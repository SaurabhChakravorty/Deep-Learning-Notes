{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Saurabh_Minimal_chat_with_SpaCy_and_WordNet_handout.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"kzyCMkpLp4PX"},"source":["# Building basic chatbots with rules, syntax and semantic nets\n","\n","It is increasingly often, that companies would like to automate internal or customer facing tasks via a chat interface. Though there are mature frameworks (like [RASA](https://rasa.com/)) or services (like [Microsoft Bot Framework](https://dev.botframework.com/) or [Chatfuel](https://chatfuel.com/)), we will attempt to set up a basic analysis pipeline based on SpaCy and WordNet, that can give us some coverage in a basic banking scenario.   \n","\n","We will use SpaCy for our basic analysis (including syntax), as well as a simple addon, that connects it to WordNet called unsurprisingly [SpaCy-WordNet](https://spacy.io/universe/project/spacy-wordnet).\n","\n","Let's take the following texts as a problem:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:51.229539Z","start_time":"2019-11-11T11:45:51.225013Z"},"id":"46QcO3myp4PX"},"source":["test_texts = [\n","    \"I would like to deposit 5000 euros.\",\n","    \"I would like to put in 5000 euros.\",\n","    \"I would like to pay in 5000 euros.\",\n","    \"I would like to pay up 5000EUR.\",\n","    \"Can I pay in 5000 euros, please?\",\n","    \n","    \n","    \"I would like to deposit money.\",\n","    \n","\n","    \"I am about to take out 5000 euros.\",\n","    \"I am about to get out 5000 euros.\",\n","    \"I am about to withdraw 5000 euros.\",\n","    \"I want to withdraw 5000 USD.\",\n","    \"Can I withdraw $5000.\",\n","\n","    \n","    \"Can I check my account, please?\",\n","    \"May I see my balance, please?\",\n","    \"Could I query my account, please?\",\n","    \"I would like to see my account balance.\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbIuaAhtp4PX"},"source":["## Let's try some syntactic analysis!\n","\n","The first goal is to see, if we can filter out, based on some common POS / Dependency structure the \"main message\", the things that people would like to say with the sentences above.\n","\n","The expected output based on our analysis would be something like:"]},{"cell_type":"markdown","metadata":{"id":"Jc0gObxwp4PX"},"source":["```\n","[deposit, 5000, euros]\n","[put, in, 5000, euros]\n","[pay, in, 5000, euros]\n","[pay, up, 5000EUR]\n","[pay, in, 5000, euros]\n","[deposit, money]\n","[take, out, 5000, euros]\n","----- No success in parsing. Original: I am about to get out 5000 euros.\n","[withdraw, 5000, euros]\n","[withdraw, 5000, USD]\n","[withdraw, $, 5000]\n","[check, my, account]\n","[see, my, balance]\n","[query, my, account]\n","[see, my, account, balance]\n","```"]},{"cell_type":"code","metadata":{"id":"syNiOzdEp4PX"},"source":["%%capture\n","import nltk\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOJVbnGbp4PX"},"source":["### Preliminaries: install SpaCy and initialize model"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:54.885571Z","start_time":"2019-11-11T11:45:51.232265Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"ynlQkUFhp4PX","executionInfo":{"status":"ok","timestamp":1606043840600,"user_tz":-60,"elapsed":4625,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"fe2b14e4-a91a-440d-a0fc-7c39939a92c4"},"source":["!python -m spacy download en_core_web_sm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n","\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:55.737042Z","start_time":"2019-11-11T11:45:54.894640Z"},"id":"iS6RuXTsp4PX"},"source":["import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:55.759307Z","start_time":"2019-11-11T11:45:55.738655Z"},"id":"125Olzxkp4PX"},"source":["# We create one document out of the array of sentences for convenience.\n","long_text = \" \".join(test_texts)\n","\n","doc = nlp(long_text ) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oRce9M3cp4PX"},"source":["### Try some syntactic matching on the texts!\n","\n","Let us use the syntactic analysis of SpaCy to get to the \"core\" of the sentences!\n","\n","Let us assume, that we are interested in **verbs** and their **minimal subtrees**!\n","\n","Please\n","\n","1. look for the verbs in the sentences, \n","2. get their subtrees,\n","3. delete every token from the \"left\" of the verb\n","4. from the \"right\" subtree, filter interjections and punctuations,\n","5. keep the shortest such subtree from the sentence and print it out!\n","\n","For the visualization of the sentence tree use [DisplaCy](https://spacy.io/usage/visualizers)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:55.767176Z","start_time":"2019-11-11T11:45:55.760799Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"6nXtYsh8p4PX","executionInfo":{"status":"ok","timestamp":1606043841238,"user_tz":-60,"elapsed":5228,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"dca797c5-1799-47e8-c31d-caff94460f15"},"source":["# Optimised code\n","for sentence in doc.sents:\n","    # Go by sents\n","    sentence_subtree = [] # For storing trees\n","    n = 0                 # For storing index\n","    #ind = []\n","    \n","    for token in sentence:\n","            # get their subtrees\n","            for i,t in enumerate(token.subtree):\n","                # delete every token from the \"left\" of the verb\n","                # look for the verbs in the sentences,\n","                if t.pos_ == \"VERB\":\n","                        if i > n:\n","                           #print(i)\n","                           n = i\n"," \n","    # keep the shortest such subtree from the sentence and print it out!\n","    sent = sentence[n:]\n","    # from the \"right\" subtree, filter interjections and punctuations,\n","    sentence_subtree = [t for t in sent if t.pos_ not in ['INTJ','PUNCT'] and n > 0]\n","    # Print it outt\n","    if sentence_subtree:\n","        print(sentence_subtree)\n","    else:\n","        print(\"----- No success in parsing. Original:\",sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[deposit, 5000, euros]\n","[put, in, 5000, euros]\n","[pay, in, 5000, euros]\n","[pay, up, 5000EUR]\n","[pay, in, 5000, euros]\n","[deposit, money]\n","[take, out, 5000, euros]\n","----- No success in parsing. Original: I am about to get out 5000 euros.\n","[withdraw, 5000, euros]\n","[withdraw, 5000, USD]\n","[withdraw, $, 5000]\n","[check, my, account]\n","[see, my, balance]\n","[query, my, account]\n","[see, my, account, balance]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7PkGRYF2p4PX"},"source":["As we can see, even in this simple case, some noise remains, that is: with our method we can not achieve success by sentence 8. Please observe, and let's discuss, why!"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:55.786989Z","start_time":"2019-11-11T11:45:55.769377Z"},"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"CFG5kI8yp4PY","executionInfo":{"status":"ok","timestamp":1606043841240,"user_tz":-60,"elapsed":5213,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"ee5638c7-77a9-4ee7-ecf9-8710c7842572"},"source":["from spacy import displacy\n","\n","doc=nlp(test_texts[7])\n","\n","displacy.render(doc, style=\"dep\", jupyter=True)\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e19770f8eaf14a7c888e71621eb4e81e-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">am</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">about</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">to</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">get</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">out</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">5000</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NUM</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">euros.</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-e19770f8eaf14a7c888e71621eb4e81e-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-e19770f8eaf14a7c888e71621eb4e81e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-e19770f8eaf14a7c888e71621eb4e81e-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-e19770f8eaf14a7c888e71621eb4e81e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-e19770f8eaf14a7c888e71621eb4e81e-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-e19770f8eaf14a7c888e71621eb4e81e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-e19770f8eaf14a7c888e71621eb4e81e-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-e19770f8eaf14a7c888e71621eb4e81e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-e19770f8eaf14a7c888e71621eb4e81e-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-e19770f8eaf14a7c888e71621eb4e81e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-e19770f8eaf14a7c888e71621eb4e81e-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-e19770f8eaf14a7c888e71621eb4e81e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-e19770f8eaf14a7c888e71621eb4e81e-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-e19770f8eaf14a7c888e71621eb4e81e-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n","</g>\n","</svg></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"xXymTt3Vp4PY"},"source":["It is worth noting, that some addon libraries, like [Textacy](https://spacy.io/universe/project/textacy) have built in functions that can come in handy in these topics.\n","\n","Like:\n","\n","`textacy.spacier.utils.get_main_verbs_of_sent(sent)`\n","Return the main (non-auxiliary) verbs in a sentence.\n","\n","`textacy.spacier.utils.get_subjects_of_verb(verb)`\n","Return all subjects of a verb according to the dependency parse.\n","\n","`textacy.spacier.utils.get_objects_of_verb(verb)`\n","Return all objects of a verb according to the dependency parse, including open clausal complements.\n","\n","`textacy.spacier.utils.get_span_for_compound_noun(noun)`\n","Return document indexes spanning all (adjacent) tokens in a compound noun.\n","\n","`textacy.spacier.utils.get_span_for_verb_auxiliaries(verb)`\n","Return document indexes spanning all (adjacent) tokens around a verb that are auxiliary verbs or negations.\n","\n","None the less, if we want to carry out some definite actions for these sentences, we have to try another route."]},{"cell_type":"markdown","metadata":{"id":"BxVLre7Pp4PY"},"source":["## Second try: detecting \"intents\" and \"entities\" with the help of WordNet\n","\n","In processing chat utterances, the two common tasks are to:\n","\n","1. Detect the overall intent of the given utterance\n","2. Extract some key parameters needed for action.\n","\n","The first is called **\"intent detection\"** the second **\"entity extraction\"**.\n","\n","More on this can be found in the Theory section on chatbots, discussed later.\n","\n","Though the standard practice for the first step is to build up a sentence classifier, and the second is done usually with some token level classifier / matching, now we will utilize the same rule based matching mechanism of SpaCy that we did before, albeit with a twist.\n","\n","One of the main problems, as we saw before is the **variety of utterances**, that is, people tend to formulate the same intent in myriad ways. We will intend to mitigate this by **increasing coverage with WorNet synonyms**.\n","\n","For this we need a connection between our analysis pipeline and WordNet. Luckily, we have it as an extension.\n"]},{"cell_type":"markdown","metadata":{"id":"arjX3B_yp4PY"},"source":["### Install extension and register it to the pipeline"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:56.816094Z","start_time":"2019-11-11T11:45:55.788687Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"VU0em4PLp4PY","executionInfo":{"status":"ok","timestamp":1606043843975,"user_tz":-60,"elapsed":7932,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"4faf95f5-9004-4bfb-b013-6215a26c41b1"},"source":["!pip install spacy-wordnet"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: spacy-wordnet in /usr/local/lib/python3.6/dist-packages (0.0.4)\n","Requirement already satisfied: nltk<3.4,>=3.3 in /usr/local/lib/python3.6/dist-packages (from spacy-wordnet) (3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk<3.4,>=3.3->spacy-wordnet) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:57.312343Z","start_time":"2019-11-11T11:45:56.822682Z"},"id":"IO-5gNAyp4PY"},"source":["from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n","\n","nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')#Register to the pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mneQYmSbp4PY"},"source":["### Setting up a custom detector for intents\n","\n","As said, we will hijack the entity detector capability of SpaCy to classify intents.\n","\n","For this, we need to define custom rules with `EntityRuler`, and some patterns that match our intents.\n","\n","We have all in all 3 intents in mind:\n","\n","`INTENTS = [\"TAKEOUT_INTENT\",\"PAYIN_INTENT\",\"BALANCE_INTENT\"]`\n","\n","First define patterns **one for each**, register it, try to run the pipeline, and see the result.\n","\n","After it, you will have to **get back to this cell and iteratively refine the pattern** based on the results of WordNet enrichment below.\n","\n","First make it run through, then refine!\n","All in all 7 patterns are enough in total to detect the three intents in all their forms seen here with the help of WordNet synsets.\n","\n","#### Set up EntityRuler"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.608620Z","start_time":"2019-11-11T11:45:57.315205Z"},"id":"hKCZXaEWp4PY"},"source":["from spacy.pipeline import EntityRuler\n","\n","INTENTS = [\"TAKEOUT_INTENT\",\"PAYIN_INTENT\",\"BALANCE_INTENT\"]\n","\n","ruler = EntityRuler(nlp)\n","\n","# Adding 8 patterns for retreival of information\n","\n","patterns = [{\"label\": INTENTS[0], \"pattern\": 'out'},\n","            {\"label\": INTENTS[0], \"pattern\": 'withdraw'},\n","           {\"label\": INTENTS[1], \"pattern\": 'pay'},\n","           {\"label\": INTENTS[1], \"pattern\": 'put'},\n","           {\"label\": INTENTS[1], \"pattern\": 'deposit'},\n","           {\"label\": INTENTS[2], \"pattern\": 'balance'},\n","           {\"label\": INTENTS[2], \"pattern\": 'query'},\n","           {\"label\": INTENTS[2], \"pattern\": 'check'}     \n","           ]\n","\n","# Add the patterns to the ruler\n","ruler.add_patterns(patterns)\n","\n","# Add the ruler to the pipeline\n","nlp.add_pipe(ruler)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fsuhymtqp4PY"},"source":["#### Define a `detect_intent` function\n","\n","The function takes in as an input an analysed sentence (`Doc`), a list on intents (eg. `INTENTS`), ad gives back the found intent or `None`."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.613673Z","start_time":"2019-11-11T11:45:58.610722Z"},"id":"nKJR9Hp_p4PY"},"source":["def detect_intent(analysed_sentence, intents):\n","    # In this case, we do not do proper intent detection,\n","    # which would be a whole sentence classification task, based on it's semantics,\n","    # but we do an intelligent entity matching based on our rules,\n","    # where we treat intents as special entities.\n","    \n","    found = None\n","    for ent in analysed_sentence.ents:\n","        if ent.label_ in intents:\n","             found  = ent.label_\n","    \n","    return found"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fIn-1k0Xp4PY"},"source":["### Setitng up a function for detecting \"real\" entities\n","\n","In SpaCy's world, monetary units and numbers are considered to be entities by default, thus the built in Named Entity Recognizer (`ner` in the pipeline) detects and tags those.\n","\n","In our case we are only interested in the monetary entities. **Please bear in mind that MULTIPLE categories can mean money, so some times normal numbers, sometimes formal money, etc. Use multiple numeric categories for detection!**\n","\n","More on this [here](https://spacy.io/usage/linguistic-features/#named-entities) and [here](https://spacy.io/api/annotation#named-entities)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.622131Z","start_time":"2019-11-11T11:45:58.614805Z"},"id":"i-ZOTurWp4PY"},"source":["MONEY = [\"MONEY\", \"CARDINAL\", \"QUANTITY\"]\n","\n","def detect_money(analysed_sentence, money):\n","    \n","    found_money = None\n","    only_numbers = ''\n","    \n","    \n","    # Please return only the numbers from the money!!!\n","    for ent in analysed_sentence.ents:\n","        #print(ent)\n","        if ent.label_ in money : \n","            found_money  = ent.text\n","            \n","    if found_money:\n","        #print(\"The found money is {}\".format(found_money))\n","        for i in found_money:\n","            if i.isdigit():\n","                only_numbers = only_numbers + i\n","    \n","    return only_numbers "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WKG0iVI4p4PY"},"source":["### Enriching intent detection with WordNet\n","\n","As we well saw, if we don't want to manually set up the patterns that match all test cases - which is unsustainable for a much bigger corpus than this - we need some semantic help.\n","\n","Let's define a super crude `enrich_sentence` function, that generates sentence variants from the input. It takes in an analysed sentence (`doc`), a set of domains (in our case eg. `ECONOMY_DOMAINS`), and **for each token in the sentence searches for the sysnonyms inside our domains, then replaces the token with it's synonym, and appends the new sentence to a list.**\n","\n","**Finally we expect to get back a set of sentence variants as texts in a list.**"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.626815Z","start_time":"2019-11-11T11:45:58.623370Z"},"id":"U-U4G7Kop4PY"},"source":["ECONOMY_DOMAINS = ['finance', 'banking']\n","\n","\n","def enrich_sentence(analysed_sentence, domains):\n","\n","    enriched_sentences = []\n","    \n","    # For each token in the sentence\n","    for token in analysed_sentence:\n","        # We get those synsets within the desired domains\n","        synsets = token._.wordnet.wordnet_synsets_for_domain(domains)\n","        #print(synsets)\n","        if synsets:\n","            lemmas_for_synset = []\n","            for s in synsets:\n","                # If we found a synset in the economy domains\n","                # we get the variants and add them to the enriched sentence\n","    \n","                #Please bear in mind that WordNet lemmas can be of multiple words, thus containing a \"_\" which we don't need.\n","                #remove_underscore = []\n","                lemmas_for_synset.extend(i.replace(\"_\",\" \") for i in s.lemma_names())\n","                enriched_sentences.append(' '.format('|'.join(set(lemmas_for_synset))))\n","        else:\n","            enriched_sentences.append(token.text)\n","    print(enriched_sentences)\n","    return enriched_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCxnbfHOp4PY"},"source":["#### Full search for intents\n","\n","Based on the `detect_intent` and `enrich_sentence` functions we set up the full logic that searches for intents.\n","\n","The function has to accept an analysed sentence (`Doc`), the list of intents and the list of domains as above, and then **try to find the intent in the default sentence. If not found, try to enrich the sentence, then search in the enriched ones. Return an intent if found.**"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.632646Z","start_time":"2019-11-11T11:45:58.628020Z"},"id":"l-GhZ81Sp4PY"},"source":["def search_for_intents(analysed_sentence, intents, domains):\n","\n","    found_intent = detect_intent(analysed_sentence, intents)\n","\n","    if found_intent == None:\n","        sent = enrich_sentence(analysed_sentence, domains)\n","        sent = nlp((\"\".join(sent)))\n","        \n","        found_intent = detect_intent(sent, intents)\n","    \n","    return found_intent"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"saOWpzWyp4PY"},"source":["#### Let's try this out!"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.694647Z","start_time":"2019-11-11T11:45:58.634052Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"gvZRdB1fp4PY","executionInfo":{"status":"ok","timestamp":1606043843982,"user_tz":-60,"elapsed":7872,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"f62b0df7-8f64-4e92-9420-f049a83eb019"},"source":["sentence = nlp(\"I would like to withdraw 5000 euros.\")\n","\n","found_intent = search_for_intents(sentence,INTENTS,ECONOMY_DOMAINS)\n","\n","print(found_intent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TAKEOUT_INTENT\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pz8NceVrp4PY"},"source":["## Finally: parse the full query\n","\n","Refine the original patterns and all the functions until the tests pass at the end of the notebook. Use **the least amount of handmade patterns possible!**"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.700317Z","start_time":"2019-11-11T11:45:58.696151Z"},"id":"SCNi-qW_p4PY"},"source":["def parse_query(query, intents, domains, money):\n","    \n","    analysed_sentence = nlp(query)\n","    found_intent = search_for_intents(analysed_sentence,intents, domains)\n","    #print(found_intent)\n","    if found_intent == intents[0] or found_intent == intents[1]:\n","        amount = detect_money(analysed_sentence,money)\n","        if amount:\n","            print(\"Executing\",found_intent,\"with\",amount)\n","            return (found_intent, amount)\n","        else:\n","            print(\"No amount was given, please add one!\")\n","            return (found_intent, None)\n","    elif found_intent == intents[-1]:\n","        print(\"Getting you your account balance, one moment...\")\n","        return (found_intent, None)\n","    else:\n","        print(\"Can't parse what you are asking for, sorry!\")\n","        return (None, None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.757534Z","start_time":"2019-11-11T11:45:58.701701Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"Nui6GjA6p4PY","executionInfo":{"status":"ok","timestamp":1606043843984,"user_tz":-60,"elapsed":7855,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"fb44a666-9e7e-4c4d-d2b3-4dc5ae04dd03"},"source":["parse_query(\"I would like to pay up 5000EUR.\",INTENTS, ECONOMY_DOMAINS, MONEY)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Executing PAYIN_INTENT with 5000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('PAYIN_INTENT', '5000')"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:58.763117Z","start_time":"2019-11-11T11:45:58.758974Z"},"id":"k2Fd98e7p4PY"},"source":["tests = [\n","   (\"I would like to deposit 5000 euros.\",(\"PAYIN_INTENT\",\"5000\")),\n","   (\"I would like to put in 5000 euros.\",(\"PAYIN_INTENT\",\"5000\")),\n","   (\"I would like to pay in 5000 euros.\",(\"PAYIN_INTENT\",\"5000\")),\n","    (\"I would like to pay up 5000EUR.\",(\"PAYIN_INTENT\",\"5000\")),\n","   (\"Can I pay in 5000 euros, please?\",(\"PAYIN_INTENT\",\"5000\")),\n","    \n","    \n","   (\"I would like to deposit money.\",(\"PAYIN_INTENT\",None)),\n","    \n","\n","    (\"I am about to take out 5000 euros.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","    (\"I am about to get out 5000 euros.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","    (\"I am about to withdraw 5000 euros.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","    (\"I want to withdraw 5000 USD.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","    (\"Can I withdraw $5000.\",(\"TAKEOUT_INTENT\",\"5000\")),\n","\n","    \n","    (\"Can I check my account, please?\",(\"BALANCE_INTENT\",None)),\n","    (\"May I see my balance, please?\",(\"BALANCE_INTENT\",None)),\n","    (\"Could I query my account, please?\",(\"BALANCE_INTENT\",None)),\n","    (\"I would like to see my account balance.\",(\"BALANCE_INTENT\",None)),\n","\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2019-11-11T11:45:59.303171Z","start_time":"2019-11-11T11:45:58.764596Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"IdksB3Ycp4PY","executionInfo":{"status":"ok","timestamp":1606043844205,"user_tz":-60,"elapsed":8056,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"6612317f-ad69-42a6-da5f-54f485e02704"},"source":["for test in tests:\n","    #print(test[0])\n","    try:\n","        assert parse_query(test[0],INTENTS, ECONOMY_DOMAINS, MONEY) == test[1]\n","    except:\n","        print(\"---ERROR: \",parse_query(test[0],INTENTS, ECONOMY_DOMAINS, MONEY))\n","        raise"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Executing PAYIN_INTENT with 5000\n","Executing PAYIN_INTENT with 5000\n","Executing PAYIN_INTENT with 5000\n","Executing PAYIN_INTENT with 5000\n","Executing PAYIN_INTENT with 5000\n","No amount was given, please add one!\n","Executing TAKEOUT_INTENT with 5000\n","Executing TAKEOUT_INTENT with 5000\n","Executing TAKEOUT_INTENT with 5000\n","Executing TAKEOUT_INTENT with 5000\n","Executing TAKEOUT_INTENT with 5000\n","Getting you your account balance, one moment...\n","Getting you your account balance, one moment...\n","Getting you your account balance, one moment...\n","Getting you your account balance, one moment...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aI34xyAzp4PY"},"source":["A more elaborate and very nice example on the power of rule based matching and it's combination with machine learning models can be found [here](https://github.com/pmbaumgartner/binder-notebooks/blob/master/rule-based-matching-with-spacy-matcher.ipynb)"]},{"cell_type":"code","metadata":{"id":"hyqdXIZzp4PY"},"source":[""],"execution_count":null,"outputs":[]}]}