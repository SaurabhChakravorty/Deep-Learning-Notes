{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background: Distributions of words\n",
    "\n",
    "\n",
    "## Psychological models of meaning: \"Osgooding\"\n",
    "\n",
    "As in many innovations in AI, the foundations of the most important NLP techniques can be __traced back to psychological research__. [Osgood](https://en.wikipedia.org/wiki/Charles_E._Osgood) in his seminal paper [\"The nature and measurement of meaning.\"](https://pdfs.semanticscholar.org/ca12/a908e86a87db152c0991ae9c5a40f1a5d2a3.pdf) proposed an __experimental method__ for __understanding__ the human subject's \"mapping\" of __meaning__.\n",
    "\n",
    "[Semantic Differential](https://en.wikipedia.org/wiki/Semantic_differential) (SD) is a type of a __rating scale__ designed to measure the __connotative meaning__ of __objects, events, and concepts__. The connotations are used to derive the attitude towards the given object, event or concept.\n",
    "\n",
    "Osgood's Semantic Differential was an application of his more general attempt to measure the semantics or meaning of words, particularly adjectives, and their referent concepts. The respondent is asked to choose where his or her position lies, on a scale between two polar adjectives (for example: \"Adequate-Inadequate\", \"Good-Evil\" or \"Valuable-Worthless\"). Semantic differentials can be used to measure opinions, attitudes and values on a psychometrically controlled scale.\"\n",
    "\n",
    "<a href=\"http://methods.sagepub.com/images/virtual/encyclopedia-of-survey-research-methods/image32.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1u3ETKwTMT72xuiRenupCvjKWPh9nTABE\" width=45%></a>\n",
    "\n",
    "This laboratory method became a standard in social sciences as well as experimental linguistics, earning the name \"osgooding\".\n",
    "\n",
    "Important things to note for this research:\n",
    "- Using __human judgements__ for the __measurement of association strengths between words__\n",
    "- Implicit assumption of a __\"space\" (distance)__ which __represents meaning relations__\n",
    "- Usage of __factor analysis__, that is the search for a __lower number of causal factors behind the observed variance__ of word affinities (dimensionality reduction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's law\n",
    "\n",
    "The other influential foundational notion we have to take into account is the phenomena described by Zipf in his 1935 work [The psycho-biology of language](https://psycnet.apa.org/record/1935-04756-000), later bacame known as \"Zipf's law\".\n",
    "\n",
    "\"..Zipf's law states that given some corpus of natural language utterances, the __frequency of any word__ is __inversely proportional to its rank in the frequency table__. Thus the __most frequent word__ will __occur approximately twice as often__ as the __second most frequent word__, __three times__ as often as the __third most frequent word__, etc.: the rank-frequency distribution is an inverse relation. For example, in the Brown Corpus of American English text, _the_ word the is the most frequently occurring word, and by itself accounts for nearly 7% of all word occurrences (69,971 out of slightly over 1 million). True to Zipf's Law, the second-place word of accounts for slightly over 3.5% of words (36,411 occurrences), followed by _and_ (28,852). Only __135 vocabulary items__ are needed to account for __half the Brown Corpus__.\"\n",
    "\n",
    "<a href=\"https://3c1703fe8d.site.internapcdn.net/newman/csz/news/800/2017/solutiontoac.png\"><img src=\"https://drive.google.com/uc?export=view&id=1KeW5cFpaezvqh7ks8wp_tNydVlEiETiV\" width=55%></a>\n",
    "\n",
    "The main takeaway form this research is:\n",
    "- The __empirical distribution__ of __written words__ has very __distinct, characteristic__\n",
    "- This __distribution__ points towards __definable generating mechanisms__ for language production\n",
    "\n",
    "### Possible explanations of Zipf's law\n",
    "\n",
    "Though Zipf's \"law\" is empirically very stable and has been observed across multiple languages and corpora, a thorough explanation of it's __causes__ is still __lacking__. Among the multiple avenues for the causes __two__ are __notable for the purpose of NLP__:\n",
    "\n",
    "On the one hand Yu et al. proposes __distinct cognitive mechanisms__ for __language processing__ as Zipf's background:\n",
    "\n",
    "\"Yu and co say the word frequencies in these languages share a common structure that differs from the one that statistical errors would produce. What’s more, they say this structure suggests that the __brain processes common words differently from uncommon ones__, an idea that has important consequences for natural-language processing and the automatic generation of text.\" ([source](https://www.technologyreview.com/s/611640/data-mining-reveals-fundamental-pattern-of-human-thinking/))\n",
    "\n",
    "However, Yu and co are able to reproduce this structure using a model of the way the brain works called the __dual-process theory__. This is the idea that the brain works in two different ways.\n",
    "\n",
    "The first is ___fast intuitive thinking___ that requires __little or no reasoning__. This type of thinking is thought to have evolved to allow humans to react quickly in threatening situations. It generally provides good solutions to difficult problems, such as pattern recognition, but can easily be tricked by non-intuitive situations.\n",
    "\n",
    "However, humans are capable of much more rational thinking. This __second type__ of thinking is __slower, more calculating, and deliberate__. It is this kind of thinking that allows us to solve complex problems like mathematical puzzles and so on.\n",
    "\n",
    "The dual-process theory suggests that __common words like the, and, if and so on are processed by fast, intuitive thinking and so are used more often__. These words form a kind of backbone for sentences.\n",
    "\n",
    "However, __less common words and phrases like \"hypothesis\" and \"Zipf’s Law\" require much more careful thought__. And because of this they occur less often.\n",
    "\n",
    "We will come back to the topic of \"informative\" words later in detail.\n",
    "\n",
    "\n",
    "\n",
    "On the other hand [Manin](https://pdfs.semanticscholar.org/b04b/1ccabc3e614ba4fe784030b41d6f1e753844.pdf)'s 2007 research proposes that the zipfian distribution is a direct result of the __processes governing hypernymy (type-of-relationship) and synonymy (same as) relations__ in the human linguistic structure. This draws a strong connection between the distributional and ontological approaches to semantics. The already discussed topics of \"relationship extraction\" can capitalize strongly on this effect, and vice versa, some distributional methods emphasize the hierarchical structure of \"meaning space\" in forms of topological constraints (see for example [this](http://arxiv.org/abs/1705.08039) paper)\n",
    "\n",
    "Interestingly enough, the __distributional properties of images tagged by humans__ also show a kind of __hierarchical distribution__, where __\"higher order\" concepts have more visual variability__ in the associated images than the \"lower level\" words in a hierarchy. This effect has been explicitly used for mining of meronymy relations for common sense ontologies. (see [here](https://people.mpi-inf.mpg.de/~ntandon/papers/pwkb-aaai2016-tandon.pdf))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Distributional hypothesis\n",
    "\n",
    "_\"You shall know a word by the company it keeps\" (Firth, 1957)_\n",
    "\n",
    "The grounding hypothesis of **distributional semantics** is that we consider **language production** (the choice of sequences of words) as a **probabilistic process**, thus we state, that meaning can and should be modeled with some kind of (conditional) probability distributions.\n",
    "\n",
    " \n",
    "-------------------\n",
    "In short:\n",
    "<font color='red'>\n",
    "Meaning of a word = distribution of it's neighbors\n",
    "</font>\n",
    "\n",
    "-------------------\n",
    "\n",
    "## Language modeling\n",
    "\n",
    "A __language model__ is a __probability distribution__ over the __sequence of words__, modeling language (production), thus if the set of words is $w$, then for arbitrary $\\mathbf w = \\langle w_1,\\dots, w_n\\rangle$ ($w_i\\in W$) sequence it defines a $P(\\mathbf w)$ probability. \n",
    "\n",
    "Probability with chain rule:\n",
    "\n",
    "$$P(\\mathbf w)= P(w_1)\\cdot P(w_2 \\vert w_1 )\\cdot P(w_3\\vert w_1, w_2)\\cdot\\dots\\cdot P(w_n\\vert w_1,\\dots, w_{n-1})$$\n",
    "\n",
    "so this means, that for the __modeling__ we need only to give the __conditional probability__ of the __\"continuation\"__, the __next word__, thus for $w$ word and $\\langle w_1,\\dots,w_n\\rangle$ sequence the probability that the next word will be $w$\n",
    "\n",
    "$$P(w ~\\vert ~ w_1,\\dots,w_n)$$\n",
    "\n",
    "There are character based models also, which take the individual characters as units, not the words, and model language as a distribution over sequences of characters (think T9...)\n",
    "\n",
    "\n",
    "\n",
    "**Language modeling is the practical application of the distribution hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows\n",
    "\n",
    "It is important to note, that this conditional probability forms a Markov chain. The problem is, that we do not know, how \"deep\" it is, that is, **how deep the causal influence of a word \"travels\" through next words**. \n",
    "\n",
    "**In practice, we do not think, that the first word of \"On war and peace\", as a book directly influences the last one. Some higher level concept, like topic, narrative, etc is in place, but for practical reasons, we will consider a causal (rolling) window.** (Rings a bell? Time series?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement of predictive performance: Perplexity\n",
    "Extrinsic vs intrinsic evaluation criteria:\n",
    "\n",
    "__Extrinsic__ - put model to task- spell checking, speech recognizer etc.\n",
    "\n",
    "__Intrinsic__ - performance on the test-set e.g. measured as perplexity\n",
    "\n",
    "Perplexity: Basic evaluation criterion of a language model - does it __prefer good sentences to bad ones__?\n",
    "- assign __higher probability__ to __\"real\"__ or __frequently observed__ sentences than to __\"ungrammatical\"__ or __\"rarely observed sentences\"__\n",
    "\n",
    "\n",
    "\n",
    "A language model $\\mathcal M$'s perplexity over the word series $\\mathbf w = \\langle w_1,\\dots, w_n\\rangle$ is:\n",
    "\n",
    "$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w) = \\sqrt[n]{\\frac{1}{P_{\\mathcal M}(\\mathbf w)}}$$\n",
    "\n",
    "With the chain rule can be rewritten as:\n",
    "\n",
    "$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w) = {\\sqrt[n]{\\frac{1}{P_{\\mathcal M}(w_1)}\\cdot \\frac{1}{P_{\\mathcal M}(w_2 \\vert w_1 )}\\cdot \\frac{1}{P_{\\mathcal M}(w_3\\vert w_1, w_2)}\\cdot\\dots\\cdot \\frac{1}{P_{\\mathcal M}(w_n\\vert w_1,\\dots, w_{n-1})}}}$$\n",
    "\n",
    "which is exactly the __geometric mean__ of the __reciprocals of the conditional probabilities__ of all __words in the corpus__.\n",
    "\n",
    "In case of a __bigram model__ this is further simplified to:\n",
    "$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w) = \\sqrt[n]{\\frac{1}{P_{\\mathcal M}(w_1)}\\cdot \\frac{1}{P_{\\mathcal M}(w_2 \\vert w_1 )}\\cdot \\frac{1}{P_{\\mathcal M}(w_3\\vert w_2)}\\cdot\\dots\\cdot \\frac{1}{P_{\\mathcal M}(w_n\\vert w_{n-1})}}$$\n",
    "\n",
    "The lower the perplexity the better (it means that our model deemed the actuall observed sequence as more likely)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to cross-entropy\n",
    "\n",
    "Taking the logarithm of perplexity, with a few simple steps of algebraic manipulations we can see that the result is \n",
    "\n",
    "$$\n",
    "\\frac{1}{n} (-\\log(P_{\\mathcal M}(w_1)) + -\\log(P_{\\mathcal M}(w_2 \\vert w_1 ))+ -\\log(P_{\\mathcal M}(w_3\\vert w_1, w_2)) + \\dots  + -\\log(P_{\\mathcal M}(w_n\\vert w_1,\\dots, w_{n-1}))\n",
    "$$\n",
    "\n",
    "which is the average cross-entropy per word. A simple consequence: minimizing cross-entropy one also minimizes the model's perplexitiy on the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
