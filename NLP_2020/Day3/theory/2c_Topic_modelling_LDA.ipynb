{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "\n",
    "## LDA Latent Dirichlet Allocation - the most common traditional (bag of words) approach\n",
    "\n",
    "__Aim__: identify topics in a document\n",
    "\n",
    "__Probabalistic__ version of topic modelling. See [__latent semantic analysis__](https://en.wikipedia.org/wiki/Latent_semantic_analysis) for a non-probabalistic version.\n",
    "\n",
    "__Applications__: search engine, customer service automation, knowledge extraction\n",
    "\n",
    "NLP version proposed by ([Bleit et al. 2013 ](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf))\n",
    "\n",
    "\n",
    "### Literal meaning\n",
    "__Latent__: Latent variables - the topics\n",
    "\n",
    "__Dirichlet__: Dirichlet distribution (distribution of distributions)\n",
    "\n",
    "__Allocation__: Allocate into clusters (aka. words to latent topics, topics to documents)\n",
    "\n",
    "Hence we are essentially talking about a __particular (probabalistic) type__ of __clustering__ algorithm\n",
    "\n",
    "A generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap - Dirichlet Distribution\n",
    "\n",
    "__Multinomial distribution__:\n",
    "- Distribution over discrete outcomes\n",
    "- Represented by non-negative vector that sums to one (forms a simplex)\n",
    "\n",
    "Representation of multinomial distribution in terms of a vector:\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1b98UA9ffOD_QmCyQ7O2vfFsHycxQ_PK7\"><img src=\"https://drive.google.com/uc?export=view&id=12nz9W3vBp2C269jvuDPvCvD2ZcijAnkq\" width=40%></a>\n",
    "\n",
    "- Multinomial distribution that favors one outcome one quarter of the trangle. \n",
    "- Equal probable point over the three different outcomes in the centre of the triange\n",
    "\n",
    "Dirichlet distribution is a distribution over multi-nomial distributions. One can draw multinomial distributions from a Dirichlet distribution.\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1TjpuF1M_Ea5M4K4zgxa9-DqqHvDG8PMP\"><img src=\"https://drive.google.com/uc?export=view&id=13Mt5OASd1D_L96yNyJfSPX_NPu1SO4WW\" width=40%></a>\n",
    "\n",
    "$\\alpha$ is the variance\n",
    "\n",
    "$m$ is the mean\n",
    "\n",
    "The first triange: If you have a uniform mean and a variance of 3 and multiply them togther, get a uniform distribution. \"1-1\" in the exponent of the multinomial vector implies that every multinomial vector is going to be raised to the $0^{th}$ power. All of the vectors p are going to be equiprobable. A uniform distribution over all possible multinomial distributions.\n",
    "\n",
    "The second triangle and third triange: If $\\alpha$ is made larger and larger this concentrates the probability distribution around the mean.\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1_MzFnR7cmrVEGjODb-gqm7AS6UDizCfR\"><img src=\"https://drive.google.com/uc?export=view&id=1OdBc-UCS0Gjlwo1fnLhsms2CSW4Dhtqx\" width=40%></a>\n",
    "\n",
    "Alternatively could have mean in different location or have the variance parameter $\\alpha$ smaller than the number of outcomes in the multinomial distribution (last triangle). Then the probability mass will be spread to the sides of the triangle. Consequently separation of topics will be prefered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap - Gibbs sampling\n",
    "- Sampling from a __probability distributions__ of __2+ dimensions__\n",
    "- Markov Chain Monte Carlo Method (MCMC). __Markov chain__: one step time dependent sampling. __Monte Carlo__: Some form of random element in deciding the next step of the sampling\n",
    "\n",
    "The underlying logic of MCMC sampling is that we can __estimate__ any __desired expectation__\n",
    "by __ergodic averages__. That is, we can __compute any statistic of a posterior distribution__ as long\n",
    "as we have __N simulated samples from that distribution__:\n",
    "\n",
    "$$ E[f(s)]_P\\approx \\frac{1}{N} \\sum_{i=1}^{N} f(s^{(i)})$$\n",
    "\n",
    "where $P$ is the posterior distribution of interest, $ f(s) $ is the desired expectation, and $f(s^{(i)})$\n",
    "is the $i^{th}$ simulated sample from $P$. For example, we can estimate the mean by $E[x]_P = \\frac{1}{N} \\sum_{i=1}^N x^{i}$.\n",
    "\n",
    "\n",
    "How do we obtain samples from the posterior distribution? Gibbs sampling is one MCMC\n",
    "technique suitable for the task. The idea in Gibbs sampling is to generate posterior samples\n",
    "by sweeping through each variable (or block of variables) to __sample from its conditional\n",
    "distribution with the remaining variables fixed to their current values__. For instance, consider\n",
    "the random variables $X_1$, $X_2$, and $X_3$. We start by setting these variables to their initial\n",
    "values $x_{1}^{(0)}$, $x_{2}^{(0)}$, and $x_{3}^{0}$\n",
    "(often values sampled from a prior distribution q). \n",
    "\n",
    "At iteration $i$, we sample \n",
    "\n",
    "$x_{1}^{(i)} ∼ p(X_1 = x_1|X_2 = x_{2}^{(i−1)}, X_3 = x_{3}^{(i−1)})$, \n",
    "\n",
    "sample \n",
    "\n",
    "$x2 ∼ p(X_2 = x_2|X_1 = x_1^{(i)}, X3 = x_3^{(i−1)})$, \n",
    "\n",
    "and sample \n",
    "\n",
    "$x3 ∼ p(X_3 = x_3|X_1 = x_1^{(i)}, X_2 = x_2^{(i)})$. \n",
    "\n",
    "This process continues\n",
    "until “convergence” (the sample values have the same distribution as if they were sampled\n",
    "from the true posterior joint distribution). Algorithm 1 details a generic Gibbs sampler.\n",
    "\n",
    "<a href=\"https://leimao.github.io/images/blog/2017-06-13-Gibbs-Sampler/gibbs_sampler_algorithm.png\"><img src=\"https://drive.google.com/uc?export=view&id=13kxNiorqbVGzmB_kD_9ngHnQZjyvj1qs\" width=80%></a>\n",
    "\n",
    "\n",
    "In Algorithm 1, we are __not directly sampling from the posterior__ distribution itself. Rather,\n",
    "we __simulate samples by sweeping through all the posterior conditionals__, one random variable\n",
    "at a time. Because we initialize the algorithm with random values, the samples simulated\n",
    "based on this algorithm at early iterations may not necessarily be representative of the\n",
    "actual posterior distribution. However, the theory of MCMC guarantees that the stationary\n",
    "distribution of the samples generated under Algorithm 1 is the target joint posterior that\n",
    "we are interested in (Gilks et al., 1996). For this reason, __MCMC algorithms__ are typically run for\n",
    "a __large number of iterations__ (in the hope that convergence to the target posterior will be\n",
    "achieved). Because __samples__ from the __early iterations__ are not from the __target posterior__, it\n",
    "is common to __discard these samples__. The discarded iterations are often referred to as the\n",
    "“burn-in” period.\n",
    "\n",
    "\n",
    "<a href=\"https://mikelove.files.wordpress.com/2008/09/gibbs.png\"><img src=\"https://drive.google.com/uc?export=view&id=1W4FvVycStv6_8kwvtsy_4G6HpG73nWe5\" width=50%></a>\n",
    "\n",
    "\n",
    "We will cover Metropolis, Metropolis-Hastings sampling and Gibbs sampling in by far more detail in the frontiers of AI class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for more detail see here:\n",
    "https://apps.dtic.mil/dtic/tr/fulltext/u2/a523027.pdf\n",
    "\n",
    "and here:\n",
    "http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More detailed explanation of LDA\n",
    "\n",
    "### The set-up\n",
    "\n",
    "According to LDA each word in each __document__ comes __from__ a __topic__ and the __topic__ is selected __from__ a __per-document distribution__ over topics. So we have two matrices:\n",
    "1. __ϴtd = P(t|d)__ which is the probability distribution of __topics in documents__\n",
    "2. __Фwt = P(w|t)__ which is the probability distribution of __words in topics__\n",
    "\n",
    "Then the probability of a word given document i.e. P(w|d) is equal to:\n",
    "\n",
    "$$P(w|d)=\\sum_{t \\epsilon T } p(w | t,d)*p(t | d)$$\n",
    "    \n",
    "where __T__ is the __total number of topics__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that there is W number of words in our vocabulary for all the documents.\n",
    "\n",
    "Assuming conditional independence:\n",
    "\n",
    "$$P(w|t,d) = P(w|t)$$\n",
    "\n",
    "And hence P(w|d) is equal to:\n",
    "    \n",
    "\n",
    "\n",
    "$$P(w|d)=\\sum_{t=1}^{T} (w |t)*p(t | d)$$\n",
    "\n",
    "that is the dot product of ϴtd and Фwt for each topic t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ccan be represented in matrix form like this:\n",
    "    \n",
    "<a href=\"https://miro.medium.com/max/936/1*QiTvyHNwvGI5UCqeKvhNsg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1giUCuG8ZUXviquTkmP5m19gS-J6GFzsI\" width=100%></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of __LDA similar__ to __matrix factorization__ or __SVD__: \n",
    "- Decompose the probability distribution matrix of word in document in two matrices consisting of distribution of topics in a document and distribution of words in a topic.\n",
    "\n",
    "Therefore, what we will get is, for example, this:\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/936/1*mnehwmSdd0w1c6pfAC4LCw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1MX8TsJ4hYeC8rlzkVZ8O3h18z6_tPN8U\" width=100%></a>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gibbs Sampling\n",
    "To __start__ assume that __we know ϴ and Ф matrices__ (e.g. by randomly initializing them). \n",
    "__Slowly change__ these __matrices__ and get to an answer that __maximizes the likelihood of the data__ that we have. \n",
    "We will do this on __word by word basis__ by __changing__ the __topic assignment of one word__. We will assume that we __don’t know the topic assignment of the given word__ __but__ we __know__ the __assignment of all other words__ in the text. __Infer__ what __topic__ to be __assigned to__ this __word__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically: find __conditional probability distribution__ of a __single word’s topic assignment__ conditioned on the rest of the topic assignments. \n",
    "What we will get is a __conditional probability equation__ that looks like this for a single word w in document d that belongs to topic k:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(z_{d,n}=k|\\overrightarrow{z}_{-d,n}, \\overrightarrow{w}, \\alpha, \\lambda) = \\frac{n_{d,k}+\\alpha_k}{\\sum_{i}^{k} n_{d,i}+\\alpha_i } \\frac{v_{k,w_{d,n}}+\\lambda_{w_{d,n}}}{\\sum_{i}v_{k,i}+ \\lambda_i}$$\n",
    "\n",
    "where\n",
    "\n",
    "$n_{d,k}$: Number of times document d uses topic $k$\n",
    "\n",
    "$v_{k,w}$: Number of times topic $k$ uses the given word $w$\n",
    "\n",
    "$α_k$: Dirichlet parameter for document to topic \n",
    "distribution\n",
    "\n",
    "$λ_w$: Dirichlet parameter for topic to word distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __two parts__ to this equation. The __First part__ tells us __how much each topic is present in a document__ and the __second part__ tells __how much each topic likes a word__. Note that for __each word__, we will get a __vector of probabilities__ that will __explain__ how __likely__ this __word belongs to each of the topics__. In the above equation, it can be seen that the Dirichlet parameters also act as __smoothing parameters__ when $n_{d,k}$ or $v_{k,w}$ is zero which means that there will still be some chance that the word will choose a topic going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through an example now:\n",
    "\n",
    "Suppose we have a document with some random word topic assignment, for example, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://miro.medium.com/max/1967/1*Sd9UEq7na8XCAQHPlg1HiA.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1pPINJzbyNXYYJO3PK43Aag-QdnLPqVyT\" width=100%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have our count matrix $v_{k,w}$ as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://miro.medium.com/max/1958/1*DoTtkQ6F3hO_LJwfj0Vtlg.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1kX008yXd1-3RLkq_5wdFCderGJLAII_w\" width=100%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s change the assignment of word \"world\" in the document.\n",
    "- First, we reduce the count of \"world\" in topic 1 from 28 to 27 as we don’t know to what topic \"world\" belongs.\n",
    "- Second let’s represent the matrix $n_{d,k}$ in the following way to show how much a document uses each topic\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/1911/1*wsTlAs6N5zT2tlxJmhQ6qg.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1XVjV4pIMLJlqtOqWQv-5bQS3waIbYL5I\" width=100%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, let’s represent $v_{k,w}$ in the following way to show how many times each topic is assigned to this word\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/1907/1*qop7pkRye81ZSWq3gpqpxQ.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=19f_D8_J-zaoelt7ZNjzihOFKdbo1OzB5\" width=100%></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, we will multiply these two matrices to get our conditional probabilities\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/2091/1*aq4DK1E0GNR7AwsmdrUSsw.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=1mIej8dxFTEySmU8s9O9iNUH81NoqjRKy\" width=100%></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we  randomly pick any of the topic according to the __distribution__ and assign the Topic to \"world\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to chose k-coherence score\n",
    "There are a number of coherence score metrics that can be employed to measure how well the topics are formed. \n",
    "\n",
    "One of them is C_V: The C_v measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity.\n",
    "\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/1986/1*cRLUGZh-Biyb_fNNnTYhig.png\"><img src=\"https://drive.google.com/uc?export=view&id=1AyeLEhkc73IATNmL0DQrJhlqqn-ICqnh\" width=70%></a>\n",
    "\n",
    "\n",
    "For more detail (see [here](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)).\n",
    "\n",
    "\n",
    "Note that in many cases it is preferable to rely on evaluation by humans in terms of how well the chosen number of topics fits and how coherent they are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of LDA\n",
    "LDA identifies topics by measuring __co-occurrence likelihoods of words__. Conceptually, there are __two challenges__ when using LDA (they both matter in term of representation and interpretation):\n",
    "\n",
    "(1) __Different words__ may have the __same or similar meanings__ (e.g. cappuccino, latte macchiato). Thus, by using LDA which is based on co-occurrences of words, we may get __multiple topics with similar meanings but different words__. The researcher thus maps those topics as concepts into larger categories. On the other hand, we may __miss topics where different words with the same meaning__ are used.\n",
    "This problem can generally be __solved__ by __word embedding__ which maps words into a latent space where similar words are mapped into a similar space. \n",
    "\n",
    "See the following two papers for how word embedding can be combined with topic modelling:\n",
    "\n",
    "\n",
    "[Topic Modeling in Embedding Spaces](https://arxiv.org/pdf/1907.04907.pdf)\n",
    "\n",
    "[Mixing Dirichlet Topic Models andWord Embeddings to Make lda2vec](https://arxiv.org/pdf/1605.02019.pdf)\n",
    "\n",
    "\n",
    "\n",
    "(2) For a topic to form it is not just the individual words that matter but also the __meaning of larger text aggregates such as paragraphs__. By counting word co-occurrences we __cannot get at such more (global meaning)__. We thus need to have something that __accounts for global structure__, such as text summarization. Once our model can __measure the similarity between say, paragraphs__ in terms of their overall meaning, we can also more easily pick prototypical paragraphs.\n",
    "\n",
    "This is mainly tackled with with RNN and other neural network models that take into account long range context (one can see this as a form of text summarization task) by creating a text summary or text context vector. Taking this global meaning into account you get by far higher semantic coherence, and thus more consistent and more interpretable topics:\n",
    "\n",
    "[TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency](https://arxiv.org/pdf/1611.01702.pdf)\n",
    "\n",
    "[Topically Driven Neural Language Model](https://arxiv.org/pdf/1704.08012.pdf)\n",
    "\n",
    "[Topic Compositional Neural Language Model](https://arxiv.org/pdf/1712.09783.pdf)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
