{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive modeling: Frequencies\n",
    "\n",
    "Based on the \"frequentist\" school of statistics it seems like a straightforward solution to __model__ the probability distribution with __simply the co-occurence statistics__ of words. \n",
    "Our solution is thus: **take huge corpora and count!**\n",
    "\n",
    "\n",
    "## On the level of words: co-occurence\n",
    "\n",
    "The most simple approach given our \"mini corpus\" is:\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1knwiBQem4174oOsc02VzBv4QpTYNbqA9\"><img src=\"https://drive.google.com/uc?export=view&id=1t-avm9QHCveWfZ2tedZZrm2Aoi2PRkr6\" width=45%></a>\n",
    "\n",
    "This table will represent our language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the level of \"documents\": Bag of ...\n",
    "\n",
    "We can treat __any units of text__, like sentences, paragraphs and documents as **\"bag-of-words\"**, thus effectively modeling them as __statistical tables of frequencies__.\n",
    "\n",
    "Some say, that this is an exhaustive and sufficient description of meaning. In certain cases it might be right. :-)\n",
    "\n",
    "<a href=\"https://i.imgur.com/L5MDTcO.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1VhKdexEdhneHmidnULg2bYl9LWRVPk6f\" width=25%></a>\n",
    "\n",
    "The data representation for this approach is:\n",
    "\n",
    "<a href=\"http://www.rdatamining.com/_/rsrc/1421498854941/examples/social-network-analysis/term-doc-matrix.png\"><img src=\"https://drive.google.com/uc?export=view&id=1DxUt0_ZqfWMVmA63Qb_LC_a0QWazoeFH\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector space\n",
    "\n",
    "With a slight reformulation of this approach we can imagine that the words are forming separate **dimensions of a space** and thus the documents are effectively forming **vectors in this space**. The presence of a __word__ in a document __influences it's \"angle\"__ and it's __frequency the \"length\"__ in this vector dimension.\n",
    "\n",
    "<a href=\"http://blog.christianperone.com/wp-content/uploads/2013/09/vector_space.png\"><img src=\"https://drive.google.com/uc?export=view&id=1-ZHcX48twMasyUzM3G7Cm6N4BRjmbNwq\" width=45%></a>\n",
    "\n",
    "\n",
    "(This is \"osgooding\", but based on corpus counts.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "Naturally, if we __only__ think in __distinct words (tokens)__, that in itself __can cause problems__, since in case of the \"United States of America\" we would not want to process the vector directions of \"United\" and especially \"of\" as separate elements, we would like to to handle this as __one expression__. (And this is definitely _not_ just about proper names, see \"get up\"...)\n",
    "\n",
    "In this case one of the __possibilities__ is to use __Part-of-speech__ information explicitly, that is to go for __\"noun phrase embeddings\"__ or such. There is literature pointing in this direction (see eg. [here](http://www.aclweb.org/anthology/Q15-1017)), and it is true also, that utilizing POS information for differentiating (some) word senses (like bank(1) and bank(2)) can be useful see for example [sense2vec](https://arxiv.org/abs/1511.06388) and [others](https://www.cs.rochester.edu/~lsong10/papers/area.pdf). \n",
    "\n",
    "None the less, the __most widespread solution__ is the **inclusion of N-grams** or **skip-grams**, that is __consecutive or \"skipped\" combinations of N words for modeling__.\n",
    "\n",
    "Example: uni-, bi- and trigrams\n",
    "\n",
    "<a href=\"https://i.stack.imgur.com/8ARA1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1pMgv_5Xd6amdyEUif-66WyRf-FzR8cxQ\" width=45%></a>\n",
    "\n",
    "Example: 1-skip-bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T06:47:12.192263Z",
     "start_time": "2020-11-04T06:47:12.184446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog . \n",
      "\n",
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.'] \n",
      "\n",
      "The quick\n",
      "The brown\n",
      "quick brown\n",
      "quick fox\n",
      "brown fox\n",
      "brown jumps\n",
      "fox jumps\n",
      "fox over\n",
      "jumps over\n",
      "jumps the\n",
      "over the\n",
      "over lazy\n",
      "the lazy\n",
      "the dog\n",
      "lazy dog\n",
      "lazy .\n",
      "dog .\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog .\"\n",
    "print(text,\"\\n\")\n",
    "splitted = text.split(\" \")\n",
    "print(splitted,\"\\n\")\n",
    "for position in range(len(splitted)):\n",
    "    if position+1 < len(splitted):\n",
    "        print(splitted[position], splitted[position+1])\n",
    "    if position+2 < len(splitted):\n",
    "        print(splitted[position], splitted[position+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We add these N-grams to the vocabulary and count the frequencies accordingly.**\n",
    "\n",
    "\n",
    "\n",
    "### Sidenote: some assumptions \n",
    "\n",
    "Observe, that with _gross_ simplification we assume, that the __distribution is only dependent on the prior $n-1$ words__ (where $n$ is typically $<=4$), thus we assume a __Markov chain of the order $n$__:\n",
    "\n",
    " $$P(w ~\\vert ~ w_1,\\dots,w_k) = P(w ~\\vert ~ w_{k- n + 1},\\dots,w_k)$$\n",
    "\n",
    "We simply compute these probabiltites in a frequentist style by calculating the $n$-gram statistics of the corpus at hand:\n",
    "\n",
    "$$P(w_2 ~\\vert ~w_1) = \\frac{c(\\langle w_1, w_2 \\rangle)}{c(w_1)}$$\n",
    "\n",
    "$$P(w_{k+1} \\vert~ w_1,\\dots,w_k)_\\mathrm = \\frac{c(\\langle w_1,...,w_k, w_{k+1} \\rangle)}{c(\\langle w_1, \\dots w_k\\rangle)}$$\n",
    "\n",
    "Please note, that in this case we are using __\"memorization\"__, a form of database learning, with __minimal compression__ - \"counting\".\n",
    "\n",
    "But what do we with the given __$n$-grams rarely or never occure__? We have to employ some __smoothing__ solutions, like: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\n",
    "Or: _\"What to do with too rare or infrequent words?\"_\n",
    "\n",
    "What do we do when the given __$n$-grams rarely or never occure__? We have to employ some __smoothing__ solutions, like: \n",
    "\n",
    "##### Additive smoothing\n",
    "We pretend, that we have seen the $n$-grams more times than we have actually did with a __fixed $\\delta$__ number, in simplest case by $n=2$:\n",
    "\n",
    "$$P(w_{i+1} ~\\vert ~w_{i-n+1},..,~w_{i}) = \\frac{c(\\langle w_{i+1},.., w_{i-n+1} \\rangle) + \\delta}{\\sum_{w\\in V} [c(\\langle w_{i-n+1},.., w_i\\rangle) + \\delta]}$$\n",
    "\n",
    "Where V is the set of all words.\n",
    "\n",
    "Widespread solution for $\\delta$ is $1$.\n",
    "\n",
    "Problem:\n",
    "- If    c(⟨w_1,w_2⟩)=0 and c(⟨w_1,w_3⟩)=0 then under additive smoothing \n",
    "\n",
    "$$ p(w_1,w_2)=p(w_1,w_3)$$\n",
    "\n",
    "- Suppose that __$w_2$__ is much __more common than $w_3$__. Then we should have:\n",
    "$$ p(w_1,w_2)>p(w_1,w_3)$$\n",
    "\n",
    "    so the result from additive smoothing seems wrong.\n",
    "\n",
    "\n",
    "- Solution: interpolate between bigram and unigram models\n",
    "\n",
    "##### Interpolation\n",
    "\n",
    "In case of bigrams, we add - with a certain weight - the probabilities coming from the unigram frequencies:\n",
    "\n",
    "$$P(w_2 ~\\vert ~w_1)_{\\mathrm{interp}} = \\lambda_1\\frac{c(\\langle w_1, w_2 \\rangle)}{c(w_1)} + (1 - \\lambda_1)\\frac{c(w_1)}{\\sum_{w\\in V}c(w)}$$\n",
    "\n",
    "Recursive solution for arbitrary $k$:\n",
    "\n",
    "$$P(w_{k+1} \\vert~ w_1,\\dots,w_k)_\\mathrm{interp} = \\lambda_k\\frac{c(\\langle w_1,...,w_k, w_{k+1} \\rangle)}{c(\\langle w_1, \\dots w_k\\rangle)} + (1-\\lambda_k)P_\\mathrm{interp}(\\langle w_2,\\dots,w_{k+1}\\rangle)$$\n",
    "\n",
    "$\\lambda_k$ is empirically set by examining the corpus, typically by [Expectation Maximization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm), which - as we have mentioned - iteratively tunes the parameters to maximize the maximum likelihood.\n",
    "\n",
    "\n",
    "Good overview about the smoothing methods: [MacCartney, NLP Lunch Tutorial: Smoothing](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Normalizing\" with meaningful words\n",
    "\n",
    "It is fairly obvious, that __not all words contribute equally to the meaning of a text__, or to put it another way: from the viewpoint of information theory, **not all words have the same added information**. (For more information theoretic considerations see: [here](https://ccc.inaoep.mx/~villasen/index_archivos/cursoTL/articulos/Aizawa-tf-idfMeasures.pdf).) \n",
    "\n",
    "Certain classes of words are used mainly for __syntactic purposes__ - like eg. pronouns - thus __their information content__ is fairly __low__ from a __semantic perspective__.\n",
    "\n",
    "It is also true, as noted above, that they are pretty frequent in the corpus as of [\"Zipf's law\"](https://en.wikipedia.org/wiki/Zipf%27s_law).\n",
    "\n",
    "\n",
    "#### TF-IDF (family)\n",
    "\n",
    "One of the possible solutions for this is to \"smooth\" based on word frequencies. The most widespread solution for this is **\"term frequency–inverse document frequency\" or [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)**.\n",
    "\n",
    "The basic intuition is, that the words which are **generally frequent** in documents are less informative, than the ones that are **generally rare, but frequent in a small subset of documents**. They represent the \"discriminative information\" for that subset.\n",
    "\n",
    "INTUITION BEHIND TF-IDF: make rare words common in a subset of the document more prominent and effectively ignore common words.\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://cdn-images-1.medium.com/max/1600/1*8XpbsR4HdAHBXy5MgpIyug.png\"><img src=\"https://drive.google.com/uc?export=view&id=1l1ocR4yuVFJFvcWQahW-DJ0ZznOdnFUQ\" width=45%></a>\n",
    "\n",
    "\n",
    "<a href=\"https://cdn-images-1.medium.com/max/1600/1*jNnpbGPxkjehlvTCXq9B8g.png\"><img src=\"https://drive.google.com/uc?export=view&id=17MPUwfV-oc785DRTzqONMMzj2o8pLy_e\" width=45%></a>\n",
    "\n",
    "The __logarithm__ turns __1 into 0__, and makes __large numbers__ (those much greater than 1) __smaller__. (More on this later.) Then a word that appears in __every single document__ will be effectively __zeroed out__, and a word that appears in very __few documents__ will have an even __larger count__ than before.\n",
    "\n",
    "For the effects of feature scaling see [here](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/ch04.html).\n",
    "\n",
    "Let’s look at some pictures to understand what it’s all about. The next figure shows a simple example that contains four sentences: “it is a puppy,” “it is a cat,” “it is a kitten,” and “that is a dog and this is a pen.” We plot these sentences in the feature space of three words: “puppy,” “cat,” and “is.”\n",
    "\n",
    "<a href=\"https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/assets/feml_0402.png\"><img src=\"https://drive.google.com/uc?export=view&id=1tZQRJrk0FHZlMOT1zV3918ZCcG-U9oD1\" width=45%></a>\n",
    "\n",
    "It is noteworthy that TF-IDF is one of the (most basic) methods for locating keywords in a document, thus we can say, that in principle **all keyword extraction methods** can be used to do smoothing for frequency counts (or even other vector representations - for that matter).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank (family)\n",
    "\n",
    "Another group of unsupervised keyword extraction techniques is **TextRank** and it's many variants (original publication [here](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)), which builds on the Google founders Page and Brin's PageRank algorithm.\n",
    "\n",
    "The basis of it is a **co-occurence graph** of words, which is in itself a filtered, \"sparsified\" co-occurence matrix.\n",
    "\n",
    "<a href=\"https://i.stack.imgur.com/ohF5r.png\"><img src=\"https://drive.google.com/uc?export=view&id=1ACpwqO2WAKGlMfy7Uwwmv0CtKuB9ENHY\" width=35%></a>\n",
    "\n",
    "We than simulate a **random walk over this graph** to come up with an approximation of some kind of \"centrality\" like metric for the nodes, that is to find the \"key\" words.\n",
    "\n",
    "<a href=\"https://i1.wp.com/1.bp.blogspot.com/-5DGkqiLF87U/Uzqm0Vah16I/AAAAAAAABIE/aPgVRreUvts/s1600/g4.gif?w=456\"><img src=\"https://drive.google.com/uc?export=view&id=1LLMmU-Msvpu0dPQ8oW1sip9EFg8Kd2Xm\" width=35%></a>\n",
    "\n",
    "[Source](https://www.r-bloggers.com/from-random-walks-to-personalized-pagerank/)\n",
    "\n",
    "\n",
    "#### Alternatives for unsupervised keyword extraction\n",
    "\n",
    "Over and beyond these techniques numerous variants exist.\n",
    "\n",
    "Some alternatives for unsupervised keyword extraction:\n",
    "\n",
    "- [RAKE](https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470689646.ch1)\n",
    "- [OKAPI BM25](https://en.wikipedia.org/wiki/Okapi_BM25)\n",
    "- [SGRank](http://www.aclweb.org/anthology/S15-1013)\n",
    "- [DivRank](http://dx.doi.org/10.1145/1835804.1835931)\n",
    "\n",
    "And more recent advancements: \n",
    "[here](http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14377/14168),\n",
    "[here](http://arxiv.org/abs/1702.04457), [here](http://arxiv.org/abs/1801.04470), [here](https://arxiv.org/abs/1803.08493) and [here](https://arxiv.org/abs/1811.10831).\n",
    "\n",
    "#### Sidenote: From vectors to keywords\n",
    "\n",
    "As it is mentioned in some of the publications above, there is a \"reverse\" way in which we __start from some available (typically pre-trained) semantic vector model__, __transform__ the __documents__ and their words __separately__ with the help of those embeddings and try to detect the __most salient keywords by comparing the vector representations of documents and texts__. \n",
    "\n",
    "**The words closest to the document vector can be considered the keywords.**\n",
    "\n",
    "In certain cases we use this \"reverse technique\".\n",
    "\n",
    "See for example [here](https://arxiv.org/abs/1710.07503)\n",
    "\n",
    "#### Inverse: The difficult case of stopwords\n",
    "\n",
    "The inverse problem of __finding \"keywords\"__, the __listing__ of so called __\"stop words\"__, which __do not add (too much) to the semantics of the given text__ is also intriguing. We have a strong assumption that the top words are not informative.\n",
    "\n",
    "There were some tries to explicitly create stopword lists based on corpus statistics. like [here](http://terrierteam.dcs.gla.ac.uk/publications/rtlo_DIRpaper.pdf).\n",
    "\n",
    "The basic idea is, that __somewhere after the most frequent stopwords__ the __\"domain keywords\" start__, then the whole distribution finishes with \"noise\".\n",
    "\n",
    "<a href=\"https://image.slidesharecdn.com/stopwords-140602111606-phpapp01/95/on-stopwords-filtering-and-data-sparsity-for-sentiment-analysis-of-twitter-13-638.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1Bf2Yh6vkQGYAItOuelD12go-Bx598azR\" width=65%></a>\n",
    "\n",
    "([source](https://www.slideshare.net/Staano/stopwords))\n",
    "\n",
    "This sounds nice, but often **does not work in practice**.\n",
    "\n",
    "More on types of stopwords:[here](http://text-analytics101.rxnlp.com/2014/10/all-about-stop-words-for-text-mining.html)\n",
    "\n",
    "Empirical observation tells us:\n",
    "It __does not always help to remove stopwords__, or at least it is controversial, see [here](https://ieeexplore.ieee.org/document/7375527).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with ngram vectorspaces\n",
    "\n",
    "### Size!\n",
    "\n",
    "On a large enough corpus, the __memory footprint__ of the __$n$-gram__ models is __huge__, eg. for the 1T n-gram corpus of Google ([see here](https://catalog.ldc.upenn.edu/LDC2006T13)) containing 1,024,908,267,229 tokens the $n$-gram counts are as follows:\n",
    "- unigram: 13,588,391, \n",
    "- bigram: 314,843,401, \n",
    "- trigram: 977,069,902, \n",
    "- fourgrams: 1,313,818,354 \n",
    "- fivegram: 1,176,470,663.\n",
    "\n",
    "### Curse of dimensionality\n",
    "\n",
    "Consider the following table, which shows the size of a hypercube that covers the given $f$ fraction of the volume of a $[0,1]^D$ unit hypercube for a number of $D$ dimensions ($\\sqrt[D]{f}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T13:48:37.578817Z",
     "start_time": "2019-02-07T13:48:37.555606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>2 dims</th>\n",
       "      <th>3 dims</th>\n",
       "      <th>4 dims</th>\n",
       "      <th>5 dims</th>\n",
       "      <th>6 dims</th>\n",
       "      <th>7 dims</th>\n",
       "      <th>8 dims</th>\n",
       "      <th>9 dims</th>\n",
       "      <th>10 dims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.584804</td>\n",
       "      <td>0.668740</td>\n",
       "      <td>0.724780</td>\n",
       "      <td>0.764724</td>\n",
       "      <td>0.794597</td>\n",
       "      <td>0.817765</td>\n",
       "      <td>0.836251</td>\n",
       "      <td>0.851340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.215443</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.398107</td>\n",
       "      <td>0.464159</td>\n",
       "      <td>0.517947</td>\n",
       "      <td>0.562341</td>\n",
       "      <td>0.599484</td>\n",
       "      <td>0.630957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.177828</td>\n",
       "      <td>0.251189</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.372759</td>\n",
       "      <td>0.421697</td>\n",
       "      <td>0.464159</td>\n",
       "      <td>0.501187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.046416</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.158489</td>\n",
       "      <td>0.215443</td>\n",
       "      <td>0.268270</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.359381</td>\n",
       "      <td>0.398107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        f    2 dims    3 dims    4 dims    5 dims    6 dims    7 dims  \\\n",
       "0  0.2000  0.447214  0.584804  0.668740  0.724780  0.764724  0.794597   \n",
       "1  0.0100  0.100000  0.215443  0.316228  0.398107  0.464159  0.517947   \n",
       "2  0.0010  0.031623  0.100000  0.177828  0.251189  0.316228  0.372759   \n",
       "3  0.0001  0.010000  0.046416  0.100000  0.158489  0.215443  0.268270   \n",
       "\n",
       "     8 dims    9 dims   10 dims  \n",
       "0  0.817765  0.836251  0.851340  \n",
       "1  0.562341  0.599484  0.630957  \n",
       "2  0.421697  0.464159  0.501187  \n",
       "3  0.316228  0.359381  0.398107  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dims = list(range(1,11))\n",
    "fracs = [0.2, 0.01, 0.001,0.0001]\n",
    "result = []\n",
    "for frac in fracs:\n",
    "    result.append([frac**(1/dim) for dim in dims])\n",
    "pd.DataFrame(result, columns = [\"f\"] +[f\"{dim} dims\" for dim in dims[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing sample requirements\n",
    "\n",
    "The figures show that if we train an ML algorithm using one feature on the 20% of a population, and start adding new features then we need dramatically more samples to maintain the same amount of coverage on the feature space: concretely, even for 3 features we'd need almost 60% of the population:\n",
    "\n",
    "<a href=\"http://www.visiondummy.com/wp-content/uploads/2014/04/curseofdimensionality.png\"><img src=\"https://drive.google.com/uc?export=view&id=1rZ3tRLxGoDRi4KgHTwaRUi5L_BSPGMx-\" width=50%></a>\n",
    "\n",
    "([image source](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/))\n",
    "\n",
    "#### Problems with \"neighborhoods\"\n",
    "\n",
    "Another strange effect of high dimensionality is that the ratio of normal/central examples that are close to the centroid of the population radically decreases, because of the strange behavior of hyperspheres. E.g., considering again an $[1,0]^D$ feature space and an inscribed hypersphere with an $0.5$ radius, the volume of the hypesphere, and consequently the ratio of central examples tends to 0 (even though the spheres touch all sides of the hypercubes they are inscibed in):\n",
    "\n",
    "<a href=\"http://www.visiondummy.com/wp-content/uploads/2014/04/sparseness.png\"><img src=\"https://drive.google.com/uc?export=view&id=1kud-R8BRUkKqSDxchlb5ZUIwuvfAHzQ2\" width=50%></a>\n",
    "\n",
    "<a href=\"http://www.visiondummy.com/wp-content/uploads/2014/04/hypersphere.png\"><img src=\"https://drive.google.com/uc?export=view&id=1eXFwO8Kalj2xUdIXryuj42RzoVvWBpDm\" width=30%></a>\n",
    "\n",
    "([image source](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/))\n",
    "\n",
    "\n",
    "Please observe, that in our raw representation of \"counts\" we did not \"learn\" too much in the sense of **representation learning**, while there is no **compression** of the data, just **memorization**.\n",
    "\n",
    "\n",
    "\n",
    "## Decomposition\n",
    "\n",
    "Or: _\"What should we do with these huge co-occurence matrices?\"_\n",
    "\n",
    "The solution is: **dimension reduction**.\n",
    "\n",
    "We would like to capture the main sources of variance for the matrices and to \"learn\" a lower dimensional, \"compressed\" representation for them. (We can also suspect, that this lower dimensional representation is something more close of the meaning space behind the produced language, that is: it's **latent structure**.)\n",
    "\n",
    "The main technique which has been used is **\"principal component analysis\"** (and it's variants).\n",
    "\n",
    "(Whoever attended the class on deep learning, is not really surprised now.)\n",
    "\n",
    "\n",
    "<a href=\"https://www.bogotobogo.com/python/scikit-learn/images/CompressData-1-DimensionalityReduction-PCA/PrincipalDirection.png\"><img src=\"https://drive.google.com/uc?export=view&id=1RyGgmJ4NeuKtnYIc4sPBVXVRj166SRWS\" width=65%></a>\n",
    "\n",
    "About dimensionality reduction:\n",
    "- [here](https://en.wikipedia.org/wiki/Dimensionality_reduction) and\n",
    "- [here](https://arxiv.org/pdf/1403.2877.pdf)\n",
    "\n",
    "\n",
    "We search for the rotation that \"explains\" the most amount of variance in our data.\n",
    "\n",
    "\n",
    "Most popular method in semantics is: **Latent Semantic Indexing (LSI)**.\n",
    "see [here](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "\n",
    "The assumption again is, that the observed co-occurences are explainable form the perspective of latent meanings, from **\"concepts\"**, which are brought forward by the dimension reduction.\n",
    "\n",
    "<a href=\"https://technowiki.files.wordpress.com/2011/08/diagram2.png\"><img src=\"https://drive.google.com/uc?export=view&id=1O45EowtprBGkL2s_qC2JlxK_5fpb9EMQ\" width=25%></a>\n",
    "\n",
    "\n",
    "LSI is not totally PCA, see [here](https://irthoughts.wordpress.com/2007/05/05/pca-is-not-lsi/), but close enough.\n",
    "\n",
    "There is also a probabilistic extension of LSI called [PLSI](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis), which explicitly states that the probabilities for generating words comes directly from the **\"topic model\"**.\n",
    "\n",
    "### Does decomposition help mitigate size?\n",
    "\n",
    "Well, if it fits into memory...\n",
    "\n",
    "The problem is, that for PCA like approaches it is a prerequisite to have the whole matrix in memory for the decomposition to happen, thus LSI and the like are many times **prohibitively expensive** in case of a decently big vocabulary and / or many documents. \n",
    "\n",
    "There were some tries to apply different algorithms like [incremental pca](https://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html) to the problem, but generally this turned out to be a limited success. \n",
    "\n",
    "**This is a major motivation for using different, \"streaming compatible\" models!**\n",
    "\n",
    "\n",
    "# How to use vector spaces?\n",
    "\n",
    "## Basic querying: Cosine distance\n",
    "\n",
    "<a href=\"https://slideplayer.com/slide/5993529/20/images/16/Documents+%26+Query+in+n-dimensional+Space.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1Yi4FdZRy8cUqdAfSYpOyz0-ZNEPQKeEa\" width=45%></a>\n",
    "\n",
    "<a href=\"http://blog.christianperone.com/wp-content/uploads/2013/09/Dot_Product.png\"><img src=\"https://drive.google.com/uc?export=view&id=1ja1csLYW2zj8_YoPBqSaZQDGj5QizQ5I\" width=25%></a>\n",
    "\n",
    "## Alternative distance metrics\n",
    "\n",
    "### Euclidean distance\n",
    "\n",
    "<a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Euclidean_distance_3d_2_cropped.png/1024px-Euclidean_distance_3d_2_cropped.png\"><img src=\"https://drive.google.com/uc?export=view&id=1QUWzWVynns5SvAXm3hhH38_Zl9v5T5SW\" width=35%></a>\n",
    "\n",
    "In case of frequency based semantic vectors we would naively be tempted to go for the full Euclidean distance calculations, but that would __disregard__ the fact, that in case of a query and a document, __radically different frequencies will be present__ in case of words, so we might be better served with the cosine approach, which focuses on relative differences in meaning regardless of frequency.\n",
    "\n",
    "\n",
    "### Earth movers's (Mahalanobis) distance\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1L1jRYCjfRbQjGyqVKssLB2pfOc47Mch7\"><img src=\"https://drive.google.com/uc?export=view&id=1bEkjN-KOAmlyNU9zw5j_btJLAuoGIjvx\" width=40%></a>\n",
    "\n",
    "\n",
    "<a href=\"https://slideplayer.com/slide/4511821/15/images/30/Option+3%3A+The+Earth+Mover+Distance+%28EMD%29.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1bcKY4nEiu7BCMdtrS3yEXVvQz-jO1Rna\" width=45%></a>\n",
    "\n",
    "<a href=\"https://vene.ro/images/wmd-obama.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Oqqr8Gr_Qh2l0YROpjE6Ri3WKkxdiwsC\" width=45%></a>\n",
    "\n",
    "The usage of EMD makes sense in this context, since we suppose again, that certain locations of our vectors are representing some meaningful units, thus some kind of \"topic mapping\" is going on in case of a distance measurement.\n",
    "\n",
    "For the usage of EMD in semantic spaces see [this](http://proceedings.mlr.press/v37/kusnerb15.pdf) article.\n",
    "\n",
    "**Fair warning:** The __runtime__ requirements of EMD are __non linear__. For production environments this can mean a lot of CPU load. (Though [Radim Řehůřek](https://radimrehurek.com/about/) in [Gensim](https://radimrehurek.com/gensim/) [wmdistance](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.wmdistance.html) did some work on speeding it up. For easy usage see [here](https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html))\n",
    "\n",
    "\n",
    "\n",
    "For a more general discussion about the different distance metrics in context of semantic clustering see [this](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.4480&rep=rep1&type=pdf) article.\n",
    "\n",
    "\n",
    "## What does this look like practically?\n",
    "\n",
    "\n",
    "The general architecture of an (linguistic) information retrieval system is similar to this:\n",
    "\n",
    "<a href=\"https://3.bp.blogspot.com/-Ix9pGH_mYNM/Wfgm064NpqI/AAAAAAAAGCA/X-P4V2917Pgr5hPnRVvRkfRD6GwgjL1rACK4BGAYYCw/s1600/information%2Bretrieval_2.PNG\"><img src=\"https://drive.google.com/uc?export=view&id=1GWJ3p8BRZ1G7325ODfUuMAKxrqtbDisK\" width=55%></a>\n",
    "\n",
    "Naturally, if we would try to __calculate__ this process __on the fly__ for all queries, that would be prohibitively expensive (think Google search...)\n",
    "\n",
    "We can thus decompose the process to __two steps__:\n",
    "\n",
    "1. __Preprocessing (vectorization)__\n",
    "\n",
    "Typically we __preprocess__ the __documents__ based on our models and store the __vector representation__ of them in an appropriate data structure (SQL is not always the best idea here, but can work).\n",
    "\n",
    "2. __Retrieval__\n",
    "\n",
    "We __vectorize__ \"on the fly\" the __incoming query__, and try to __calculate__ a __ranking__ for the pre-processed database enrties. This step in itself is not a linearly scaling problem, so good care has to be taken, that the lookup remains quick enough. We can use approximate nearest neighbor search for the shortlist of items just like Spotify does with [Annoy](https://github.com/spotify/annoy).\n",
    "\n",
    "\n",
    "## Main takeaway\n",
    "\n",
    "---------------------\n",
    "\n",
    "<font color='red'>\n",
    "Whoever can learn a right transformation from objects to a meaningful vector space and can do similarity comparisons at scale has a good search / recommender engine!\n",
    "</font>\n",
    " \n",
    "---------------------\n",
    "\n",
    " \n",
    "# General problems with frequency based models\n",
    "\n",
    "\n",
    "1. Extreme __memory hunger__ (as discussed already).\n",
    "\n",
    "2. Their basic __assumptions are not realistic__, since the probabilities of words are _indeed_ **influenced by far away words and sequence information**, but we can not capture these with this methods (even increasing N-gram count is infeasible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
