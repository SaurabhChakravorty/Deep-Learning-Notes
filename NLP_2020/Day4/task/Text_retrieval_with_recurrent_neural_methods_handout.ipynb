{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1LpjNSceR85m"
   },
   "source": [
    "# LSTM in practice -- NLP\n",
    "\n",
    "## Language modeling\n",
    "\n",
    "A language model is a probability distribution over the sequence of words, modeling language (production), thus if the set of words is $w$, then for arbitrary $\\mathbf w = \\langle w_1,\\dots, w_n\\rangle$ ($w_i\\in W$) sequence it defines a $P(\\mathbf w)$ probability. \n",
    "\n",
    "Probability with chain rule:\n",
    "\n",
    "$$P(\\mathbf w)= P(w_1)\\cdot P(w_2 \\vert w_1 )\\cdot P(w_3\\vert w_1, w_2)\\cdot\\dots\\cdot P(w_n\\vert w_1,\\dots, w_{n-1})$$\n",
    "\n",
    "so this means, that for the modeling we need only to give the conditional probability of the \"continuation\", the next word, thus for $w$ word and $\\langle w_1,\\dots,w_n\\rangle$ sequence the probability that the next word will be $w$\n",
    "\n",
    "$$P(w ~\\vert ~ w_1,\\dots,w_n)$$\n",
    "\n",
    "There are character based models also, which take the individual characters as units, not the words, and model language as a distribution over sequences of characters (think T9...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moYnnb1-jKT4"
   },
   "source": [
    "### Measurement of performance: Perplexity\n",
    "\n",
    "A language model $\\mathcal M$'s perplexity over the word series $\\mathbf w = \\langle w_1,\\dots, w_n\\rangle$ is:\n",
    "\n",
    "$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w) = \\sqrt[n]{\\frac{1}{P_{\\mathcal M}(\\mathbf w)}}$$\n",
    "\n",
    "With the chain rule can be rewritten as:\n",
    "\n",
    "$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w) = {\\sqrt[n]{\\frac{1}{P_{\\mathcal M}(w_1)}\\cdot \\frac{1}{P_{\\mathcal M}(w_2 \\vert w_1 )}\\cdot \\frac{1}{P_{\\mathcal M}(w_3\\vert w_1, w_2)}\\cdot\\dots\\cdot \\frac{1}{P_{\\mathcal M}(w_n\\vert w_1,\\dots, w_{n-1})}}}$$\n",
    "\n",
    "which is exactly the geometric mean of the reciprocals of the conditional probabilities of all words in the corpus.\n",
    "\n",
    "In case of a bigram model this is further simplified to:\n",
    "$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w) = \\sqrt[n]{\\frac{1}{P_{\\mathcal M}(w_1)}\\cdot \\frac{1}{P_{\\mathcal M}(w_2 \\vert w_1 )}\\cdot \\frac{1}{P_{\\mathcal M}(w_3\\vert w_2)}\\cdot\\dots\\cdot \\frac{1}{P_{\\mathcal M}(w_n\\vert w_{n-1})}}$$\n",
    "\n",
    "\n",
    "### But what is it good for?\n",
    "For example:\n",
    "- Predictive text input (\"autocomplete\")\n",
    "- Generating text\n",
    "- Spell checking\n",
    "- Language understanding\n",
    "- And most importantly representation learning - this we will be studiying in detail in a next lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fn4iPehcyda2"
   },
   "source": [
    "### Generating text with a language model\n",
    "\n",
    "The language model produces a tree with probable continuations of the text:\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-Jjpb7iyB37A/WBZI4ImGQII/AAAAAAAAA9s/ululnUWt2vw9NMKuEr-F9H8tR2LEv36lACLcB/s1600/prefix_probability_tree.png\" width=400 heigth=400>\n",
    "\n",
    "Using this tree we can try different algorithms to search for the best \"continuations\". A full breadth-first search oi usually impossible, due to the high branching factor of the tree.\n",
    "\n",
    "Alternatives:\n",
    "- \"Greedy\": we choose the continuation which has the highest direct probability, This will most probably be suboptimal, since the probability of the full sequence is tha product of the continuations, and if we would have chosen a different path, we might ahve been able to choose later words with hihg probabilities.\n",
    "- Beam-search: we always store a fixed $k$ number of partial sequences, and we always try to expand these, always keeping the most probable $k$ from the possible continuations. \n",
    "\n",
    "Example ($k$=5):\n",
    "\n",
    "<img src=\"http://opennmt.net/OpenNMT/img/beam_search.png\" width=600 heigth=600>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pO8BuLsDWWAE"
   },
   "source": [
    "### The \"old way\": N-gram based solutions\n",
    "\n",
    "With _gross_ simplification we assume, that the distribution is only dependent on the prior $n-1$ words (where $n$ is typically $<=4$), thus we assume a Markov chain of the order $n$:\n",
    "\n",
    " $$P(w ~\\vert ~ w_1,\\dots,w_k) = P(w ~\\vert ~ w_{k- n + 2},\\dots,w_k)$$\n",
    "\n",
    "We simply compute these probabilities in a frequentist style by calculating the $n$-gram statistics of the corpus at hand:\n",
    "\n",
    "$$P(w_2 ~\\vert ~w_1) = \\frac{c(\\langle w_1, w_2 \\rangle)}{c(w_1)}$$\n",
    "\n",
    "$$P(w_{k+1} \\vert~ w_1,\\dots,w_k)_\\mathrm = \\frac{c(\\langle w_1,...,w_k, w_{k+1} \\rangle)}{c(\\langle w_1, \\dots w_k\\rangle)}$$\n",
    "\n",
    "Please note, that in this case we are using \"memorization\", a form of database learning, with minimal compression - \"counting\".\n",
    "\n",
    "But what do we do the given $n$-grams rarely or never occur? We have to employ some __smoothing__ solutions, like: \n",
    "\n",
    "##### Additive smoothing\n",
    "We pretend that we have seen the $n$-grams more times than we have actually did with a fixed $\\delta$ number, in the simplest case with $n=2$:\n",
    "\n",
    "$$P(w_2 ~\\vert ~w_1) = \\frac{c(\\langle w_1, w_2 \\rangle) + \\delta}{\\sum_{w\\in V} [c(\\langle w_1, w\\rangle) + \\delta]}$$\n",
    "\n",
    "Widespread solution for $\\delta$ is $1$.\n",
    "\n",
    "The main problem with this kind of smoothing is that it does not take into account by \"supplementing\" the data the frequency of components of shorter $n$-grams, eg. if neither $\\langle w_1, w_2 \\rangle$  nor $\\langle w_1, w_3 \\rangle$ occurs in the corpus, it assumes the frequency of both bigrams to be $\\delta$, irrespective of the ratio of frequencies of $w_2$ and $w_3$.\n",
    "Most smoothing techniques are trying to accomodate this, eg: simple interpolation:\n",
    "\n",
    "##### Interpolatcion\n",
    "\n",
    "In case of bigrams, we add - with a certain weight - the probabilities coming from the individual frequencies:\n",
    "\n",
    "$$P(w_2 ~\\vert ~w_1)_{\\mathrm{interp}} = \\lambda_1\\frac{c(\\langle w_1, w_2 \\rangle)}{c(w_1)} + (1 - \\lambda_1)\\frac{c(w_1)}{\\sum_{w\\in V}c(w)}$$\n",
    "\n",
    "Recursive solution for arbitrary $k$:\n",
    "\n",
    "$$P(w_{k+1} \\vert~ w_1,\\dots,w_k)_\\mathrm{interp} = \\lambda_k\\frac{c(\\langle w_1,...,w_k, w_{k+1} \\rangle)}{c(\\langle w_1, \\dots w_k\\rangle)} + (1-\\lambda_k)P_\\mathrm{interp}(\\langle w_2,\\dots,w_{k+1}\\rangle)$$\n",
    "\n",
    "$\\lambda_k$ is empirically set by examining the corpus, typically by [Expectation Maximization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm), which - as we have mentioned - iteratively tunes the parameters to maximize the likelihood.\n",
    "\n",
    "\n",
    "Good overview about the smoothing methods: [MacCartney, NLP Lunch Tutorial: Smoothing](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)\n",
    "\n",
    " \n",
    "#### General problems\n",
    "\n",
    "- Even the core assumption is not too realistic, since the probabilities are for sure influenced in a way by words further than $n$, but for practical reasons, it has to be limited (sparsity, computation capacity).\n",
    "- On a large enough corpus, the memory footprint of the $n$-gram models is _huge_, eg. for the 1T n-gram corpus of Google ([see here](https://catalog.ldc.upenn.edu/LDC2006T13)) containing 1,024,908,267,229 tokens the $n$-gram counts are as follows:\n",
    "    - unigram: 13,588,391, \n",
    "    - bigram: 314,843,401, \n",
    "    - trigram: 977,069,902, \n",
    "    - fourgrams: 1,313,818,354 \n",
    "    - fivegram: 1,176,470,663."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgLdc8JQrH-4"
   },
   "source": [
    "## Language modeling with LSTMs\n",
    "\n",
    "One way to circumvent the Markov assumption is to use RNN-s, which are capable of modeling the long-ter dependencies inside the sequence of words. The text is thus considered to be a time-series, and thus an appropriate architecture can be used (as we have already seen):\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1y8QYr9ftTvXAxgzS-ldnGlijVpmK2l21\" width=600 heigth=600>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHVR_jd62LHI"
   },
   "source": [
    "Notable features:\n",
    "\n",
    "- Input is a \"one-hot\" encoded vector, wchic we on the spot transform into an \"embedding vector\"\n",
    "- For each output step, we get a probability distribution over the whole vocabulary with softmax\n",
    "- This above is a simple RNN, but LSTMs can be used without any problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqQY-P9XP6Ro"
   },
   "source": [
    "### Teaching\n",
    "\n",
    "_In theory_ an RNN could be trained with full GD on the corpus in one go:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1XsBoRp7cNay3svFLRDv2JEDyC7m7CUdC\" width=600 heigth=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXIBEAhM8dzU"
   },
   "source": [
    "- The loss is generally the well-kown crossentropy, which is in this case (since the input is a one-hot vector):\n",
    "  $$J^{(i)}(\\Theta) = -\\log (\\hat y[x^{(i+1)}])$$\n",
    "  the negative logarithm of the probability assigned by the network to the right word / next word.\n",
    "\n",
    "- For the sake of more frequent updates, and since BPTT for long sequences is very expensive, teaching is done in smaller units with not necessarily the same length.\n",
    "- The unit is typically one or more sentece, or if the length allows, and we have enough material, a paragraph can be a good candidate.\n",
    "- Initial state in case of the time-series units: if the boundaries are inside a unit of text, it is important to _transfer the hidden state_ from the previous unit, in other cases initialization can be done by some fixed value.\n",
    "- (Somewhat misleading) terminology: the length of the \"time\" unit is _time step_, but sometimes certain implementations call it _minibatch_, though that would generally mean the number of units processed in one go for the sake of computaitonal efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8IYutdnW_SG"
   },
   "source": [
    "### LSTM as layers\n",
    "\n",
    "+ An LSTM - how ever strange that may sound - can be considered to be a complete layer. The most important parameter of it is the \"number of (memory) units\", which is the length of the hidden state vector, thus, the memory capacity. **Warning: this does not have any relationship to input size, thus can be considered a freely chosen parameter.**\n",
    "+ It is quite widespread to use multiple LSTM layers (\"stacked LSTMs\") -- as in the case of ConvNets the hope is, that the layers learn a hierarchy of abstract representations:\n",
    "\n",
    "<img src=\"http://wenchenli.github.io/assets/img/GNMT_residual.png\" width=60%>\n",
    "\n",
    "(on the right side a network is shown with skip/residual connections!)\n",
    "\n",
    "In this case it makes sense, that we do not only get on top of the LSTM a final prediction $h$ (or even prediction + inner state vector $c$) for a sequence, but **we ask it to output the whole sequence of predictions**, so that the next layer can also operate on full sequences. Please bear this in mind during implementation, since this can be a common source of failure.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3R04Si0M8KzK"
   },
   "source": [
    "## An LSTM language model in Keras\n",
    "\n",
    "For this task the inspiration comes from the famous [reference work of Andrej Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
    "\n",
    "Note, that in this case we will not use regularization, since we are willing to overfit - for the sake of play with the text. This is now an \"overfitting competition\", so _not_ a generally good practice!\n",
    "\n",
    "## Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T16:48:43.620252Z",
     "start_time": "2018-10-06T16:48:43.607851Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "egfH6gkf8WQg",
    "outputId": "67e0a903-02eb-480f-c9b3-f06f90336115"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1212)\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# This can be an important parameter, so be aware of it...\n",
    "max_seq_length = 15\n",
    "max_num_of_sents = 57200\n",
    "# max_num_of_sents = 50 # How many sentences should we read from the corpus (max=57200)\n",
    "\n",
    "def generate_brown_word_to_id_map():\n",
    "    \"\"\"Return a dictionary mapping downcased Brown-words to their ids.\n",
    "    Numbering starts from 1 since we use 0 for masking (!!!).\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    for word in brown.words():\n",
    "        words.add(word.lower())\n",
    "    return {word: idx + 1 for idx, word in enumerate(sorted(words))}\n",
    "\n",
    "\n",
    "class BrownReader:\n",
    "    \"\"\"A reader class for the Brown corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word_to_id_map = generate_brown_word_to_id_map()\n",
    "        self.id_to_word_map = {idx: word for word, idx in self.word_to_id_map.items()}\n",
    "\n",
    "    def n_words(self):\n",
    "        return len(self.word_to_id_map)\n",
    "\n",
    "    def sentence_to_ids(self, sentence):\n",
    "        \"\"\"Return the word ids of a sentence.\n",
    "        \"\"\"\n",
    "        return [self.word_to_id_map[word.lower()] for word in sentence]\n",
    "        \n",
    "    def sentences(self):\n",
    "        \"\"\"Generator yielding features from the Brown corpus.\n",
    "        \"\"\"\n",
    "        return (self.sentence_to_ids(sentence) for sentence in brown.sents())\n",
    "\n",
    "    def sentence_matrixes(self):\n",
    "        x = np.zeros((max_num_of_sents, max_seq_length-1))\n",
    "        y = np.zeros((max_num_of_sents, max_seq_length-1))\n",
    "        sents = self.sentences()\n",
    "        for idx, sent in enumerate(sents):\n",
    "            if idx == max_num_of_sents:\n",
    "                break\n",
    "            np_array = np.asarray(sent)\n",
    "            length  = min(max_seq_length, len(np_array))\n",
    "            x[idx, :length - 1] = np_array[:length - 1]\n",
    "            y[idx, :length - 1] = np_array[1:length]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pvjdF2_p9_YV"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SavNwc5m9_Ya"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CXHNzCXj9_Ye",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "br = BrownReader()\n",
    "n_words = br.n_words()\n",
    "\n",
    "max_input_length = max_seq_length - 1 # since our x/y input does not contain the last/first element of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JkhDDIwl9_ZC",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_x, data_y = br.sentence_matrixes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rJdzjP6R9_ZK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_y = np.expand_dims(data_y, -1) # It seems that Keras needs this for the \"one-cold\" and softmax dims to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "\n",
    "lstm_size = ???\n",
    "embedding_size = ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWPOCc6q9_Yn"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "FXth0CDm9_Yq",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "915247f0-9294-4cb8-c3d7-ed48d4cc0cf8"
   },
   "outputs": [],
   "source": [
    "# Import:\n",
    "# Import the appropriate layers from tf.keras!\n",
    "# Think about it, that one layer should map (\"embed\") tha input into a dense vector!\n",
    "# Don't forget the main model class - according to functional or sequential API\n",
    "# And eventually the optimizer and backend\n",
    "# Later one for resetting the graph - good practice!\n",
    "# And think about it what loss function you will use. The output is a classification\n",
    "# (categorical) task, it is sparse, so...\n",
    "\n",
    "\n",
    "# Please reset the graph here!\n",
    "\n",
    "# Model\n",
    "########\n",
    "# Build the model!\n",
    "# Start with the input layer, it receives a vector of the length of the maximal text span (sentence)\n",
    "# And: vector or not, shape is a tuple...\n",
    "# After this, use the \"mapping\" layer.\n",
    "# WARNING:\n",
    "# 1. width = number of words +1\n",
    "# 2. it's size is defined in a parameter, somewhere above\n",
    "# 3. length of input: max input size -1\n",
    "# 4. Zero values are to be masked in it!!! \n",
    "#    the constructor has a named argument which has to be given with the True value!!!\n",
    "#\n",
    "# TASK: Can you please verbally elaborate to the instructor, why the points above are true?\n",
    "\n",
    "\n",
    "# In the next TWO layers there should be LSTM-s\n",
    "# For them being able to be stacked, they have to give back not only the predictions at sequence end\n",
    "# Somewhere there has to be a nice parameter for this... ;-)\n",
    "\n",
    "# Important: for certain purposes (hint: search engine...) it's very useful to have the last\n",
    "# hidden state and cell state also included in the results, so for the 2nd LSTM please turn \n",
    "# on the return_state option. Be aware that thereby the 2nd LSTM cell will return three\n",
    "# tensors (the series of outputs, last hidden state, last cell state), and you will need only \n",
    "# the first of these as input for your next layer (as always, ask when in doubt)   \n",
    "\n",
    "# Finally project the output with a fully connected layer and a softmax.\n",
    "# What is it's width? (Help: If you have varbally elaborated well above, you already know.)\n",
    "\n",
    "\n",
    "# Finally, create a model instance!\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKXuebno9_Y1"
   },
   "source": [
    "### Error, optimizer, compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4Mob9-6s9_Y3",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Loss \n",
    "\n",
    "loss = ??? # Our output is One-hot encoded. What do we use?\n",
    "\n",
    "# Optimizer\n",
    "optimizer = ??? # According to taste...\n",
    " \n",
    "# Compilation\n",
    "#############\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLLqU3c_9_Y8"
   },
   "source": [
    "### Training\n",
    "\n",
    "We generate the trainig data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rJdzjP6R9_ZK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_y = np.expand_dims(data_y, -1) # It seems that Keras needs this for the \"one-cold\" and softmax dims to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMPFHd-M9_ZX"
   },
   "source": [
    "And train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "EJboxaY_9_Zd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "80dacba0-9177-4b12-cbd6-641659f376f0"
   },
   "outputs": [],
   "source": [
    "# Fit a language model to the data!\n",
    "# Use 10% validation - not so important in case of language models.\n",
    "# Use default alidation split of Keras.\n",
    "# And try to guess a realistic batch size!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eao03t2U9gBy"
   },
   "source": [
    "## Demo 1: Predict next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-06T17:04:58.584741Z",
     "start_time": "2018-10-06T16:48:43.630Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RM1WVrua9qaY",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "275659e7-1141-414a-c73b-cd25d8c599c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "############\n",
    "\n",
    "def str_to_input(s):\n",
    "    \"\"\"Convert a string to appropriate model input.\n",
    "    \"\"\"\n",
    "    words = [x.lower() for x in s.split()[:max_input_length]]\n",
    "    ids = [br.word_to_id_map[word] for word in words]\n",
    "    ids_array = np.asarray(ids)\n",
    "    length = min(max_input_length, len(ids_array))\n",
    "    result = np.zeros((1, max_input_length))\n",
    "    result[0, :length] = ids_array[:length]\n",
    "    return result, length\n",
    "    \n",
    "\n",
    "while True:\n",
    "    s = input(\"\\nEnter a few starting words of a sentence or <return> to stop: \")\n",
    "    if s == \"\":\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            x, length = str_to_input(s)\n",
    "            predictions = model.predict(x)\n",
    "            probs = predictions[0][length - 1]\n",
    "            most_probable = np.argmax(probs)\n",
    "            print(\"Predicted next word:\", br.id_to_word_map[most_probable])\n",
    "        except KeyError:\n",
    "            print(\"Unknown words -- please try again!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4AkEJxBzPfm"
   },
   "source": [
    "## Demo 2: Similarity of sentences\n",
    "\n",
    "First we define a function that generates the hidden state of the LSTM from an input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "thYtAla6gDbi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_layer = model.get_layer(\"input_1\")\n",
    "lstm_2_layer = model.get_layer(\"lstm_1\")\n",
    "\n",
    "cell_state_fun = K.function([input_layer.input],[lstm_2_layer.output[2]])\n",
    "\n",
    "def get_embedding(x):\n",
    "    \"\"\"Return the final cell state associated with the input.\n",
    "       Returns the last cell state as a vector.\n",
    "    \"\"\"\n",
    "    return cell_state_fun([x])[0].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the vectors for calculating the cosine distance between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "LKUgmCrG7Itw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ae970159-4ca0-48c8-ba74-cb967c085aed"
   },
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "\treturn np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "while True:\n",
    "    s1 = input(\"\\nEnter the first sentence or <return> to quit: \")\n",
    "    if s1 == \"\": break\n",
    "    s2 = input(\"\\nEnter the second sentence: \")\n",
    "    try:\n",
    "        x1, _ = str_to_input(s1)\n",
    "        x2, _ = str_to_input(s2)\n",
    "        e1 = get_embedding(x1)\n",
    "        e2 = get_embedding(x2)\n",
    "        print(\"The cosine similarity between the two sentences is\", cos_sim(e1, e2))\n",
    "    except KeyError:\n",
    "        print(\"Unknown words -- please try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-3W1oxZv4DI"
   },
   "source": [
    "## Demo 3: Mini search engine\n",
    "\n",
    "We use the library [Annoy](https://github.com/spotify/annoy) published by Spotify to create a vector space index of the Brown corpus from the LSTM's cell state. We assign a vector for each sentence, and then store it to be able to run nearest neighbor queries on it. With this we effectively created a **semantic search engine**.\n",
    "\n",
    "(There are multiple solutions for approximate nearest neighbor search a scale which are worth looking into, one of them is [FAISS](https://code.fb.com/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) from Facebook Research.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MWp-rofueeS0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def brown_sent_to_input(ids):\n",
    "  ids_array = np.asarray(ids)\n",
    "  length = min(max_input_length, len(ids_array))\n",
    "  result = np.zeros((1, max_input_length))\n",
    "  result[0, :length] = ids_array[:length]\n",
    "  return result, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wda7o8ntfov1",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sentlist = list(br.sentences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "eaAJ7Pz5i8Cp",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f94ff59a-e327-4d02-e725-66ea3e5682fc"
   },
   "outputs": [],
   "source": [
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o8SO85y_w8sX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "INDEX_COVERAGE_PERCENT = 1.0 #How much of the corpus you want ot index? 1.0 means whole, 0.5 means half.\n",
    "NEAREST_NEIGHBOR_NUM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UfwnfqYff6h8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "930b77b2-3ce1-4b5d-bbd4-03e57b40fdb7"
   },
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm\n",
    "\n",
    "index = AnnoyIndex(512, metric=\"angular\")\n",
    "\n",
    "for i in tqdm(range(int(len(sentlist)*INDEX_COVERAGE_PERCENT))):\n",
    "  inputs,length = brown_sent_to_input(sentlist[i])\n",
    "  vector = get_embedding(inputs)\n",
    "  index.add_item(i,vector)\n",
    "\n",
    "print(\"Building index...\")\n",
    "index.build(100)\n",
    "print(\"Index done, ready to query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ex6AGjVrh6Sb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_brown_index(sentences, indices):\n",
    "  for i in indices:\n",
    "    word_ids_list = sentences[i]\n",
    "    for j in word_ids_list:\n",
    "      print(br.id_to_word_map[j]+\" \", end='')\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "V6y-1aVGi573",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "68d4ef41-2398-4e56-a170-25bae99df5c7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "  query = input(\"\\nEnter the query or <return> to quit: \")\n",
    "  if query == \"\": break\n",
    "  try:\n",
    "    in_ids, length = str_to_input(query)\n",
    "    in_vector = get_embedding(in_ids)\n",
    "    nearest_sentence_indices = index.get_nns_by_vector(in_vector, NEAREST_NEIGHBOR_NUM)\n",
    "    #print(\"nearest indices:\", nearest_sentence_indices)\n",
    "    print_brown_index(sentlist, nearest_sentence_indices)\n",
    "\n",
    "  except KeyError:\n",
    "    print(\"Unknown words -- please try again!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_LM_Task.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
