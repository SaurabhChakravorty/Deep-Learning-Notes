{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Saurabh_Text_retrieval_with_recurrent_neural_methods_handout.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"1LpjNSceR85m"},"source":["# LSTM in practice -- NLP\n","\n","## Language modeling\n","\n","A language model is a probability distribution over the sequence of words, modeling language (production), thus if the set of words is $w$, then for arbitrary $\\mathbf w = \\langle w_1,\\dots, w_n\\rangle$ ($w_i\\in W$) sequence it defines a $P(\\mathbf w)$ probability. \n","\n","Probability with chain rule:\n","\n","$$P(\\mathbf w)= P(w_1)\\cdot P(w_2 \\vert w_1 )\\cdot P(w_3\\vert w_1, w_2)\\cdot\\dots\\cdot P(w_n\\vert w_1,\\dots, w_{n-1})$$\n","\n","so this means, that for the modeling we need only to give the conditional probability of the \"continuation\", the next word, thus for $w$ word and $\\langle w_1,\\dots,w_n\\rangle$ sequence the probability that the next word will be $w$\n","\n","$$P(w ~\\vert ~ w_1,\\dots,w_n)$$\n","\n","There are character based models also, which take the individual characters as units, not the words, and model language as a distribution over sequences of characters (think T9...)"]},{"cell_type":"markdown","metadata":{"id":"moYnnb1-jKT4"},"source":["### Measurement of performance: Perplexity\n","\n","A language model $\\mathcal M$'s perplexity over the word series $\\mathbf w = \\langle w_1,\\dots, w_n\\rangle$ is:\n","\n","$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w) = \\sqrt[n]{\\frac{1}{P_{\\mathcal M}(\\mathbf w)}}$$\n","\n","With the chain rule can be rewritten as:\n","\n","$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w) = {\\sqrt[n]{\\frac{1}{P_{\\mathcal M}(w_1)}\\cdot \\frac{1}{P_{\\mathcal M}(w_2 \\vert w_1 )}\\cdot \\frac{1}{P_{\\mathcal M}(w_3\\vert w_1, w_2)}\\cdot\\dots\\cdot \\frac{1}{P_{\\mathcal M}(w_n\\vert w_1,\\dots, w_{n-1})}}}$$\n","\n","which is exactly the geometric mean of the reciprocals of the conditional probabilities of all words in the corpus.\n","\n","In case of a bigram model this is further simplified to:\n","$$\\mathbf{PP}_{\\mathcal M}(\\mathbf w) = \\sqrt[n]{\\frac{1}{P_{\\mathcal M}(w_1)}\\cdot \\frac{1}{P_{\\mathcal M}(w_2 \\vert w_1 )}\\cdot \\frac{1}{P_{\\mathcal M}(w_3\\vert w_2)}\\cdot\\dots\\cdot \\frac{1}{P_{\\mathcal M}(w_n\\vert w_{n-1})}}$$\n","\n","\n","### But what is it good for?\n","For example:\n","- Predictive text input (\"autocomplete\")\n","- Generating text\n","- Spell checking\n","- Language understanding\n","- And most importantly representation learning - this we will be studiying in detail in a next lecture"]},{"cell_type":"markdown","metadata":{"id":"Fn4iPehcyda2"},"source":["### Generating text with a language model\n","\n","The language model produces a tree with probable continuations of the text:\n","\n","<img src=\"https://4.bp.blogspot.com/-Jjpb7iyB37A/WBZI4ImGQII/AAAAAAAAA9s/ululnUWt2vw9NMKuEr-F9H8tR2LEv36lACLcB/s1600/prefix_probability_tree.png\" width=400 heigth=400>\n","\n","Using this tree we can try different algorithms to search for the best \"continuations\". A full breadth-first search oi usually impossible, due to the high branching factor of the tree.\n","\n","Alternatives:\n","- \"Greedy\": we choose the continuation which has the highest direct probability, This will most probably be suboptimal, since the probability of the full sequence is tha product of the continuations, and if we would have chosen a different path, we might ahve been able to choose later words with hihg probabilities.\n","- Beam-search: we always store a fixed $k$ number of partial sequences, and we always try to expand these, always keeping the most probable $k$ from the possible continuations. \n","\n","Example ($k$=5):\n","\n","<img src=\"http://opennmt.net/OpenNMT/img/beam_search.png\" width=600 heigth=600>\n"," "]},{"cell_type":"markdown","metadata":{"id":"pO8BuLsDWWAE"},"source":["### The \"old way\": N-gram based solutions\n","\n","With _gross_ simplification we assume, that the distribution is only dependent on the prior $n-1$ words (where $n$ is typically $<=4$), thus we assume a Markov chain of the order $n$:\n","\n"," $$P(w ~\\vert ~ w_1,\\dots,w_k) = P(w ~\\vert ~ w_{k- n + 2},\\dots,w_k)$$\n","\n","We simply compute these probabilities in a frequentist style by calculating the $n$-gram statistics of the corpus at hand:\n","\n","$$P(w_2 ~\\vert ~w_1) = \\frac{c(\\langle w_1, w_2 \\rangle)}{c(w_1)}$$\n","\n","$$P(w_{k+1} \\vert~ w_1,\\dots,w_k)_\\mathrm = \\frac{c(\\langle w_1,...,w_k, w_{k+1} \\rangle)}{c(\\langle w_1, \\dots w_k\\rangle)}$$\n","\n","Please note, that in this case we are using \"memorization\", a form of database learning, with minimal compression - \"counting\".\n","\n","But what do we do the given $n$-grams rarely or never occur? We have to employ some __smoothing__ solutions, like: \n","\n","##### Additive smoothing\n","We pretend that we have seen the $n$-grams more times than we have actually did with a fixed $\\delta$ number, in the simplest case with $n=2$:\n","\n","$$P(w_2 ~\\vert ~w_1) = \\frac{c(\\langle w_1, w_2 \\rangle) + \\delta}{\\sum_{w\\in V} [c(\\langle w_1, w\\rangle) + \\delta]}$$\n","\n","Widespread solution for $\\delta$ is $1$.\n","\n","The main problem with this kind of smoothing is that it does not take into account by \"supplementing\" the data the frequency of components of shorter $n$-grams, eg. if neither $\\langle w_1, w_2 \\rangle$  nor $\\langle w_1, w_3 \\rangle$ occurs in the corpus, it assumes the frequency of both bigrams to be $\\delta$, irrespective of the ratio of frequencies of $w_2$ and $w_3$.\n","Most smoothing techniques are trying to accomodate this, eg: simple interpolation:\n","\n","##### Interpolatcion\n","\n","In case of bigrams, we add - with a certain weight - the probabilities coming from the individual frequencies:\n","\n","$$P(w_2 ~\\vert ~w_1)_{\\mathrm{interp}} = \\lambda_1\\frac{c(\\langle w_1, w_2 \\rangle)}{c(w_1)} + (1 - \\lambda_1)\\frac{c(w_1)}{\\sum_{w\\in V}c(w)}$$\n","\n","Recursive solution for arbitrary $k$:\n","\n","$$P(w_{k+1} \\vert~ w_1,\\dots,w_k)_\\mathrm{interp} = \\lambda_k\\frac{c(\\langle w_1,...,w_k, w_{k+1} \\rangle)}{c(\\langle w_1, \\dots w_k\\rangle)} + (1-\\lambda_k)P_\\mathrm{interp}(\\langle w_2,\\dots,w_{k+1}\\rangle)$$\n","\n","$\\lambda_k$ is empirically set by examining the corpus, typically by [Expectation Maximization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm), which - as we have mentioned - iteratively tunes the parameters to maximize the likelihood.\n","\n","\n","Good overview about the smoothing methods: [MacCartney, NLP Lunch Tutorial: Smoothing](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf)\n","\n"," \n","#### General problems\n","\n","- Even the core assumption is not too realistic, since the probabilities are for sure influenced in a way by words further than $n$, but for practical reasons, it has to be limited (sparsity, computation capacity).\n","- On a large enough corpus, the memory footprint of the $n$-gram models is _huge_, eg. for the 1T n-gram corpus of Google ([see here](https://catalog.ldc.upenn.edu/LDC2006T13)) containing 1,024,908,267,229 tokens the $n$-gram counts are as follows:\n","    - unigram: 13,588,391, \n","    - bigram: 314,843,401, \n","    - trigram: 977,069,902, \n","    - fourgrams: 1,313,818,354 \n","    - fivegram: 1,176,470,663."]},{"cell_type":"markdown","metadata":{"id":"DgLdc8JQrH-4"},"source":["## Language modeling with LSTMs\n","\n","One way to circumvent the Markov assumption is to use RNN-s, which are capable of modeling the long-ter dependencies inside the sequence of words. The text is thus considered to be a time-series, and thus an appropriate architecture can be used (as we have already seen):\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1y8QYr9ftTvXAxgzS-ldnGlijVpmK2l21\" width=600 heigth=600>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YHVR_jd62LHI"},"source":["Notable features:\n","\n","- Input is a \"one-hot\" encoded vector, wchic we on the spot transform into an \"embedding vector\"\n","- For each output step, we get a probability distribution over the whole vocabulary with softmax\n","- This above is a simple RNN, but LSTMs can be used without any problems"]},{"cell_type":"markdown","metadata":{"id":"wqQY-P9XP6Ro"},"source":["### Teaching\n","\n","_In theory_ an RNN could be trained with full GD on the corpus in one go:\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1XsBoRp7cNay3svFLRDv2JEDyC7m7CUdC\" width=600 heigth=600>\n"]},{"cell_type":"markdown","metadata":{"id":"lXIBEAhM8dzU"},"source":["- The loss is generally the well-kown crossentropy, which is in this case (since the input is a one-hot vector):\n","  $$J^{(i)}(\\Theta) = -\\log (\\hat y[x^{(i+1)}])$$\n","  the negative logarithm of the probability assigned by the network to the right word / next word.\n","\n","- For the sake of more frequent updates, and since BPTT for long sequences is very expensive, teaching is done in smaller units with not necessarily the same length.\n","- The unit is typically one or more sentece, or if the length allows, and we have enough material, a paragraph can be a good candidate.\n","- Initial state in case of the time-series units: if the boundaries are inside a unit of text, it is important to _transfer the hidden state_ from the previous unit, in other cases initialization can be done by some fixed value.\n","- (Somewhat misleading) terminology: the length of the \"time\" unit is _time step_, but sometimes certain implementations call it _minibatch_, though that would generally mean the number of units processed in one go for the sake of computaitonal efficiency.\n"]},{"cell_type":"markdown","metadata":{"id":"R8IYutdnW_SG"},"source":["### LSTM as layers\n","\n","+ An LSTM - how ever strange that may sound - can be considered to be a complete layer. The most important parameter of it is the \"number of (memory) units\", which is the length of the hidden state vector, thus, the memory capacity. **Warning: this does not have any relationship to input size, thus can be considered a freely chosen parameter.**\n","+ It is quite widespread to use multiple LSTM layers (\"stacked LSTMs\") -- as in the case of ConvNets the hope is, that the layers learn a hierarchy of abstract representations:\n","\n","<img src=\"http://wenchenli.github.io/assets/img/GNMT_residual.png\" width=60%>\n","\n","(on the right side a network is shown with skip/residual connections!)\n","\n","In this case it makes sense, that we do not only get on top of the LSTM a final prediction $h$ (or even prediction + inner state vector $c$) for a sequence, but **we ask it to output the whole sequence of predictions**, so that the next layer can also operate on full sequences. Please bear this in mind during implementation, since this can be a common source of failure.  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"3R04Si0M8KzK"},"source":["## An LSTM language model in Keras\n","\n","For this task the inspiration comes from the famous [reference work of Andrej Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n","\n","Note, that in this case we will not use regularization, since we are willing to overfit - for the sake of play with the text. This is now an \"overfitting competition\", so _not_ a generally good practice!\n","\n","## Reader"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-10-06T16:48:43.620252Z","start_time":"2018-10-06T16:48:43.607851Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"egfH6gkf8WQg","executionInfo":{"elapsed":3471,"status":"ok","timestamp":1606213170248,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"},"user_tz":-60},"outputId":"fa2b824d-a815-4d57-de9f-c03d37b5304e"},"source":["import numpy as np\n","import tensorflow as tf\n","import nltk\n","\n","from numpy.random import seed\n","seed(1212)\n","\n","tf.random.set_seed(1234)\n","\n","nltk.download(\"brown\")\n","\n","from nltk.corpus import brown\n","\n","# This can be an important parameter, so be aware of it...\n","max_seq_length = 15\n","max_num_of_sents = 57200\n","# max_num_of_sents = 50 # How many sentences should we read from the corpus (max=57200)\n","\n","def generate_brown_word_to_id_map():\n","    \"\"\"Return a dictionary mapping downcased Brown-words to their ids.\n","    Numbering starts from 1 since we use 0 for masking (!!!).\n","    \"\"\"\n","    words = set()\n","    for word in brown.words():\n","        words.add(word.lower())\n","    return {word: idx + 1 for idx, word in enumerate(sorted(words))}\n","\n","\n","class BrownReader:\n","    \"\"\"A reader class for the Brown corpus.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.word_to_id_map = generate_brown_word_to_id_map()\n","        self.id_to_word_map = {idx: word for word, idx in self.word_to_id_map.items()}\n","\n","    def n_words(self):\n","        return len(self.word_to_id_map)\n","\n","    def sentence_to_ids(self, sentence):\n","        \"\"\"Return the word ids of a sentence.\n","        \"\"\"\n","        return [self.word_to_id_map[word.lower()] for word in sentence]\n","        \n","    def sentences(self):\n","        \"\"\"Generator yielding features from the Brown corpus.\n","        \"\"\"\n","        return (self.sentence_to_ids(sentence) for sentence in brown.sents())\n","\n","    def sentence_matrixes(self):\n","        x = np.zeros((max_num_of_sents, max_seq_length-1))\n","        y = np.zeros((max_num_of_sents, max_seq_length-1))\n","        sents = self.sentences()\n","        for idx, sent in enumerate(sents):\n","            if idx == max_num_of_sents:\n","                break\n","            np_array = np.asarray(sent)\n","            length  = min(max_seq_length, len(np_array))\n","            x[idx, :length - 1] = np_array[:length - 1]\n","            y[idx, :length - 1] = np_array[1:length]\n","        return x, y\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pvjdF2_p9_YV"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"SavNwc5m9_Ya"},"source":["### Parameters"]},{"cell_type":"code","metadata":{"id":"CXHNzCXj9_Ye"},"source":["br = BrownReader()\n","n_words = br.n_words()\n","\n","max_input_length = max_seq_length - 1 # since our x/y input does not contain the last/first element of the sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JkhDDIwl9_ZC"},"source":["data_x, data_y = br.sentence_matrixes()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJdzjP6R9_ZK"},"source":["data_y = np.expand_dims(data_y, -1) # It seems that Keras needs this for the \"one-cold\" and softmax dims to match"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0toQ5Ugl5k5n"},"source":["# Tasks\n","\n","See below"]},{"cell_type":"code","metadata":{"id":"grrGNfJT5k5n"},"source":["# Network parameters\n","lstm_size = 512\n","embedding_size = 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GWPOCc6q9_Yn"},"source":["### Network"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FXth0CDm9_Yq","executionInfo":{"elapsed":14386,"status":"ok","timestamp":1606213181232,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"},"user_tz":-60},"outputId":"7796d8a8-bfc9-4304-ef50-22d350e293d1"},"source":["# Import:\n","# Import the appropriate layers from tf.keras!\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, Embedding, Flatten,Input\n","from keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.models import Model\n","from keras.utils import multi_gpu_model\n","from tensorflow.python.client import device_lib\n","import tensorflow as tf\n","# Think about it, that one layer should map (\"embed\") tha input into a dense vector!\n","# Don't forget the main model class - according to functional or sequential API\n","# And eventually the optimizer and backend\n","# Later one for resetting the graph - good practice!\n","# And think about it what loss function you will use. The output is a classification\n","# (categorical) task, it is sparse, so...\n","\n","################################################################################\n","\n","# Please reset the graph here!\n","# We use FUNCTIONAL API!\n","from keras import backend as K\n","K.clear_session()\n","\n","################################################################################\n","# Model\n","# Build the model!\n","model = Model()\n","# Start with the input layer, it receives a vector of the length of the maximal text span (sentence)\n","# And: vector or not, shape is a tuple...\n","# After this, use the \"mapping\" layer.\n","ip = Input(shape = (max_input_length,))\n","\n","################################################################################\n","# WARNING:\n","# 1. width = number of words +1\n","# 2. it's size is defined in a parameter, somewhere above\n","# 3. length of input: max input size -1\n","# 4. Zero values are to be masked in it!!! \n","#    the constructor has a named argument which has to be given with the True value!!!\n","################################################################################\n","\n","# TASK: Can you please verbally elaborate to the instructor, why the points above are true?\n","# The above points are true because:\n","# 1. Width is the vocab size which is always embedded in the values between 0-vocab_size, to add extra layer of dimension we add +1 in it\n","# 2. Here, in this case our train and test data do not contain the first and last elements of sentences\n","# 3. Zero values are padded in extra dimensional space to make length of embeddings' equal in maximum sentence sequence length\n","\n","\n","input_1 = Embedding(n_words + 1,embedding_size, input_length=max_input_length, mask_zero=True)(ip)\n","\n","################################################################################\n","# In the next TWO layers there should be LSTM-s\n","# For them being able to be stacked, they have to give back not only the predictions at sequence end\n","# Somewhere there has to be a nice parameter for this... ;-)\n","lstm_1 = LSTM(lstm_size, activation='relu',return_sequences = True)(input_1)\n","\n","################################################################################\n","# Important: for certain purposes (hint: search engine...) it's very useful to have the last\n","# hidden state and cell state also included in the results, so for the 2nd LSTM please turn \n","# on the return_state option. Be aware that thereby the 2nd LSTM cell will return three\n","# tensors (the series of outputs, last hidden state, last cell state), and you will need only \n","# the first of these as input for your next layer (as always, ask when in doubt) \n","\n","output, cell_h, cell_s = LSTM(lstm_size, activation='relu',return_sequences = True, return_state=True)(lstm_1)\n","# print(output.shape)\n","# print(cell_h.shape)\n","################################################################################\n","\n","# Finally project the output with a fully connected layer and a softmax.\n","# What is it's width? (Help: If you have varbally elaborated well above, you already know.)\n","\n","output = Dense(n_words + 1, activation = 'softmax')(output)\n","\n","################################################################################\n","# Finally, create a model instance!\n","model = Model(ip,output)\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","Model: \"functional_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 14)]              0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 14, 100)           4981600   \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 14, 512)           1255424   \n","_________________________________________________________________\n","lstm_1 (LSTM)                [(None, 14, 512), (None,  2099200   \n","_________________________________________________________________\n","dense (Dense)                (None, 14, 49816)         25555608  \n","=================================================================\n","Total params: 33,891,832\n","Trainable params: 33,891,832\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FKXuebno9_Y1"},"source":["### Error, optimizer, compilation"]},{"cell_type":"code","metadata":{"id":"4Mob9-6s9_Y3"},"source":["# Loss \n","\n","loss = 'sparse_categorical_crossentropy' # Our output is One-hot encoded. What do we use?\n","\n","# Optimizer\n","optimizer ='adam' # According to taste...\n"," \n","# Early stop\n","#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n","# Compilation\n","#############\n","\n","model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WLLqU3c_9_Y8"},"source":["### Training\n","\n","We generate the trainig data."]},{"cell_type":"code","metadata":{"id":"k4kNDFsE5k5o"},"source":["data_y = np.expand_dims(data_y, -1) # It seems that Keras needs this for the \"one-cold\" and softmax dims to match"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NMPFHd-M9_ZX"},"source":["And train!"]},{"cell_type":"markdown","metadata":{"id":"q-PEK0bgwh7u"},"source":["Training done in less **epochs of 50** due to resource unavailability and keeping in mind the overfitting as we aren't suppose to use **Regularization**\n","\n","We will see how...."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJboxaY_9_Zd","executionInfo":{"elapsed":5610355,"status":"ok","timestamp":1606218793321,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"},"user_tz":-60},"outputId":"c467d30b-690c-4ca9-fbd5-437b39f42cc4"},"source":["# Fit a language model to the data!\n","# Use 10% validation - not so important in case of language models.\n","# Use default validation split of Keras.\n","# And try to guess a realistic batch size!\n","with tf.device('/gpu:0' and '/gpu:1' and '/gpu:2'):\n","      model.fit(data_x, data_y, epochs=50, validation_split=0.10, batch_size = 128, use_multiprocessing=True, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","403/403 [==============================] - 112s 279ms/step - loss: 6.1342 - accuracy: 0.0775 - val_loss: 4.8088 - val_accuracy: 0.1023\n","Epoch 2/50\n","403/403 [==============================] - 112s 278ms/step - loss: 5.4131 - accuracy: 0.1115 - val_loss: 4.7125 - val_accuracy: 0.1144\n","Epoch 3/50\n","403/403 [==============================] - 113s 279ms/step - loss: 5.1789 - accuracy: 0.1221 - val_loss: 4.7108 - val_accuracy: 0.1281\n","Epoch 4/50\n","403/403 [==============================] - 112s 279ms/step - loss: 4.9976 - accuracy: 0.1340 - val_loss: 4.6306 - val_accuracy: 0.1392\n","Epoch 5/50\n","403/403 [==============================] - 113s 280ms/step - loss: 4.8266 - accuracy: 0.1456 - val_loss: 4.5948 - val_accuracy: 0.1478\n","Epoch 6/50\n","403/403 [==============================] - 113s 279ms/step - loss: 4.6761 - accuracy: 0.1530 - val_loss: 4.6333 - val_accuracy: 0.1527\n","Epoch 7/50\n","403/403 [==============================] - 113s 279ms/step - loss: 4.5348 - accuracy: 0.1592 - val_loss: 4.6331 - val_accuracy: 0.1554\n","Epoch 8/50\n","403/403 [==============================] - 112s 279ms/step - loss: 4.3953 - accuracy: 0.1645 - val_loss: 4.6403 - val_accuracy: 0.1568\n","Epoch 9/50\n","403/403 [==============================] - 112s 279ms/step - loss: 4.2659 - accuracy: 0.1702 - val_loss: 4.6536 - val_accuracy: 0.1569\n","Epoch 10/50\n","403/403 [==============================] - 112s 278ms/step - loss: 4.1351 - accuracy: 0.1765 - val_loss: 4.7254 - val_accuracy: 0.1554\n","Epoch 11/50\n","403/403 [==============================] - 112s 279ms/step - loss: 4.0033 - accuracy: 0.1861 - val_loss: 4.7857 - val_accuracy: 0.1553\n","Epoch 12/50\n","403/403 [==============================] - 113s 280ms/step - loss: 3.8936 - accuracy: 0.1959 - val_loss: 4.8315 - val_accuracy: 0.1554\n","Epoch 13/50\n","403/403 [==============================] - 113s 279ms/step - loss: 3.7857 - accuracy: 0.2062 - val_loss: 4.8501 - val_accuracy: 0.1554\n","Epoch 14/50\n","403/403 [==============================] - 112s 279ms/step - loss: 3.6698 - accuracy: 0.2190 - val_loss: 4.8353 - val_accuracy: 0.1555\n","Epoch 15/50\n","403/403 [==============================] - 112s 279ms/step - loss: 3.5619 - accuracy: 0.2313 - val_loss: 4.8037 - val_accuracy: 0.1546\n","Epoch 16/50\n","403/403 [==============================] - 112s 279ms/step - loss: 3.4622 - accuracy: 0.2434 - val_loss: 4.8387 - val_accuracy: 0.1540\n","Epoch 17/50\n","403/403 [==============================] - 113s 280ms/step - loss: 3.3615 - accuracy: 0.2558 - val_loss: 4.9102 - val_accuracy: 0.1529\n","Epoch 18/50\n","403/403 [==============================] - 112s 279ms/step - loss: 3.2738 - accuracy: 0.2671 - val_loss: 4.9742 - val_accuracy: 0.1506\n","Epoch 19/50\n","403/403 [==============================] - 112s 277ms/step - loss: 3.1963 - accuracy: 0.2777 - val_loss: 5.0605 - val_accuracy: 0.1472\n","Epoch 20/50\n","403/403 [==============================] - 111s 277ms/step - loss: 3.1212 - accuracy: 0.2889 - val_loss: 5.1497 - val_accuracy: 0.1436\n","Epoch 21/50\n","403/403 [==============================] - 112s 277ms/step - loss: 3.0518 - accuracy: 0.2989 - val_loss: 5.1775 - val_accuracy: 0.1423\n","Epoch 22/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.9800 - accuracy: 0.3103 - val_loss: 5.2336 - val_accuracy: 0.1406\n","Epoch 23/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.9083 - accuracy: 0.3212 - val_loss: 5.2762 - val_accuracy: 0.1417\n","Epoch 24/50\n","403/403 [==============================] - 111s 277ms/step - loss: 2.8547 - accuracy: 0.3286 - val_loss: 5.2374 - val_accuracy: 0.1418\n","Epoch 25/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.8095 - accuracy: 0.3361 - val_loss: 5.2070 - val_accuracy: 0.1443\n","Epoch 26/50\n","403/403 [==============================] - 111s 276ms/step - loss: 2.7474 - accuracy: 0.3458 - val_loss: 5.2052 - val_accuracy: 0.1449\n","Epoch 27/50\n","403/403 [==============================] - 111s 277ms/step - loss: 2.6961 - accuracy: 0.3540 - val_loss: 5.1701 - val_accuracy: 0.1455\n","Epoch 28/50\n","403/403 [==============================] - 111s 276ms/step - loss: 2.6542 - accuracy: 0.3607 - val_loss: 5.1859 - val_accuracy: 0.1446\n","Epoch 29/50\n","403/403 [==============================] - 111s 276ms/step - loss: 2.6055 - accuracy: 0.3693 - val_loss: 5.2519 - val_accuracy: 0.1429\n","Epoch 30/50\n","403/403 [==============================] - 111s 276ms/step - loss: 2.5574 - accuracy: 0.3777 - val_loss: 5.3056 - val_accuracy: 0.1434\n","Epoch 31/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.5007 - accuracy: 0.3878 - val_loss: 5.3557 - val_accuracy: 0.1411\n","Epoch 32/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.4690 - accuracy: 0.3924 - val_loss: 5.3538 - val_accuracy: 0.1397\n","Epoch 33/50\n","403/403 [==============================] - 112s 278ms/step - loss: 2.4530 - accuracy: 0.3953 - val_loss: 5.3379 - val_accuracy: 0.1393\n","Epoch 34/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.4358 - accuracy: 0.3974 - val_loss: 5.3710 - val_accuracy: 0.1378\n","Epoch 35/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.4003 - accuracy: 0.4044 - val_loss: 5.4588 - val_accuracy: 0.1346\n","Epoch 36/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.3567 - accuracy: 0.4131 - val_loss: 5.4896 - val_accuracy: 0.1336\n","Epoch 37/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.3127 - accuracy: 0.4208 - val_loss: 5.5524 - val_accuracy: 0.1319\n","Epoch 38/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.2745 - accuracy: 0.4282 - val_loss: 5.6162 - val_accuracy: 0.1298\n","Epoch 39/50\n","403/403 [==============================] - 111s 277ms/step - loss: 2.2394 - accuracy: 0.4344 - val_loss: 5.6766 - val_accuracy: 0.1267\n","Epoch 40/50\n","403/403 [==============================] - 111s 276ms/step - loss: 2.2086 - accuracy: 0.4397 - val_loss: 5.7313 - val_accuracy: 0.1252\n","Epoch 41/50\n","403/403 [==============================] - 111s 276ms/step - loss: 2.1799 - accuracy: 0.4451 - val_loss: 5.7469 - val_accuracy: 0.1257\n","Epoch 42/50\n","403/403 [==============================] - 111s 276ms/step - loss: 2.1392 - accuracy: 0.4531 - val_loss: 5.7647 - val_accuracy: 0.1281\n","Epoch 43/50\n","403/403 [==============================] - 111s 276ms/step - loss: 2.1068 - accuracy: 0.4596 - val_loss: 5.7439 - val_accuracy: 0.1287\n","Epoch 44/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.0745 - accuracy: 0.4657 - val_loss: 5.7333 - val_accuracy: 0.1289\n","Epoch 45/50\n","403/403 [==============================] - 111s 276ms/step - loss: 2.0460 - accuracy: 0.4707 - val_loss: 5.7327 - val_accuracy: 0.1303\n","Epoch 46/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.0265 - accuracy: 0.4744 - val_loss: 5.7675 - val_accuracy: 0.1304\n","Epoch 47/50\n","403/403 [==============================] - 112s 277ms/step - loss: 2.0018 - accuracy: 0.4786 - val_loss: 5.7963 - val_accuracy: 0.1305\n","Epoch 48/50\n","403/403 [==============================] - 111s 276ms/step - loss: 1.9759 - accuracy: 0.4842 - val_loss: 5.8300 - val_accuracy: 0.1284\n","Epoch 49/50\n","403/403 [==============================] - 111s 275ms/step - loss: 1.9499 - accuracy: 0.4895 - val_loss: 5.8456 - val_accuracy: 0.1280\n","Epoch 50/50\n","403/403 [==============================] - 112s 277ms/step - loss: 1.9217 - accuracy: 0.4956 - val_loss: 5.8849 - val_accuracy: 0.1278\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kMJ4WC-EHAfH"},"source":["As observed, the model is prone to overfitting so we rstrict in in certain epochs. The aim is to not learn the data totally rather generalizing it."]},{"cell_type":"markdown","metadata":{"id":"Eao03t2U9gBy"},"source":["## Demo 1: Predict next word"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-10-06T17:04:58.584741Z","start_time":"2018-10-06T16:48:43.630Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"RM1WVrua9qaY","scrolled":true,"executionInfo":{"elapsed":5648759,"status":"ok","timestamp":1606218853826,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"},"user_tz":-60},"outputId":"ce6fcb3e-aee1-4b1f-9647-8ca193568bb7"},"source":["# Prediction\n","############\n","\n","def str_to_input(s):\n","    \"\"\"Convert a string to appropriate model input.\n","    \"\"\"\n","    words = [x.lower() for x in s.split()[:max_input_length]]\n","    ids = [br.word_to_id_map[word] for word in words]\n","    ids_array = np.asarray(ids)\n","    length = min(max_input_length, len(ids_array))\n","    result = np.zeros((1, max_input_length))\n","    result[0, :length] = ids_array[:length]\n","    return result, length\n","    \n","\n","while True:\n","    s = input(\"\\nEnter a few starting words of a sentence or <return> to stop: \")\n","    if s == \"\":\n","        break\n","    else:\n","        try:\n","            x, length = str_to_input(s)\n","            predictions = model.predict(x)\n","            probs = predictions[0][length - 1]\n","            most_probable = np.argmax(probs)\n","            print(\"Predicted next word:\", br.id_to_word_map[most_probable])\n","        except KeyError:\n","            print(\"Unknown words -- please try again!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Enter a few starting words of a sentence or <return> to stop: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election  produced no evidence that any irregularities took place. The jury further said in term-end presentments that the City Executive Committee\n","Predicted next word: produced\n","\n","Enter a few starting words of a sentence or <return> to stop: \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L4AkEJxBzPfm"},"source":["## Demo 2: Similarity of sentences\n","\n","First we define a function that generates the hidden state of the LSTM from an input sentence:"]},{"cell_type":"code","metadata":{"id":"thYtAla6gDbi"},"source":["input_layer = model.get_layer(\"input_1\")\n","lstm_2_layer = model.get_layer(\"lstm_1\")\n","\n","cell_state_fun = K.function([input_layer.input],[lstm_2_layer.output[2]])\n","\n","def get_embedding(x):\n","    \"\"\"Return the final cell state associated with the input.\n","       Returns the last cell state as a vector.\n","    \"\"\"\n","    return cell_state_fun([x])[0].flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHh365It5k5o"},"source":["Then we use the vectors for calculating the cosine distance between sentences."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"LKUgmCrG7Itw","jupyter":{"outputs_hidden":true},"executionInfo":{"elapsed":5658402,"status":"ok","timestamp":1606218865578,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"},"user_tz":-60},"outputId":"7888182d-b9bd-40fc-a078-a8c98dd21dd0"},"source":["def cos_sim(a, b):\n","\treturn np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n","\n","while True:\n","    s1 = input(\"\\nEnter the first sentence or <return> to quit: \")\n","    if s1 == \"\": break\n","    s2 = input(\"\\nEnter the second sentence: \")\n","    try:\n","        x1, _ = str_to_input(s1)\n","        x2, _ = str_to_input(s2)\n","        e1 = get_embedding(x1)\n","        e2 = get_embedding(x2)\n","        print(\"The cosine similarity between the two sentences is\", cos_sim(e1, e2))\n","    except KeyError:\n","        print(\"Unknown words -- please try again!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Enter the first sentence or <return> to quit: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election  produced no evidence that any irregularities took place. The jury further said in term-end presentments that the City Executive Committee.\n","\n","Enter the second sentence: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election  produced no evidence that any irregularities took place. The jury further said in term-end presentments that the City Executive Committee.\n","The cosine similarity between the two sentences is 1.0\n","\n","Enter the first sentence or <return> to quit: \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1-3W1oxZv4DI"},"source":["## Demo 3: Mini search engine\n","\n","We use the library [Annoy](https://github.com/spotify/annoy) published by Spotify to create a vector space index of the Brown corpus from the LSTM's cell state. We assign a vector for each sentence, and then store it to be able to run nearest neighbor queries on it. With this we effectively created a **semantic search engine**.\n","\n","(There are multiple solutions for approximate nearest neighbor search a scale which are worth looking into, one of them is [FAISS](https://code.fb.com/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) from Facebook Research.)"]},{"cell_type":"code","metadata":{"id":"MWp-rofueeS0"},"source":["def brown_sent_to_input(ids):\n","  ids_array = np.asarray(ids)\n","  length = min(max_input_length, len(ids_array))\n","  result = np.zeros((1, max_input_length))\n","  result[0, :length] = ids_array[:length]\n","  return result, length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wda7o8ntfov1"},"source":["sentlist = list(br.sentences())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaAJ7Pz5i8Cp","executionInfo":{"elapsed":5668772,"status":"ok","timestamp":1606218877673,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"},"user_tz":-60},"outputId":"2285ad48-285b-402f-dd2b-506e0a9bffe3"},"source":["!pip install annoy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting annoy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n","\u001b[K     |████████████████████████████████| 655kB 13.0MB/s eta 0:00:01\n","\u001b[?25hBuilding wheels for collected packages: annoy\n","  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for annoy: filename=annoy-1.17.0-cp36-cp36m-linux_x86_64.whl size=393350 sha256=7b7e5dee13588909ed48924fb4cf504d9bb3b314c4bb4a64d82cddcde1632d09\n","  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n","Successfully built annoy\n","Installing collected packages: annoy\n","Successfully installed annoy-1.17.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o8SO85y_w8sX"},"source":["#How much of the corpus you want ot index? 1.0 means whole, 0.5 means half.\n","INDEX_COVERAGE_PERCENT = 1.0\n","NEAREST_NEIGHBOR_NUM = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UfwnfqYff6h8","executionInfo":{"elapsed":8116521,"status":"ok","timestamp":1606221326765,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"},"user_tz":-60},"outputId":"6b356290-1e00-4213-a797-1681ed291305"},"source":["from annoy import AnnoyIndex\n","from tqdm import tqdm\n","\n","index = AnnoyIndex(512, metric=\"angular\")\n","\n","for i in tqdm(range(int(len(sentlist)*INDEX_COVERAGE_PERCENT))):\n","  inputs,length = brown_sent_to_input(sentlist[i])\n","  vector = get_embedding(inputs)\n","  index.add_item(i,vector)\n","\n","print(\"Building index...\")\n","index.build(100)\n","print(\"Index done, ready to query!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 57340/57340 [40:34<00:00, 23.56it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Building index...\n","Index done, ready to query!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ex6AGjVrh6Sb"},"source":["def print_brown_index(sentences, indices):\n","  for i in indices:\n","    word_ids_list = sentences[i]\n","    for j in word_ids_list:\n","      print(br.id_to_word_map[j]+\" \", end='')\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6y-1aVGi573","scrolled":true,"executionInfo":{"elapsed":8176790,"status":"ok","timestamp":1606221388351,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"},"user_tz":-60},"outputId":"0a17bebb-e6a5-4d56-f1d9-de913e9d8045"},"source":["while True:\n","  query = input(\"\\nEnter the query or <return> to quit: \")\n","  if query == \"\": break\n","  try:\n","    in_ids, length = str_to_input(query)\n","    in_vector = get_embedding(in_ids)\n","    nearest_sentence_indices = index.get_nns_by_vector(in_vector, NEAREST_NEIGHBOR_NUM)\n","    #print(\"nearest indices:\", nearest_sentence_indices)\n","    print_brown_index(sentlist, nearest_sentence_indices)\n","\n","  except KeyError:\n","    print(\"Unknown words -- please try again!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Enter the query or <return> to quit: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election  produced no evidence that any irregularities took place. The jury further said in term-end presentments that the City Executive Committee\n","the fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place . \n","thus , the combined efficiency of the elements replaced by the two fiber plates ( with a combined efficiency of 0.25 ) is 0.043 or about six times less than that of the two fiber plates . \n","the effect of device and quantum noise , associated with such low input levels , will be described . \n","the local community maintains responsibility for the financial support of its own library program , facilities , and services , but wider resources and additional services become available through membership in a system . \n","for less than a dozen miles from the unplowed land of the dead man lived another settler who had ignored the warnings that his existence might be foreclosed on -- a blatant and defiant rustler named fred powell . \n","\n","Enter the query or <return> to quit: \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"okBJ7ECuQUJ0"},"source":[""],"execution_count":null,"outputs":[]}]}