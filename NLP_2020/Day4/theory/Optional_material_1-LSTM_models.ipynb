{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, but what is an RNN?\n",
    "\n",
    "We would like to define a neural network architecture, that in case of an input data point  $x_{t}$ can take into account the effect of prior datapoints without providing those as explicit inputs (as in the feedforward case), but as **(hidden) state**, using the activations of the NN itslef.\n",
    "\n",
    "Till this point we only used the activation for backpropagation, but now we'll use it also for something else.\n",
    "\n",
    "**Basic idea:**\n",
    "\n",
    "<a href=\"https://cdn-images-1.medium.com/max/1400/1*lQ4izz9ZbhKYD8NClZpsmQ.png\"><img src=\"https://drive.google.com/uc?export=view&id=197zCScRaBJypUb0giZzBh_0plHAOzlYJ\"></a>\n",
    "\n",
    "As a naive solution we could store the activations at $x_{t_-1}$ for all neurons and use a common $\\lambda$ \"dampening factor\" before adding them to the activations arising at $x_{t}$\n",
    "\n",
    "$$ f(x_t) = \\sigma(w x_t +\\lambda a_{t-1} + b)$$ \n",
    "\n",
    "This is not a complete recurrent network, but something along that direction (it's worth trying wether it works :-)\n",
    "\n",
    "But if we have a coplete neural architecture with weight matrices, why shouldn't we do this in a more clever way?\n",
    "\n",
    "### Elman network\n",
    "\n",
    "The first real recurrent network:\n",
    "\n",
    "[Elman 1990](https://crl.ucsd.edu/~elman/Papers/fsit.pdf): \n",
    "Jeffrey L. Elman: Finding structure in time, Cognitive Science 14, p179-p211\n",
    "<a href=\"https://cdn-images-1.medium.com/max/1400/1*E6OMkLY8vbPdJ7b5R27FQA.jpeg\"><img src=\"https://drive.google.com/uc?export=view&id=17s60w9qBrwdrazopSAIPHjQ6ttiBgtTn\"></a>\n",
    "\n",
    "**The general notation for recurrent networks:**\n",
    "\n",
    "**$$h_t=\\sigma(Wx_t+Uh_{t-1})$$**\n",
    "\n",
    "\"Legend\":\n",
    "\n",
    "|Symbol| Meaning|\n",
    "|:---|:---|\n",
    "|$h_t$| \"hidden state\" at time $t$ |\n",
    "|$\\sigma$| activation function (typically sigmoid or tanh)|\n",
    "|$x_t$| input at time $t$ |\n",
    "|$W$| input to hidden weigth matrix|\n",
    "|$U$| hidden to hidden weigth matrix|\n",
    "|$h_{t-1}$| \"hidden state\" at time $t-1$ |\n",
    "\n",
    "\n",
    "**In this model we learn two weigth matrices $W$ and $U$.**\n",
    "\n",
    "Summary  [here](https://medium.com/lingvo-masino/introduction-to-recurrent-neural-network-d77a3fe2c56c).\n",
    "\n",
    "## Long short-term memory network (LSTM)\n",
    "Get ready to be Schmidhubered! :-)\n",
    "\n",
    "<a href=\"https://www.xing.com/img/custom/content/klartext/asset_images/images/000/157/804/x137/image.png?1453288186\"><img src=\"https://drive.google.com/uc?export=view&id=1Ea5cmpxJnHZmZTPNO2DwVr7rfxMAw_wi\"></a>\n",
    "\n",
    "Or more famously [Hochreiter and Schmidhuber 1997](http://www.bioinf.jku.at/publications/older/2604.pdf), in which the heroes are strongly inspired by human short term memory, and would like to create a network which can store signals for arbitrary, but learned length of time, enabling it to effectively infinitely \"memorize\" them.\n",
    "\n",
    "(The joke term \"to be Schmidhubered\" comes from the fact that Prof. Schmidhuber has a long time feud with the Hinton group, and he tries to diminish the importance of their findings, claiming that others - amongst them he-  invented key methods before the \"deep learning conspiracy\". Disturbingly he is sometimes even right. see [this](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html) :-)\n",
    "\n",
    "## Architecture\n",
    "\n",
    "**Gold standard explanation: [Colah's Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)** The personal blog of the researcher Chris Olah.\n",
    "\n",
    "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\"><img src=\"https://drive.google.com/uc?export=view&id=1UOtmNeaimbcn3H7CBSeIUfOcjRaubXgY\" heigth=300 width=600></a>\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "Cell state \"travels\" through the process.\n",
    "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\"><img src=\"https://drive.google.com/uc?export=view&id=1IQEqkYz9ThTT1p_9EnO1NJ8xVLQwRsDv\"></a>\n",
    "\n",
    "\"Gates\" are to be understood as combinations of non-linearities (sigmoid or tanh) and pointwise operations (addition or multiplication).\n",
    "\n",
    "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png\"><img src=\"https://drive.google.com/uc?export=view&id=1tUFbRRllulugrK5OBmPGBVidOnqUIMWF\"></a>\n",
    "\n",
    "**Some help: Let us imagine if a vector gets multiplied pointwise with something between 0 and 1, this is equivalent to \"deleting\" (0) or \"leaving intact\" (1) some parts of the data.**\n",
    "\n",
    "### 1. step: What would we like to \"forget\"?\n",
    "\n",
    "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\"><img src=\"https://drive.google.com/uc?export=view&id=1J1_LbATvWLASY4OJsJ95txk9SCWjeYHR\"></a>\n",
    "\n",
    "### 2. step: What and where would we like to store?\n",
    "\n",
    "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\"><img src=\"https://drive.google.com/uc?export=view&id=1ZpIjdziGKlIacF4M7GpO9GRvxVPxGK7j\"></a>\n",
    "\n",
    "### 3. step: Update the cell state!\n",
    "\n",
    "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\"><img src=\"https://drive.google.com/uc?export=view&id=1jjM4MSIqgqNIz69K4jQUZtVjzTskezF_\"></a>\n",
    "\n",
    "### 4. step: Choose and produce the output!\n",
    "\n",
    "<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\"><img src=\"https://drive.google.com/uc?export=view&id=1_gjSEe6CW1yc_QTCPhGEqM5Bst3D0SId\"></a>\n",
    "\n",
    "#### Alternative explanation [here](http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/)\n",
    "\n",
    "This may be worth reading through, since it shows in detail each vector's dimensionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
