{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation:\n",
    "\n",
    "\"- We would like to have a chatbot!\"\n",
    "\n",
    "\"- Sure! Do you have enough data?\"\n",
    "\n",
    "\"- Absolutely, we are overwhelmed!'\n",
    "    \n",
    "\t(Turns out to be 150 emails...) \n",
    "    \n",
    "\"- OOOK, wel, then, let's see what we can do?\"\n",
    "\n",
    "**Options:**\n",
    "\n",
    "1. Try to learn a whole language with deep meaning from 150 emails?\n",
    "2. Give it up and go for a walk?\n",
    "3. ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sidenote:\n",
    "\n",
    "It is in itself a more nuanced problem, if we have enough data or not, see:\n",
    "\n",
    "[“Do I have enough data for Machine Learning?” — Um… maybe…?](https://medium.com/@haomiao/do-i-have-enough-data-for-machine-learning-um-maybe-d45f41234d2d)\n",
    "\n",
    "The answer is: **\"It depends.\"**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And it does **not** just depend on dimensionality.\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/4470/1*Ox1nUWMV4TUAdb5kOPGYjg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1aKB6v3pwxlBnSSNNUDYZ_w-dQFStwE0P\" width=55%></a>\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/4449/1*3BTvgPA-cYQ1y1mTUStcVg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1E_56MoXBZkAklOUJSE7WHTnH6QVvup-0\" width=55%></a>\n",
    "\n",
    "You can simply **distinguish peacocks from peahen** by measuring the **amount of green** on the picture - even though the picture is of high dimensionality (as many as number of pixels). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The amount of data is roughly proportionate on **how difficult of a decision you have to make**.\n",
    "<a href=\"https://miro.medium.com/max/3353/1*pao0zUAyGYuo2uA4tGqWLA.png\"><img src=\"https://drive.google.com/uc?export=view&id=1x5n8kOIr5QHsBD7zNL5ZgE7LO3e2L0Z3\" width=45%></a>\n",
    "<a href=\"https://miro.medium.com/max/3241/1*ENZI_EOozpPDR4-jwrISdw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1D7CInZg5RB01sD5WQq8lbqvGHVhuOAq3\" width=45%></a>\n",
    "\n",
    "Which in many cases can be only found out by trying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is important to note that **all learning is a form of memorization**.\n",
    "\n",
    "<a href=\"https://datascienceplus.com/wp-content/uploads/2016/12/outliers_effect.png\"><img src=\"https://drive.google.com/uc?export=view&id=1p5Fr3IiObdO6Ho7L0gYjkmms_5Q5zO_w\" width=55%></a>\n",
    "\n",
    "We would like to \"distill\" the essence, remember what is the **defining characteristic** of the data, and **disregard \"noise\"**, eg. outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This in turn has strong connections with **overfitting**, since if we \"memorize\" also the **\"unimportant\", random properties** of the data, we will have a **bad generalization** performance out of sample\n",
    "\n",
    "<a href=\"https://hackernoon.com/hn-images/1*SBUK2QEfCP-zvJmKm14wGQ.png\"><img src=\"https://drive.google.com/uc?export=view&id=1_7v7GoNr8fVXgmNsC4zBPQQw3NazesOJ\" width=55%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In turn, good generalization is crucial in case of **\"covariate shift\"**, when the data distribution changes **out of sample**.\n",
    "\n",
    "<a href=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/07230628/plot1-300x200.png\"><img src=\"https://drive.google.com/uc?export=view&id=1dxNp7v_qpR4tD6JSdyhuRndd1wCK-EIr\" width=45%></a>\n",
    "\n",
    "That is: ALWAYS :-)\n",
    "\n",
    "So if we have learned the right function, we can still generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a href=\"https://i.ytimg.com/vi/WJjPVj2bBVQ/hqdefault.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1Ma6E5MbpZ58T3WB-jHizr6KaTE4tLfUv\" width=45%></a>\n",
    "\n",
    "And since Deep Learning models have **huge memory capacity**, we would like to see if they can memorize some **generally useful patterns** across domains!\n",
    "\n",
    "That is: train a model on an abundant, general dataset, (preferredly unsupervised), and apply it to the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The other answer to the enough data question is: **\"Look at what others did, maybe you get a clue!\"**\n",
    "\n",
    "This latter strategy we will follow. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer learning\n",
    "\n",
    "- Presupposition till this point, that we build models specifically **for the given task** and always **start training \"from scratch\"**.\n",
    "- Do not take intuitions, insights, models and hyperparameters (that is: **knowledge**) from **one domain** and apply it **to another** one. \n",
    "- If **generalization of models** is strong, should be able to utilize knowledge stored in one learned model in another, similar case. (The concept of \"similar task\" is non-obvious!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Main source for discussion: [Michéle Sebag: Representation Learning, Domain Adaptation and Generative Models with Deep Learning. DeepLearn2018 Summer School, Genova](https://drive.google.com/file/d/1XZjoYuOxdPUPGZFAVw2_9Ob78msCKzNF/view?usp=sharing)\n",
    "\n",
    "A very good summary can be found at the blog of [Sebastian Ruder](http://ruder.io/transfer-learning/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Continuum between Domain Adaptation, Transfer Learning, and Multi-Task learning.\n",
    "\n",
    "- **Domain Adaptation**: same task, but \"domain\" for the task different (adapting spam filter from one user to another user)\n",
    "- **Trasfer learning**: different task (telling apart cars and telling apart animals, or even a classification and a regression), usually different tasks done sequentially\n",
    "- **Multi-Task learning**: strengthen model representation by giving it multiple different tasks  (with the idea that the tasks at least have something in common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting point\n",
    "\n",
    "* Task: classification, or regression\n",
    "* A source domain source distribution $D_s$ \n",
    "* A target domain target distribution $D_t$\n",
    "\n",
    "**Idea:**\n",
    "* Source and target are “sufficiently” related\n",
    "* ... one wants to use source data to improve learning from target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Settings\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1nu7qrEhJz0obfwEzcTqB9FbJe4MSHK-T\"><img src=\"https://drive.google.com/uc?export=view&id=1aGxmOevZIK8Zrzm2iOy3T5QGC885Cqr0\" width=65%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"http://drive.google.com/uc?export=view&id=11neGgvY5dy5BaS34edW7KfjY9uCdmspl\"><img src=\"https://drive.google.com/uc?export=view&id=1eNEreV9KtsQ0DolXt2zzPHix_Va1Ww1X\" width=65%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connections with covariate shift\n",
    "<br> </br>\n",
    "We can see, that the frequently occuring phenomenon of **\"covariate shift\"**, is highly relevant here, when the data distribution changes **out of sample**. Basicylly transfer learning is **\"induced covariate shift\"**\n",
    "\n",
    "<a href=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/07230628/plot1-300x200.png\"><img src=\"https://drive.google.com/uc?export=view&id=1dxNp7v_qpR4tD6JSdyhuRndd1wCK-EIr\" width=45%></a>\n",
    "\n",
    "Our hope is: if we have learned the right function, we can still generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Warning! - Transferring bias\n",
    "\n",
    "The myth of the \"Tank detector\":\n",
    "https://www.gwern.net/Tanks#could-something-like-it-happen\n",
    "\n",
    "<a href=\"https://i.ytimg.com/vi/yvuKQkGVjJE/hqdefault.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1vxqGrvyWaAPNbTeMSnRl9f-gmRf6yvzg\" width=400 heigth=400></a>\n",
    "\n",
    "It can be an \"urban legend\", but shows that a mere environmental constant in a dataset can becom a a biasing factor, so it is pretty easy to imagine that these biases will hurt performance in a different setting. (As well as again draw attention to the paramount importance of model validation!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some baseline supervised methods:\n",
    "\n",
    "### Learn source and target domain in union\n",
    "\n",
    "[J. Huang, A. Smola, A. Gretton, K. M. Borgwardt, and B. Scholk ¨ opf. Correcting sample selection\n",
    "bias by unlabeled data. In B. Scholk ¨ opf, J. Platt, and T. Hoffman, editors, Advances in Neural\n",
    "Information Processing Systems 19. MIT Press, Cambridge, MA, 2007](https://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data.pdf)\n",
    "\n",
    "- Default assumption in many learning scenarios: training and test data independently and identically (iid) drawn from the same distribution \n",
    "- While the available data have been collected in a biased manner, the test is usually performed over a more general target population\n",
    "- Training and test data are drawn from different distributions, commonly referred to as sample selection bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Covariance shift\n",
    "- Matching distributions between training and testing sets in feature space\n",
    "- Account for the difference between Pr(x, y) and Pr′ (x, y) by reweighting the training points such that the means of the training and test points are close (in a reproducing kernel Hilbert space (RKHS))\n",
    "\n",
    "\n",
    "Since the source has much more data, target can get \"oppressed\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Label based on source classifier, add that as input, learn new classifier\n",
    "\n",
    "[H. Daumé, D. Marcu, Domain Adaptation for Statistical Classifiers](https://www.aaai.org/Papers/JAIR/Vol26/JAIR-2603.pdf)\n",
    "\n",
    "\n",
    "- Situation: labeled out-of-domain data plentiful, but labeled in-domain data scarce\n",
    "- Statistical formulation of problem in terms of a simple mixture model\n",
    "- Treat in-domain data as drawn from a mixture of two distributions: a “truly in-domain” distribution and a “general domain” distribution\n",
    "- Out-of-domain data is treated as if drawn from a mixture of a “truly out-of-domain” distribution and a “general domain” \n",
    "- Inference algorithm case based on the technique of conditional expectation maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learn two classifiers, one for source, one for target and weight their outputs as \"votes\"\n",
    "\n",
    "[H. Daumé, D. Marcu, Domain Adaptation for Statistical Classifiers](https://www.aaai.org/Papers/JAIR/Vol26/JAIR-2603.pdf)\n",
    "\n",
    "### Use source model as prior\n",
    "\n",
    "Initialize the target classifier with the weights learned from source, during training regularize towards the original weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised transfer learning - Distance of tasks \n",
    "\n",
    "###  Feature augmentation\n",
    "\n",
    "[H. Daumé, Frustratingly Easy Domain Adaptation In ACL 2007](https://arxiv.org/abs/0907.1815)\n",
    "- Take each feature in the original problem and make three versions of it: a general version, a source-specific version and a target-specific version. \n",
    "- Augmented source data will contain only general and source-specific versions. \n",
    "- Augmented target data contains general and target-specific versions \n",
    "- Then define a systematic mapping between source and target data as a transformation.\n",
    "- Distance of tasks\" relevant: presume systematic mapping transforming source to target. \n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1lhdJQBM61Dxg8fH0nMbBezJK9qT2oQrf\"><img src=\"https://drive.google.com/uc?export=view&id=1zDTSMLFNzIJ9P6njCIi6UoC4Sjt_tuOw\" width=45%></a>\n",
    "\n",
    "For one method see [Baochen Sun, Jiashi Feng, Kate Saenko: Return of Frustratingly Easy\n",
    "Domain Adaptation. AAAI 2016](https://arxiv.org/abs/1511.05547) quoted by Sebag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detour: Catastrophic forgetting\n",
    "\n",
    "- What if during learning the new task, we don't just acquire new abilities, but \"forget\" what we have learned so far? \n",
    "- No guarantee that updates based on error gradients on the new task do not lead the model weights away from the optimum learned before (or better to say: it is nearly certain it does so...) \n",
    "- It is a difficult task to ensure, that learning does not happen with strong degradation of the prior knowledge (what prior knowledge to degrade)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a href=\"http://drive.google.com/uc?export=view&id=1dU9SRJdhcgl2YMclPk_Qu38BLnuAqoCY\"><img src=\"https://drive.google.com/uc?export=view&id=1vtKPG6waIgXE3EO8jnxDcPeDtZt0r3M5\" width=55%></a>\n",
    "\n",
    "Even in the \"natural\" case, things can be easily forgotten.\n",
    "\n",
    "Combine this with the possibility, that the **early gradient updates cause much noise** since the last classification layers can be newly initialized for the task (more on that below), and you will get pretty weak benefit from transfer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**[Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/1612.00796)**\n",
    "\n",
    "- One Idea: make the change of weights that were important for solving prior tasks more difficult. \n",
    "- We can use a quadratic error term we can try to keep these weights close to their original values. The authors call this \"elastic weight consolidation\". \n",
    "\n",
    "<a href=\"https://image.slidesharecdn.com/overcomingcatastrophicforgettinginneuralnetwork-170429024916/95/overcoming-catastrophic-forgetting-in-neural-network-9-638.jpg?cb=1493434190\"><img src=\"https://drive.google.com/uc?export=view&id=1Wh7u-waBs_hV2lWcawgjwVwdAHcWOA2q\" width=35%></a>\n",
    "\n",
    "An interesting description of \"catastrophic forgetting\" in NLP context can be found [here](https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning of \"general\" representations\n",
    "\n",
    "- Can argue, that by learning on a big enough dataset, a model will learn useful \"general\" features, that can be later on reusable. \n",
    "- This is indeed one of the motivation behind teaching autoencoders (large amounts of unlabeled data), or **re-using model representations** from discriminative models - trained eg. on ImageNet.\n",
    "\n",
    "**Capitalize on the notion, that the deep networks are gradually learning hierarchic representations of features, and only at the last couple layers do they learn a classifier.**\n",
    "\n",
    "**Thus a common practice to take a trained ConvNet, throw away the last 1-2 layers and train it for a new task - remember, last is a softmax over the classes...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A baseline approach for deep NLP\n",
    "\n",
    "Especially in NLP, with the advent of the predictive paradigm, namely word2vec, it became quasi the gold standard to use some transfer learning.\n",
    "\n",
    "The most basic approach was (and is still today) to **initialize the embedding layer of complex models with simple ones** (Like GloVe or word2vec). This means, that the models at least start with a decent knowledge about the individual words, and can start to learn \"context\" on top of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a href=\"https://guillaumegenthial.github.io/assets/bi-lstm.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Bl3dsZcffwk0xGr1xO85Ot9Q_j-1kvOK\" width=45%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Detour: LMAO\n",
    "\n",
    "It is also worth noting that since softmax over a huge vocabulary is one of the biggest bottlenecks for NLP task training, some people proposed **replacing the output layer to be a vector output, instead of a softmax**. (Details see [here](https://arxiv.org/abs/1812.04616)).\n",
    "\n",
    "For this to be achievable, they needed to come up with a new loss function, that can compare a **distribution over word vectors** to the real word vector instead of a simple softmax.\n",
    "\n",
    "The authors claim:\n",
    "\n",
    "_\"We evaluate this new class of sequence-to-sequence models with continuous outputs on the task of neural machine translation. We show that our models train up to **2.5x faster** than the state-of-the-art models while achieving **comparable translation quality**. These models are capable of handling **very large vocabularies** without compromising on translation quality or speed. They also produce **more meaningful errors** than the softmax-based models, as these errors typically lie in a subspace of the vector space of the reference translations.\"_\n",
    "\n",
    "The NLP toolkit Spacy also included this method in their 2.1 release, calling it \"LMAO - Language Modeling on Approximate Outputs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to prevent catastrophic forgetting in practice?\n",
    "\n",
    "To answer the question properly, we have to think over **where and how the forgetting happens** at all.\n",
    "\n",
    "Since the topmost (eg. Softmax) layers are initialized randomly, most probably they contribute most to the error, thus it can be argued, that until we get them to \"mature\" a bit more, they are the main culprits we have to work with, thus there is growing evidence, that **the top layers have to be trained first, then the lower layers can be fine tuned.**\n",
    "\n",
    "(This makes all the more sense, since the lower the layers, the more basic features they represent, thus most probably they generalize all the more, so it makes no sense to hit them hard with some big modifications at first.)\n",
    "\n",
    "Enter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradual unfreezing\n",
    "\n",
    "The idea is pretty simple: for the early part of the transfer learning based training do not allow updates on weights for the majority of the network, just **gradually unfreeze** the layers later on.\n",
    "\n",
    "<a href=\"https://humboldt-wi.github.io/blog/img/seminar/group4_ULMFiT/Figure_21.png\"><img src=\"https://drive.google.com/uc?export=view&id=1KxNr1UqL_1q7FTNjLv8SXivkuFKB343H\" width=65%></a>\n",
    "\n",
    "See a detailed analysis of Ruder and co.'s UMLFIT [here](https://medium.com/explorations-in-language-and-learning/transfer-learning-in-nlp-2d09c3dfaeb6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Soft version: Differential learning rates\n",
    "\n",
    "The concept of [differential learning rates](https://blog.slavv.com/differential-learning-rates-59eff5209a4f) is in a sense a soft version of freezing: instead of fixing the lower layers, a smaller learning rate is used for their updates, which - at least in theory - should help them avoid forgetting.\n",
    "\n",
    "**In practice, though, many times an appropriately small (think: one order of magnitude smaller) global learning rate is sufficient.**\n",
    "\n",
    "<a href=\"https://cdn-images-1.medium.com/max/2400/1*BEyoI-p1FGTV0PUoGTxz4Q.png\"><img src=\"https://drive.google.com/uc?export=view&id=1fdjxkQ-MHqM42k5PPU4A-pfmfZmfAu9J\" width=55%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Verdict is still out\n",
    "\n",
    "The development of more effective transfer learning methods is far from finished, there are quite [recent papers](https://arxiv.org/abs/1812.01640) which offer a good survey:\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1V03HbhaHSRboTubR5afsMh0G6TTbkMMw\"><img src=\"https://drive.google.com/uc?export=view&id=1PvsLWLN9aWwYFqINCLpG4gymki-eZcUD\"  width=40%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detour: Weak supervision - An interesting current solution\n",
    "\n",
    "[Exploring the Limits of\n",
    "Weakly Supervised Pretraining](https://research.fb.com/wp-content/uploads/2018/05/exploring_the_limits_of_weakly_supervised_pretraining.pdf)\n",
    "\n",
    "- Research group at Facebook wanted to teach a discriminative model with exceptionally large dataset, bigger than manual tagging allows\n",
    "- Decided to capitalize on \"weak supervision\", that is the noisy \"labeling\" based on the # tags people give for their Instagram pictures\n",
    "- Took **3.5 billion (!!!)** Instagram pictures and trained a ConvNet on it. \n",
    "- Annotation quality inferior, but the sheer amount of data causes the model to beat the state of the art in transfer scenarios.\n",
    "\n",
    "This was also a _huge_ implementation challenge, well worth a read [here](https://www.facebook.com/ross.girshick/posts/10160363792300261)  and [here](https://code.facebook.com/posts/1700437286678763/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi task learning\n",
    "\n",
    "Basic idea: advantage to force a model to learn multiple tasks concurrently!\n",
    "It promises, that we generally learn something about the world - model constrained by variety of data. (AGI, rings a bell? :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General approach\n",
    "\n",
    "The general approach on multi-task learning capitalizes on the fact, that learned representations for neural models are typically having two somewhat distinguishable structural elements:\n",
    "\n",
    "- **the first** (MANY!) **layers are for representation learning** (\"embedding\")\n",
    "- **the last** (typically 1 or so) **layers are for classification** (\"cut\", ie. a linear classifier)\n",
    "\n",
    "With this in mind, we can propose to utilize a **joint representation for the embedding**, and only keep the classification layers separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a href=\"https://miro.medium.com/max/565/0*ZwCyanzdxYyWXLKK.png\"><img src=\"https://drive.google.com/uc?export=view&id=1GFQsWJFwN3kqD5bVmjefvRlmFoe6eczu\" width=45%></a>\n",
    "\n",
    "This method promises to produce more \"generalizable\" embeddings, thus, it can be of importance for out-of-training performance as well as model generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Specific models\n",
    "\n",
    "#### \"Zero-shot translation\"\n",
    "[Google’s Multilingual Neural Machine Translation System:\n",
    "Enabling Zero-Shot Translation](https://arxiv.org/pdf/1611.04558.pdf)\n",
    "\n",
    "Or in a more readable format [here](https://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html)\n",
    "\n",
    "- Seq2seq, \"normal\" encoder-decoder + attention mechanism machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a href=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/0245d8cbc0a405e553311bd20a01099c9aa11c14/4-Figure1-1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1UjKks5TT-bf45YkoLh3ACipKLFiGJaA-\" width=45%></a>\n",
    "\n",
    "- Parallel corpora on _language pairs_\n",
    "- Only addition, that a separate tag for the given language is put before the input sequence (to inform the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Result:**\n",
    "\n",
    "1. It learns to translate to language pairs for which _it was never given data_\n",
    "2. It's inner representation is _language independent_\n",
    "\n",
    "<a href=\"https://2.bp.blogspot.com/-AmBczBtfi3Q/WDSB0M3InDI/AAAAAAAABbQ/1U_51u5ynl4FK4L0KOEllfRCq0Oauzy5wCEw/s640/image00.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Nd9tmG6SBlixVpIKyd7crfbPa3sVf7dw\" width=55%></a>\n",
    "\n",
    "This result amazed even the researchers, much analysis went into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Multi-modality\n",
    "\n",
    "Even more ambitious experiment:  Google [\"One model to rule them all\"]( https://arxiv.org/abs/1706.05137)\n",
    "\n",
    "- Single model that yields good results on a number of problems spanning multiple domains: trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. \n",
    "- Model architecture: convolutional layers, an attention mechanism, and sparsely-gated layers\n",
    "- Each of these computational blocks is crucial for a subset of the tasks we train on.\n",
    "- Even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. \n",
    "- We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.\n",
    "- A model has been given a host of tasks in parallel (language as well as visual tasks!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### General description\n",
    "\n",
    "[Ruder: Multi task learning](https://arxiv.org/abs/1706.05098)\n",
    "Or more \"friendly\" description [here](http://ruder.io/multi-task-learning-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outlook\n",
    "\n",
    "It is obvious, that transfer learning - because of great emphasis on generalization -, and especially multi-task learning is also on the forefront of development towards general AI models.\n",
    "\n",
    "A recently published amazing example proposes to use VGG CNN-s, **pre-trained on standard ImageNet** dataset for **processing of sound spectrograms** with remarkable efficiency.  A material presenting state of the art toolkits can be found here: [Björn Schuller: Deep Learning for signal analysis, DeepLearn2018 summer school, Genova](https://drive.google.com/file/d/1GCt1PAISMH-4L7fGuSvp_4DbzIiSXrZk/view?usp=sharing) [paper](https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0434.PDF) and [toolkit](https://github.com/DeepSpectrum/DeepSpectrum). This shows that we still have to be rather humble, and we still have much to learn about invariances across domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Is there an end in sight?\n",
    "\n",
    "Well, for certain domains, one can argue, that we do not yet see an obvious plateau of performance with regard to adding magnitudes of more data for unsupervised pre-training. Language modeling being the prime example:\n",
    "\n",
    "<a href=\"https://ruder.io/content/images/2019/08/scaling_up_pretraining.png\"><img src=\"https://drive.google.com/uc?export=view&id=1WZBcjTK1UQ8Ss46bRkSom3fqV7X8-7QM\" width=75%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations\n",
    "\n",
    "We have to consider, that this question is in strong connection with generalization, albeit over the train-test split or even the \"new\", \"live\" data.\n",
    "\n",
    "As such, it is also subject of the:\n",
    "\n",
    "**[No free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)**\n",
    "\n",
    "_\"states that any two optimization algorithms are equivalent when their performance is averaged across all possible problems\"_\n",
    "\n",
    "There is no universal model, not even a human one. **There will always be some things we don't know.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to do transfer learning in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Recently some progress has been made in more effective regimes of transfer learning, like \"gradual unfreezing\" and [\"differential learning rates\"](https://blog.slavv.com/differential-learning-rates-59eff5209a4f), which influenced the [UMLFiT](https://arxiv.org/abs/1801.06146) approach (more on that later), as well as new architecture were proposed, specializing in transfer learning, like [adapters](https://arxiv.org/abs/1902.00751).\n",
    "\n",
    "There are also experimental approaches that try to avoid task specific pre-training, and add some auxiliary loss instead to achieve the same purpose, like SiATL from [\"An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models\"](https://arxiv.org/pdf/1902.10547v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a href=\"https://hackernoon.com/hn-images/1*DkyoDbCWRJ_8uKgin24QNA.png\"><img src=\"https://drive.google.com/uc?export=view&id=1FtrFJSpVCefkJmMXKXkctvh2V7s0CEN1\" width=45%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recently a more **systematic analysis of transfer learning with BERT model** has been carried out in context of sentiment classification.\n",
    "\n",
    "Multiple approaches are being studied:\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=12XWIaWTgR16TD6pBQ3Q1uyu1vaZ7vkar\"><img src=\"https://drive.google.com/uc?export=view&id=1PTeLtLMh1ltPsO8sci4hLpYU4mypNtCo\" width=45%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"...**some experimental findings:**\n",
    "\n",
    "1. The top layer of BERT is more useful for text classification; \n",
    "2. With an appropriate layer-wise decreasing learning rate, BERT can overcome the catastrophic forgetting problem; \n",
    "3. Within-task and in-domain further pre-training can significantly boost its performance; \n",
    "4. A preceding multi-task fine-tuning is also helpful to the single-task fine-tuning, but its benefit is smaller than further pre-training; \n",
    "5. BERT can improve the task with small-size data.\"\n",
    "\n",
    "\n",
    "[How to fine-tune BERT for text classification?](https://arxiv.org/abs/1905.05583) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For a very nice analysis of [The state of transfer learning in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/) was carried out by Ruder. Definitely worth studying.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
