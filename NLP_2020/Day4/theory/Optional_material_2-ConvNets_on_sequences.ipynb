{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are LSTM-s really necessary?\n",
    "\n",
    "LSTMs, when combined with dropout and other techniques became **hugely successful** and were considered the workhorse of NLP and time series applications, as well as sequence to sequence problems, moreover they serve as basis for all memory network architectures. They were and are still dominant in these fields.\n",
    "\n",
    "None the less as of 2017-8, multiple findings emerged that question the necessity for LSTMs in many fields. The leading field in this regard was neural machine translation, where Facebook Research developed it's [ConvNet based  machine translation](https://code.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/), as well as Google publishing the [transformer architecture](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html). Both approaches were motivated by the fact that LSTM computation is not easily parallelized, but ConvNets and transformers are, so training can be scaled up rapidly.\n",
    "\n",
    "Based on these networks multiple analyses tried to justify the usage of LSTMs and found, that though they have in theory infinite memory ability, in practice, a limited memory is good enough, which can be modeled by ConvNets, especially 1D and dilated convolutions.\n",
    "\n",
    "For more information see [here](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0) [here](http://www.offconvex.org/2018/07/27/approximating-recurrent/) [here](http://blog.aylien.com/acl-2018-highlights-understanding-representations-and-evaluation-in-more-challenging-settings/) [here](https://arxiv.org/abs/1803.01271)\n",
    "\n",
    "(Interesting snippets or remarks from the above sources:\n",
    "“LSTMs work in practice, but can they work in theory?”\n",
    "\" “According to Chomsky, sequential recency is not the right bias for learning human language. RNNs thus don’t seem to have the right bias for modeling language, which in practice can lead to statistical inefficiency and poor generalization behaviour.\")\n",
    "\n",
    "\n",
    "Alternative approaches as [\"recursive neural networks\"](https://en.wikipedia.org/wiki/Recursive_neural_network) and [\"recurrent neural network grammars\"](https://arxiv.org/abs/1602.07776) also exist, but not that widespread.\n",
    "\n",
    "\n",
    "### Convolutions for sequential data\n",
    "\n",
    "We use convolution operators, but only in one dimension over the data.\n",
    "\n",
    "<a href=\"http://mblogthumb2.phinf.naver.net/MjAxNjEyMTBfMjMx/MDAxNDgxMjk1ODk2NDAz.kn9JN93v9X2Xn9vJloqupV5c5GB09YNYwPrvDB8yKU8g.Hh1wT30ySu0JFWNqj2qoSTiX-pRnrjH2VWhMI2EAo30g.PNG.atelierjpro/%EC%8A%AC%EB%9D%BC%EC%9D%B4%EB%93%9C2.PNG?type=w2\"><img src=\"https://drive.google.com/uc?export=view&id=1VlPqHI2s8NyI2kaaHIrjGwtd4FClAbVZ\" width=50%></a>\n",
    "\n",
    "Look, look, we have reinvented sliding windows! :-(\n",
    "\n",
    "<a href=\"https://qph.fs.quoracdn.net/main-qimg-523434af0d21bb0b59454aa9563cc90b-c\"><img src=\"https://drive.google.com/uc?export=view&id=1uuL9sC25DnbdL-bD-DmqNp1tFX6-xVaX\" width=50%></a>\n",
    "\n",
    "Though if we [calculate the receptive fileds](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807) of these models, with enough depth they can be formidable!\n",
    "\n",
    "### Dilated convolutions\n",
    "\n",
    "Dilated convolutions are used to radically increase the total receptive field of a network.\n",
    "\n",
    "<a href=\"http://sergeiturukin.com/assets/2017-02-23-155956_803x294_scrot.png\"><img src=\"https://drive.google.com/uc?export=view&id=1x12ed5qvn3Ls-glPeYBArzC6w3Wb2Br9\" width=50%></a>\n",
    "\n",
    "\n",
    "<a href=\"https://mlblr.com/images/dilated.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1meYEWWcBbrvJy21PBTSUSUd0oTjAveJh\" width=50%></a>\n",
    "\n",
    "Original paper [here](https://arxiv.org/abs/1511.07122)\n",
    "\n",
    "[Wavenet](http://sergeiturukin.com/2017/03/02/wavenet.html) and other successful models (especially in voice recognition) use this approach effectively.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
