{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are parts of speech?\n",
    "\n",
    "A __syntactic theory__ for a language is a theoretical characterization of the language's well formed sentences. Although there is a plethora of syntactic theories, basically all of them rely on some notion of \"parts of speech\" (POS for short), i.e., basic syntactic roles that expressions can have in well formed sentences. For instance, in the standard constituency analysis of the English sentence\n",
    "\n",
    "> John hit the ball.\n",
    "\n",
    "<a href=\"https://upload.wikimedia.org/wikipedia/commons/5/54/Parse_tree_1.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1-e4Vqo7pMZdCx0der8WxyaD-VUw-tY6s\"></a>\n",
    "\n",
    "(Image source: [Wikipedia: Parse tree](https://en.wikipedia.org/wiki/Parse_tree))\n",
    "\n",
    "\n",
    "\"John\" and \"ball\" are nouns, \"hit\" is a verb, and \"the\" is a determinant -- these are all parts of speech categorizations that are an important part of the full syntactic analysis.\n",
    "\n",
    "__Context dependence__\n",
    "\n",
    "It is important to note that the part of speech role played by an expression can be context dependent. For instance, in contrast to the previous sentence, \"hit\" in the sentence\n",
    "\n",
    "> His first song was a huge hit in Europe.\n",
    "\n",
    "is a noun. \n",
    "\n",
    "__Theory and language dependence__\n",
    "\n",
    "Although the full list of POS categories is language and syntactic theory dependent, some parts of speech are pretty much universal, e.g. the categories __noun__, __verb__, __adjective__ and __adverb__ can be found in almost all languages and syntactic theories.\n",
    "\n",
    "__Open vs closed POS categories__\n",
    "\n",
    "- Closed POS categories, e.g., the category of determiners in English, consist of relatively small sets of words, and these sets do not change easily: it's a rare phenomenon that a new determiner is added to a language.\n",
    "- Open POS categories, on the other hand, like that of English verbs, contain a large number of words and new members are added on daily basis.\n",
    "\n",
    "A strongly related distinction is that of between __function words__ and __content words__: while words belonging to open POS categories are content words in the sense that they typically have a more or less well characterizable lexical semantic content on their own (many verbs refer to actions, many proper nouns to indviduals etc.), closed POS categories contain words without much independent semantic content -- their semantics is closely tied to their semantic function within sentences.\n",
    "\n",
    "__Why are POS categories useful?__\n",
    "\n",
    "Determining the POS category of each word in a text is the first linguistic analysis step after tokenization/sentence segmentation: it is necessary for all later stages, i.e. full syntactic analysis, semantics etc.\n",
    "\n",
    "In addition to being an important part of syntax, part of speech information contains useful information about a word's\n",
    "- __distributional properties__, i.e., in which context the word can occur, e.g., in English nouns can be preceded by a determiner, but not verbs, and, especially for content words, about its \n",
    "- __semantics__, e.g., verbs frequently (but not exclusively) refer to some sort of actions/events while nouns frequently refer to participants of these actions/events.\n",
    "\n",
    "this type of information is routinely exploited in NLP applications, e.g. in information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagsets\n",
    "\n",
    "In NLP, POS categories are typically encoded with shorthands, so called POS tags.\n",
    "## Penn Treebank tagset\n",
    "\n",
    "A historically very influential English POS tagset was developed for the [Penn Treebank (PTB) project](https://catalog.ldc.upenn.edu/LDC99T42) (1989-1996):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||||\n",
    "|--- |--- |--- |\n",
    "|Number|Tag|Description|\n",
    "|1.|CC|Coordinating conjunction|\n",
    "|2.|CD|Cardinal number|\n",
    "|3.|DT|Determiner|\n",
    "|4.|EX|Existential there|\n",
    "|5.|FW|Foreign word|\n",
    "|6.|IN|Preposition or subordinating conjunction|\n",
    "|7.|JJ|Adjective|\n",
    "|8.|JJR|Adjective, comparative|\n",
    "|9.|JJS|Adjective, superlative|\n",
    "|10.|LS|List item marker|\n",
    "|11.|MD|Modal|\n",
    "|12.|NN|Noun, singular or mass|\n",
    "|13.|NNS|Noun, plural|\n",
    "|14.|NNP|Proper noun, singular|\n",
    "|15.|NNPS|Proper noun, plural|\n",
    "|16.|PDT|Predeterminer|\n",
    "|17.|POS|Possessive ending|\n",
    "|18.|PRP|Personal pronoun|\n",
    "|19.|PRP\\$|Possessive pronoun|\n",
    "|20.|RB|Adverb|\n",
    "|21.|RBR|Adverb, comparative|\n",
    "|22.|RBS|Adverb, superlative|\n",
    "|23.|RP|Particle|\n",
    "|24.|SYM|Symbol|\n",
    "|25.|TO|to|\n",
    "|26.|UH|Interjection|\n",
    "|27.|VB|Verb, base form|\n",
    "|28.|VBD|Verb, past tense|\n",
    "|29.|VBG|Verb, gerund or present participle|\n",
    "|30.|VBN|Verb, past participle|\n",
    "|31.|VBP|Verb, non-3rd person singular present|\n",
    "|32.|VBZ|Verb, 3rd person singular present|\n",
    "|33.|WDT|Wh-determiner|\n",
    "|34.|WP|Wh-pronoun|\n",
    "|35.|WP$|Possessive wh-pronoun|\n",
    "|36.|WRB|Wh-adverb|\n",
    "\n",
    "(Source: [Penn Treebank P.O.S. Tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is was designed specifically for English, the tagset is very fine grained, and some of the tags encode _morphological_ information (which we will talk about later) in addition to the basic POS category, e.g. the JJ tags for adjectives indicate the _degree_ of the adjective, and VB tags indicate tense and person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Dependencies POS tagset\n",
    "\n",
    "Recently, the [Universal Dependencies treebank project](https://universaldependencies.org/introduction.html) (2014) has developed a cross-linguistic set of POS tags. The tagset consists only of 17 POS categories, but word annotations can contain additional language-specific morphological tags, and, therefore, can reach the level of detail that is provided by more fine-grained POS tagsets like the PTB tagset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Universal Dependencies (UD) POS tagset contains the following tags:\n",
    "\n",
    "__Open class tags__\n",
    "\n",
    "||||\n",
    "|-- |-- |-- |\n",
    "|Tag|Description|Examples|\n",
    "|ADJ|adjective|big, old, green, African, first|\n",
    "|ADV|adverb|very, well, exactly, tomorrow, where, here, somewhere|\n",
    "|INTJ| interjection|psst, ouch, bravo, hello|\n",
    "|NOUN|noun|girl, cat, tree, air|\n",
    "|PROPN|proper noun|Mary, John, London, NATO|\n",
    "|VERB|verb|run, eat, runs, ate|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Closed class tags__\n",
    "\n",
    "||||\n",
    "|--|--|--|\n",
    "|Tag|Description|Examples|\n",
    "|ADP|adposition|in, to, during|\n",
    "|AUX|auxiliary|has, is (as in \"He is a teacher.\"), should, was, must|\n",
    "|CCONJ|coordinating conjunction|and, or, but|\n",
    "|DET|determiner|a, an, the, this, which, any, no (as in \"I have no car.\")|\n",
    "|NUM|numeral|0,1,2,one,two|\n",
    "|PART|particle|not, 's (as in \"Andrew's table\")|\n",
    "|PRON|pronoun|I, myself, who|\n",
    "|SCONJ|subordinating conjunction|that, if|\n",
    "\n",
    "__Other tags__\n",
    "\n",
    "||||\n",
    "|--|--|--|\n",
    "|Tag|Description|Examples|\n",
    "|PUNCT|punctuation|.  ,  ;|\n",
    "|SYM|symbol|$,  ¬ß, ¬©, üòù|\n",
    "|X|other|for unanalyizable elements, as in \"And then he just xfgh pdl jklw\"|\n",
    "\n",
    "(the examples are from the official [UD tagset description](https://universaldependencies.org/u/pos/all.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also\n",
    "\n",
    "- [Detailed official explanation of all UD POS categories with more examples](https://universaldependencies.org/u/pos/all.html)\n",
    "- Paper about the UD treebank project: [Joakim Nivre et al, Universal Dependencies v1: A Multilingual Treebank Collection (2016)](http://www.lrec-conf.org/proceedings/lrec2016/pdf/348_Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The POS-tagging task\n",
    "\n",
    "The POS-tagging task is simply to assign correct tags from a given POS tagset to each word/token in a tokenized (and possibly sentence-segmented) input text. This _sequence labeling_ task is naturally supervised: POS-taggers are trained and evaluated on already tagged text corpora. If the tokenized text is also sentence segmented then there can be special sentence start and sentence end tokens (e.g., &lt;s&gt;, &lt;/s&gt;) indicating sentence boundaries.\n",
    "\n",
    "## Performance metrics\n",
    "\n",
    "The most common performance metric for POS-tagging is __accuracy__.\n",
    "\n",
    "## Corpora\n",
    "\n",
    "Treebanks automatically contain POS-information, so the standard POS-data sets usually coincide with the standard treebank data sets for a language (if they exist). \n",
    "\n",
    "+ In the case of English, the Wall Street Journal part of the [Penn Treebank](https://catalog.ldc.upenn.edu/LDC99T42) is the most common POS data set on which models are evaluated. Unfortunately, the PTB can be obtained from the Linguistic Data Consortium and  is not free.\n",
    "\n",
    "+ An important free multilanguage POS data set is provided by the [Universal dependencies project](https://universaldependencies.org/) which contains corpora (of varying size) for more than 70 languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging with classic sequence labeling ML methods\n",
    "\n",
    "As we have seen, POS-tagging is a __sequence labeling__ or sequence tagging task: given a $\\langle w_1,\\dots,w_n\\rangle$ __sequence of word \"observations\"__, we have to find the correct $\\langle t_1,\\dots, t_n \\rangle$ __sequence of tags__, classifying each element in the input sequence.\n",
    "\n",
    "## Baseline: most frequent tag\n",
    "\n",
    "If we disregard context then our task boils down to a simple classification: label a $w$ word with the correct POS-tag. Given a labeled training corpus, the simplest possible strategy is to predict for any $w$ word the tag with which it is most frequently associated in the corpus. Somewhat surprisingly, this primitive baseline can achieve over 90% accuracy in certain settings: e.g., [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/8.pdf) reports that\n",
    "\n",
    ">If we train on the WSJ training corpus and test on sections 22-24 of the same corpus the most-frequent-tag baseline achieves an accuracy of 92.34%.\n",
    "\n",
    "Naturally, this baseline does not address two crucial problems:\n",
    "\n",
    "+ __Context dependence__: In real life, POS-tags are context dependent (if they were not, the baseline accuracy would be 100% on the corpus it was trained on).\n",
    "+ __Unknown words__: The baseline has no strategy to deal with words that do not occur in the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM-based tagging\n",
    "\n",
    "A way more sophisticated approach is to build a probabilistic model to assign probabilities to possible __$\\langle t_1,\\dots, t_n \\rangle$ tag sequences__ for the input sequence. Of course, in order to make this feasible, we have to make some independence assumptions, but we also want to capture dependencies, e.g. regularities like \n",
    "\n",
    "> determiners are frequently followed by nouns but very rarely by verbs\n",
    "\n",
    "or\n",
    "\n",
    "> prepositions are frequently followed by verbs \n",
    "\n",
    "and a relatively obvious, and __historically important__ approach has been to work with a __hidden Markov model (HMM)__. \n",
    "\n",
    "In contrast to the ASR scenario, here the hidden states of the HMM will be the class labels (POS-tags), and the observable states will be words:\n",
    "\n",
    "<a href=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/04/16134154/pos2.png\"><img src=\"https://drive.google.com/uc?export=view&id=1I1Sn6QVQfOKAogrGSm8zqP-mcMzYeTcT\" width=\"400px\"></a>\n",
    "\n",
    "(Image source: [Part of Speech (POS) tagging with Hidden Markov Model](https://www.mygreatlearning.com/blog/pos-tagging/))\n",
    "\n",
    "Since HMMs are __generative__ (as opposed to __discriminative__) models, it is worth thinking about this POS-tagging model in terms of language generation. What our HMM says is that a text can be generated by\n",
    "\n",
    "1. generating a __sequence of POS-classes according to the transition probabilities__ (a \"syntactic skeleton\")\n",
    "2. generating a __word for each chosen POS-class according to the emission probabilities__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Again, unlike in ASR, the training corpus gives us full __access to the hidden states (POS-tags)__, so given a large enough corpus, we can perform the MLE of transition and emission probabilities simply by __counting__:\n",
    "\n",
    "__Transition probability:__\n",
    "$$\n",
    "\\hat P(t_j \\mid t_i) = \\frac{Count(t_i, t_j)}{Count(t_j)}\n",
    "$$\n",
    "\n",
    "__Emission probability:__\n",
    "$$\n",
    "\\hat P(w_k \\mid t_i) = \\frac{Count(w_k, t_i)}{Count(t_i)}\n",
    "$$\n",
    "\n",
    "\n",
    "A serious complication is that some combinations can be missing from the corpus. In these cases __smoothing methods__ can be used, which will be discussed in some detail later when are going to talk about language modeling.\n",
    "\n",
    "Decoding the most likely tag sqeuence for a sentence:\n",
    "\n",
    "$$\\hat{y}=argmax \\prod\\nolimits_{n=1}^N P(w_n|t_n)P(t_n|t_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown words\n",
    "\n",
    "How can we handle unknown words with this type of model? If __no emission probabilities__ are allocated to unknown words then the __probability__ of any observation sequence containing one will be __zero__, so there will __not be any difference__ between the probability of __tag sequences__. This means that we have to allocate __some emission probability__ mass to __unknown words__. The simplest way to do this is (again) smoothing: tags (for open classes) should have some probability of \"emitting\" unknown words, which can be handled by introducing a new symbol, e.g. UNK. into the model with smoothed probabilities.\n",
    "\n",
    "#### Features for unknown words\n",
    "\n",
    "A radically better approach is to recognize that even __unknown/unseen words__ can __have features__ that are useful for predicting the correct label and, crucially, _can_ be learned from the training corpus. E.g., even if we have never seen the word\n",
    "\n",
    "> supercalifragilisticexpialidocious \n",
    "\n",
    "we can have the educated guess that there is a high probability that it is an adjective, based on its ending. This leads to the thought of adding unknown word features to the model, e.g.\n",
    "\n",
    "+ word suffix (e.g., the last \"4\" character)\n",
    "\n",
    "instead of emitting UNK, now the model can emit and handle UNK_with_suffix:ious, UNK_with_suffix:cely etc. symbols, and the ratio of the emission probabilities for these alternatives can be learned from the training corpus.\n",
    "\n",
    "Variants of this strategy actually work quite well: the most important example is the [TnT](http://www.coli.uni-saarland.de/~thorsten/tnt/) POS-tagger, which used a 2-order Markov model and handled unknown words based on their suffixes achieving 96.7% accuracy on the PTB (see the [paper](https://arxiv.org/pdf/cs/0003055) for details).\n",
    "\n",
    "#### The problem of generative modeling\n",
    "\n",
    "An important problem with the above strategy for unknown words that it is __hard to add other type of features__ (e.g. whether the word is capitalized or not) because this would require the modeling of the probabilistic interdependencies between these features, as HMMs are __generative models__. This motivates switching to __discriminative models__, that model the $P(T \\mid W)$ conditional probabilities instead of the the full $P(T, W)$ joint distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging with a maximum entropy Markov model (MEMM)\n",
    "Based on: Multinomial regression, Maximum entropy regression, Multiclass linear regression\n",
    "\n",
    "- Based on the idea of __logistic regression as a discriminative model__ \n",
    "- But logistic regression isn‚Äôt a __sequence model__: assigns __class__ to a __single observation__\n",
    "- Solution: turn logistic regression into __discriminative sequence__ model\n",
    "- Run on successive words: use __class assigned to the prior word as feature__ in the classification of the next word. \n",
    "- Apply logistic regression for __each possible outcome class__\n",
    "- Chose __most likely class__\n",
    "\n",
    "When we apply logistic regression\n",
    "in this way (to a multi-class output), it‚Äôs called the __Maximum Entropy Markov model__ or __MEMM__.\n",
    "\n",
    "Maximum entropy: distrubutes the probability mass to the correct label using a softmax function. Whilst a single label with probability mass P=1 has the smallest entropy a uniform distribution has the largest entropy.\n",
    "\n",
    "\n",
    "Keeping the Markov chain model for the tags but conditioning on the __input sequence (\"changing the direction of the arrows\")__ we get a probabilistic model that does not require us to model the distribution of the input: \n",
    "\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1WKQNlnpkcUljDCdxdJL7RuA_2uZZS2U3\"><img src=\"https://drive.google.com/uc?export=view&id=1yzkwMAjdseQW29wKDRTv955ngb_UQyLU\" width=\"450\"></a>\n",
    "\n",
    "(Image source: [HMM, MEMM, and CRF: A comparative analysis](https://medium.com/@Alibaba_Cloud/hmm-memm-and-crf-a-comparative-analysis-of-statistical-modeling-methods-49fc32a73586))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature templates\n",
    "\n",
    "__Switch to discriminative modeling__:\n",
    "- In contrast to our HMM-based solution, can use __all types of useful features__ about the input elements without having to model their interactions\n",
    "\n",
    "For POS-tagging, like in many other sequence labeling scenarios, it is customary to actually condition only on __local features__ of the element to be labeled by considering only a __context window__. \n",
    "\n",
    "The following is a typical POS-tagging feature template for an individual input word $x_i$:\n",
    "\n",
    "- Elements in a context window around $x_i$, e.g. $\\langle x_{i-1}, x_{i}, x_{i+1} \\rangle$ (using the presence of concrete words as features is called __lexicalization__)\n",
    "- __Suffixes__ (of a fixed length) of the context window's elements\n",
    "- __Prefixes__ (of a fixed length) of the context window's elements\n",
    "- __Capitalization__ information of the context window's elements\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1ZRaHHiP2vPbbLNna8SpK5Rz7lOpWJkxg\"><img src=\"https://drive.google.com/uc?export=view&id=1txD5NRw22UmTw8O4LVBQhf-J-kN7RibI\" width=\"450\"></a>\n",
    "\n",
    "__WARNING__: A typical feature template like the above can actually produce __thousands of features__ because the categoricals have to be one-hot encoded. In addition to performance implications this also has consequences for data sparsity: smoothing becomes very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the probabilities\n",
    "\n",
    "Let the sequence of words be $W = w_{1}^{n}$\n",
    "and the sequence of tags $T = t_{1}^{n}$. In an\n",
    "HMM to compute the best tag sequence that maximizes $P(T|W)$ we rely on Bayes‚Äôrule and the likelihood P(W|T):\n",
    "\n",
    "$ \\hat{T}= \\max\\limits_{T}P(T|W)$\n",
    "\n",
    "$ \\hat{T}= \\max\\limits_{T}P(W|T)*P(T)$\n",
    "\n",
    "$ \\hat{T}= \\max\\limits_{T}\\prod_{i}P(word_i|tag_{i})\\prod_{i}P(tag_i|tag_{i-1})$\n",
    "\n",
    "In an MEMM, by contrast, we compute the posterior P(T|W) directly, training it to\n",
    "discriminate among the possible tag sequences:\n",
    "\n",
    "$ \\hat{T}= \\max\\limits_{T}P(W|T)$\n",
    "\n",
    "$ \\hat{T}= \\max\\limits_{T}\\prod_{i}P(tag_i|word_{i}, tag_{i-1})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap - logistic regression\n",
    "want output of probabilities between 0 and 1\n",
    "\n",
    "$0\\leq h_{\\theta} \\leq 1$\n",
    "\n",
    "$h_{\\theta}(x)=g(\\theta^t*x)$\n",
    " \n",
    "and as activation we take the sigmoid or ligistic function\n",
    "\n",
    "$g(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/728/1*Xu7B5y9gp0iL5ooBj7LtWw.png\"><img src=\"https://drive.google.com/uc?export=view&id=1QZ_UdtUIkpJnbtDlWvsdj4J0HdHEIW8s\" width=\"450\"></a>\n",
    "\n",
    "$h_{\\theta}(x)=\\frac{1}{1+e^{\\theta^T*x}}$\n",
    "\n",
    "This can be illustrated as the activation of a single neuron:\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/1001/0*rtJ7w5lrNFwW1Per.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1NrwjXk_HsuflYK4CDBYcmq23IpILTDV8\" width=\"450\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Entropy Regression\n",
    "The main difference is that we now multiple output classes\n",
    "\n",
    "<a href=\"https://i.stack.imgur.com/0rewJ.png\"><img src=\"https://drive.google.com/uc?export=view&id=1t6M-DaC_ZzYgwJAFcZFVpSMlP96oUaXF\" width=\"450\"></a>\n",
    "\n",
    "\n",
    "This is equivalent to:\n",
    "- __Neural network__ with a __multi-class__ output \n",
    "- __Without a hidden layer__ \n",
    "- With a __sigmoid activation__\n",
    "- With a __softmax transformation__ over the output \n",
    "\n",
    "Note: except for the sigmoid and softmax function, it is very close to a linear perceptron with multiple classes as output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for taking the __softmax function__ is to normalize the output in terms of forming a probability distribution with probability mass 1.\n",
    "\n",
    "Works as follows:\n",
    "\n",
    "let $y \\in (1,2,..,c)$ be the class labels\n",
    "\n",
    "and let w(1), w(2),..,w(c) be the weight vector\n",
    "\n",
    "$\\hat{p}=  \\frac{exp(w^T(i)*X_n)}{\\sum\\limits_{j=1}^c exp(w^T(j)*X_n})$\n",
    "\n",
    "where $exp(w^T(i)*X_n)$ \n",
    "\n",
    "is the score for the particular label, also known as scoring function\n",
    "\n",
    "and $\\sum\\limits_{j=1}^c exp(w^T(j)*X_n)$ \n",
    "\n",
    "is the partition function or normalization. Simply adds up all scoring functions to ensure that the total probability mass will be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function\n",
    "\n",
    "let $y_n=1$ be the true label\n",
    "\n",
    "In logistic regression we take the log-loss:\n",
    "\n",
    "$L(w)=-log \\big(\\hat{p} (y_n|x_n)\\big)$\n",
    "\n",
    "which, for our function translates into:\n",
    "\n",
    "$L(w)=-log \\big(  \\frac{exp(w^T(i)*X_n)}{\\sum\\limits_{j=1}^c exp(w^T(j)*X_n)}\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfering Maximum Entropy Regression into a Maximum Entropy Markov Model\n",
    "\n",
    "The above general formula can be adjusted to __account for the feature template__ as well as the __time dependence__.\n",
    "\n",
    "Using an  $f$  feature template function which generates local features for the $i$ index from $X$ and $y_{i-1}$,\n",
    "with __multiclass logistic regression__ (this is where the \"maximum entropy\" in MEMM comes from) we can model each individual\n",
    "$P(y_i \\mid y_{i-1}, X)$ \n",
    "\n",
    "The most likely sequence of tags is then computed by combining these features of the input word $w_i$, its neighbors within l words $w_{i+l}^{i‚àíl}$, and the previous k tags $t_{i‚àí1}^{i‚àík}$\n",
    "as follows (using Œ∏ to refer to feature weights instead of w to avoid the confusion with\n",
    "w meaning words):\n",
    "\n",
    "\n",
    "$ \\hat{T}= \\max\\limits_{T}P(W|T)$\n",
    "\n",
    "$ \\hat{T}= \\max\\limits_{T}\\prod_{i}P(tag_i|word_{i}, tag_{i-1})$\n",
    "\n",
    "$\n",
    "\\hat{T} =  \\frac{ \\sum\\limits_{j} \\exp\\big(\\mathbf \\theta_{j} \\cdot f(t_{i}, w_{i+l}^{i‚àíl}, t_{i‚àí1}^{i‚àík})\\big)}{\\sum\\limits_{t \\in tagest }\\exp\\big(\\sum\\limits_{j}\\mathbf \\theta_{j} \\cdot f(t_{i}, w_{i+l}^{i‚àíl}, t_{i‚àí1}^{i‚àík})\\big)}\n",
    "$\n",
    "\n",
    "where the $\\mathbf \\theta_j$ vectors ($j=1,\\dots,K$) are learned weight vectors for the features provided by $f$ for the $j$-th class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher order MEMM variants \n",
    "Similarly to HMMs, MEMMs it is straightforward to produce higher-order variants, at the expense of costlier inference and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "+ Greedy search is the simplest method and can work reasonably well, but\n",
    "+ similarly to the case of HMMs, the most probable $Y$ sequence for a given $X$ input can be precisely calculated with the Viterbi algorithm.\n",
    "+ If even Viterbi is too slow then beam search can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Since we managed to reduce our original sequence labeling problem to a simple linear classification, we can choose from a variety of learning algorithms, most importantly, we can use\n",
    "\n",
    "+ gradient descent,\n",
    "+ quasi-Newton methods like (L)BFGS\n",
    "+ (as an approximation of the gradient) perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured perceptron\n",
    "\n",
    "Since in practice it works very well and (as we will see), its use can be generalized to other \"structured prediction\" settings, the application of the perceptron algorithm is worth detailing a bit. First, recall the perceptron update rule for multiclass classification:\n",
    "\n",
    "For each $\\langle \\mathbf x, y\\rangle$ data point, if the $\\hat y$ prediction with the current weight is incorrect, then:\n",
    "1. Update the weights for the correct class: $\\mathbf w_y \\leftarrow \\mathbf w_y + \\eta \\mathbf x$\n",
    "2. Update the weights for the incorrect class: $\\mathbf w_\\hat{y} \\leftarrow \\mathbf w_\\hat{y} - \\eta \\mathbf x$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "This can be adapted to the sequence labeling setting as follows: For each $\\langle X, Y\\rangle$ input sequence, correct label sequence data point:\n",
    "\n",
    "1. Calculate the $\\hat Y=\\langle \\hat y_1,\\dots, \\hat y_N \\rangle $  most probable predicted label  sequence using the current weights, e.g., by Viterbi.\n",
    "2. Apply the above perceptron rule for each sequence index $i \\in \\{1,\\dots, N\\}$ using  $\\langle f(\\hat y_{i-1}, X, i), y_i \\rangle$ as the data point and $\\hat y_i$ as the predicted label.\n",
    "\n",
    "As mentioned, in practice, structured perceptron works very well for optimizing MEMMs, especially the weighted variants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMM problems and their solutions\n",
    "\n",
    "Unfortunately, MEMMs have some important limitations:\n",
    "+ __One-directionality__: Although indirectly Viterbi makes it possible for the next tag(s) to have some influence on the current one, this information cannot be part of the features on which the linear classifier is trained.\n",
    "+ __The so-called label bias problem__: Since outgoing state transition probabilities are normalized (add up to 1 since they are conditional probabilities), labels with few outgoing transitions (1 in the extreme case) are preferred to those with a large number of outgoing edges and can be chosen as part of the most probable sequence disregarding observations.\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"http://www.davidsbatista.net/assets/images/2017-11-13-Label_Bias_Problem.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Y1wwj_IbgRI8KIIpMCWJT4Sqb3HyHUJ0\" width=\"800\"></a>\n",
    "\n",
    "Transitions from a given state are competing against each other only.\n",
    "\n",
    "Per state normalization, i.e. sum of transition probability for any state has to sum to 1.\n",
    "\n",
    "MEMM are normalized locally over each observation where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.\n",
    "\n",
    "States with a single outgoing transition effectively ignore their observations.\n",
    "\n",
    "Causes bias: states with fewer arcs are preferred.\n",
    "\n",
    "__Solutions to achieve bidirectionality__\n",
    "+ Making multiple one-directional passes: starting from the second pass later labels can also be used from the previous pass.\n",
    "+ Using both a forward and a backward MEMM, and choosing the higher scoring label-sequence when decoding -- either tag-by-tag or from the two enitre Viterbi decoded label sequences.\n",
    "+ Finally, using bidirectional probabilistic models is also a possibility:\n",
    "    + The Stanford POS tagger uses a special, bidirectional MEMM version, see [Toutanova et al.:  Feature-rich part-of-speech tagging with a cyclic dependency network](https://www.aclweb.org/anthology/N03-1033.pdf)\n",
    "    + A more principled solution is to use a Conditional Random Fields model (CRF), to which we will dedicate a separate section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also\n",
    "- For some more details on MEMM-based taggers see [chapter 8 of Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/8.pdf), on which the present discussion was partly based.\n",
    "- The first version of spaCy used MEMMs and weighted perceptrons for most of its NLP models, see [A Good Part-of-Speech Tagger in about 200 Lines of Python](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python) for some details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear chain conditional random fields (CRFs)\n",
    "\n",
    "The critical difference between CRF and MEMM is that the latter uses __per-state exponential models__ for the conditional probabilities of next states given the current state, whereas CRF uses a __single exponential model to determine the joint probability of the entire sequence__ of labels, given the observation sequence. Therefore, in CRF, the weights of different features in different states compete against each other.\n",
    "\n",
    "This means that in the MEMMs there is a model to compute the probability of the next state, given the current state and the observation. On the other hand __CRF computes all state transitions globally, in a single model__\n",
    "\n",
    "In contrast to HMMs and MEMMs, linear-chain CRFs are undirected graphical models with the following structure:\n",
    "\n",
    "<a href=\"http://www.davidsbatista.net/assets/images/2017-11-13-Conditional_Random_Fields.png\"><img src=\"https://drive.google.com/uc?export=view&id=1qxU2orll4pNMczGWxS0MOMORY_ViMbMi\" width=\"450\"></a>\n",
    "\n",
    "(Image source: [Conditional Random Fields for Sequence Prediction](http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/))\n",
    "\n",
    "Similarly to MEMMs, CRFs are discriminative models that conditionalize on the input, but they are\n",
    "+ undirected, solving the one-directionality problem\n",
    "+ allow observations to \"modulate\" the local probability mass for state transitions, solving the label bias problem.\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=18t3BCoHhsZpdeUI7qy-bnuoweNoQjtzu\"><img src=\"https://drive.google.com/uc?export=view&id=1vcAg3QMZON52d3aNp0WAQa0vhTgalhHq\" width=\"450\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling Linear CRF\n",
    "\n",
    "Suppose we want to classify a whole sequence and take regular classification. Then:\n",
    "\n",
    "$p(\\bar{y}|x)= \\prod_{k=1}^{n} p({y_k}|x_k)=\\prod_{k=1}^{n}\\big( \\frac{exp(a_k*x_k)}{z(x_k)}\\big)$\n",
    "\n",
    "which tells us that the __probability of a sequence of labels__ given that they are independent is simply the __product of these labels__\n",
    "\n",
    "the __product of the exponentials__ is the __expeonentioal of the sum__, so \n",
    "\n",
    "=$\\frac {exp \\sum\\limits_{k=1}^k a_k*x_k} {\\sum\\limits_{k=1}^{n}z(x_k))}$\n",
    "\n",
    "\n",
    "We __adjust__ this to __obtain sequence classification__ that also takes into account the interdependence between particular labels:\n",
    "\n",
    "$p(\\bar{y}|x)=\\frac{exp \\big(\\sum\\limits_{k=1}^k a_k*x_k \\sum\\limits_{k=1}^k V_{y_k,y_{k-1}} \\big)}{ \\sum\\limits_{k=1}^{n}z(x_k)}$\n",
    "\n",
    "where $\\sum\\limits_{k=1}^k a_k*x_k $ describes how likelx $y_k$ is given the input\n",
    "\n",
    "and $\\sum\\limits_{k=1}^k V_{y_k,y_{k-1}}$ is a matrix of $y_k$ followed by $y_{k+1}$ which shows preference of the model for particular pairs of sequences for $y_k$, $y_{k+1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This can be slighly rewritten to point out the general structure:__\n",
    "\n",
    "Define \n",
    "\n",
    "  $F(y,x)= \\big(\\sum\\limits_{k=1}^k a_k*x_k +\\sum\\limits_{k=1}^k V_{y_k,y_{k-1}} \\big)$ as the scoring function\n",
    "\n",
    "and  \n",
    "\n",
    "$Z(x)= \\sum\\limits_{k=1}^{n}z(x_k)= \\sum\\limits_{l=1}^c \\bigg(\\big(\\sum\\limits_{k=1}^k a_k*x_k +\\sum\\limits_{k=1}^k V_{y_k,y_{k-1}} \\big)\\bigg)$ as the partition function\n",
    "\n",
    "so,\n",
    "\n",
    "$P(y,x)=\\frac {exp(F(y,x))} {Z(x)}$\n",
    "\n",
    "which is another way in which these types of models are frequently presented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Calculating the most probable label sequence for a given $X$ input sequence can be done analogously to MEMMs, i.e., with Viterbi or beam search --  the computation of the normalizer factor is not required as it is the same for all alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "The methods used for MEMMs (GD variants, quasi-Newton and structured perceptron) can all be used, but global normalization makes the training significantly slower than in the case of MEMMs, as computing the normalizer requires computing the sum of scores for all possible label variations for the input sequences in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher order variants \n",
    "Similarly to HMMs and MEMMs, there are higher-order CRF variants, see, e.g., [Chuon & Cieu (2014): Conditional Random Field with High-order Dependencies for Sequence Labeling and Segmentation](http://www.jmlr.org/papers/volume15/cuong14a/cuong14a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also\n",
    "\n",
    "+ The paper which introduced CRFs: [Lafferty et al (2001):Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)\n",
    "+ A detailed introduction to CRFs: [Sutton and McCallum: An introduction to CRFs](https://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)\n",
    "+ The most important CRF implementation is the [CRFSuite](http://www.chokkan.org/software/crfsuite/)\n",
    "+ CRFSuite has a Python wrapper as well, see (python-crfsuite)(https://python-crfsuite.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural methods\n",
    "\n",
    "## The standard: word embeddings +  RNN(s) + softmax\n",
    "\n",
    "Starting from the early 2000s, neural network-based solutions started to appear and eventually outperformed the traditional ML-based, manually feature engineered approaches.\n",
    "\n",
    "The most common, standard neural architecture has been to use RNNs (mostly LSTM variants) to classify the tokenized and embedded input text token by token with a softmax output layer, which can be taught, e.g., with the usual cross-entropy loss:\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/1481/1*wf9iOTO853P5ewjPX079RQ.png\"><img src=\"https://drive.google.com/uc?export=view&id=1q42Ab7-I-d5_zqAIXNwdvCMluAeu7Kr6\" width=\"800\"></a>\n",
    "\n",
    "(Image source: [Taming LSTMs](https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e))\n",
    "\n",
    "__Word embeddings__\n",
    "\n",
    "Word embeddings, used as the input of the RNN layer, are __dense representations__ of the __input tokens__ that are typically produced in an unsupervised manner. The details will be discussed later, but it is worth noting that they play the __role of the manually engineered feature vectors__ of the old ML models, although -- at least in the simplest case -- they are context independent, characterizing the token in general, without reflecting its actual context in the input sequence.\n",
    "\n",
    "__Bidirectional RNN layers__\n",
    "\n",
    "One-directionality is an important limitation of individual RNN-layers: the output is based on a one-sided context only. A standard solution to this problem is to use __bidirectional RNN layers__ (or bi-RNNs),  which combine the output of a forward and backward processing RNN at each position:\n",
    "\n",
    "<a href=\"https://www.i2tutorials.com/wp-content/media/2019/05/Deep-Dive-into-Bidirectional-LSTM-i2tutorials.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1_1QDv9PB190BW9janTjzqdMdvT4sLt2g\" width=\"700px\"></a>\n",
    "\n",
    "(Image source: [Deep dive into bidirectional LSTM](https://www.i2tutorials.com/technology/deep-dive-into-bidirectional-lstm/))\n",
    "\n",
    "Naturally, bidirectional RNNs can be stacked similarly to one-directional ones:\n",
    "\n",
    "<a href=\"https://d3i71xaburhd42.cloudfront.net/828dbeb7cf922dc9b6657dd169b8d26d2b58eedb/3-Figure1-1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1dnSzM0YKQOXZJ3zJy2Liwnsu9f9nqu0l\" width=\"500px\"></a>\n",
    "\n",
    "(Image source: [A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering](https://www.semanticscholar.org/paper/A-Long-Short-Term-Memory-Model-for-Answer-Sentence-Wang-Nyberg/828dbeb7cf922dc9b6657dd169b8d26d2b58eedb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-based alternatives\n",
    "\n",
    "In the last few years CNN-based sequence labeling architectures have also played an important role. Their performance can be competitive with RNNs but are typically faster (because of parallelizability). spaCy 2.x versions, for instance, use  dilated 1d convolutions (see https://github.com/explosion/spaCy/issues/1057 for some details):\n",
    "\n",
    "\n",
    "<a href=\"https://d3i71xaburhd42.cloudfront.net/1478075a10ea2e0a1b5bdc170468dcea81e6fcb2/2-Figure1-1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1gCEkzatLbCo7YVWEpe_FBdTg6SAxKH_W\"></a>\n",
    "\n",
    "(Image source: [Strubel et al (2017): Fast and Accurate Entity Recognition with Iterated Dilated Convolutions](https://arxiv.org/pdf/1702.02098.pdf))\n",
    "\n",
    "Dilated convolutions give an especially large context window for classification. In theory, RNNs are not limited to a context window at all, but in practice truncated backpropagation limits RNNs to a window as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural-CRF hybrids\n",
    "\n",
    "The RNN and CNN based architectures mentioned so far __do not model explicitly the interaction between tags__ -- the final tagging decisions are made independently from each other (although, of course, on the basis of context-dependent representations). This fact led to the emergence of hybrid models that contain an __explicit \"inference\" layer on top of RNN or CNN layers which represent the label interactions__, the __typical choice being a CRF__.\n",
    "\n",
    "From the point of view of performance, __neural-CRF hybrids__ represent the state of the art in many sequence labeling tasks, adding a CRF top layer to existing models typically improves results a bit, at the expense of a performance penalty. In fact, hybrid sequence taggers are so common that there is even a dedicated platform to build them: [NCRF++](https://github.com/jiesutd/NCRFpp). NCRF++ supports building models that follow the following general schema:\n",
    "\n",
    "<a href=\"https://d3i71xaburhd42.cloudfront.net/67d40c3f7470287a3bccfccdb506bcb6d522ac8c/2-Figure2-1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1kT4wv0rSyCBEu_FASkBZZf2GbdrhYHxS\" width=\"750px\"></a>\n",
    "\n",
    "(Image from the NCRF++ paper: [NCRF++: An Open-source Neural Sequence Labeling Toolkit](https://arxiv.org/pdf/1806.05626.pdf))\n",
    "\n",
    "## The role of subword/character based embeddings\n",
    "\n",
    "Although our ongoing discussion has concentrated on token/word level models, it is very important to keep in mind that the performance of these models can be only as good as the token-level representations (embeddings) they rely on, especially for unseen/unknown words, for which there is no token level-information in the training data. Modeling tokens on a subword (character, morpheme etc.) level is therefore a crucial part of neural sequence tagging -- we will return to this problem in the word embedding class.\n",
    "\n",
    "## See also\n",
    "\n",
    "+ The textbooks by [Jurafsky & Manning](https://web.stanford.edu/~jurafsky/slp3/9.pdf) and [Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) both contain good introductory chapters on neural sequence tagging.\n",
    "+ The paper [Bai et al. (2018): An Empirical Evaluation of Generic Convolutional and Recurrent Networksfor Sequence Modeling](https://arxiv.org/pdf/1803.01271.pdf) is a useful comparison of the performance of CNNs and RNNs in sequence modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When will we reach 100% accuracy?\n",
    "\n",
    "The current state of the art in POS tagging on the industry standard PTB/WSJ data set is (according to [NLP progress](https://github.com/sebastianruder/NLP-progress/blob/master/english/part-of-speech_tagging.md)) 97.96%, but the TnT HMM tagger was already at 96.7% in 2000 (!), so progress has been far from fast. In a very interesting paper entitled [Part-of-Speech Tagging from 97% to 100%:Is It Time for Some Linguistics?](https://nlp.stanford.edu/pubs/CICLing2011-manning-tagging.pdf) Christopher D. Manning conducted an error analysis and discusses reasons why we will probably __not__ reach 100%.\n",
    "\n",
    "In addition to the fact that the standard data sets actually contain incorrect annotations in certain cases, and in general interannotator agreement itself seems to be around 97%, the most important barrier might be __theoretical__: Manning asks the question \n",
    "\n",
    "> Are part-of-speech labels well-defined discrete properties enabling us to assign each word a single symbolic label?\n",
    "\n",
    "and suggests that the answer is not necessarily affirmative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "377.833px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
