{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax\n",
    "\n",
    "Syntactic theories aim to characterize\n",
    "\n",
    "> the set of rules or principles that govern how words are put together to form phrases, well formed\n",
    "sequences of words. \n",
    "\n",
    "([Koopman et al.: An Introduction to Syntactic Analysis and Theory, p. 1](https://linguistics.ucla.edu/people/stabler/isat.pdf))\n",
    "\n",
    "The most important \"well formed sequences\" in this context are __sentences__: the central goal of syntactic theories for a given language is to find structural rules or principles that characterize/delineate __well formed/grammatical sentences__ of the language in question.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern syntactic theories do this by defining structural rules for generating, and -- in the other direction -- parsing, i.e. structurally describing sentences. A sentence is considered to be syntactically well-formed or __grammatical__ if there is a parse or structural description of it which satisfy the syntactic constraints of the theory in question.\n",
    "\n",
    "Grammaticality doesn't necessarily mean coherence or meaningfulness: To use Chomsky's famous example, the sentence\n",
    "\n",
    "> Colorless green ideas sleep furiously.\n",
    "\n",
    "is totally well formed, but semantically nonsensical, in contrast to the similar\n",
    "\n",
    "> Furiously sleep ideas green colorless.\n",
    "\n",
    "which is ungrammatical as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NLP purposes, we won't be interested so much in deciding whether a sentence is grammatical (although grammatical correction is a valid task), but rather in __parsing__, since parsing can discover structure in sentences which can be highly useful for a variety of tasks: semantic processing, IR, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there exist a plethora of different of syntactic theories, __constituency (aka phrase structure) grammars__ and __dependency grammars__ have been especially important for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constituency grammars\n",
    "\n",
    "(Not totally unexpectedly,) Constituency grammars are based on the concept of a __constituent__, i.e., a word or a group of words that form a \"natural unit\", \"belong together\". This vague notion can be operationalized using various test,  e.g., the phrase \n",
    "\n",
    "> a nice little city\n",
    "\n",
    "is a constituent, e.g., since it can be \n",
    "\n",
    "+ put into various sentence frames like \"I wanted to visit ....\", \"Frankfurt is ...\" \n",
    "+ coordinated: \"Frankfurt is a nice little city and a metropolis at the same time.\"\n",
    "+ substituted by pronouns: \"I have visited a nice little city.\" -> \"I have visited it.\"\n",
    "+ an answer to a question: \"What did you want to visit?\" -- \"A nice little city\"\n",
    "\n",
    "an so on, see [the \"Constituents\" entry of Wikipedia](https://en.wikipedia.org/wiki/Constituent_(linguistics)) for more tests. \n",
    "\n",
    "The theoretical significance of constituents is that smaller ones can be grouped together to build a larger one, for example our \"a nice little city\" can be grouped with \"visited\" to form\n",
    "\n",
    "> visited a nice little city\n",
    "\n",
    "which is a constituent again. \n",
    "\n",
    "Constituency grammars\n",
    "+ categorize constituents, and\n",
    "+ specify rules according to which constituents can be grouped together to build larger ones, eventually building up a whole sentence.\n",
    "\n",
    "### Context-free grammars (CFGs)\n",
    "\n",
    "Context-free grammars are basic, but still very useful constituency grammars, which, because of their simplicity and good computational properties have been frequently used in NLP applications. These grammars can be specified as finite set of __context free production rules__  of the form\n",
    "\n",
    "$\\alpha \\rightarrow \\beta_1 \\dots \\beta_n$\n",
    "\n",
    "where $\\alpha$ is a so called non-terminal symbol (constituent category), while $\\beta_1 \\dots \\beta_n$ are one or more non-terminal or terminal symbols (the latter are words in the language's lexicon). Among the non-terminals there is a distinguished $S$ start/sentence symbol. The rules are context-free, because the production rules for the categories are context independent (there can be only one symbol on the left).\n",
    "\n",
    "An example grammar for an (extremely small) fragment of English:\n",
    "\n",
    "<a href=\"https://www.researchgate.net/profile/Peter_Hellwig/publication/267632391/figure/fig9/AS:668635244789772@1536426473380/A-grammar-fragment-of-English_W640.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1wo__5Zb9BQc4-gljwVsqnUj7lxqzVjhC\"></a>\n",
    "\n",
    "(Example from [Hellwig (2006): Parsing with dependency grammars](https://www.researchgate.net/publication/267632391_Parsing_with_Dependency_Grammars))\n",
    "\n",
    "(The lexicon specifies so called insertion rules in a compressed form: if a $w$ word is in a category $c$ in the lexicon, then $c \\rightarrow w$ is a production rule of the grammar.)\n",
    "\n",
    "__Grammatical (well formed) sentences__\n",
    "\n",
    "The __grammatical (well formed) sentences of a CFG__ are simply those word (terminal symbol) sequences $W=\\langle w_1,\\dots,w_n\\rangle$ for which there exist a derivation sequence $S \\Rightarrow\\dots\\Rightarrow W$ such that each consecutive element pair in the sequence is of the form \n",
    "\n",
    "$$\\gamma_1\\alpha\\gamma_2 \\Rightarrow \\gamma_1\\beta_1 \\dots \\beta_n\\gamma_2$$\n",
    "\n",
    "where "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha \\rightarrow \\beta_1 \\dots \\beta_n$$ is a production rule of the grammar, and $\\gamma_1, \\gamma_2$ are possibly empty sequences of (terminal or non-terminal) symbols. In other words, every grammatical sentence can be __generated__ from the $S$ symbol by steps of replacing symbols according to the production rules. (As most modern formal syntactic theories, CFGs are __generative grammars__.) For example, the sentence \n",
    "\n",
    "> The students love their professors\n",
    "\n",
    "can be derived by the steps\n",
    "\n",
    "S $\\Rightarrow$ NP VP    (R-1)\n",
    "\n",
    "NP VP $\\Rightarrow$  Det Noun VP (R-3)\n",
    "\n",
    "Det Noun VP $\\Rightarrow$  Det Noun Vt NP (R-6)\n",
    "\n",
    "Det Noun Vt NP $\\Rightarrow$ Det Noun Vt Det Noun (R-3)\n",
    "\n",
    "Det Noun Vt Det Noun $\\Rightarrow$ The students love their professors (By repeated application of the lexical insertion rules)\n",
    "\n",
    "Parse trees provide a more perspicuous, essentially equivalent representation of the derivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://drive.google.com/uc?export=view&id=1blCWLNK4_mKoRiTuPGOkhLVa8pQY3omP\"><img src=\"https://drive.google.com/uc?export=view&id=1zLZy4wbAuRgvXP9l9XjbDEdjXg3yWWXk\" width=\"350\"></a>\n",
    "\n",
    "The edges of the tree indicate that the phrases are (direct) constituents of each other, that is, it shows the __constituency relation__ between constituents of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Structural ambiguity__\n",
    "\n",
    "As in the case of other syntactic theories, there is no guarantee that a grammatical sentence has only one valid analysis (parse tree), even if we fix the meaning of the individual words. For example, compare\n",
    "\n",
    "<a href=\"https://www.nltk.org/book/tree_images/ch08-tree-1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1ZyXXD0TWeEqgEfCoKbBWL4LgTwPjw8yi\" width=\"400\"></a>\n",
    "\n",
    "with\n",
    "\n",
    "<a href=\"https://www.nltk.org/book/tree_images/ch08-tree-2.png\"><img src=\"https://drive.google.com/uc?export=view&id=1JQEU6pSg_TfT7FVr4nbgjl58t-NhGFTO\" width=\"400\"></a>\n",
    "\n",
    "(Image from the [NLTK book](https://www.nltk.org/book/ch08.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the syntax of natural languages context-free?\n",
    "\n",
    "While context-free grammars can describe large fragments of natural languages, most linguists think that natural languages are actually not entirely context-free, since there are constructions that can be modeled only by context-sensitive rules. Since parsing context-sensitive languages (where the left side of rules can specify context requirements) is in general an NP-complete task, research has focused on so-called __mildly context sensitive languages__, that allow some context-sensitivity but still can be parsed in polynomial time. See the [Wikipedia entry on mildly context-sensitive languages](https://en.wikipedia.org/wiki/Mildly_context-sensitive_grammar_formalism) for some details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency grammars\n",
    "\n",
    "### Headedness and dependency\n",
    "\n",
    "Although CFGs do not, many important constituency grammars (e.g., [Head-driven Phrase Structure Grammar](https://en.wikipedia.org/wiki/Head-driven_phrase_structure_grammar)) make heavy use of the assumption that every multi-word constituent contains a __head word__,  which determines the type of the constituent, and determines the syntactic organization and properties of the other parts. Continuing our example, the heads of the NPs \"the students\", \"their professor\" would be their nouns \"students\" and \"professors\", the VP \"love their professor\" would be headed by \"love\", which would be the head of the whole sentence as well:\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1msxgf6N62CAuffvbib2aVZEeGQeojTBo\"><img src=\"https://drive.google.com/uc?export=view&id=1oY_mWOMIpFph2FlwAVe2UyFueu6JgzCW\" width=\"350\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of a constituent being \"headed\" by a word leads naturally to a notion of __dependency__ relationship between _words:_ if $c_2$ is a direct constituent of $c_1$ and their (different) heads are $h_2$ and $h_1$, then $h_2$ depends on $h_1$. Accordingly, the dependencies in the sentence would be\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1oTMteU6X05Yn7Nj01ILAqP1LKeqrH4mr\"><img src=\"https://drive.google.com/uc?export=view&id=1i3bR8gi-HRVMVWqIDd2qlW-_swjfogyL\" width=\"400\"></a>\n",
    "\n",
    "### Dependency without constituents\n",
    "\n",
    "Although as we have seen, the notion of dependency can be motivated from a constituency perspective, __dependency grammars__ make the dependency relationship between words the fundamental component of syntactic analysis. Similarly to the constituent-tests we have seen earlier, a number of tests/criterions have been suggested in the literature for dependency between words, e.g. \n",
    "\n",
    "$d$ depends on $h$ when\n",
    "\n",
    "+ $d$ modifies the meaning of $h$, makes it more specific, e.g. \"eats\" --> \"eats bread\", \"eats slowly\" etc.\n",
    "+ there is asymmetric relationship of omissibility between them: $d$ can be omitted from the sentence keeping $h$ but not vice versa\n",
    "\n",
    "Dependency grammars impose some important _global_ constraints on dependencies within a grammatical sentence:\n",
    "+ There is exactly one independent word (the root of the sentence).\n",
    "+ All other words depend directly on exactly one word.\n",
    "\n",
    "As a consequence, the the direct dependency graph of a sentence is a __tree__.\n",
    "\n",
    "In addition to this structure, typical dependency grammars also rely on a finite list of __dependency types__, which label the edges of the dependency trees. For instance, using the dependency types of the [Universal Dependencies treebank project](https://universaldependencies.org/), the spaCy dependency parser parses our example sentence as\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1XI-Usp7HVClkuoqm8-6XEfUV9Azr5gnD\"><img src=\"https://drive.google.com/uc?export=view&id=1F10wvEAxttZ_4gFVwjb9zF6obVk1i2P2\" width=\"500px\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency grammars impose a number of constraints/rules on the permissible dependency relations between words of a sentence, and can therefore define a grammatical/well formed sentence as a sentence which has a permissible dependency tree analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projectivity\n",
    "\n",
    "Some axiomatizations of dependency grammar also include the following, so called __projectivity__ condition on dependency trees:\n",
    "\n",
    "> If a $w$ word depends directly on $h$ and a $w'$ word lies between them in the sentence's word order, then the head of this $w'$ is either $w$ or $h$, or another word between them.\n",
    "\n",
    "Less formally, the projectivity condition states that dependencies are __nested__, there cannot be __crossing__ dependencies between words:\n",
    "\n",
    "<a href=\"http://languagelog.ldc.upenn.edu/myl/McDonaldSattaFig1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1cWO7ve4Yus4q3JVBRldFXsf1lw356DLk\" width=\"600px\"></a>\n",
    "\n",
    "(Image from [Language Log: Non-projective flavor](https://languagelog.ldc.upenn.edu/nll/?p=7851))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projectivity is very important for efficient parsing, since it greatly limits the positions in which the dependents of a word can be found, reducing the search space. Nonetheless, virtually all serious, theoretically motivated dependency grammars accept that some well-formed sentences have non-projective dependency trees, but they consider projectivity the general norm, which is sometimes violated in some special and relatively rare constructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constituency vs dependency grammars in NLP\n",
    "\n",
    "Historically, modern formal linguistics was dominated by constituency/phrase structure focused theories of syntax, and dependency grammar researchers were only a small minority group of dissenters. (Even though the notion of dependency arguably played central role in the ancient Indian and Greek grammars.) As a result, syntactic analysis in early NLP applications was based on constituency grammars (frequently on CFGs), but this gradually changed in the 2000s, when dependency grammars started to dominate the field. Some of the reasons for this change have been computational:\n",
    "\n",
    "+ dependency trees are in many respect simpler structures than phrase structure parse trees\n",
    "\n",
    "but the needs of semantic processing were also important: \n",
    "\n",
    "+ the predicate-argument analysis of sentences provided by dependency graphs is a very good starting point for event or frame-oriented semantic analysis. \n",
    "\n",
    "E.g.,  which representation seems more useful if your task is to extract events and participants from sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://drive.google.com/uc?export=view&id=1blCWLNK4_mKoRiTuPGOkhLVa8pQY3omP\"><img src=\"https://drive.google.com/uc?export=view&id=1zLZy4wbAuRgvXP9l9XjbDEdjXg3yWWXk\" width=\"350\"></a>\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1XI-Usp7HVClkuoqm8-6XEfUV9Azr5gnD\"><img src=\"https://drive.google.com/uc?export=view&id=1F10wvEAxttZ_4gFVwjb9zF6obVk1i2P2\" width=\"500px\"></a>\n",
    "\n",
    "(We will talk about these semantic tasks later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "Given a syntactic theory, the central syntactic task is to analyze the structure of putative sentences by constructing a representation of their structure which shows that they satisfy the conditions/constraints the theory specifies for grammatical sentences. For constituency grammars this means the identification of constituents and the construction of a constituency parse tree, while in the case of dependency grammars the identification of direct dependencies and the construction of a dependency tree.\n",
    "\n",
    "In modern NLP practice, the syntactic theories are typically specified, a least in part, implicitly, using so called __treebanks__, that is, a data set consisting of sentences annotated with their parse trees. This makes parsing a __structured supervised learning task__: given a training set of $\\langle \\mathrm{sentence}, \\mathrm{parse}~\\mathrm{tree} \\rangle$ pairs, learn to predict the parse tree of unseen sentences.\n",
    "\n",
    "## Performance metrics\n",
    "\n",
    "__Constituency grammar parsers__\n",
    "\n",
    "For constituency grammar parser, the standard evaluation is based on the number of correctly identified constituents relative to the ground truth (\"gold corpus\"). Recall and precision can be calculated in the usual way, and from those the F1 score, which is the most common metric:\n",
    "\n",
    "$$\\mathrm{Parser~~precision} = \\frac{\\#(\\mathrm{correct~~parser~~constituents)}}{\\#(\\mathrm{all~~parser~~constituents)}}$$\n",
    "\n",
    "$$\\mathrm{Parser~~recall}=\\frac{\\#(\\mathrm{correct~~parser~~constituents)}}{\\#(\\mathrm{all~~correct~~constituents)}}$$\n",
    "\n",
    "$$\\mathrm{F1~score}=\\frac{2pr}{p+r}$$\n",
    "\n",
    "__Dependency grammar parsers__\n",
    "\n",
    "Here the most common metrics are\n",
    "\n",
    "+ __UAS (Unlabeled Attachment Score)__: The percentage of words that are attached to the correct head.\n",
    "+ __UAS (Labeled Attachment Score)__: The percentage of words that are attached to the correct head with the correct dependency label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow parsing (chunking)\n",
    "\n",
    "Although very efficient algorithms and parsers have been developed for various constituency (phrase structure) grammars including vanilla CFGs to produce full parse trees for sentences, from a practical point of view, shallow parsing or chunking, i.e. the __segmentation of input sentences into various types of (non-overlapping) phrases__ proved to be more important. Shallow parsing tasks differ in their target phrase types, e.g.\n",
    "\n",
    "+ NP-chunking is limited to finding (maximal) NPs in the input, while \n",
    "+ a more complete shallow parsing task may require the identification of a larger number of phrases, e,g. NPs, VPs and PPs.\n",
    "\n",
    "<a href=\"http://www.nltk.org/images/chunk-segmentation.png\"><img src=\"https://drive.google.com/uc?export=view&id=1pUgrhfA04nkg8AYkvJYjWik4kch_vRY5\" width=\"500px\"></a>\n",
    "\n",
    "(Image from the [Natural Language Processing with Python](http://www.nltk.org/book/ch07.html))\n",
    "\n",
    "Although rule-based solutions are available, e.g. the NLTK toolkit contains a chunker module, which segments phrases based on regex rules (see [Natural Language Processing with Python](https://www.nltk.org/book/ch07.html)), modern chunkers are based on statistical models that are trained on annotated corpora. The dominant approach is to treat the problem as a sequence tagging with the so-called __IOB tagging__: if the segment types to be found are $T_1,\\dots,T_n$, then there will be $2n +1$ tags in the task:\n",
    "\n",
    "+ a $B$ (beginning) tag for all segment types ($B-T_1,\\dots, B-T_n)$ indicating the first token of a chunk of a given type,\n",
    "+ an $I$ (inside) tag for all segment types ($I-T_1,\\dots, I-T_n)$ indicating that a token is inside (as second or later element)  a chunk, and, finally\n",
    "+ a unique $O$ tag for tokens that do not belong to any chunk/segment type to be found.\n",
    "\n",
    "<a href=\"https://www.nltk.org/images/chunk-tagrep.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Kdw2DRXjDmevmHQRCILKy6S_YgOxzWcp\" width=\"500px\"></a>\n",
    "\n",
    "For IOB tagging the same types of sequence-tagging models can be used that we discussed with respect to POS-tagging:\n",
    "+ MEMMs\n",
    "+ CRFs \n",
    "+ neural sequence taggers\n",
    "+ neural + CRF hybrids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing\n",
    "\n",
    "Similarly to sequence tagging, dependency parsing algorithms use the same general strategy of breaking down prediction into individual decisions over elements of the structure, in this case about individual dependencies between words. The tricky part is, of course, to ensure that the individual decisions lead to a coherent dependency tree at the end. \n",
    "\n",
    "Dependency parsers typically use one of the following two approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition-based dependency parsing\n",
    "\n",
    "This approach can be seen as a formal model of a parsing process which moves from left to right in the sentence to be parsed and at every step chooses one of the following actions:\n",
    "\n",
    "> + assign the current word as the head of some previously seen word,\n",
    "+ Assign some previously seen word as the head of the current word,\n",
    "+ Or postpone doing anything with the current word, adding it to a store for later processing. ([Jurafsky and Martin, Chapter 15, p. 8](https://web.stanford.edu/~jurafsky/slp3/15.pdf))\n",
    "\n",
    "The formal model of this process consists of the following component:\n",
    "+ a __buffer__, in which the unprocessed tokens of the input are contained\n",
    "+  __a stack__ for current operation and storing postponed elements\n",
    "+ __a dependency graph__, which is being built for the input sentence\n",
    "\n",
    "The model is in a certain __configuration__ at every step of the process:\n",
    "\n",
    "<a href=\"https://miro.medium.com/max/494/1*rf-PcmxwvdnblDNDxw1Bhg.png\"><img src=\"https://drive.google.com/uc?export=view&id=1t_bK7ublj3XkPcKqpMiqBMQ6fC0Uc733\"></a>\n",
    "\n",
    "(Image from [Jurafsky and Martin, Chapter 15](https://web.stanford.edu/~jurafsky/slp3/15.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsing process starts with the configuration in which \n",
    "+ the buffer contains all words of the input,\n",
    "+ the stack contains the single root node of the dependency graph\n",
    "+ and the dependency graph is empty (contains no dependency edges)\n",
    "\n",
    "At every step, one of the permitted configuration manipulating actions (configuration transitions) are performed. The permitted actions vary, a very simple set of actions is used in the so called \"__arc standard__\" approach:\n",
    "\n",
    "+ __left arc with label l__: add a head-dependent edge with label l between words s1 and s2 on the top of the stack, and remove s2 (whose head has been found)\n",
    "+ __right arc with label l__: add a head-dependent edge with label l between words s2 and s1 on the top of the stack, and remove s1 (whose head has been found)\n",
    "+ __shift__: remove the first word w1  from the buffer and put it on the top of the stack.\n",
    "\n",
    "It is easy to see that the process is guaranteed to end after a finite number of steps, when we reach a configuration in which the  buffer is empty and the created dependency graph is a well-formed dependency tree for the whole input.\n",
    "\n",
    "An example run from [Jurafsky and Martin, Chapter 15](https://web.stanford.edu/~jurafsky/slp3/15.pdf) (edge labels are omitted):\n",
    "\n",
    "<a href=\"https://d3i71xaburhd42.cloudfront.net/273f54ea6f3631a78d9dd442609bb2033cfb1ffe/10-Figure14.7-1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1MW7hmqPNSqtI56zW_BR5Y6f2ol_VIBFU\" width=\"750px\"></a>\n",
    "\n",
    "#### Choosing the right action\n",
    "\n",
    "The big question is obviusly how does a parser know which action to choose in a certain configuration? The answer is that the model has to act as a __classifier__ over the model configurations: it has to assign the correct action/transition to any valid configuration. If there are $n$ labels then there will be $2n+1$ actions/classes.\n",
    "\n",
    "In order to have training data for this classifier, dependency treebank annotations have to be turned into supervised data sets containing (parser configuration, correct action) pairs, in other words treebanks have to be turned into a data set of the actions of a parsing __oracle__, which always chooses the right action.\n",
    "\n",
    "#### Converting a dependency tree into a series of \"oracle actions\"\n",
    "\n",
    "Fortunately, given the correct dependency graph, the configurations and actions of the \"oracle\" can be reconstructed using a straightforward algorithm:\n",
    "\n",
    "- (obviously) start with a stack containing only root and a buffer with the full input;\n",
    "- choose the \"left arc\" action with the correct label if it leads to a correct edge, else\n",
    "- choose the \"right arc\" action with the correct label if (i) it leads to a correct edge (ii) all dependencies with s1 as head were already added to to the dependency graph;\n",
    "- otherwise choose shift.\n",
    "\n",
    "#### Alternative action/transition systems\n",
    "\n",
    "Arc-standard is not the only transition system used for transition-based parsers, several alternatives have been developed over the time, most importantly the [arc-eager transition system](https://natural-language-understanding.fandom.com/wiki/Arc-eager_dependency_parsing) which can greatly simplify the derivations of certain parse trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The problem of non-projectivity__\n",
    "\n",
    "An important feature of the arc-standard and arc-eager systems is that they can produce only projective trees, although \n",
    "many languages use (and their treebanks contain) a considerable amount of non-projective sentences:\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1sDfLuEF9y_ITTvVlYO95KByK-zK6Tn30\"><img src=\"https://drive.google.com/uc?export=view&id=1t-YyFzV9SfFjWpbEaOQKWOx3dmVFVbOk\" width=\"400px\"></a>\n",
    "\n",
    "(Image from the presentation [Beyond MaltParser](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.269.2400&rep=rep1&type=pdf) by Joakim Nivre)\n",
    "\n",
    "__Pseudo-projective parsing__\n",
    "\n",
    "Within the transition-based parsing paradigm, non-projectivity can be handled either by using alternative transition-systems that do allow (limited types of) non-projective edges, or, and this is the more typical solution, by so-called __pseudo-projective__ dependency parsing, in which a projective-only transition-system is used, and non-projective graphs are mapped to projective-ones by special graph transformations. In the training phase, the whole training set is \"projectivized\", and the parser is trained on the transformed data set. In prediction/inference, the inverse transformation is applied to the parser's output to get the final (possibly projective results). See, e.g., [Nivre and Nilsson: Pseudo-projective parsing (2005)](https://www.aclweb.org/anthology/P05-1013.pdf) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier features and architectures\n",
    "\n",
    "Transition-based dependency parsers are, as our discussion showed so far, classifiers over stack/buffer/partial dependency graph configurations. Although there are not too many classes, the input to be classified has a complicated structure, so feature extraction is an important part of developing a high-performance transition based parser.\n",
    "\n",
    "Although details differ, similarly to sequence tagging, before DL-based solutions the classifiers in transition-based dependency parsers used complex, manually engineered feature templates that contained features based on the top words on the stack and the first few words in the buffer. E.g., the perceptron-based parser presented in [Liang et al (2009): Bilingually-constrained  (monolingual)  shift-reduceparsing](http://delivery.acm.org/10.1145/1700000/1699668/p1222-huang.pdf) used the following feature templates:\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1KiFMkV5V7mHo850-eMLhl0emSioiP-aH\"><img src=\"https://drive.google.com/uc?export=view&id=1Y7elRJ2Ht4VWsLUwoqEU79sY66qowDJi\" width=\"800\"></a>\n",
    "\n",
    "(Table from the paper, p. 4.)\n",
    "\n",
    "Similarly to sequence labeling, the problems with manual feature engineering and data sparsity led to the development of neural parsers, which rely on embeddings for classification. The architectures are analogous to the ones we discussed with respect to sequence tagging. A simple but representative example is provided by the [Stanford neural dependency parser](https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf), which uses a one-hidden layer feed-forward network over word and POS-embeddings from selected positions in the input configuration:\n",
    "\n",
    "<a href=\"https://d3i71xaburhd42.cloudfront.net/a14045a751f5d8ed387c8630a86a3a2861b90643/4-Figure2-1.png\"><img src=\"https://drive.google.com/uc?export=view&id=1agXGcZIPe263KL_f7DmjE55typ8IJubS\" width=\"600px\"></a>\n",
    "\n",
    "(Image from the [paper](https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf))\n",
    "\n",
    "More recently, more complex architectures have been used, e.g. spaCy 2 uses a CNN for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph-based dependency parsing\n",
    "\n",
    "In contrast to transition-based dependency parsing, which decomposes scoring a dependency-graph in terms of scoring the steps of a somewhat complicated _graph building process,_ graph-based parsers score the graphs themselves and try to find the dependency graph with the maximal score:\n",
    "\n",
    "$$\\hat g =\\underset{g\\in G}{\\operatorname{argmax}}S(g)$$\n",
    "\n",
    "A simple but efficient and surprisingly well performing approach is\n",
    "+ scoring all the possible edges individually (this requires scoring $n(n-1)l$ directed edges if there are $n$ tokens and $l$ labels), and then\n",
    "+ find the (correctly directed) tree with the largest sum total score.\n",
    "\n",
    "This means, of course, that we chose the simplest possible factorization of the graph score: our assumption is that it is just the sum of the edge scores:\n",
    "\n",
    "$$\n",
    "S(g) = \\sum_{e\\in g} S(e)\n",
    "$$\n",
    "\n",
    "for rather obvious reasons, this way of scoring a graph is called the edge- or arc-factored approach.\n",
    "\n",
    "#### Finding the maximally scoring tree\n",
    "\n",
    "A brute-force search over all possible graphs would be obviously unfeasible. Fortunately, there are some fast algorithms for finding the maximally scoring tree (the so-called __maximum spanning tree__) which we are after. \n",
    "\n",
    "The basic idea is a greedy search: having calculated the score of all possible edges, build a graph by choosing for each node an incoming edge with the maximal score among the incoming edges (except the root):\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1HXObh4TYS5ywXEufTQIbyqHqKDwfgvbS\"><img src=\"https://drive.google.com/uc?export=view&id=1fvVPfQg6Sh_2v6mXuZurqbZk4zhF3_ZV\" width=\"500\"></a>\n",
    "\n",
    "(Image from [Jurafsky & Martin, chap. 15](https://web.stanford.edu/~jurafsky/slp3/15.pdf))\n",
    "\n",
    "__Removing cycles__\n",
    "\n",
    "The \"slight\" problem with greedily choosing the edges is that it finds the right solution, __except when it leads to a graph with circles__. So we have to find a way of finding the maximally scoring _circle-less_ graph if the greedy graph contains circles. The __Chu-Liu Edmonds  algorithm__ does exactly that.\n",
    "\n",
    "It starts with a score _normalization_ step, in which for each node we subtract the maximal score among the incoming edges from the score of each incoming edge. This results in 0 scores for all edges in the greedy graph (and negative scores for others), but does not change the relative scores of the graphs we are interested in, because we subtract the same value from all of them.\n",
    "\n",
    "The second step is to choose a circle in the greedy graph and collapse it into a single point (edges within the circle are removed). Now if we knew the maximum spanning tree for this reduced graph then we would be finished, because restoring the circle we can easily determine which edge of the circle has to be deleted in the original setting. Consequently we managed to reduce our problem to the same problem but for a strictly smaller graph, that is, we have a good recursive algorithm to find the maximum spanning tree (since sooner or later we will reach a circle-less greedy graph, at worst when only one node remains.)\n",
    "\n",
    "An example of one step of the algorithm:\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1pTJ_5qDdJW5QSbl31BceuLYGwkPZ_4HQ\"><img src=\"https://drive.google.com/uc?export=view&id=1yfY32RCYIKW1ufhIj_rKmh7HJ7g9MNWg\" width=\"90%\"></a> \n",
    "\n",
    "(Image from [Jurafsky & Martin, chap. 15](https://web.stanford.edu/~jurafsky/slp3/15.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge scoring features and architectures\n",
    "\n",
    "Graph-based dependency parsers are regressors: they have to produce scores for the possible edges between the input tokens. The used feature templates are analogous to those in transition-based parsers:\n",
    "\n",
    "- the dependent and its affixes, POS etc.\n",
    "- the head and its affixes, POS etc.\n",
    "- the edge label\n",
    "- the relationship between the head and the dependent in the sentence, e.g. their distance\n",
    "- for neural architectures, embeddings for the nodes and the label of the edge\n",
    "\n",
    "Also analogously to the transition-based case, both classic ML and neural graph-based parsers have been developed over the years, the highest performing parsers being RNN-based. An important aspect of the latest architectures, introduced by the paper [Dozat & Manning (2017): Deep Biaffine Attention for Neural Dependency Parsing](https://arxiv.org/pdf/1611.01734) is that they use different sets of embeddings for the head and dependent representations of the same words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition-based vs graph-based parsing\n",
    "\n",
    "There are important trade-offs between the two approaches:\n",
    "\n",
    "+ __Complexity__: the time-complexity of transition-based parsers is typically linear with respect to the length of input, while graph-based parsers precompute scores for all possible edges, so they start with an $O(n^2 l)$ operation ($n$ is the number of tokens and $l$ the number of labels) and the time of finding the maximum spanning tree is added to this. Even if we handle finding labels as a separate task the $O(n^2)$ complexity is unescapable.\n",
    "\n",
    "+ __Non-projectivity__: as we have seen, non-projectivity is a serious problem for the most wide-spread transition systems which needs special treatment. Graph-based approaches don not suffer from this problem.\n",
    "\n",
    "+ __Performance__: Transition-based systems tend to have problems with long-distance dependencies, graph-based models do not have this performance issue. As a consequence, the dependency parser leader boards are dominated by graph-based systems.\n",
    "\n",
    "## See also\n",
    "\n",
    "+ The above discussion of dependency parsing is very heavily indebted and closely follows that of [Jurafsky & Martin, chap. 15](https://web.stanford.edu/~jurafsky/slp3/15.pdf) which is well worth reading in full.\n",
    "+ A more technical, very detailed description of dependency can be found in Chapter 11 of [Eisenstein's NLP introduction](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
