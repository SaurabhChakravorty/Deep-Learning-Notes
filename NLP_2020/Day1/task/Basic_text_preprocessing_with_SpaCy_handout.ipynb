{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the SpaCy pipeline\n",
    "\n",
    "This task is aiming to demonstrate the tokenization capabilites of [SpaCy](https://spacy.io/), as well as to serve as an introduction to the pipeline's capabilities combined with [rule based matching](https://spacy.io/usage/rule-based-matching).\n",
    "\n",
    "Our goal will be to process the demonstration text, as well as to correct for some peculiarities, like special pronunciation marks, wide-spread abbreviations and foreign language insertions into our text.\n",
    "\n",
    "It is mandatory, to stick to SpaCy based pipeline operations so as to make our analysis reproducible by running the pipeline on other texts presumably coming from the same corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our demonstration text\n",
    "\n",
    "Original from [Deutsche Sprache](https://de.wikipedia.org/wiki/Deutsche_Sprache) Wikipedia entry - with some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:03.801971Z",
     "start_time": "2019-11-08T11:44:03.798661Z"
    }
   },
   "outputs": [],
   "source": [
    "text= '''Die deutsche Sprache bzw. Deutsch ([dɔʏ̯t͡ʃ]; abgekürzt dt. oder dtsch.) ist eine westgermanische Sprache.\n",
    "\n",
    "And this is an English sentence inbetween.\n",
    "\n",
    "Ihr Sprachraum umfasst Deutschland, Österreich, die Deutschschweiz, Liechtenstein, Luxemburg, Ostbelgien, Südtirol, das Elsass und Lothringen sowie Nordschleswig. Außerdem ist sie eine Minderheitensprache in einigen europäischen und außereuropäischen Ländern, z. B. in Rumänien und Südafrika, sowie Nationalsprache im afrikanischen Namibia.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "\n",
    "After installing SpaCy, let us demonstrate it's basic usage by analysing our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:04.978496Z",
     "start_time": "2019-11-08T11:44:03.810849Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tabulate >> /dev/null\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the German language models for Spacy\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, we installed SpaCy, but do we have a model for German?\n",
    "# Something has to be done here to get it!\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.365430Z",
     "start_time": "2019-11-08T11:44:04.981233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please do the appropriate imports for SpaCy and it's rule based Matcher class!\n",
    "\n",
    "....\n",
    "\n",
    "# Please don't forget to instantiate the language model that we will use later on for analysis\n",
    "\n",
    "nlp=...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.385990Z",
     "start_time": "2019-11-08T11:44:06.366784Z"
    }
   },
   "outputs": [],
   "source": [
    "# And please use the model to analyse the text from above!\n",
    "\n",
    "doc=...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for nice printout\n",
    "\n",
    "We just define some helper functions for nice printout. Nothing to do here, except to observe the ways one can iterate over a corpus or sentence, as well as the nice output of [Tabulate](https://bitbucket.org/astanin/python-tabulate/src/master/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.395057Z",
     "start_time": "2019-11-08T11:44:06.387362Z"
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_sentences(doc):\n",
    "    for sentence in doc.sents:\n",
    "        print(sentence,\"\\n\")\n",
    "\n",
    "def print_tokens_for_sentence(doc,sentence_num, stopwords=False):\n",
    "    attribs=[]\n",
    "    for token in list(doc.sents)[sentence_num]:\n",
    "        if token.has_extension(\"is_lemma_stop\"):\n",
    "            if stopwords and token._.is_lemma_stop:\n",
    "                pass\n",
    "            else:\n",
    "                attribs.append([token.text, token.lemma_, token.pos_])\n",
    "        else:\n",
    "            attribs.append([token.text, token.lemma_, token.pos_])\n",
    "    print(tabulate(attribs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.401099Z",
     "start_time": "2019-11-08T11:44:06.397107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die deutsche Sprache bzw. Deutsch ([dɔʏ̯t͡ʃ]; abgekürzt dt. oder dtsch.) ist eine westgermanische Sprache.\n",
      "\n",
      " \n",
      "\n",
      "And this is an English sentence inbetween.\n",
      "\n",
      " \n",
      "\n",
      "Ihr Sprachraum umfasst Deutschland, Österreich, die Deutschschweiz, Liechtenstein, Luxemburg, Ostbelgien, Südtirol, das Elsass und Lothringen sowie Nordschleswig. \n",
      "\n",
      "Außerdem ist sie eine Minderheitensprache in einigen europäischen und außereuropäischen Ländern, z. B. in Rumänien und Südafrika, sowie Nationalsprache im afrikanischen Namibia. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_sentences(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.410817Z",
     "start_time": "2019-11-08T11:44:06.404499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------  -------------------  -----\n",
      "Außerdem             Außerdem             ADV\n",
      "ist                  sein                 AUX\n",
      "sie                  ich                  PRON\n",
      "eine                 einen                DET\n",
      "Minderheitensprache  Minderheitensprache  NOUN\n",
      "in                   in                   ADP\n",
      "einigen              einig                DET\n",
      "europäischen         europäisch           ADJ\n",
      "und                  und                  CONJ\n",
      "außereuropäischen    außereuropäisch      ADJ\n",
      "Ländern              Land                 NOUN\n",
      ",                    ,                    PUNCT\n",
      "z.                   z.                   ADP\n",
      "B.                   B.                   NOUN\n",
      "in                   in                   ADP\n",
      "Rumänien             Rumänien             PROPN\n",
      "und                  und                  CONJ\n",
      "Südafrika            Südafrika            PROPN\n",
      ",                    ,                    PUNCT\n",
      "sowie                sowie                CONJ\n",
      "Nationalsprache      Nationalsprache      NOUN\n",
      "im                   im                   ADP\n",
      "afrikanischen        afrikanisch          ADJ\n",
      "Namibia              Namibia              PROPN\n",
      ".                    .                    PUNCT\n",
      "-------------------  -------------------  -----\n"
     ]
    }
   ],
   "source": [
    "print_tokens_for_sentence(doc,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching \"zum Beispiel\"\n",
    "\n",
    "We are a bit frustrated, that the standard analysis pipeline does not know, that in German, \"z. B.\" is the abbreviation of \"zum Beispiel\" (like eg. is for \"for example\"), thus we would like to correct this.\n",
    "\n",
    "Our approach is to extend the pipeline and do a matching, whereby we replace the `lemma` form of \"z. B.\" to the appropriate long form.\n",
    "\n",
    "**IMPORTANT** design principle by SpaCy is, that one **always keeps the possibility to restore the original text**, so we are **NOT to modify `token.text`**. In the analysed form, we can do whatever we want.\n",
    "\n",
    "It is typical to add layers to the pipeline which modify the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, we will use rule based matching to achieve our goals.\n",
    "\n",
    "A detailed description on rule based matching in SpaCy can be found [here](https://spacy.io/usage/rule-based-matching), or [here](https://medium.com/@ashiqgiga07/rule-based-matching-with-spacy-295b76ca2b68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the matcher\n",
    "\n",
    "With the help of rule based matching we create a matcher that reacts to the presence of \"z. B.\" exactly, then we use this matcher to define a pipeline step, that after matching, replaces the lemmas of the tokens \"z.\" and \"B.\" to  their full written equivalent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.417129Z",
     "start_time": "2019-11-08T11:44:06.412535Z"
    }
   },
   "outputs": [],
   "source": [
    "zb_matcher = ... # Please instantiate a matcher with the appropriate parameters - think about all the words of the corpus...\n",
    ".... # Please add an appropriate pattern to the matcher to match \"z. B.\"\n",
    "\n",
    "def zb_replacer(doc):\n",
    "    matched_spans = []\n",
    "    # Please use the matcher to get matches!\n",
    "    matches = ....\n",
    "    # Plsease iterate over the matches!\n",
    "    for ....:\n",
    "        span = ... # get the span of text based on the matches coordinates!\n",
    "        matched_spans.append(span)\n",
    "        print(\"ZB MATCH!!!\")\n",
    "\n",
    "    # Please iterate over matched spans\n",
    "    for ....:  \n",
    "        # And replace their lemmas to the appropriate ones!\n",
    "        # Please observe, that you don't have the ID of the desired lemmas, just the their string form.\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register it to the pipeline\n",
    "\n",
    "After creating this processing step, we register it to be part of the pipeline and then run our analysis again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.421259Z",
     "start_time": "2019-11-08T11:44:06.418628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plase register the new zb_replacer to the pipeline!\n",
    "# Think about, where to place it!\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-do the analysis and observe results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.442000Z",
     "start_time": "2019-11-08T11:44:06.422574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.448772Z",
     "start_time": "2019-11-08T11:44:06.443720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------  -------------------  -----\n",
      "Außerdem             Außerdem             ADV\n",
      "ist                  sein                 AUX\n",
      "sie                  ich                  PRON\n",
      "eine                 einen                DET\n",
      "Minderheitensprache  Minderheitensprache  NOUN\n",
      "in                   in                   ADP\n",
      "einigen              einig                DET\n",
      "europäischen         europäisch           ADJ\n",
      "und                  und                  CONJ\n",
      "außereuropäischen    außereuropäisch      ADJ\n",
      "Ländern              Land                 NOUN\n",
      ",                    ,                    PUNCT\n",
      "z.                   zum                  ADP\n",
      "B.                   Beispiel             NOUN\n",
      "in                   in                   ADP\n",
      "Rumänien             Rumänien             PROPN\n",
      "und                  und                  CONJ\n",
      "Südafrika            Südafrika            PROPN\n",
      ",                    ,                    PUNCT\n",
      "sowie                sowie                CONJ\n",
      "Nationalsprache      Nationalsprache      NOUN\n",
      "im                   im                   ADP\n",
      "afrikanischen        afrikanisch          ADJ\n",
      "Namibia              Namibia              PROPN\n",
      ".                    .                    PUNCT\n",
      "-------------------  -------------------  -----\n"
     ]
    }
   ],
   "source": [
    "print_tokens_for_sentence(doc,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are those ugly pronunciation signs doing there?\n",
    "\n",
    "OK, so far so good. Let's observe, what is the problem with the first sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.473528Z",
     "start_time": "2019-11-08T11:44:06.450123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.484081Z",
     "start_time": "2019-11-08T11:44:06.476136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------  ---------------  -----\n",
      "Die              der              DET\n",
      "deutsche         deutsch          ADJ\n",
      "Sprache          Sprache          NOUN\n",
      "bzw.             beziehungsweise  CONJ\n",
      "Deutsch          Deutsch          NOUN\n",
      "(                (                PUNCT\n",
      "[                [                NOUN\n",
      "dɔʏ̯t͡ʃ            dɔʏ̯t͡ʃ            PROPN\n",
      "]                ]                PROPN\n",
      ";                ;                PUNCT\n",
      "abgekürzt        abkürzen         VERB\n",
      "dt               dt               PROPN\n",
      ".                .                NOUN\n",
      "oder             oder             CONJ\n",
      "dtsch            dtsch            ADJ\n",
      ".                .                PUNCT\n",
      ")                )                PUNCT\n",
      "ist              sein             AUX\n",
      "eine             einen            DET\n",
      "westgermanische  westgermanische  ADJ\n",
      "Sprache          Sprache          NOUN\n",
      ".                .                PUNCT\n",
      "                                  SPACE\n",
      "---------------  ---------------  -----\n"
     ]
    }
   ],
   "source": [
    "print_tokens_for_sentence(doc,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, poor pipeline can not really cope with the pronunciation markings of the phonetic alphabet, and thus thinks, that the signs are representing a foreign proper noun. \n",
    "\n",
    "We would like to remedy this, and since we do expect further texts from the corpus to contain these inserted phonetics, we would like to match, merge and replace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building up matcher for PRONUNCIATION\n",
    "\n",
    "To be more specific, we again first build up a matcher, that aims at the \"square brackets\" markings around the pronunciation. The task is to match everything between square brackets, or to be more specific: **everything that starts with an opening square bracket, and finishes with \";\"**.\n",
    "\n",
    "This matcher can then be used to:\n",
    "\n",
    "1. Merge the resulting matching `span` into one token\n",
    "2. Replace the token's lemma to \"PRONUNCIATION\"\n",
    "\n",
    "For this to be achievable, we have to first register \"PRONUNCIATION\" as part of the vocabulary, moreover mark it as [\"stopword\"](https://en.wikipedia.org/wiki/Stop_words). (More on SpaCy's stopword handling [here](https://medium.com/@makcedward/nlp-pipeline-stop-words-part-5-d6770df8a936)) See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.497568Z",
     "start_time": "2019-11-08T11:44:06.486489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please instantiate and build the matcher as before with the appropriate pattern!\n",
    "# Make it so, that the pattern will match ALL future pronunciations, not just the present one!\n",
    "....\n",
    "\n",
    "\n",
    "# We set the properties for the new word \"PRONUNCIATION\"\n",
    "lex = nlp.vocab['PRONUNCIATION']\n",
    "lex.is_oov = False\n",
    "lex.is_stop = True\n",
    "\n",
    "def pronunciation_replacer(doc):\n",
    "    \n",
    "    # Using the template above, please build a pronunciation replacer, that\n",
    "    # 1. gets the matches\n",
    "    # 2. merges them into one\n",
    "    # 3. Replaces their lemma string and lemma ID\n",
    "    # 4. sets it's POS to \"NOUN\"\n",
    "    ....\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp.add_pipe(pronunciation_replacer, after=\"zb_replacer\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.623259Z",
     "start_time": "2019-11-08T11:44:06.500096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n",
      "PRONUNCIATION MATCH!!!\n",
      "---------------  ---------------  -----\n",
      "Die              der              DET\n",
      "deutsche         deutsch          ADJ\n",
      "Sprache          Sprache          NOUN\n",
      "bzw.             beziehungsweise  CONJ\n",
      "Deutsch          Deutsch          NOUN\n",
      "(                (                PUNCT\n",
      "[dɔʏ̯t͡ʃ];         PRONUNCIATION    NOUN\n",
      "abgekürzt        abkürzen         VERB\n",
      "dt               dt               NOUN\n",
      ".                .                NOUN\n",
      "oder             oder             CONJ\n",
      "dtsch            dtsch            ADJ\n",
      ".                .                PUNCT\n",
      ")                )                PUNCT\n",
      "ist              sein             AUX\n",
      "eine             einen            DET\n",
      "westgermanische  westgermanische  ADJ\n",
      "Sprache          Sprache          NOUN\n",
      ".                .                PUNCT\n",
      "                                  SPACE\n",
      "---------------  ---------------  -----\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)\n",
    "print_tokens_for_sentence(doc,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, we decide, we would not want to include the pronunciation tokens in our view. So we have to mark them as wtopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering PRONUNCIATION as a stopword\n",
    "\n",
    "Stopwords are typically those words, which do not contribute to the meaning of the sentence, are just there for syntactic reasons. There is a vague running list of these for languages. We will use and extend the German one in SpaCy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.627929Z",
     "start_time": "2019-11-08T11:44:06.625286Z"
    }
   },
   "outputs": [],
   "source": [
    "# import stop words from GERMAN language data\n",
    "....\n",
    "\n",
    "# Add PRONUNCIATION to stopwords\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we will only be able to manipulate the lemmas of the pronunciation markings, we would have to let SpaCy know, that - in contrast to the default behavior, where stopwords are filtered on `text` level, we would like to have a new property for words, that is based on `lemma` level stopword filtering.\n",
    "\n",
    "For these we will use extensions!\n",
    "\n",
    "For more info please see [here](https://spacy.io/api/token#set_extension)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.635721Z",
     "start_time": "2019-11-08T11:44:06.629472Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Please define a function (or lambda expression!) that checks if a Token, or its lower case for, \n",
    "# OR it's lemma string is contained it he stopword list above.\n",
    "stop_words_getter ...\n",
    "\n",
    "# Set the above defined function as a extension for Token under the name \"is_lemma_stop\" as a getter!\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.672287Z",
     "start_time": "2019-11-08T11:44:06.637068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n",
      "PRONUNCIATION MATCH!!!\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.677941Z",
     "start_time": "2019-11-08T11:44:06.673660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------  ---------------  -----\n",
      "deutsche         deutsch          ADJ\n",
      "Sprache          Sprache          NOUN\n",
      "bzw.             beziehungsweise  CONJ\n",
      "Deutsch          Deutsch          NOUN\n",
      "(                (                PUNCT\n",
      "abgekürzt        abkürzen         VERB\n",
      "dt               dt               NOUN\n",
      ".                .                NOUN\n",
      "dtsch            dtsch            ADJ\n",
      ".                .                PUNCT\n",
      ")                )                PUNCT\n",
      "westgermanische  westgermanische  ADJ\n",
      "Sprache          Sprache          NOUN\n",
      ".                .                PUNCT\n",
      "                                  SPACE\n",
      "---------------  ---------------  -----\n"
     ]
    }
   ],
   "source": [
    "print_tokens_for_sentence(doc,0, stopwords=True)\n",
    "\n",
    "assert len(list(doc.sents)[0]) == 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language detection\n",
    "\n",
    "We could also observe, that there is some English text inbetween our nice German sentences. We would like to detect foreign sentences and by later processing, ignore / skip them.\n",
    "\n",
    "For this to be achievable, we need some language detection capabilities.\n",
    "\n",
    "Luckily enough, we can make it part of our pipeline via [this extension](#https://spacy.io/universe/project/spacy-langdetect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.709209Z",
     "start_time": "2019-11-08T11:44:06.679245Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spacy-langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.732209Z",
     "start_time": "2019-11-08T11:44:07.714434Z"
    }
   },
   "outputs": [],
   "source": [
    "#Please import the language detector!\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding language detection to our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.736304Z",
     "start_time": "2019-11-08T11:44:07.733828Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please register it to the pipeline as the final step of processing!\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.776361Z",
     "start_time": "2019-11-08T11:44:07.737797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZB MATCH!!!\n",
      "PRONUNCIATION MATCH!!!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:08.097242Z",
     "start_time": "2019-11-08T11:44:07.780914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------  ---  -----------------------------------------------\n",
      "[Die, deutsche, Sprache, bzw., Deutsch]          ...  {'language': 'de', 'score': 0.9999968355426437}\n",
      "[And, this, is, an, English]                     ...  {'language': 'en', 'score': 0.9999969089435032}\n",
      "[Ihr, Sprachraum, umfasst, Deutschland, ,]       ...  {'language': 'de', 'score': 0.9999950244100722}\n",
      "[Außerdem, ist, sie, eine, Minderheitensprache]  ...  {'language': 'de', 'score': 0.999997164861766}\n",
      "-----------------------------------------------  ---  -----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "attribs = []\n",
    "for sentence in doc.sents:\n",
    "    attribs.append([list(sentence)[:5],\"...\", sentence._.language])\n",
    "print(tabulate(attribs))\n",
    "\n",
    "# Please observe how one accesses anextension!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating final generator for cleaned text\n",
    "\n",
    "Typically for a later stage of NLP, we would like to have a generator like function, which allows us to iteratively access the corpus, albeit in it's cleaned and encoded form. Integer encoding (as well as one hot encoding) are quite typical representations of text.\n",
    "\n",
    "In this spirit, we would like to implement a generator, that gives back an **array of lemmas OR lemma IDs for each sentence in the corpus, filtering out non-German sentences and punctuation / space marks**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:08.105282Z",
     "start_time": "2019-11-08T11:44:08.101443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please implement a generator function that yields the text of the corpus as lists of sentences\n",
    "# Based on the parameters either as a list of strings or a list of IDs\n",
    "# It should filter out non-German sentences\n",
    "# as well as topwords based on lemmas\n",
    "# and punctuation and \"space like\" characters!\n",
    "\n",
    "def sentence_generator(doc, ids=False):\n",
    "    \n",
    "    ....\n",
    "   \n",
    "    yield out_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:08.144266Z",
     "start_time": "2019-11-08T11:44:08.106651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deutsch', 'Sprache', 'beziehungsweise', 'Deutsch', 'abkürzen', 'dt', '.', 'dtsch', 'westgermanische', 'Sprache'] \n",
      "\n",
      "['Sprachraum', 'umfasst', 'Deutschland', 'Österreich', 'Deutschschweiz', 'Liechtenstein', 'Luxemburg', 'Ostbelgien', 'Südtirol', 'Elsass', 'Lothringen', 'Nordschleswig'] \n",
      "\n",
      "['Minderheitensprache', 'einig', 'europäisch', 'außereuropäisch', 'Land', 'Beispiel', 'Rumänien', 'Südafrika', 'Nationalsprache', 'afrikanisch', 'Namibia'] \n",
      "\n",
      "[5968319817064592459, 8431935777423264011, 16143637279988465102, 13347145995516113707, 12068858602874567954, 5135506797272647618, 12646065887601541794, 2552743035069842888, 7654685629011980891, 8431935777423264011] \n",
      "\n",
      "[11854469037278879099, 7289263729939212449, 3491614202785599281, 16047064563126251420, 3469156011154928224, 10833980334450146958, 15216956676957942053, 14493420987399493547, 14425170055224073740, 14854674721094831692, 5682654018506929560, 10694615845175474381] \n",
      "\n",
      "[13853446524293058697, 2130075938147343825, 512110525822973470, 15751849195492229329, 731233208058718707, 176351906757609250, 16018282812866072734, 14398131728093720111, 13884865873598079458, 9226656959411645728, 2911802427415368037] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sentence_generator(doc):\n",
    "    print(i,\"\\n\")\n",
    "    \n",
    "for i in sentence_generator(doc, ids=True):\n",
    "    print(i,\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
