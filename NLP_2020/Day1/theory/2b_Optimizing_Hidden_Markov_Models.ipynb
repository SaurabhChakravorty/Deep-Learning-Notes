{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition: The Hidden Markov Model can be described by the following parameters\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "H M M=\\{N, M, A, B, \\pi\\} \\\\\n",
    "O=\\left\\{O_{1}, O_{2}, O_{3}, \\ldots, O_{T}\\right\\}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where:\n",
    "<br>\n",
    "__N__ is the number of all the possible states (e.g. the words for speech recognition)\n",
    "<br>\n",
    "__M__ is the set of observations that can be made (e.g. spectra of acoustic signal for speech recognition)\n",
    "<br>\n",
    "__A__ is the state transition matrix - probability of moving from one state i to another state j\n",
    "<br>\n",
    "__B__ is the distribution of probabilities of seeing one of the observable symbols given that one is in a particular state (exists for each state)\n",
    "<br>\n",
    "$\\Pi$is the probability of beginning in a particular state\n",
    "<br>\n",
    "__O__ is the observations made, where the index indicates at what point in time the observations were made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three key problems to sovle\n",
    "\n",
    "(1) What is the probability that a model $\\lambda=(A, B, \\pi)$ generated a sequence of observations $O =(O_{1}, O_{2}, O_{3},...,O_T)$?\n",
    "\n",
    "$$\n",
    "P(O \\mid \\lambda)?\n",
    "$$\n",
    "\n",
    "e.g. what model bests fits with the observations - what particular sentence was written by an author?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "(2) Given a model, $\\lambda=(A, B, \\pi)$, what sequence of states, $Q=\\left\\{q_{1}, q_{2}, q_{3}, \\ldots, q_{T}\\right\\}$,  best explains a sequence of observations $O=\\left\\{O_{1}, O_{2}, O_{3}, \\ldots, O_{T}\\right\\}$ ?\n",
    "\n",
    "\n",
    "e.g. what sequence of words best explains a series of sound spectra?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(3) Given a set of observation sequences, how do we learn the model probabilities that would generate them (how do we learn the parameters of the model)? \n",
    "\n",
    "$$O=\\left\\{O_{1}, O_{2}, O_{3}, \\ldots, O_{T}\\right\\} \\quad \\lambda=(A, B, \\pi) ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-backward algorithm to solve question what is the probability that a particular type of model generated sequence of observations?\n",
    "\n",
    "\n",
    "\n",
    "Let's start by imagining all possible state sequences\n",
    "<br>\n",
    "$$\n",
    "Q=q_{1}, q_{2}, q_{3}, \\ldots, q_{T}\n",
    "$$\n",
    "\n",
    "Given the model and the observations\n",
    "\n",
    "<br>\n",
    "$$O=\\left\\{O_{1}, O_{2}, O_{3}, \\ldots, O_{T}\\right\\} \\quad \\lambda=(A, B, \\pi)$$\n",
    "\n",
    "__Probability of seeing observations given those states is__\n",
    "$$\n",
    "P(O \\mid Q, \\lambda)=\\prod_{t=1}^{T} P\\left(O_{t} \\mid q_{t}, \\lambda\\right)\n",
    "$$\n",
    "\n",
    "Which can be written as \n",
    "\n",
    "<br>\n",
    "$$\n",
    "P(O \\mid Q, \\lambda)=b_{q_{1}}\\left(O_{1}\\right) \\cdot b_{q_{2}}\\left(O_{2}\\right) \\cdots b_{q_{T}}\\left(O_{T}\\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "where $b_{qT}$ is the probability of a particular observation in a particular state, e.g. the probability of a particular phone for a particular word\n",
    "\n",
    "<br>\n",
    "\n",
    "e.g. $b_{q_{1}}\\left(O_{1}\\right)$ tells us what is the probability of seeing observation $O_1$ when we are in state $Q_1$\n",
    "\n",
    "<br>\n",
    "\n",
    "__Probability of seeing those state transitions, given the other parameters in the model__\n",
    "\n",
    "<br>\n",
    "$$P(Q \\mid \\lambda)=\\pi_{q_{1}} a_{q_{1} q_{2}} a_{q_{2} q_{3}} \\cdots a_{q_{T-1} q_{T}}$$\n",
    "<br>\n",
    ", where $\\pi_{q_{1}}$ is the probability of being in the initial state and $a_{q_{1} q_{2}}$ is the probability of transitioning from $q_1$ to $q_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have probabilites that tells us what's the probability of seeing a particular sequence of observations and have an expression that tells us what is the probability of seeing a particular sequence of states\n",
    "\n",
    "__Joined Probability of those seeing observations AND those state transitions is__\n",
    "$$\n",
    "P(O, Q \\mid \\lambda)=P(O \\mid Q, \\lambda) P(Q \\mid \\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we want the probability of the observations regardless of the particular state sequence, so we have to iterate over __all__ possible state sequences\n",
    "$$\n",
    "P(O \\mid \\lambda)=\\sum_{\\text {all } Q} P(O \\mid Q, \\lambda) P(Q \\mid \\lambda)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(O \\mid \\lambda)=\\sum_{q_{1}, q_{2}, \\ldots, q_{T}} \\pi_{q_{1}} b_{q_{1}}\\left(O_{1}\\right) a_{q_{1} q_{2}} b_{q_{2}}\\left(O_{2}\\right) a_{q_{2} q_{3}} \\cdots a_{q_{T-1} q_{T}} b_{q_{T}}\\left(O_{T}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "i.e. summing over all possible state sequences\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculating this is infeasible__ \n",
    "\n",
    "How many state sequences are there? $N^{T}$\n",
    "How many multiplications per state sequence?\n",
    "$$\n",
    "2 T-1\n",
    "$$\n",
    "Total number of operations?\n",
    "$$\n",
    "(2 T-1) N^{T}+\\left(N^{T}-1\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "$\\mathrm{T}=100$ and $\\mathrm{N}=5,$ How many operations?\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "(2 T-1) N^{T}+\\left(N^{T}-1\\right) \\\\\n",
    "(2(100)-1) 5^{100}+\\left(5^{100}-1\\right) \\\\\n",
    "199 \\cdot 5^{100}+5^{100}-1 \\\\\n",
    "200 \\cdot 5^{100}-1 \\\\\n",
    "\\approx 5^{103} \\\\\n",
    "\\approx 10^{72}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "__Need a better way to calculate this, which is the forward-backward algorithm__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward backward algorithm \n",
    "#### Forward Algorithm - Alpha helper function\n",
    "\n",
    "\n",
    "Main motivation a lot of repeated calculations in above formula for likelihood of a given sequence of observations given a model. Can reduce number of repeated calculations by introducing alpha (note that alpha is different from a)\n",
    "\n",
    "$$\n",
    "\\alpha_{t}(i)=P\\left(O_{1}, O_{2}, O_{3}, \\ldots, O_{t}, q_{t}=S_{i} \\mid \\lambda\\right)\n",
    "$$\n",
    "\n",
    "Alpha sub t at i is the probability of seeing observations $O_1,..O_t$ and then ending up at state $S_i$ at time $q_t$, given our model. \n",
    "\n",
    "The helper function is limiting the time at which it is considering the probability (only to lower case t not capital T) and ending at one particular state- considering all possible state sequences up to lower case t and then transitioning to state $q_t$. The state that we are going to represent it $i$.\n",
    "\n",
    "__Calculate inductively (iteratively)__:\n",
    "\n",
    "__(1) base case:__\n",
    "$$\\alpha_{1}(i)=\\pi_{i} b_{i}\\left(O_{1}\\right) \\quad 1 \\leq i \\leq N$$\n",
    "\n",
    "__(2) inductive step:__\n",
    "$$\\alpha_{t+1}(j)=\\left[\\sum_{i=1}^{N} \\alpha_{t}(i) a_{i j}\\right] b_{j}\\left(O_{t+1}\\right) \\quad \\begin{array}{l}1 \\leq t \\leq T-1 \\\\ 1 \\leq j \\leq N\\end{array}$$\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1uFCxHKH072QOQ6BW66ZgYEgBDI8XhLSv\"><img src=\"https://drive.google.com/uc?export=view&id=11erhIXSww_4BAa27mpWY7T_ZN7En5TM_\" width=\"300px\"></a>\n",
    "\n",
    "__(3) final step:__\n",
    "$$\n",
    "P(O \\mid \\lambda)=\\sum_{i=1}^{N} \\alpha_{T}(i)\n",
    "$$\n",
    "\n",
    "\n",
    "Finally sum up over all the possible states that we could have ended up in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We are effectively using a lattice of calculations__\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=14i6uKOVcSGOHXrlVyJl2s-qtVUo4RMTm\"><img src=\"https://drive.google.com/uc?export=view&id=1zOuBNuZAfl3UfVVIXZn4rk5S-Xgmk8dr\" width=\"300px\"></a>\n",
    "\n",
    "in the _final step_ we are summing over the final column\n",
    "\n",
    "At each step we have to calculate the flow of probabilities from one N stat to the ext N states, hence $N^2$. Have to do it for each of our T observations.\n",
    "\n",
    "Hence the total number of calculations is roughly:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "O\\left(N^{2} T\\right) \\\\\n",
    "N=5 \\\\\n",
    "T=100 \\\\\n",
    "\\quad \\approx 3000 \\text { calculations }\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward algorithm: beta helper function\n",
    "\n",
    "Moving backward step by step to answer the question: what is the probability of being in state t given what we are seeing in the future (cannot be done in real time but only once we know what the future looks like).\n",
    "\n",
    "$$\n",
    "\\beta_{t}(i)=P\\left(O_{t+1}, O_{t+2}, \\cdots O_{T} \\mid q_{t}=S_i, \\lambda\\right)\n",
    "$$\n",
    "\n",
    "We solve this inductively\n",
    "\n",
    "\n",
    "base case: $$\\quad \\beta_{T}(i)=1 \\quad 1 \\leq i \\leq N$$\n",
    "\n",
    "inductive (recursive) step:\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "\\beta_{t}(i)=\\sum_{j=1}^{N} a_{i j} b_{j}\\left(O_{t+1}\\right) \\beta_{t+1}(j) \\\\\n",
    "t=T-1, T-2, \\cdots, 1 \\quad 1 \\leq i \\leq N\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Tells us what is the probability that I am in this state given the observations that are coming. Have to calculate what beta t+1 is and then moving backwards step by step.\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1OVHjHsQEDGOWO1rftIqU-2fDW5CtxgjZ\"><img src=\"https://drive.google.com/uc?export=view&id=1a1JPutEjOFYlbID3NbmVbXD4xbiAL1eu\" width=\"300px\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Viterbi Algorithm\n",
    "\n",
    "(2) Given a model, $\\lambda=(A, B, \\pi)$, what sequence of states, $Q=\\left\\{q_{1}, q_{2}, q_{3}, \\ldots, q_{T}\\right\\}$,  best explains a sequence of observations $O=\\left\\{O_{1}, O_{2}, O_{3}, \\ldots, O_{T}\\right\\}$ ?\n",
    "\n",
    "e.g. the observations are frequencies that we are hearing and hidden states are components of words that generated these sounds.\n",
    "\n",
    "By best we mean the sequence of states that has the highest likelihood given the observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) choose states that are __individually__ most likely\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\gamma_{t}(i)=P\\left(q_{t}=S_{i} \\mid O, \\lambda\\right) \\\\\n",
    "\\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)} \\\\\n",
    "\\sum_{i=1}^{N} \\gamma_{t}(i)=1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where we divide by the total sum to normalize and ensure that the probabilities add up to 1. And where gamma gives us the probability of being in a state at a particular point in time.\n",
    "\n",
    "$$\n",
    "q_{t}=\\underset{1 \\leq i \\leq N}{\\operatorname{argmax}}\\left[\\gamma_{t}(i)\\right], \\quad 1 \\leq t \\leq T\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1pqM6JrA-xR4SCHfZsMZve_v0MufOFwSB\"><img src=\"https://drive.google.com/uc?export=view&id=15Ikc1GgaGk9zYz5dfmWu42P4yYfrBVcl\" width=\"300px\"></a>\n",
    "\n",
    "\n",
    "But these states may not be connected (so it is not the best sequence - we solved each step independently)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chose the sequence that Maximises the probability - the Viterby algoirthm__\n",
    "\n",
    "Chose the max at each time step give the max chosen at the previous state\n",
    "\n",
    "\n",
    "What is the path with the highest probability that accounts for the first $t$ observations and\n",
    ":\n",
    "ends at state $S_{i} ?$\n",
    "$$\n",
    "\\delta_{t}(i)=\\max _{q_{1}, q_{2}, \\cdots q_{t}-1} P\\left(\\left\\{q_{1}, q_{2}, q_{3} \\cdots q_{t}=i\\right\\},\\left\\{O_{1}, O_{2}, O_{3}, \\cdots O_{t}\\right\\} \\mid \\lambda\\right)\n",
    "$$\n",
    "induction step\n",
    "$$\n",
    "\\delta_{t+1}(j)=\\left[\\max _{i} \\delta_{t}(i) a_{i j}\\right] \\cdot b_{j}\\left(O_{t+1}\\right)\n",
    "$$\n",
    "We need to keep track of which i maximized the result at each time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to keep track of the variables where we came from..\n",
    "\n",
    "\n",
    "Initialization \n",
    "\n",
    "$$\\delta_{1}(i)=\\pi_{i} b_{i}\\left(O_{1}\\right)$$\n",
    "$$\n",
    "\\psi_{1}(i)=0\n",
    "$$\n",
    "Inductive step\n",
    "$$\n",
    "\\begin{array}{rlr}\n",
    "\\delta_{t}(j) & =\\max _{1 \\leq i \\leq N}\\left[\\delta_{t-1}(i) a_{i j}\\right] \\cdot b_{j}\\left(O_{t}\\right) & 2 \\leq t \\leq T \\\\\n",
    "\\psi_{t}(j) & =\\underset{1 \\leq i \\leq N}{\\operatorname{argmax}}\\left[\\delta_{t-1}(i) a_{i j}\\right] & 1 \\leq j \\leq N\n",
    "\\end{array}\n",
    "$$\n",
    "Termination\n",
    "$$\n",
    "P^{*}=\\max _{1 \\leq i \\leq N}\\left[\\delta_{T}(i)\\right]\n",
    "$$\n",
    "$$\n",
    "q_{T}^{*}=\\underset{1<i<N}{\\operatorname{argmax}}\\left[\\delta_{T}(i)\\right] \\quad q_{t}^{*}=\\psi_{t+1}\\left(q_{t+1}^{*}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Baum-Welsh Algorithm\n",
    "\n",
    "__(3) Given a set of observations sequences how do we learn the model probabilities that would generate them (how do we learn the parameters of the model)?__ \n",
    "\n",
    "$$O=\\left\\{O_{1}, O_{2}, O_{3}, \\ldots, O_{T}\\right\\} \\quad \\lambda=(A, B, \\pi) ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no known way to solve for the globally optimal parameters of lambda\n",
    "- We will search for a locally optimal result\n",
    "- A result that converges to a stable good answer but isn't guaranteed to be the best answer (depends on initialization).\n",
    "\n",
    "\n",
    "\n",
    "- This is an EM method\n",
    "- Expectation-Maximization (EM)\n",
    "- Iterative, converges to a local optimum\n",
    "- Aka gradient \"descent\"\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1_dNL0mOAfiEPzNpmaXkbNDHL8k9Br0pC\"><img src=\"https://drive.google.com/uc?export=view&id=1fepHmt1pg4x_NShj5jC1DWLZTwW7U7Lf\" width=\"300px\"></a>\n",
    "\n",
    "\n",
    "We need a new mathematical tool\n",
    "$$\n",
    "\\xi_{t}(i, j)=P\\left(q_{t}=S_{i}, q_{t+1}=S_{j} \\mid O, \\lambda\\right)\n",
    "$$\n",
    "\n",
    "Probability of being in one state and then transitioning into another.\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1QfNEwmnTYR5CdlIFVam842W1lsTWLcmw\"><img src=\"https://drive.google.com/uc?export=view&id=1pTZSg44rdktNxVtn60hJUEyupK-3lZxU\" width=\"300px\"></a>\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\xi_{t}(i, j)=\\frac{\\alpha_{t}(i) a_{i j} b_{j}\\left(O_{t+1}\\right) \\beta_{t+1}(j)}{P(O \\mid \\lambda)} \\\\\n",
    "\\xi_{t}(i, j)=\\frac{\\alpha_{t}(i) a_{i j} b_{j}\\left(O_{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(O_{t+1}\\right) \\beta_{t+1}(j)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "with gamma on the left and zhe on the right\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1xYxaAwdX6_oBwsfI3M3AYDflhngjVpp8\"><img src=\"https://drive.google.com/uc?export=view&id=1vTOsS7w-YaEzcMJg2DDMcFLTmVxEMGB9\" width=\"300px\"></a>\n",
    "\n",
    "can move visually from the left to the right side by doing the summation\n",
    "\n",
    "\n",
    "\n",
    "$\\xi_{t}(i, j)$ is related to $\\gamma_{t}(i)$\n",
    "$$\n",
    "\\gamma_{t}(i)=\\sum_{j=1}^{N} \\xi_{t}(i, j)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "if we sum over all time observations, $t$, then we have a number that can be treated as the expected number of times $S_{i}$ is ever visited (the probability of ever being in state $S_{i}$). \n",
    "\n",
    "<br>\n",
    "$\\xi_{t}(i, j)$ is the probability of ever transitioning from $S_{i}$ to $S_{j}$ at time $t$ (regardless of the time).\n",
    "\n",
    "<br>\n",
    "if we sum over all $t$ then we have a number that can be treated as the expected number of times $S_{i}$ ever transitions to $S_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "\\sum_{t=1}^{T-1} \\gamma_{t}(i)=\\text { expected number of transitions from } S_{i} \\\\\n",
    "\\sum_{t=1}^{T-1} \\xi_{t}(i, j)=\\text { expected number of transitions from } S_{i} \\text { to } S_{j}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we use these to improve our model (generate updates)?\n",
    "$$\n",
    "\\bar{\\lambda}=?\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\bar{\\pi_i}=\\gamma_{1}(i)=\\text { expected frequency in } S_{i} \\text { at time }(t=1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\bar{a}_{i j}=\\frac{\\text { expected number of transitions from } S_{i} \\text { to } S_{j}}{\\text { expected number of transitions from } S_{i}} \\\\\n",
    "\\bar{a}_{i j}=\\frac{\\sum_{t=1}^{T-1} \\xi_{t}(i, j)}{\\sum_{t=1}^{T-1} \\gamma_{t}(i)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "calculated from the previous lambda and our observations\n",
    "\n",
    "$$\n",
    "\\bar{b}_{j}(k)=\\frac{\\text { expected number of times in state } j \\text { and observing } v_{k}}{\\text { expected number of times in state } \\mathrm{j}}\n",
    "$$\n",
    "\n",
    "where $v_k$ is the observation of the particular symbol we are interested in\n",
    "\n",
    " \n",
    "$$\n",
    "\\bar{b}_{j}(k)=\\frac{\\sum_{t=1}^{T} \\gamma_{t}(i)\\ emitting\\ v_{k}}{\\sum_{t=1}^{T}  \\gamma_{t}(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text { Given } \\lambda=(A, B, \\pi) \\text { and } O \\text { we can produce } \\alpha_{t}(i), \\beta_{t}(i), \\gamma_{t}(i), \\xi_{t}(i, j)\\\\\n",
    "&\\text { Given } \\alpha_{t}(i), \\beta_{t}(i), \\gamma_{t}(i), \\xi_{t}(i, j) \\text { we can produce } \\bar{\\lambda}=(\\bar{A}, \\bar{B}, \\bar{\\pi})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1Qj5JDwCky7xxkSR2PHMR_92_UmdB98J0\"><img src=\"https://drive.google.com/uc?export=view&id=1GMQsatf44KCuXpDxVGruQ7HPyW1i6VgF\" width=\"300px\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
