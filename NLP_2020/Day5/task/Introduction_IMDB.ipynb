{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1606962496387,
     "user": {
      "displayName": "Saurabh Chakravorty",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64",
      "userId": "01575942328847603880"
     },
     "user_tz": -60
    },
    "id": "sz1oJorubHGi",
    "outputId": "e4c442cd-cd21-4b69-b853-74324596c5a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<marquee style='width: 100%; color: red;'><b><li style=\"font-size:75px;\">Introduction</li></b></marquee>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<marquee style='width: 100%; color: red;'><b><li style=\"font-size:75px;\">Introduction</li></b></marquee>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MK47ky6GbIi7"
   },
   "source": [
    "The following notebook gives information about the project and the geenral pipeline followed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR4PsyxybM8h"
   },
   "source": [
    "## Requirements\n",
    "\n",
    "1. All the files are designed to run in Colab environment although it can be run in Local system with the help of Jupyter IDE too\n",
    "\n",
    "2. Please run the Notebooks one by one following a certain order which is discussed below\n",
    "\n",
    "3. **!pip install dependencies** which are necessary\n",
    "\n",
    "4. Please give access to the Google **drive** folder for storing the files required for running the code\n",
    "\n",
    "5. All the files which are required are present in the following drive [folder](https://drive.google.com/drive/folders/1DXLnY2IFHQCfkV76UUxdUgCQxvxH4ojO?usp=sharing)\n",
    "\n",
    "6. The scripts will automatically generate .pkl files in the user's local directory upon giving access under the folder named \"IMDB_Data\"\n",
    "\n",
    "The code architecture followed here is\n",
    "\n",
    " <img src=\"http://drive.google.com/uc?export=view&id=12TKZ0Tmd5pbX_ZH-tbpgPTxSziO6_onO\" width=1200 heigth=1500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRk2iS8bbO0u"
   },
   "source": [
    "## File description\n",
    "\n",
    "The following python files are created in .ipynb format with following details:\n",
    "\n",
    "1. **Data Loading** : The file *Data_IMDB_with_multiple_models_handout.ipynb* is the file which generates \"train_data.pkl\" and \"test_data.pkl\" in .pkl format. The general **train and test split** followed is **50%**.\n",
    "\n",
    "2. **Data Preprocessing:** Here the data is preprocessed with cleaning of data and feature extraction. We generate files over here for baseline modelling with features which are named as \"train_tfid_features.pkl\" and \"test_tfid_features\". this file takes around **60 min** to run overall. The file is named as *Data_Processing_IMDB.ipynb*.\n",
    "\n",
    "3. **Baseline Models:** With the help of features generated we train the baseline models *Baseline_Models_IMDB.ipynb* for baseline accuracy. Here, different models are created which takes a long time for training the model. the accuracy of all the models are almost same.\n",
    "\n",
    "4. **LSTM Models:** With the help of cleaned text after stop word removal and feature extraction we *LSTM_Models_IMDB.ipynb* for training the model on large corpus of word embedding with the help of Glove and Fast Text embeddings. The embedding files can be found in the above link. These files are too large and the LSTM models requires high computational power to train extensively.\n",
    "\n",
    "5. **Fast Text Model:** Fast text models are run in the form of API with *Fast_Text_Model.ipynb*. The model is loaded and trained in local directory after the train and test files are generated in proper format for sentiment classification.\n",
    "\n",
    "6. **BERT Models:** The BERT model is trained with pretrained embeddings and using the Ktrain wrapper with *BERT_Model.ipynb*. The file takes around 60 min to run with good computational power.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZfEU24mbQgt"
   },
   "source": [
    "## Description\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "\n",
    "1. The ETA of all the notebooks is close to **180 minutes** with good computational resources\n",
    "\n",
    "2. We acheive a good accuracy overall gradually as we use more sophisticated models\n",
    "\n",
    "3. The models are trained on 50/50 train and test ratio so implications of generalization would be more on deployment\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notable mentions of link where inspiration is take from:\n",
    "\n",
    "1. https://github.com/Neerajj9/Sentiment-Analysis-with-Word-Embeddings/blob/master/Sentiment_Analysis.ipynb\n",
    "\n",
    "2. https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews\n",
    "\n",
    "3. https://medium.com/analytics-vidhya/classification-using-long-short-term-memory-glove-global-vectors-for-word-representation-254d02d5e158\n",
    "\n",
    "All other sources are mentioned in each notebook"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7whXfGPzW05s8TvplACPH",
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
