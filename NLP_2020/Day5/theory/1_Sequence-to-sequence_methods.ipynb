{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Our main motivation in exploring more complex neural architectures is:\n",
    "\n",
    "1. Scaling up to \"multiple levels\" of texts in one model (native modeling of compositionality)\n",
    "2. Solving sequence in, sequence out scenarios.\n",
    "\n",
    "For this we capitalize again on the representation learning abilities of RNN-s.\n",
    "\n",
    "# Skip thought vectors\n",
    "\n",
    "On of the methods for directly learning representations of bigger units of texts is the \"skip thought vectors\" approach that borrows some elements from **word2vec** and uses it with **LSTM**-s.\n",
    "\n",
    "We practically try to **re-generate the the preceding and following sentences from the dense representation of a sentence in the middle**. (This can also be tried with paragraphs.)\n",
    "\n",
    "[Original paper](https://arxiv.org/abs/1506.06726)\n",
    "[Elaboration](https://sanyam5.github.io/my-thoughts-on-skip-thoughts/)\n",
    "\n",
    "\n",
    "## Structure\n",
    "\n",
    "<a href=\"https://sanyam5.github.io/images/skip-thoughts/skip-overview.png\"><img src=\"https://drive.google.com/uc?export=view&id=1lDdso_MgVRZaTkblmERh2VZIspD8V3g2\" width=65%></a>\n",
    "\n",
    "\"Skip-Thoughts model has three parts:\n",
    "\n",
    "**Encoder Network:** Takes the sentence x(i) at index i and generates a fixed length representation z(i). This is a recurrent network (generally GRU or LSTM) that takes the words in a sentence sequentially.\n",
    "\n",
    "**Previous Decoder Network:** Takes the embedding z(i) and “tries” to generate the sentence x(i-1). This also is a recurrent network (generally GRU or LSTM) that generates the sentence sequentially.\n",
    "\n",
    "**Next Decoder Network:** Takes the embedding z(i) and “tries” to generate the sentence x(i+1). Again a recurrent network similar to the Previous Decoder Network.\"\n",
    "\n",
    "\n",
    "**Main takeaway:**\n",
    "\n",
    "-------------------\n",
    "<font color=red>\n",
    "The inner representation of memory models at the end of the sequence are good dense representations for the full sequence.\n",
    "</font>\n",
    "\n",
    "-------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# Generalized seq2seq architecture\n",
    "\n",
    "As mentioned before the inner states of LSTMs represent an arbitrary long sequence of inputs as a fixed length hidden state vector, thus LSTMs can be regarded as sequence encoders.\n",
    "\n",
    "The produced representations can be for example used to:\n",
    "\n",
    "- classification (eg. sentiment analysis in NLP)\n",
    "- the measurement of similarities of series, thus __search__\n",
    "- for **sequence to sequence transformations**, where we generate a new series just in case of language models by applying for example \"beam search\" from the hidden representations.\n",
    "\n",
    "<a href=\"http://suriyadeepan.github.io/img/seq2seq/seq2seq2.png\"><img src=\"https://drive.google.com/uc?export=view&id=1slnyfW87l_HqBLXiIHUKzuMD91CN143_\"></a>\n",
    "\n",
    "LSTM based sequence-to-sequence transformations are used at:\n",
    "\n",
    "- **neural machine translation**\n",
    "- Summarization\n",
    "- Question answering\n",
    "- Dialogue systems\n",
    "\n",
    "## Visualization of a seq2seq model\n",
    "\n",
    "Source: [Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:10:16.378848Z",
     "start_time": "2019-02-08T15:10:16.365710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"100%\" height=\"auto\" loop=\"\" controls=\"\">\n",
       "  <source src=\"https://jalammar.github.io/images/seq2seq_6.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(data='''\n",
    "<video width=\"100%\" height=\"auto\" loop=\"\" controls=\"\">\n",
    "  <source src=\"https://jalammar.github.io/images/seq2seq_6.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some libraries: Google seq2seq\n",
    "\n",
    "The encoder-decoder based seq2seq RNN architectures became so important, but so complex in recent years, that some dedicated frameworks for building up these kind of models appeared. One of the most well known of them is Google's  [seq2seq](https://github.com/google/seq2seq) based on TensorFlow, with which we can define complex seq2seq architectures with the help of simple `yml` description files (naturally more atypical architectrues are also possible). An example of a simple seq2seq model in `yml`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```model: BasicSeq2Seq\n",
    "model_params:\n",
    "  bridge.class: seq2seq.models.bridges.InitialStateBridge\n",
    "  embedding.dim: 128\n",
    "  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n",
    "  encoder.params:\n",
    "    rnn_cell:\n",
    "      cell_class: GRUCell\n",
    "      cell_params:\n",
    "        num_units: 128\n",
    "      dropout_input_keep_prob: 0.8\n",
    "      dropout_output_keep_prob: 1.0\n",
    "      num_layers: 1\n",
    "  decoder.class: seq2seq.decoders.BasicDecoder\n",
    "  decoder.params:\n",
    "    rnn_cell:\n",
    "      cell_class: GRUCell\n",
    "      cell_params:\n",
    "        num_units: 128\n",
    "      dropout_input_keep_prob: 0.8\n",
    "      dropout_output_keep_prob: 1.0\n",
    "      num_layers: 1\n",
    "  optimizer.name: Adam\n",
    "  optimizer.params:\n",
    "    epsilon: 0.0000008\n",
    "  optimizer.learning_rate: 0.0001\n",
    "  source.max_seq_len: 50\n",
    "  source.reverse: false\n",
    "  target.max_seq_len: 50```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple other solutions available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Main use cases\n",
    "\n",
    "## Machine translation\n",
    "\n",
    "Machine translation is in itself one of the foundational concerns of AI research, it has been explicitly mentioned in the Dartmouth Manifesto, and has a long [history of it's own](https://en.wikipedia.org/wiki/History_of_machine_translation). This made it all the more remarkable, that the advent of seq2seq machine translation marked a breakthrough, thus when Google decided to deploy such models into Translate, the appropriate media attention was also given.\n",
    "\n",
    "The performance of NMT models is still progressing rapidly.\n",
    "\n",
    "[source](http://nlpprogress.com/english/machine_translation.html)\n",
    "\n",
    "**Models are evaluated on the English-German dataset of the Ninth Workshop on Statistical Machine Translation (WMT 2014) based on BLEU**\n",
    "\n",
    "|Model |\tBLEU |\tPaper / Source|\n",
    "|------|------|------|\n",
    "|ConvS2S (Gehring et al., 2017)|\t25.16|\t[Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)|\n",
    "|MoE (Shazeer et al., 2017)|\t26.03\t| [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)|\n",
    "|Transformer Base (Vaswani et al., 2017)|\t27.3|\t[Attention Is All You Need](https://arxiv.org/abs/1706.03762)|\n",
    "|Transformer Big (Vaswani et al., 2017)|\t28.4|\t[Attention Is All You Need](https://arxiv.org/abs/1706.03762)|\n",
    "|RNMT+ (Chen et al., 2018)|\t28.5*|\t[The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation](https://arxiv.org/abs/1804.09849)|\n",
    "|Transformer Big (Ott et al., 2018)|\t29.3|\t[Scaling Neural Machine Translation](https://arxiv.org/abs/1806.00187)|\n",
    "|DeepL|\t33.3|\t[DeepL Press release](https://www.deepl.com/press.html)|\n",
    "|Transformer Big + BT (Edunov et al., 2018)\t|35.0\t|[Understanding Back-Translation at Scale](https://arxiv.org/pdf/1808.09381.pdf)|\n",
    "\n",
    "\n",
    "More information on the BLEU metric can be found [here](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat agents\n",
    "\n",
    "The other very promising area of application for seq2seq models is the field of end-to-end dialog modeling. The final goal is to use the available corpora of domain specific dialogs to build up useful chat agents in an unsupervised (better to say: self-supervised) manner.\n",
    "\n",
    "As pointed out elsewhere, though the learning ability of such models in \"language modeling like\" scenarios is remarkable, the **semantic control of output production is rather problematic**, it is a yet unsolved area of research. This hinders the rollout of end-to-end learned chatbot solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for fun\n",
    "\n",
    "Sequence to sequence \"translation\" can mean different things, and a surprisingly large body of problems can be cast into this category. Beside serious applications, like chemical reaction modeling (see eg. [here](https://pubs.acs.org/doi/full/10.1021/acscentsci.7b00303)), there are fun projects one can attempt with this approach, like the one below, where the author tries to compute written probability exercises with an external symbolic calculator.\n",
    "\n",
    "<a href=\"https://reiinakano.github.io/images/sp/calcnet3.gif\"><img src=\"https://drive.google.com/uc?export=view&id=1ZSn6kP8Q1hnTtWh8j9LHrZEJTw8WbHk4\" width=50%></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention!\n",
    "\n",
    "Many of the seq2seq tasks behave in a \"non holistic\" way, meaning that during the solution generation it is not true that all of the prior input information is always equally important, it is well worth \"attending to\" certain elements of it at times, when at other occasions they can be thought of as completely unnecessary. Despite this the encoder-decoder model is constrained to only one summarized representation and can not access the relevant parts of prior hidden states. In early times some tricks were applied to mitigate this effect: entering the input twice or in reverse order, but the real solution proved to be tha so called **\"attention mechanism\"** (coming from ConvNets).\n",
    "\n",
    "<a href=\"https://cdn-images-1.medium.com/max/1600/0*SY3nv8-J6qX1GUxk.png\"><img src=\"https://drive.google.com/uc?export=view&id=19Fckva14TW5FpNKkdIVVGGVPqkmuAlXB\"></a>\n",
    "\n",
    "The decoder receives in each step the prior hidden state and output, as well a _weighted sum_ of all prior states of the encoder as context. \n",
    "\n",
    "Context in the $i$ step of the decoder:\n",
    "\n",
    "$$ c_i = \\sum_{j=1}^{T}\\alpha_{ij}h_j$$\n",
    "\n",
    "where for all $h_k$ hidden states there is weight generated by a trained feedforward network $A$:\n",
    "\n",
    "$$e_{ik} = A(h_k, s_{i-1})$$ \n",
    "\n",
    "(where input is $h_k$ encoder state and $s_{i-1}$, the prior hidden state of the decoder) and uses $\\alpha_{ij}$ weights to generate a softmax:\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp e_{ij}}{\\sum_{k=1}^{T}\\exp e_{ik}}$$\n",
    "\n",
    "\n",
    "The classic paper about attention mechanisms is: [Bahdanau et al: \"Neural machine translation by jointly learning to align and translate.\" (2014).](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "\n",
    "## Visualization of attention mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:15:51.841687Z",
     "start_time": "2019-02-08T15:15:51.825366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"100%\" height=\"auto\" loop=\"\" controls=\"\">\n",
       "  <source src=\"https://jalammar.github.io/images/seq2seq_7.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(data='''\n",
    "<video width=\"100%\" height=\"auto\" loop=\"\" controls=\"\">\n",
    "  <source src=\"https://jalammar.github.io/images/seq2seq_7.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:16:04.627391Z",
     "start_time": "2019-02-08T15:16:04.610105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"100%\" height=\"auto\" loop=\"\" controls=\"\">\n",
       "  <source src=\"https://jalammar.github.io/images/attention_process.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(data='''\n",
    "<video width=\"100%\" height=\"auto\" loop=\"\" controls=\"\">\n",
    "  <source src=\"https://jalammar.github.io/images/attention_process.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:17:12.313406Z",
     "start_time": "2019-02-08T15:17:12.295334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"100%\" height=\"auto\" loop=\"\" controls=\"\">\n",
       "  <source src=\"https://jalammar.github.io/images/attention_tensor_dance.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(data='''\n",
    "<video width=\"100%\" height=\"auto\" loop=\"\" controls=\"\">\n",
    "  <source src=\"https://jalammar.github.io/images/attention_tensor_dance.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the model attend to?\n",
    "\n",
    "<a href=\"https://jalammar.github.io/images/attention_sentence.png\"><img src=\"https://drive.google.com/uc?export=view&id=1vOPMVqpsqmY6S21RC0PtRDiaZN1HfU9o\" width=45%></a>\n",
    "\n",
    "It is noteworthy in in the picture above, that the sequence of English and French text is not the same, and the model learns to pay attention to the relevant positions even in a nonstandard sequence, thus effectively learning syntactic rules of the two languages, as well as their mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Maybe that is all we need?\n",
    "\n",
    "As some of the above mentioned paper titles implied, the idea arose, that the attention mechanism itself forms the crucial part in the success of seq2seq models, thus some brave experiments were made to get rid of RNN-s )and even CNN-s) altogether and focus on purely attention based models.\n",
    "\n",
    "It turned out, that: **\"Attention is all you need!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will talk about a [generalized attention model](https://arxiv.org/pdf/1902.02181.pdf) next, which proves, that the idea of attention started a \"life of it's own\", and became a major modeling in itself, without any kind of recurrent or convolutional elements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
