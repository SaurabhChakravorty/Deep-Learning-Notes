{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the MountainCar problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we have a car, located at the bottom of a valley. The car can go to left or right with full throttle or do nothing.\n",
    "Here the environment is a bit changed because the reward function is redefined to provide a dense feedback. In the original game -1 reward is received all the time. However this makes the convergence slow. \n",
    "\n",
    "To speed it up, the reward function will be:\n",
    "\n",
    "$$r = \\sin(p' \\cdot 3) \\cdot 0.45 + 0.55$$\n",
    "\n",
    "where $p'$ is the position in the next state.\n",
    "\n",
    "**State:** the $x$ coordinate of the car and the speed\n",
    "\n",
    "**Action:** full throttle to left or right (left: 0, do nothing: 1 \\[we will ignore this\\], right: 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution will use the radial basis functions for feature extraction. You will implement Q-learning with a linear function approximator (based on the features) to train the Q-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym  # this library contains environments defined in advance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1XJj3Bju-mqZO8S9JT9QEfMAnZqAjj5X2\" width=45%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(num_coarse):\n",
    "    # num_coarse - how coarse is the feature space\n",
    "    # the Q-function will be represented by a linear-approximator\n",
    "    # initialize the weights matrix \n",
    "    # each weight correpsonds to a specific feature\n",
    "    \n",
    "    # ----- implement this single line -----\n",
    "    return   # create a numpy matrix with the appropriate size, now we have only two actions!\n",
    "\n",
    "\n",
    "def center_points(num_coarse):\n",
    "    # in case of the radial functions we need center points\n",
    "    # the state space is 2D\n",
    "    max_speed = 0.07\n",
    "    min_speed = -0.07\n",
    "    max_pos = 0.6\n",
    "    min_pos = -1.2\n",
    "    speeds = np.linspace(min_speed, max_speed, num_coarse)  \n",
    "    positions = np.linspace(min_pos, max_pos, num_coarse)\n",
    "    sigma = (max_pos - min_pos) / num_coarse\n",
    "    ratio = (max_pos - min_pos) / (max_speed - min_speed)  # the scale of the speed and position is different, this would harm the exponential terms calculated from the speed, causing no effect of the speed\n",
    "    return positions, speeds, sigma, ratio\n",
    "\n",
    "\n",
    "def feature(state, centers):\n",
    "    # calculate the feature values\n",
    "    # take into account the ratio too\n",
    "    pos, velocity = state  # position, velocity\n",
    "    positions, speeds, sigma, ratio = centers\n",
    "    \n",
    "    # ----- implement the radial basis features according to the formula -----\n",
    "    \n",
    "    \n",
    "    return f  # the feature values in a numpy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the feature function\n",
    "# this is a visualization to understand better the feature space\n",
    "# the feature hashigh value at the location of the state in the state space\n",
    "# the state space is spanned by the position and velocity of the car\n",
    "# therefore the state space is 2-dimensional\n",
    "cps = center_points(5)\n",
    "\n",
    "state = (0.3, 0.04)\n",
    "f = feature(state, cps)\n",
    "plt.matshow(f)\n",
    "\n",
    "state = (-0.3, -0.04)\n",
    "f = feature(state, cps)\n",
    "plt.matshow(f)\n",
    "\n",
    "state = (0.1, 0.01)\n",
    "f = feature(state, cps)\n",
    "plt.matshow(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see for the first case:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1WyxbvdQm43yxuTRARRRpV5PtxwmF950G\" width=45%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving/loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end we can save the weights.\n",
    "# This is good for experimenting with the agent after training.\n",
    "\n",
    "def save_weights(weights, file_path):\n",
    "    np.save(file_path, weights, allow_pickle=False)\n",
    "\n",
    "\n",
    "def load_weights(file_path):\n",
    "    return np.load(file_path, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_sa(state, action, weights, centers):\n",
    "    # this is the Q-function\n",
    "    # ----- implement this -----\n",
    "    f =   # calculate the features for the current state\n",
    "    return   # calculate the Q-value from the weights, features and action (action: 0, 1)\n",
    "\n",
    "\n",
    "def policy(state, weights, centers, epsilon):\n",
    "    # epsilon-greedy for choosing the next action\n",
    "    # return 0 for left, 2 for right (1 is ignored now)\n",
    "    # ----- implement this -----\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return action  # the sampled action\n",
    "\n",
    "\n",
    "def update(state, action, r, state_next, weights, centers, gamma, alpha):\n",
    "    # Q-learning update rule for the linear approximation\n",
    "    # ----- implement this -----\n",
    "    q =   # choose the best Q-value\n",
    "    u =   # one-step return\n",
    "    action = action // 2  # action can be 0 or 2 but 2 is not a valid index\n",
    "    weights[action] +=   # update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # init the environment\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    # set the seeds to make the results easier to reproduce\n",
    "    np.random.seed(0)\n",
    "    env.seed(1)\n",
    "    \n",
    "    # hyper-parameters (you can play with this parameters)\n",
    "    num_iter = 5000\n",
    "    num_coarse = 10\n",
    "    epsilon = 1\n",
    "    alpha = 0.35  # learning rate\n",
    "    gamma = 0.98\n",
    "    \n",
    "    weights = init_weights(num_coarse)\n",
    "    centers = center_points(num_coarse)\n",
    "\n",
    "    for counter in range(num_iter):\n",
    "        state = env.reset()  # state - [position, velocity]\n",
    "        done = False\n",
    "        score = 0  # to see the return (for status log)\n",
    "        length = 0  # the length of the current episode (for status log)\n",
    "        max_pos = -1.2  # how much further moved the car in the right direction (for status log)\n",
    "        while not done:\n",
    "            length += 1\n",
    "            action =   # ---- get the action in the current state -----\n",
    "            state_next, _, done, _ =   # ----- make one step with the environment -----\n",
    "            if state_next[0] > max_pos:\n",
    "                max_pos = state_next[0]  # this will show whether the agent is getting closer to the flag\n",
    "            reward = np.sin(state_next[0]*3)*0.45+0.55  # redefined reward to get more feedback\n",
    "            update(state, action, reward, state_next, weights, centers, gamma, alpha)\n",
    "            score += reward\n",
    "            state = state_next\n",
    "        epsilon = max(epsilon - 1.0/num_iter , 0.1)  # decrease the epsilon (exploration -> exploitation)\n",
    "        if counter % 10 == 0:  # logging\n",
    "            print(\"{} -> {} -> {} -> {}\".format(counter, length,  score, max_pos))\n",
    "    # saving the weights at the end\n",
    "    save_weights(weights, \"wgt.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video for seeing what the agent is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(weights, num_coarse):\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    centers = centers = center_points(num_coarse)\n",
    "    for counter in range(10):\n",
    "        done = False\n",
    "        state = env.reset()  # state - [position, velocity]\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = policy(state, weights, centers, 0.0)  # during test there is no need for exploration\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            env.render()  # to show a window with the moving car\n",
    "            score += reward\n",
    "        print(\"At iteration {} the score: {}\".format(counter, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will not work on colab but it can run on a common laptop\n",
    "# No need for gpu and computationally not expensive\n",
    "weights = load_weights(\"wgt.npy\")\n",
    "num_coarse = 10\n",
    "visualize(weights, num_coarse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
