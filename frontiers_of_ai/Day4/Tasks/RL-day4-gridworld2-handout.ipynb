{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld example with Q-learning and Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=18q7KL4aV6McMtaid_1Let2aGkw6d4QYn\" width=45%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    The gridworls is a frequently used demo environment in reinforcement learning\n",
    "    to try and test ideas.\n",
    "    Today, we will use it to understand the concepts so far.\n",
    "    \n",
    "    The environment: (see image)\n",
    "    * cells: the agent can step on a cell. There is exactly one cell to start from.\n",
    "    This is the top left corner. There is one terminal cell where the walking ends, \n",
    "    the agent can not leave it (blue).\n",
    "    * obstacles: there are cells where the agent can not step. (gray)\n",
    "    * agent: it can move from one cell to an other neighboring cell. \n",
    "    Possible directions: up, down, left, right. Each transition happens with probability 1.\n",
    "    * reward: after each transition the agent receives -1 point. In the terminal cell, no reward\n",
    "    received anymore.\n",
    "    \n",
    "    Implement the environment below! You can use the implementation from the previous assignment!\n",
    "    \"\"\"\n",
    "    def __init__(self, size, start_cell, obstacles, terminating_state):\n",
    "        self.size = size\n",
    "        self.start = start_cell\n",
    "        self.obstacles = obstacles\n",
    "        self.termin = terminating_state\n",
    "        self.current_cell = self.start\n",
    "    \n",
    "    def reset(self):\n",
    "        # ----- reset the current cell to the start cell to start again -----\n",
    "    \n",
    "    def transition(self, cell, action):\n",
    "        # ----- IMPLEMENT FUNCTION -----\n",
    "        # cell = (row, column) indices\n",
    "        # action: 0 left, 1 up, 2 right, 3 down\n",
    "        # returns: What will be the next state\n",
    "        # Take care of the borders of the grid!\n",
    "        \n",
    "        # ....\n",
    "        \n",
    "        return (r_next, c_next)\n",
    "\n",
    "    def reward(self, cell, action):\n",
    "        # ----- RETURN REWARD -----\n",
    "        # -1 if not in the terminal state\n",
    "    \n",
    "    def in_terminal(self):\n",
    "        return self.current_cell == self.termin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\"\n",
    "    In this class you can implement the Q-learning algorithm.\n",
    "    The algorithm will run trajectories in the environment (grid world)\n",
    "    and according to the transitions ()\n",
    "    \"\"\"\n",
    "    def __init__(self, gridworld, gamma, alpha, episodes):\n",
    "        self.gridworld = gridworld\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.episodes = episodes\n",
    "        # create table\n",
    "        size = gridworld.size\n",
    "        # ----- implement this -----\n",
    "        self.q_table =     # create a numpy matrix for storing the q-values for each actions and states\n",
    "        # epsilon greedy\n",
    "        self.eps = 0.9\n",
    "        self.episode = 0\n",
    "        # preformance summary\n",
    "        self.sum_rewards = []\n",
    "        self.path = []\n",
    "    \n",
    "    def update(self, cell, action, reward, next_cell):\n",
    "        # the update rule for q-learning\n",
    "        r_t, c_t = cell  # current state\n",
    "        r_tp1, c_tp1 = next_cell  # next state\n",
    "        # ----- implement the update rule -----\n",
    "        self.q_table[action, r_t, c_t] = # ...\n",
    "    \n",
    "    def choose_action(self, cell):\n",
    "        r, c = cell\n",
    "        # ----- choose the next action accroding to epsilon-greedy -----\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def anneal_epsilon(self):\n",
    "        # 1.5 - anneal earlier then the last episode\n",
    "        # ----- Question: how does this function look like? -----\n",
    "        # Answere: ...\n",
    "        self.eps = max(0, self.eps * (1 - self.episode / self.episodes * 1.5))\n",
    "    \n",
    "    def one_episode(self):  # plays an episode\n",
    "        # This function is responsible for running the agent \n",
    "        # for one episode.\n",
    "        # During each transition, the Q-function is updated.\n",
    "        # ----- implement the missing parts -----\n",
    "        cntr = 0  # counter to avoid infinite loops when the agent stucks in the grid and can not reach the terminal state\n",
    "        # ----- reset the gridworld -----\n",
    "\n",
    "        # ----- append zero at the end of sum_rewards -----\n",
    "\n",
    "        # ---- cycle until termination (end of the current episode) or an upper limit (e.g. 5000)\n",
    "        while not self.gridworld.in_terminal() and cntr < 5000:\n",
    "            # ----- increase the counter -----\n",
    "            \n",
    "            # ----- get the current cell -----\n",
    "            cell = # ...\n",
    "            # ----- choose the action -----\n",
    "            action = # ...\n",
    "            # ----- get the reward -----\n",
    "            reward = # ...\n",
    "            # ----- make one transition and store -----\n",
    "            next_cell = # ...\n",
    "            # ----- update the q-table -----\n",
    "\n",
    "            # ----- add the reward to the last element in sum_rewards -----\n",
    "            \n",
    "        # ----- anneal epsilon -----\n",
    "\n",
    "        # ----- increase the episode counter -----\n",
    "\n",
    "    \n",
    "    def trajectory(self):\n",
    "        self.gridworld.reset()\n",
    "        self.path = []\n",
    "        sum_rewards = 0\n",
    "        itr = 0\n",
    "        while not self.gridworld.in_terminal() and itr < 20:\n",
    "            r, c = self.gridworld.current_cell\n",
    "            action = np.argmax(self.q_table[:, r, c])\n",
    "            self.gridworld.transition((r, c), action)\n",
    "            sum_rewards += self.gridworld.reward((r, c), action)\n",
    "            itr += 1\n",
    "            self.path.append((r, c))\n",
    "        return sum_rewards\n",
    "\n",
    "    def is_learning_finished(self):  # depands on the number of episodes\n",
    "        return self.episode > self.episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    \n",
    "    def __init__(self, gridworld, gamma, alpha, episodes):\n",
    "        self.gridworld = gridworld\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.episodes = episodes\n",
    "        # create table\n",
    "        size = gridworld.size\n",
    "        # ----- implement this -----\n",
    "        self.q_table =     # create a numpy matrix for storing the q-values for each actions and states\n",
    "        # epsilon greedy\n",
    "        self.eps = 0.9\n",
    "        self.episode = 0\n",
    "        # preformance summary\n",
    "        self.sum_rewards = []\n",
    "        self.path = []\n",
    "    \n",
    "    def update(self, cell, action, reward, next_cell, next_action):\n",
    "        # the update rule for sarsa learning\n",
    "        r_t, c_t = cell  # current state\n",
    "        r_tp1, c_tp1 = next_cell  # next state\n",
    "        # ----- implement the update rule -----\n",
    "        self.q_table[action, r_t, c_t] = # ...\n",
    "    \n",
    "    def choose_action(self, cell):\n",
    "        r, c = cell\n",
    "        # ----- choose the next action accroding to epsilon-greedy -----\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def anneal_epsilon(self):\n",
    "        # 1.5 - anneal earlier then the last episode\n",
    "        self.eps = max(0, self.eps * (1 - self.episode / self.episodes * 1.5))\n",
    "    \n",
    "    def one_episode(self):  # plays an episode\n",
    "        # ----- implement this -----\n",
    "        # use the same logic like in Q-learning\n",
    "        # be aware that this is an on-policy algorithm!\n",
    "        # Question: Can you explain concisely what is the main difference\n",
    "        # between on-policy and off-policy?\n",
    "        # Answer: ...\n",
    "    \n",
    "    def trajectory(self):\n",
    "        self.gridworld.reset()\n",
    "        self.path = []\n",
    "        sum_rewards = 0\n",
    "        itr = 0\n",
    "        while not self.gridworld.in_terminal() and itr < 20:\n",
    "            r, c = self.gridworld.current_cell\n",
    "            action = np.argmax(self.q_table[:, r, c])\n",
    "            self.gridworld.transition((r, c), action)\n",
    "            sum_rewards += self.gridworld.reward((r, c), action)\n",
    "            itr += 1\n",
    "            self.path.append((r, c))\n",
    "        return sum_rewards\n",
    "\n",
    "    def is_learning_finished(self):\n",
    "        return self.episode > self.episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(ql):\n",
    "    values = ql.sum_rewards\n",
    "    x = list(range(len(values)))\n",
    "    y = values\n",
    "    plt.plot(x, y, 'ro')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid world parameters\n",
    "size = (6, 6)\n",
    "start_cell = (0, 0)\n",
    "obstacles = [(3, 3)]\n",
    "terminating_state = (3, 5)\n",
    "# q learning parameters\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "episodes = 500\n",
    "\n",
    "gw = GridWorld(size, start_cell, obstacles, terminating_state)\n",
    "solver = QLearning(gw, gamma, alpha, episodes)  # ----- try both of them -----\n",
    "#solver = Sarsa(gw, gamma, alpha, episodes)\n",
    "\n",
    "while not solver.is_learning_finished():\n",
    "    solver.one_episode()\n",
    "    sum_rewards = solver.sum_rewards[-1]\n",
    "    print(sum_rewards)\n",
    "\n",
    "sum_rewards = solver.trajectory()\n",
    "print(sum_rewards)\n",
    "plot_learning_curve(solver)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
