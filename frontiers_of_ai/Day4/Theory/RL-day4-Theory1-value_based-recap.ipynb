{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction into value-based RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Bellman-equation again\n",
    "* Taxonomy of RL algorithms\n",
    "* Model-based vs model-free learning\n",
    "\n",
    "* Value-based RL with classic methods\n",
    "* Value-based RL with DNNs\n",
    "* Examples\n",
    "\n",
    "* Practice session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Bellman-equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1VqFRFYo8Tv2YDG__6ntVliUNMb-RlEmU\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Key concepts we covered last time:**\n",
    "\n",
    "* state\n",
    "* action\n",
    "* reward (and return)\n",
    "* policy\n",
    "* transition matrix\n",
    "* dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Objective of reinforcement learning:**\n",
    "\n",
    "To find the optimal behavior of the agent which maximizes its expected return. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1J-jj-LQLnKIMFy9KV6cdKONtXQlMHhWI\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Value-function, fixed stochastic policy\n",
    "$$V^\\pi(s) = \\sum_{s', a}{\\pi(s, a) \\cdot T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma \\cdot V^\\pi(s') \\right]}$$\n",
    "\n",
    "Value-function, fixed deterministic policy\n",
    "$$V^\\pi(s) = \\sum_{s'}{T(s, \\pi(s), s') \\cdot \\left[ r(s, \\pi(s)) + \\gamma \\cdot V^\\pi(s') \\right]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Action value-function, fixed stochastic policy\n",
    "$$Q^\\pi(s, a) = \\sum_{s', a'}{\\pi(s', a') \\cdot T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma \\cdot Q^\\pi(s', a') \\right]}$$\n",
    "\n",
    "Action value-function, fixed deterministic policy\n",
    "$$Q^\\pi(s, a) = \\sum_{s'}{T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma \\cdot Q^\\pi(s', \\pi(s')) \\right]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimal value-function\n",
    "$$\\tilde{V}(s) = \\max_a \\sum_{s'}{T(s, a, s') \\cdot \\left[ r(s, a, s') + \\gamma \\cdot \\tilde{V}(s') \\right]}$$\n",
    "\n",
    "Optimal action value-function\n",
    "$$\\tilde{Q}(s, a) = \\sum_{s'}{T(s, a, s') \\cdot \\left[ r(s, a, s') + \\gamma \\cdot \\max_{a'} \\tilde{Q}(s', a') \\right]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taxonomy of RL algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1Gz0WBOtTxYrZ91uAidFE_Tuw0RBSfl9O\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1WaYwpMZ5O0pUpT41TlfSQh2ov42xIHQb\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Value-based methods:**\n",
    "\n",
    "* DQN and its variants (DQN with prioritized sweeping, Double DQN)\n",
    "* Deep Sarsa\n",
    "\n",
    "**Policy-based method:**\n",
    "\n",
    "* REINFORCE\n",
    "\n",
    "**Actor-Critic methods:**\n",
    "\n",
    "* A3C, A2C\n",
    "* TRPO\n",
    "* PPO\n",
    "* SAC\n",
    "\n",
    "and lots of others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model-based or model-free learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Think about how the policy can be calculated when the value-function or the action-value function is known:\n",
    "\n",
    "$$\\pi(s) = \\arg \\max_a{ \\tilde{Q}(s, a) }$$\n",
    "\n",
    "When the value-function is given:\n",
    "\n",
    "$$\\pi(s) = \\arg\\max_a \\left( T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma \\tilde{V}(s') \\right]\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main difference between the two is the necessity of $T$ and $r$, the transition probability and the reward.\n",
    "These functions describe the dynamics of the environment, the model.\n",
    "\n",
    "Algorithms using the model of the environment, are the model-based algorithms. Otherwise, it is model-free. If the $Q$ function is calculated then the algorithm can be model-free. This is one of the reasons why Q-learning is so popular and well-known (see later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model-based algorithms require the knowledge of the model. Unfortunately, it is rarely known ahead. Therefore we need a so called model-identification to learn it. There are two types of model representation:\n",
    "\n",
    "1. distribution models\n",
    "2. sampling models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Distribution models vs sampling models**\n",
    "\n",
    "If we have a state and an already chosen action then we can have a **distribution over the possible next state**. Formally:\n",
    "\n",
    "$$p(s') = T(s, a, s')$$\n",
    "\n",
    "A distribution model can give the probabilities over the possible next actions. However, a **sampling model only gives the next state with probability $p(s')$** but we have no idea that what are the probabilities of the other states.\n",
    "\n",
    "Therefore the distribution model is more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the MDP's point of view, the learning of distribution model is the same as learning the $T(s, a, s')$. The simplest approach to learn the model is to use a \"policy\" or sampling method that ensures that all of the transitions are visited enough time, therefore the transition probabilities will be closed to the empirical probabilities. The empirical probability:\n",
    "\n",
    "$$T(s, a, s') = \\frac{N_{s, a, s'}}{N_{s, a}}$$\n",
    "\n",
    "$N_{s, a, s'}$ means how many transition ($s, a \\rightarrow s'$) happened so far. $N_{s, a}$ means how many times the $s, a$ pair appeared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Having a large and complex environment results in a need for a large amount of sampling. This can be infeasable in real life. However, there are some RL learning methods that tries to learn the model and find the optimal policy concurrently, for instance [Dina-Q](http://papers.nips.cc/paper/388-integrated-modeling-and-control-based-on-reinforcement-learning-and-dynamic-programming.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In practice, it is much feasable to build a simulator and use it as a sampling model then apply a **model-free method** in the simulator. Of course, the transition from the simulator to real world is still requires effort. From now, we are interested in model-free methods. Most of the successful RL algorithms are model-free."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
