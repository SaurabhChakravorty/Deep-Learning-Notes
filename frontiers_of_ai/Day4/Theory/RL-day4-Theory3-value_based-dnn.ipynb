{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value-based RL DNN approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, consider again the curses we covered in the previous lesson:\n",
    "\n",
    "* curse of dimension\n",
    "* curse of modeling\n",
    "* curse of credit assignment\n",
    "\n",
    "The problem with the first two was the size of the state space and action space. Simply, it is not possible to store the values for each state separately. Therefore we turned toward feature extraction and linear methods. We have also seen, there are pretty good performance guarantees. Here are the tables again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Prediction algorithms:**\n",
    "\n",
    "| On/Off-policy | Algorithm | Tabular | Linear | Non-linear|\n",
    "|------|------|------|------|------|\n",
    "| On-policy | MC | YES | YES | YES |\n",
    "| On-policy | TD($\\lambda$) | YES | YES | NO |\n",
    "| On-policy | **Gradient TD** | YES | YES | YES |\n",
    "| Off-policy | MC | YES | YES | YES |\n",
    "| Off-policy | TD($\\lambda$) | YES | NO | NO |\n",
    "| On-policy | **Gradient TD** | YES | YES | YES |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Control algorithms:**\n",
    "\n",
    "| Algorithm | Tabular | Linear | Non-linear|\n",
    "|------|------|------|------|\n",
    "| MC | YES | YES | NO |\n",
    "| Sarsa | YES | YES | NO |\n",
    "| Q-learning | YES | NO | NO |\n",
    "| **Gradient Q** | YES | YES | NO |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the tables above, two new algorithms are highlighted which have better convergence properties. We do not discuss them in detail but it can be useful to know about them. Details: [Gradient TD](http://incompleteideas.net/Talks/gradient-TD-2011.pdf), [Gradient Q-learning 1](http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_21.pdf) and [Gradient Q-learning 2](https://arxiv.org/pdf/1705.03967.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** can we eliminate or handle the problem of the curse of modeling and the curse of dimension?\n",
    "\n",
    "So far, we have seen that the model can be eliminated by applying the Q-function with sampling (e.g.: Q-learning, Sarsa-learning and their variants). Those algorithms are called model-free learning algorithms.\n",
    "\n",
    "The **curse of credit assignment** is more challenging. n-step return, $\\lambda$-return can make the learning more stable and by using a long horizon the credit assignment can be easier. But there is no clear-cut in this.\n",
    "\n",
    "The **curse of dimension** requires a different approach instead of tabular methods. Manual feature extraction has a long history in machine learning but deep learning makes it possible to learn the features automatically. It is tempting to apply deep learning for representing the state values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DQN - Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The DQN algorithm was one of the first RL algorithms where the RL framework was combined with DNNs and the result was satisfying. The properties of the algorithm:\n",
    "\n",
    "* relatively easy to implement\n",
    "* simple but powerful\n",
    "* difficult convergence\n",
    "* sensitive for the hyper-paramters\n",
    "\n",
    "You will implement this algorithm in today's handout [cartpole-dqn-handout](CartPole-with-DQN.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DQN is based on Q-learning but the $Q$-function is approximated by a neural network: $Q_\\theta(s, a)$. Then the update rule for the $\\theta$ parameters is given as:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\left( r_t + \\max_{a'}Q_\\theta(s', a') - Q_\\theta(s, a) \\right)\\cdot \\nabla_\\theta Q_\\theta(s, a)|_{\\theta = \\theta_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How can we represent the state?** The problem is when something is moving on the image (e.g.: a ball) then a static frame is not able to represent it at a time point. The convolutional networks are memory less therefore they can not store informations across the consequtive frames. Remember, RL assumes an MDP (Markov decision process), which requires the state contains all the information about the environment. \n",
    "\n",
    "A good approximation of this is to use a bunch of consequtive frames. Therefore a moving object appears different places on the consequtive frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DQN has two major tricks to avoid the instability caused by the neural network approximator:\n",
    "\n",
    "1. experience replay\n",
    "2. iterative update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The next slide shows the pseudo code of the DQN algorithm. The experience replay is the $D$ buffer in the code. The algorithm stores and samples experiences from the buffer. The iterative update implemented with $\\hat{Q}$ and $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1EaDj9o9-ACuMsf9PtmMw4p3CMtknTVAg\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Experience replay:**\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1rqQQyPxhDTSFScMwmXDLGAE6eUpAWoSu\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are two main reasons why experience replay can help to converge faster:\n",
    "\n",
    "1. If all of the samples are taken consequtively before feeding it into the network then the data samples will be correlated. This correlation makes the learning slower and harms the generalization. The replay buffer gathers the experiences in a buffer and the training batches are sampled according to a uniform distribution.\n",
    "2. There are valuable states (experiences) which should be used more times because it affects the policy strongly. However, may be the state is visited rarely because it is hard to reach it. Because the replay buffer stores a long history of experiences, the rare experiences can be reused several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Iterative update:**\n",
    "\n",
    "One of the reasons behind the instability of the Q-learning combined with a deep neural network is the fast change (high variance) of the one-step return. The one-step return depends on the network itself and the network weight is updated frequently. The network has no time to adapt and follow up changes.\n",
    "\n",
    "Iterative update or (delayed update) uses two networks for representing the $Q$-function. The architecture is the same but the weights are different. The weights are synchronized after a given number of steps.\n",
    "\n",
    "The goal of the first network is to calculate the return and it is not updated until synchronization. The second network is responsible for selecting the next step and it is always updated according to the update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The update rule changes for the following one:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\left( r_t + \\max_{a'}\\hat{Q}_{\\theta^-}(s', a') - Q_\\theta(s, a) \\right)\\cdot \\nabla_\\theta Q_\\theta(s, a)|_{\\theta = \\theta_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Preprocessing steps:**\n",
    "\n",
    "The frames arriving from the simulator needs preprocession before feeding into the network.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1v6xXmKxSbElHF8RDgwmjP4eou2DxMHDj\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The preprocessing of the raw input frames consists of the following steps, as the above image illustrates:\n",
    "\n",
    "* grayscale the image\n",
    "* cropping (only the interesting part of the image will remain)\n",
    "* downsampling (or resizing) the image for $84\\times 84$\n",
    "* stacking four frames together to form the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Network architecture:**\n",
    "\n",
    "* Conv2D(kernel\\_num=32, kernel\\_size=(8, 8), padding='valid', input_shape=(84, 84, 4), strides=(4, 4))\n",
    "* Activation('relu')\n",
    "* Conv2D(kernel\\_num=64, kernel\\_size=(4, 4), padding='valid', strides=(2, 2))\n",
    "* Activation('relu')\n",
    "* Conv2D(kernel\\_num=64, kernel\\_size=(3, 3), padding='valid', strides=(1, 1))\n",
    "* Activation('relu')\n",
    "* Flatten()\n",
    "* Dense(units=512, activation='relu')\n",
    "* Dense(num\\_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Video playing Atari](https://www.youtube.com/watch?v=V1eYniJ0Rnk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "\n",
    "$Q$-learning can overestimate the real value of $Q(s, a)$ and this can harm the convergence. Double $Q$-learning is an approach to handle the problem of overestimation.\n",
    "\n",
    "The main idea is to use two separate $Q$ functions. One for choosing the best action, and one for boostrapping. \n",
    "\n",
    "Update rule for simple $Q$-learning:\n",
    "$$Y^Q = r_t + \\gamma Q(s_t, \\arg\\max_a Q(s_t, a; \\theta); \\theta)$$\n",
    "\n",
    "Update rule for **double $Q$-learning**:\n",
    "$$Y^{DoubleQ} = r_t + \\gamma Q(s_t, \\arg\\max_a Q(s_t, a; \\theta); \\theta')$$\n",
    "\n",
    "The role of $\\theta$ and $\\theta'$ is switched update by update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=1jaH7q73Pc_GzuIDt2nkHPzo3rgAq7z16\"  height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DQN with prioritized experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1511.05952.pdf)\n",
    "\n",
    "Remember, in case of the DQN we sample the experiences uniformly (with equal probability) from the experience replay.\n",
    "Unfortunately, this approach assumes that all of the experiences has equal impact on learning. It is easy to understand that his is not true. There are experiences with more relevance. If we define a metric or indicator, to decide which experience is the more important, then we can create a prioritized experience replay. The experiences with higher priority are chosen more frequently.\n",
    "\n",
    "How can we measure the importnace? A common way for that is to calculate the TD-error (we have already seen it), which indicates how suprising or unexpected the transition is:\n",
    "\n",
    "$$\\delta = r + \\gamma \\max_{a'}Q(s', a') - Q(s, a)$$\n",
    "\n",
    "Then, the probability of sampling transition $i$ is:\n",
    "\n",
    "$$P(i) = \\frac{p^\\alpha}{\\sum_k p^\\alpha_k}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$p_i = |\\delta_i| + \\varepsilon$$\n",
    "\n",
    "$\\alpha$ is a hyperparameter and $\\alpha=0$ is equal with the uniform sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=1UK6o9wAMGZ_IC8UM2JD9yUt34YwZD7re\"  height=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=1Foki_zqVOJLkE2eJrpXmZJVxQ3w8prDB\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dueling network for DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1511.06581.pdf)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1EH5T77x-RqDyXm1u3rAf8RNBobOv71jm\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Definition of the **advantage**:\n",
    "\n",
    "$$A(s, a) = Q(s, a) - V(s)$$\n",
    "\n",
    "Combining the estimated $V$ and $A$ in the network is tricky because of the identification problem, mentioned in the paper. For instance by adding a constant to the $V$ and substracting the same constant from $A$, the $Q$ will be the same.\n",
    "\n",
    "However, the following is true for the relation between $V$ and $Q$ in case of a deterministic policy:\n",
    "\n",
    "$$V(s) = Q(s, a^*)$$\n",
    "\n",
    "If we compare the two equations then:\n",
    "\n",
    "$$A(s, a^*) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to avoid the identification problem, the authors suggest to force the last equation to hold by:\n",
    "\n",
    "$$Q(s, a; \\theta, \\alpha, \\beta) = V(s; \\theta, \\beta) + \\left( A(s, a; \\theta, \\alpha) - \\max_a' A(s, a'; \\theta, \\alpha) \\right)$$\n",
    "\n",
    "However, to further improve the stability of optimization, the paper proposed the following module to combine $V$ and $A$ at the output:\n",
    "\n",
    "$$Q(s, a; \\theta, \\alpha, \\beta) = V(s; \\theta, \\beta) + \\left( A(s, a; \\theta, \\alpha) - \\frac{1}{|D|} \\sum_{a' \\in D}A(s, a'; \\theta, \\alpha) \\right)$$\n",
    "\n",
    "$D$ now is the action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main contribution of this approach, that it helps generalizing among different environments. In case of the Atari, the games differ in the action space too (e.g.: the number of actions and their meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TreeQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1710.11417.pdf)\n",
    "\n",
    "DQN combined with the model. The model is represented as a network. The architecture is like a tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1fX44HRPKzRJI6U7vxaAZPa6bg7tHEGpY\" width=75%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
