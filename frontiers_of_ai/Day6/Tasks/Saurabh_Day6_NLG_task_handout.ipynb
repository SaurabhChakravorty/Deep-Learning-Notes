{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Saurabh_Day6_NLG_task_handout.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rYZcjkVRxmXN"},"source":["# Task: MLE and Policy Gradient trained sequence to sequence model in Texar\n","\n","We build two versions of a simple (but attention using) sequence-to-sequence model:\n","\n","- The first version will be trained only with an MLE objective,\n","- The second will be trained both with MLE as a pretraining, and then Policy Gradient.\n","\n","The goal of the task is simply to get to know Texar a bit, nothing else -- the dataset is a toy dataset from Google which simply reverses the input, and without proper hyperparameter tuning (which we won't do) Policy Gradient quickly collapses, even after the MLE pretraining.\n","\n","Consequently, the task is simply to build the models and get the training running, there is no target performance which you'd have to achieve. In addition, almost all code that is needed can be found by looking at the Texar documentation..."]},{"cell_type":"markdown","metadata":{"id":"PuXRvWgdicfJ"},"source":["# Texar prerequisites"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":802},"id":"UzsX_RkyftLG","executionInfo":{"status":"ok","timestamp":1610882631539,"user_tz":-60,"elapsed":11588,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"33be2db0-3793-4d42-8305-e9cf24dda0a1"},"source":["%tensorflow_version 1.x # Texar needs TF 1.x!!!\r\n","!pip install texar"],"execution_count":1,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.x # Texar needs TF 1.x!!!`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n","Collecting texar\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/57/1b8af36cca480965e3d50ec218017f1c3c8732e07aa8d8d802ef4ecd9561/texar-0.2.4.tar.gz (290kB)\n","\u001b[K     |████████████████████████████████| 296kB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: regex>=2018.01.10 in /usr/local/lib/python3.6/dist-packages (from texar) (2019.12.20)\n","Collecting numpy<1.17.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/b1/ba7e59da253c58aaf874ea790ae71d6870255a5243010d94688c41618678/numpy-1.16.6-cp36-cp36m-manylinux1_x86_64.whl (17.4MB)\n","\u001b[K     |████████████████████████████████| 17.4MB 209kB/s \n","\u001b[?25hRequirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.6/dist-packages (from texar) (1.0.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from texar) (3.13)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from texar) (2.23.0)\n","Collecting funcsigs>=1.0.2\n","  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n","Collecting sentencepiece>=0.1.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 73.3MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from texar) (20.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->texar) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->texar) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->texar) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->texar) (2020.12.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->texar) (2.4.7)\n","Building wheels for collected packages: texar\n","  Building wheel for texar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for texar: filename=texar-0.2.4-cp36-none-any.whl size=444370 sha256=6637c6b74d2223475ab9146f0f725a98b39b23a604eef59479e9b3c1fd377fbe\n","  Stored in directory: /root/.cache/pip/wheels/d3/10/64/40ab4b8563fe393750ada8484c27da019b15b0ff51ffcbaf95\n","Successfully built texar\n","\u001b[31mERROR: tensorflow 1.15.2 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, funcsigs, sentencepiece, texar\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","Successfully installed funcsigs-1.0.2 numpy-1.16.6 sentencepiece-0.1.95 texar-0.2.4\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"q3uOKU7Kgxfi","executionInfo":{"status":"ok","timestamp":1610882651977,"user_tz":-60,"elapsed":8519,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["import texar.tf as tx\n","import tensorflow as tf\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_J6Mn2JhQIFI"},"source":["# Downloading the data\n","We download and extract the toy dataset with Texar's download utility function:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhhQOSmRjWIF","executionInfo":{"status":"ok","timestamp":1610882653832,"user_tz":-60,"elapsed":1276,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"1108e2a5-652c-4370-e5e6-8c1130997d26"},"source":["tx.data.maybe_download(\n","            urls='https://drive.google.com/file/d/'\n","                 '1fENE2rakm8vJ8d3voWBgW4hGlS6-KORW/view?usp=sharing',\n","            path='./',\n","            filenames='toy_copy.zip',\n","            extract=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Successfully downloaded toy_copy.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['./toy_copy.zip']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"CSGSsLRU6E_t"},"source":["After extraction this will already be in the format needed for Texar's data readers."]},{"cell_type":"markdown","metadata":{"id":"5_ufSlPcqfPn"},"source":["# The basic model: RNN Seq2seq with attention trained with MLE"]},{"cell_type":"markdown","metadata":{"id":"hlFYiXU7m5XC"},"source":["## Model parameters\n","\n","In Texar, hyperparameters are typically represented by multi-level dictionaries (or  dictionary-like texar.HParams intstances). Before building the model, we define a minimal set of hyperparameter dictionaries for the embedding, encoder, decoder and attention."]},{"cell_type":"code","metadata":{"id":"Z6pTkpWkUBse","executionInfo":{"status":"ok","timestamp":1610883022375,"user_tz":-60,"elapsed":620,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["# We want to use the same dimensionality (\"number of units\") for the embedding, and the encoder and decoder RNNs\n","num_units = 20 # Please, specify a reasonable number\n","\n","# For inference, the model will use beam search\n","beam_width = 10 # Specify a reasonable number (remember that the search time is not \n","                 # linear with respect to this parameter!)\n","\n","embedder_hparams = {\"dim\": num_units}\n","encoder_hparams = {\n","    'rnn_cell_fw': {\n","        'kwargs': {\n","            'num_units': num_units\n","        }\n","    }\n","}\n","decoder_hparams = {\n","    'rnn_cell': {\n","        'kwargs': {\n","            'num_units': num_units\n","        },\n","    },\n","    'attention': {\n","        'kwargs': {\n","            'num_units': num_units,\n","        },\n","        'attention_layer_size': num_units\n","    }\n","}"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"33QyQ3ZoQXnR"},"source":["## Building the model\n","\n","First we build a simple seq2seq model with attention."]},{"cell_type":"code","metadata":{"id":"YrA46wqeRRNK","executionInfo":{"status":"ok","timestamp":1610883437713,"user_tz":-60,"elapsed":623,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["def build_mle_model(batch, train_data):\n","    \"\"\"Build a basic seq2seq model with attention for MLE training.\n","    \"\"\"\n","    \n","    # Please define a word embedding layer for the Encoder using Texar's API.\n","    # For hyperparameters, use the embedder hparams defined in the previous cell.\n","    source_embedder = tx.modules.WordEmbedder(\n","        vocab_size=train_data.source_vocab.size, hparams=embedder_hparams)\n","    \n","    # For encoder, use a Bidirectional RNN encoder from the Texar API.\n","    # hparams were defined above.\n","    encoder = tx.modules.BidirectionalRNNEncoder(\n","        hparams=encoder_hparams)\n","\n","    enc_outputs, _ = encoder(source_embedder(batch['source_text_ids']))\n","    \n","    # Please define a word embedding layer for the Decoder using Texar's API.\n","    # For hyperparameters, use the embedder hparams defined in the previous cell.\n","    target_embedder = tx.modules.WordEmbedder(\n","        vocab_size=train_data.target_vocab.size, hparams=embedder_hparams)\n","\n","    # The decoder should be a Texar Attention RNN decoder with the hyperparameters \n","    # defined above\n","    decoder = tx.modules.AttentionRNNDecoder(\n","        memory=tf.concat(enc_outputs, axis=2),\n","        memory_sequence_length=batch['source_length'],\n","        vocab_size=train_data.target_vocab.size,\n","        hparams=decoder_hparams)\n","\n","    # For MLE training, we use greedy decoding and teacher forcing, \n","    # this is why the input is coming from the target text\n","    mle_training_outputs, _, _ = decoder(\n","        decoding_strategy='train_greedy', # Please specify greedy training decoding here \n","                               # see the possible values in the Texar \"Decoders\" documentation section\n","        inputs=target_embedder(batch['target_text_ids'][:, :-1]),\n","        sequence_length=batch['target_length'] - 1)\n","\n","    # The loss for MLE training is the familiar sparse softmax cross entropy\n","    # Please use the Texar version of it here!\n","    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n","        labels=batch['target_text_ids'][:, 1:],\n","        logits=mle_training_outputs.logits,\n","        sequence_length=batch['target_length'] - 1)\n","\n","    # Texar produces a train op from the loss for us:\n","    mle_train_op = tx.core.get_train_op(mle_loss)\n","\n","    # For inference (text generation), we need the start (bos) tokens from the data set\n","    # and we produce here a whole vector of them, for the entire batch.\n","    start_tokens = tf.ones_like(batch['target_length']) * train_data.target_vocab.bos_token_id\n","\n","    # Inference (text generation) by beam search -- nothing to do here, just observe!\n","    beam_search_outputs, _, _ = \\\n","        tx.modules.beam_search_decode(\n","            decoder_or_cell=decoder,\n","            embedding=target_embedder,\n","            start_tokens=start_tokens,\n","            end_token=train_data.target_vocab.eos_token_id,\n","            beam_width=beam_width,\n","            max_decoding_length=60)\n","\n","    # Having built the model, we need to return two things that will be needed for the training\n","    # and evaluation of the model: the mle training op  and the beam search output \n","    # please add these in the next line (in this order)!!\n","    return mle_train_op, beam_search_outputs"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YopoVmiVqtl4"},"source":["## Data sets and training parameters"]},{"cell_type":"code","metadata":{"id":"WzgvL-Dgm9X4","executionInfo":{"status":"ok","timestamp":1610883440072,"user_tz":-60,"elapsed":680,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["source_vocab_file = './data/toy_copy/train/vocab.sources.txt'\n","target_vocab_file = './data/toy_copy/train/vocab.targets.txt'\n","\n","mle_training_num_epochs  = 10 # Please specify the number of training epochs\n","steps_per_train_epochs = 312 # Don't touch this, this is the correct value for the toy dataset\n","batch_size = 128 # Please specify a batch size\n","\n","display = 50\n","\n","# Texar hparams for the toy dataset\n","\n","train_hparams = {\n","    'num_epochs': 500, # We set this to an unexhaustible number bec. of a Texar bug!!\n","    'batch_size': batch_size,\n","    'allow_smaller_final_batch': False,\n","    'source_dataset': {\n","        \"files\": './data/toy_copy/train/sources.txt',\n","        'vocab_file': source_vocab_file\n","    },\n","    'target_dataset': {\n","        'files': './data/toy_copy/train/targets.txt',\n","        'vocab_file': target_vocab_file\n","    }\n","}\n","val_hparams = {\n","    'batch_size': batch_size,\n","    'allow_smaller_final_batch': False,\n","    'source_dataset': {\n","        \"files\": './data/toy_copy/dev/sources.txt',\n","        'vocab_file': source_vocab_file\n","    },\n","    'target_dataset': {\n","        \"files\": './data/toy_copy/dev/targets.txt',\n","        'vocab_file': target_vocab_file\n","    }\n","}\n","test_hparams = {\n","    'batch_size': batch_size,\n","    'allow_smaller_final_batch': False,\n","    'source_dataset': {\n","        \"files\": './data/toy_copy/test/sources.txt',\n","        'vocab_file': source_vocab_file\n","    },\n","    'target_dataset': {\n","        \"files\": './data/toy_copy/test/targets.txt',\n","        'vocab_file': target_vocab_file\n","    }\n","}\n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8k1wKtP4xX9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610883444628,"user_tz":-60,"elapsed":2623,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"41a0da20-ab9a-46c5-fd4e-ff5a382bf0bf"},"source":["# All of our data sets consist of paired texts -- please specify the correct\n","# Texar data class in the next three lines:\n","\n","train_data  =  tx.data.PairedTextData(hparams=train_hparams)\n","val_data    =  tx.data.PairedTextData(hparams=val_hparams)\n","test_data   =  tx.data.PairedTextData(hparams=test_hparams)\n","\n","# Texar's data iterators are thin wrappers around the Tensorflow Dataset API\n","# Please put Texar's data iterator here which can switch between train, test and validation data\n","iterator = tx.data.TrainTestDataIterator(train=train_data, val=val_data, test=test_data)\n","batch = iterator.get_next()\n","train_op, infer_outputs = build_mle_model(batch, train_data) # build the model, get train and inference outputs"],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING: Entity <function PairedTextData._process_dataset.<locals>.<lambda> at 0x7f48c0778e18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function PairedTextData._process_dataset.<locals>.<lambda> at 0x7f48c06c2510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function PairedTextData._process_dataset.<locals>.<lambda> at 0x7f48c06868c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1kGOUcw5sJlu"},"source":["## MLE training"]},{"cell_type":"code","metadata":{"id":"9M2cVhBYpOZb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610883888707,"user_tz":-60,"elapsed":281231,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"0594f082-7ba5-4f9f-c87f-5628e78fcb1f"},"source":["# Now we manually write the training loops...\n","# not as cosy as Keras, for sure..\n","# Nothing to do in this cell, just observe\n","\n","def mle_train_epoch(sess, iterator, train_op):\n","    \"\"\"Train the Seq2Seq model for an epoch.\n","    sess is a TF session to use, \n","    iterator is a TrainTestDataIterator with the data,\n","    train_op is training op in the model's graph.\n","    \"\"\"\n","    iterator.switch_to_train_data(sess)\n","    for step in range(steps_per_train_epochs):\n","        try:\n","            loss = sess.run(train_op) # Run graph until the train op\n","            if step % display == 0:\n","                print(\"step={}, loss={:.4f}\".format(step, loss))\n","        except tf.errors.OutOfRangeError:\n","            break\n","\n","\n","def eval_epoch(sess, mode, iterator, batch):\n","    \"\"\" Evaluate an epoch. Mode is 'test' or 'val'.\n","    \"\"\"\n","    if mode == 'val':\n","        iterator.switch_to_val_data(sess)\n","    else:\n","        iterator.switch_to_test_data(sess)\n","\n","    refs, hypos = [], []\n","    while True:\n","        try:\n","            # fetches are what we want to get back from the session\n","            # in this case the target texts and the predicted texts\n","            fetches = [\n","                batch['target_text'][:, 1:],\n","                infer_outputs.predicted_ids[:, :, 0]\n","            ]\n","            feed_dict = {\n","                tx.global_mode(): tf.estimator.ModeKeys.PREDICT,\n","            }\n","            target_texts, output_ids = \\\n","                sess.run(fetches, feed_dict=feed_dict)\n","\n","            target_texts = tx.utils.strip_special_tokens(target_texts)\n","            output_texts = tx.utils.map_ids_to_strs(\n","                ids=output_ids, vocab=val_data.target_vocab)\n","\n","            for hypo, ref in zip(output_texts, target_texts):\n","                hypos.append(hypo)\n","                refs.append([ref])\n","        except tf.errors.OutOfRangeError:\n","            break\n","    # For evaluation we want to use a BLEU variant:\n","    # please put here Texar's \"moses\" corpus BLEU variant.\n","    return tx.evals.corpus_bleu_moses(list_of_references=refs,\n","                                        hypotheses=hypos)\n","\n","\n","def mle_train_and_eval(sess, iterator, batch, train_op):\n","    \"\"\"Train the model with MLE and eval.\n","    \"\"\"\n","    best_val_bleu = -1.\n","    for i in range(mle_training_num_epochs):\n","        mle_train_epoch(sess, iterator, train_op)\n","\n","        val_bleu = eval_epoch(sess, 'val', iterator, batch)\n","        best_val_bleu = max(best_val_bleu, val_bleu)\n","        print('val epoch={}, BLEU={:.4f}; best-ever={:.4f}'.format(\n","            i, val_bleu, best_val_bleu))\n","\n","        test_bleu = eval_epoch(sess, 'test', iterator, batch)\n","        print('test epoch={}, BLEU={:.4f}'.format(i, test_bleu))\n","\n","        print('=' * 50)\n","\n","\n","# The only thing left is to run the training and evaluation:\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    sess.run(tf.local_variables_initializer())\n","    sess.run(tf.tables_initializer())\n","    mle_train_and_eval(sess, iterator, batch, train_op)\n","   "],"execution_count":16,"outputs":[{"output_type":"stream","text":["step=0, loss=35.2633\n","step=50, loss=32.0072\n","step=100, loss=30.4397\n","step=150, loss=27.6329\n","step=200, loss=27.4042\n","step=250, loss=26.3508\n","step=300, loss=23.8906\n","val epoch=0, BLEU=4.7400; best-ever=4.7400\n","test epoch=0, BLEU=4.7600\n","==================================================\n","step=0, loss=22.6689\n","step=50, loss=19.9428\n","step=100, loss=16.1234\n","step=150, loss=12.1089\n","step=200, loss=9.5302\n","step=250, loss=7.2472\n","step=300, loss=5.1733\n","val epoch=1, BLEU=85.6000; best-ever=85.6000\n","test epoch=1, BLEU=86.2100\n","==================================================\n","step=0, loss=4.6976\n","step=50, loss=2.8876\n","step=100, loss=2.7292\n","step=150, loss=1.5512\n","step=200, loss=1.5769\n","step=250, loss=0.9650\n","step=300, loss=0.8511\n","val epoch=2, BLEU=98.3200; best-ever=98.3200\n","test epoch=2, BLEU=99.1400\n","==================================================\n","step=0, loss=0.6466\n","step=50, loss=0.6676\n","step=100, loss=0.6413\n","step=150, loss=0.4958\n","step=200, loss=0.3775\n","step=250, loss=0.2764\n","step=300, loss=0.2814\n","val epoch=3, BLEU=99.7200; best-ever=99.7200\n","test epoch=3, BLEU=99.6800\n","==================================================\n","step=0, loss=0.2933\n","step=50, loss=0.2498\n","step=100, loss=0.2815\n","step=150, loss=0.1895\n","step=200, loss=0.1522\n","step=250, loss=0.1727\n","step=300, loss=0.1087\n","val epoch=4, BLEU=99.8200; best-ever=99.8200\n","test epoch=4, BLEU=99.9800\n","==================================================\n","step=0, loss=0.1172\n","step=50, loss=0.0827\n","step=100, loss=0.0714\n","step=150, loss=0.0772\n","step=200, loss=0.0668\n","step=250, loss=0.0902\n","step=300, loss=0.0712\n","val epoch=5, BLEU=99.9300; best-ever=99.9300\n","test epoch=5, BLEU=100.0000\n","==================================================\n","step=0, loss=0.0662\n","step=50, loss=0.0629\n","step=100, loss=0.0480\n","step=150, loss=0.0593\n","step=200, loss=0.0622\n","step=250, loss=0.0487\n","step=300, loss=0.0395\n","val epoch=6, BLEU=99.9900; best-ever=99.9900\n","test epoch=6, BLEU=99.9800\n","==================================================\n","step=0, loss=0.0476\n","step=50, loss=0.0497\n","step=100, loss=0.0346\n","step=150, loss=0.0498\n","step=200, loss=0.0307\n","step=250, loss=0.0244\n","step=300, loss=0.0305\n","val epoch=7, BLEU=100.0000; best-ever=100.0000\n","test epoch=7, BLEU=100.0000\n","==================================================\n","step=0, loss=0.0180\n","step=50, loss=0.0237\n","step=100, loss=0.0269\n","step=150, loss=0.0216\n","step=200, loss=0.0196\n","step=250, loss=0.0201\n","step=300, loss=0.0188\n","val epoch=8, BLEU=100.0000; best-ever=100.0000\n","test epoch=8, BLEU=100.0000\n","==================================================\n","step=0, loss=0.0188\n","step=50, loss=0.0146\n","step=100, loss=0.0192\n","step=150, loss=0.0161\n","step=200, loss=0.0129\n","step=250, loss=0.0123\n","step=300, loss=0.0136\n","val epoch=9, BLEU=100.0000; best-ever=100.0000\n","test epoch=9, BLEU=100.0000\n","==================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cdi8ep_6J88v"},"source":["# Attention Seq2Seq with Policy gradient"]},{"cell_type":"markdown","metadata":{"id":"hdvW_Y9oxb-l"},"source":["## Building the model"]},{"cell_type":"code","metadata":{"id":"6pQD8DO2Vju1","executionInfo":{"status":"ok","timestamp":1610884962208,"user_tz":-60,"elapsed":643,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["def build_rl_model(batch, train_data):\n","    \"\"\"Build a seq2seq model trained with Policy Gradient.\n","    \"\"\"\n","    \n","    # Our RL-trained model will be almost totally the same as the previous one,\n","    # except that we add sampled outputs for Policy Gradient.\n","    # So, please repeat here the missing elements of the previous model. (Copy & paste...)\n","    \n","    source_embedder = tx.modules.WordEmbedder(\n","        vocab_size=train_data.source_vocab.size, hparams=embedder_hparams)\n","    \n","    \n","    encoder = tx.modules.BidirectionalRNNEncoder(\n","        hparams=encoder_hparams)\n","    \n","    enc_outputs, _ = encoder(source_embedder(batch['source_text_ids']))\n","\n","    target_embedder =tx.modules.WordEmbedder(\n","        vocab_size=train_data.target_vocab.size, hparams=embedder_hparams)\n","\n","    decoder = tx.modules.AttentionRNNDecoder(\n","        memory=tf.concat(enc_outputs, axis=2),\n","        memory_sequence_length=batch['source_length'],\n","        vocab_size=train_data.target_vocab.size,\n","        hparams=decoder_hparams)\n","\n","    # MLE pretraining\n","\n","    mle_training_outputs, _, _ = decoder(\n","        decoding_strategy='train_greedy', # Please specify greedy training decoding here \n","                               # see the possible values in the Texar \"Decoders\" documentation section\n","        inputs=target_embedder(batch['target_text_ids'][:, :-1]),\n","        sequence_length=batch['target_length'] - 1)\n","    \n","    mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n","        labels=batch['target_text_ids'][:, 1:],\n","        logits=mle_training_outputs.logits,\n","        sequence_length=batch['target_length'] - 1)\n","\n","    mle_train_op = tx.core.get_train_op(mle_loss)\n","\n","    start_tokens = tf.ones_like(batch['target_length']) * train_data.target_vocab.bos_token_id\n","\n","    beam_search_outputs, _, _ = \\\n","        tx.modules.beam_search_decode(\n","            decoder_or_cell=decoder,\n","            embedding=target_embedder,\n","            start_tokens=start_tokens,\n","            end_token=train_data.target_vocab.eos_token_id,\n","            beam_width=beam_width,\n","            max_decoding_length=60)    \n","    \n","    # Here comes the novelty...\n","    # We need random sampling for Policy Gradient\n","    sampled_outputs, _, sequence_length = decoder(\n","        decoding_strategy= 'infer_sample', # Please add here the correct 'decoding strategy' for random sampling\n","        start_tokens=start_tokens,\n","        end_token=train_data.target_vocab.eos_token_id,\n","        embedding=target_embedder,\n","        max_decoding_length=30)\n","\n","    # We need to return a bit more things from the graph for Policy Gradient\n","    return sampled_outputs, mle_train_op, sequence_length, beam_search_outputs"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"riMzhKObyC-j"},"source":["## Data sets and iterator"]},{"cell_type":"code","metadata":{"id":"52KC7jzajrQs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610884966960,"user_tz":-60,"elapsed":3024,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"ae385c55-944c-491a-abb0-906a5bd2fcb5"},"source":["tf.reset_default_graph()\n","\n","# Please repeat here the previous definitions for our data sets and iterator!\n","\n","train_data  =  tx.data.PairedTextData(hparams=train_hparams)\n","val_data    =  tx.data.PairedTextData(hparams=val_hparams)\n","test_data   =  tx.data.PairedTextData(hparams=test_hparams)\n","\n","iterator = tx.data.TrainTestDataIterator(train=train_data, val=val_data, test=test_data)\n","\n","batch = iterator.get_next()\n","\n","# We build the model:\n","sampled_outputs, mle_train_op, sequence_length, infer_outputs = build_rl_model(batch, train_data)\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["WARNING: Entity <function PairedTextData._process_dataset.<locals>.<lambda> at 0x7f48e31e5d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function PairedTextData._process_dataset.<locals>.<lambda> at 0x7f48e3179400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","WARNING: Entity <function PairedTextData._process_dataset.<locals>.<lambda> at 0x7f48e313c840> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hoezIYR9hFHo"},"source":["## Agent definition"]},{"cell_type":"code","metadata":{"id":"PPWbxLrvhEPT","executionInfo":{"status":"ok","timestamp":1610884972978,"user_tz":-60,"elapsed":1536,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["# Now a crucial point: we need te create a Texar Sequence Policy Gradient Agent\n","# Please specify the correct Texar class!\n","agent = tx.agents.SeqPGAgent(\n","    samples=sampled_outputs.sample_id,\n","    logits=sampled_outputs.logits,\n","    sequence_length=sequence_length,\n","    hparams={'discount_factor': 0.95, 'entropy_weight': 0.5})"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1iL-E0xg0baR"},"source":["## Policy gradient training"]},{"cell_type":"code","metadata":{"id":"P9uDJwtFaKg5","executionInfo":{"status":"ok","timestamp":1610885247273,"user_tz":-60,"elapsed":682,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["# PG training and evaluation function\n","def pg_train_and_eval_epoch(sess, agent, iterator, batch):\n","    best_val_bleu = -1.\n","    for step in range(steps_per_train_epochs):\n","        iterator.switch_to_train_data(sess)\n","\n","        \n","        extra_fetches = {\n","            'truth': batch['target_text_ids'],\n","        }\n","\n","        # The agent needs to get the samples with the current policy.\n","        # Please add the the correct agent method in the next line!!\n","        # to be clear: you will need something like \n","        # fetches = agent.<METHOD_NAME>(extra_fetches=extra_fetches) \n","        # here.\n","        fetches = agent.get_samples(extra_fetches=extra_fetches)\n","\n","        sample_text = tx.utils.map_ids_to_strs(\n","            fetches['samples'], train_data.target_vocab,\n","            strip_eos=False, join=False)\n","        truth_text = tx.utils.map_ids_to_strs(\n","            fetches['truth'], train_data.target_vocab,\n","            strip_eos=False, join=False)    \n","\n","        # Compute the rewards\n","        reward = []\n","        for ref, hyp in zip(truth_text, sample_text):\n","            r = tx.evals.sentence_bleu([ref], hyp, smooth=True)\n","            reward.append(r)\n","\n","        # Now we need to do the actual weight updates with the policy gradient,\n","        # in the Texar API this is called \"observing\".\n","        # Please add, again, the correct method name in the next line!\n","        loss = agent.observe(reward=reward)\n","\n","        # Displays & evaluates\n","        if step == 1 or step % display == 0:\n","            print(\"step={}, loss={:.4f}, reward={:.4f}\".format(\n","                step, loss, np.mean(reward)))\n","\n","        if step % display == 0:\n","            val_bleu = eval_epoch(sess, 'val', iterator, batch)\n","            best_val_bleu = max(best_val_bleu, val_bleu)\n","            print('val step={}, BLEU={:.4f}; best-ever={:.4f}'.format(\n","                step, val_bleu, best_val_bleu))\n","\n","            test_bleu = eval_epoch(sess, 'test', iterator, batch)\n","            print('test step={}, BLEU={:.4f}'.format(step, test_bleu))\n","            print('=' * 50)\n","\n"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7c3twV451aAH"},"source":["## Running it"]},{"cell_type":"code","metadata":{"id":"v6aQAZQl1LUl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610886276068,"user_tz":-60,"elapsed":1026526,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"fc234105-42b8-479a-82cc-aaf0e5a55990"},"source":["mle_training_num_epochs = 10 # Specify the number of MLE training epochs!\n","pg_train_num_epochs = 5 # Specify the number of PG training epochs!\n","\n","# Now we can run the training and see how the (untuned) Policy Gradient training quickly ruins \n","# the MLE results...\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    sess.run(tf.local_variables_initializer())\n","    sess.run(tf.tables_initializer())\n","\n","    print(\"== Starting MLE pretraining ==\")\n","\n","    mle_train_and_eval(sess, iterator, batch, mle_train_op)\n","\n","    print(\"== Starting PG training ==\")\n","\n","    agent.sess = sess\n","\n","    for epoch in range(pg_train_num_epochs):\n","        print('=' * 50)\n","        print('== EPOCH NO', epoch, '==')\n","        print('=' * 50)\n","        pg_train_and_eval_epoch(sess, agent, iterator, batch)\n","        "],"execution_count":26,"outputs":[{"output_type":"stream","text":["== Starting MLE pretraining ==\n","step=0, loss=34.9225\n","step=50, loss=32.7071\n","step=100, loss=30.9545\n","step=150, loss=29.0667\n","step=200, loss=28.2161\n","step=250, loss=26.1942\n","step=300, loss=24.1638\n","val epoch=0, BLEU=4.0500; best-ever=4.0500\n","test epoch=0, BLEU=4.2800\n","==================================================\n","step=0, loss=22.8864\n","step=50, loss=20.0676\n","step=100, loss=17.2732\n","step=150, loss=13.3734\n","step=200, loss=10.2090\n","step=250, loss=7.3831\n","step=300, loss=5.5184\n","val epoch=1, BLEU=82.3900; best-ever=82.3900\n","test epoch=1, BLEU=82.6600\n","==================================================\n","step=0, loss=4.8400\n","step=50, loss=3.4950\n","step=100, loss=2.7525\n","step=150, loss=2.2772\n","step=200, loss=1.9515\n","step=250, loss=1.3341\n","step=300, loss=1.2279\n","val epoch=2, BLEU=97.4700; best-ever=97.4700\n","test epoch=2, BLEU=97.2400\n","==================================================\n","step=0, loss=1.0864\n","step=50, loss=0.7294\n","step=100, loss=0.7317\n","step=150, loss=0.8132\n","step=200, loss=0.8026\n","step=250, loss=0.4211\n","step=300, loss=0.4446\n","val epoch=3, BLEU=98.9300; best-ever=98.9300\n","test epoch=3, BLEU=98.8100\n","==================================================\n","step=0, loss=0.3842\n","step=50, loss=0.4737\n","step=100, loss=0.3425\n","step=150, loss=0.3096\n","step=200, loss=0.3330\n","step=250, loss=0.2145\n","step=300, loss=0.2530\n","val epoch=4, BLEU=99.5200; best-ever=99.5200\n","test epoch=4, BLEU=99.1900\n","==================================================\n","step=0, loss=0.2399\n","step=50, loss=0.1895\n","step=100, loss=0.1500\n","step=150, loss=0.1672\n","step=200, loss=0.2371\n","step=250, loss=0.1618\n","step=300, loss=0.1295\n","val epoch=5, BLEU=99.5800; best-ever=99.5800\n","test epoch=5, BLEU=99.3500\n","==================================================\n","step=0, loss=0.1687\n","step=50, loss=0.1095\n","step=100, loss=0.0885\n","step=150, loss=0.1138\n","step=200, loss=0.0967\n","step=250, loss=0.0609\n","step=300, loss=0.0634\n","val epoch=6, BLEU=99.7300; best-ever=99.7300\n","test epoch=6, BLEU=99.7500\n","==================================================\n","step=0, loss=0.1285\n","step=50, loss=0.0743\n","step=100, loss=0.0622\n","step=150, loss=0.0795\n","step=200, loss=0.0555\n","step=250, loss=0.0547\n","step=300, loss=0.0468\n","val epoch=7, BLEU=99.7400; best-ever=99.7400\n","test epoch=7, BLEU=99.4200\n","==================================================\n","step=0, loss=0.0508\n","step=50, loss=0.0583\n","step=100, loss=0.0492\n","step=150, loss=0.0440\n","step=200, loss=0.0475\n","step=250, loss=0.0421\n","step=300, loss=0.0347\n","val epoch=8, BLEU=99.9800; best-ever=99.9800\n","test epoch=8, BLEU=99.6300\n","==================================================\n","step=0, loss=0.0373\n","step=50, loss=0.0271\n","step=100, loss=0.0340\n","step=150, loss=0.0344\n","step=200, loss=0.0294\n","step=250, loss=0.0232\n","step=300, loss=0.0324\n","val epoch=9, BLEU=99.9300; best-ever=99.9800\n","test epoch=9, BLEU=99.7100\n","==================================================\n","== Starting PG training ==\n","==================================================\n","== EPOCH NO 0 ==\n","==================================================\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/texar/tf/utils/dtypes.py:120: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(str_list)\n"],"name":"stderr"},{"output_type":"stream","text":["step=0, loss=6.8981, reward=66.5499\n","val step=0, BLEU=86.2100; best-ever=86.2100\n","test step=0, BLEU=86.6800\n","==================================================\n","step=1, loss=40.2444, reward=57.7852\n","step=50, loss=52.7256, reward=36.7673\n","val step=50, BLEU=58.3800; best-ever=86.2100\n","test step=50, BLEU=59.2500\n","==================================================\n","step=100, loss=43.6967, reward=37.2814\n","val step=100, BLEU=36.0800; best-ever=86.2100\n","test step=100, BLEU=37.3300\n","==================================================\n","step=150, loss=35.8899, reward=33.0831\n","val step=150, BLEU=43.1800; best-ever=86.2100\n","test step=150, BLEU=42.6700\n","==================================================\n","step=200, loss=34.8923, reward=17.6722\n","val step=200, BLEU=19.8400; best-ever=86.2100\n","test step=200, BLEU=19.1000\n","==================================================\n","step=250, loss=27.5854, reward=11.5137\n","val step=250, BLEU=10.7200; best-ever=86.2100\n","test step=250, BLEU=10.6000\n","==================================================\n","step=300, loss=28.2578, reward=11.5083\n","val step=300, BLEU=11.3700; best-ever=86.2100\n","test step=300, BLEU=11.9200\n","==================================================\n","==================================================\n","== EPOCH NO 1 ==\n","==================================================\n","step=0, loss=29.4794, reward=11.6021\n","val step=0, BLEU=8.5800; best-ever=8.5800\n","test step=0, BLEU=8.9700\n","==================================================\n","step=1, loss=33.8628, reward=10.7056\n","step=50, loss=21.9623, reward=9.4223\n","val step=50, BLEU=5.5900; best-ever=8.5800\n","test step=50, BLEU=5.9500\n","==================================================\n","step=100, loss=20.3172, reward=6.9088\n","val step=100, BLEU=1.1600; best-ever=8.5800\n","test step=100, BLEU=1.4800\n","==================================================\n","step=150, loss=25.3003, reward=7.1379\n","val step=150, BLEU=1.1900; best-ever=8.5800\n","test step=150, BLEU=1.3600\n","==================================================\n","step=200, loss=26.2240, reward=7.2641\n","val step=200, BLEU=1.5800; best-ever=8.5800\n","test step=200, BLEU=1.8900\n","==================================================\n","step=250, loss=26.6693, reward=7.2051\n","val step=250, BLEU=1.3400; best-ever=8.5800\n","test step=250, BLEU=1.6100\n","==================================================\n","step=300, loss=28.4046, reward=6.8516\n","val step=300, BLEU=1.6500; best-ever=8.5800\n","test step=300, BLEU=1.5800\n","==================================================\n","==================================================\n","== EPOCH NO 2 ==\n","==================================================\n","step=0, loss=29.0226, reward=7.3251\n","val step=0, BLEU=1.7500; best-ever=1.7500\n","test step=0, BLEU=1.7000\n","==================================================\n","step=1, loss=26.2345, reward=7.0758\n","step=50, loss=31.7117, reward=7.2505\n","val step=50, BLEU=1.3900; best-ever=1.7500\n","test step=50, BLEU=1.4500\n","==================================================\n","step=100, loss=37.9095, reward=6.6292\n","val step=100, BLEU=1.2500; best-ever=1.7500\n","test step=100, BLEU=1.2800\n","==================================================\n","step=150, loss=32.3643, reward=6.5004\n","val step=150, BLEU=1.3900; best-ever=1.7500\n","test step=150, BLEU=1.3900\n","==================================================\n","step=200, loss=35.7137, reward=6.9452\n","val step=200, BLEU=1.4700; best-ever=1.7500\n","test step=200, BLEU=1.7100\n","==================================================\n","step=250, loss=34.8378, reward=7.0912\n","val step=250, BLEU=1.5100; best-ever=1.7500\n","test step=250, BLEU=1.7800\n","==================================================\n","step=300, loss=45.2811, reward=7.3744\n","val step=300, BLEU=1.2100; best-ever=1.7500\n","test step=300, BLEU=1.1900\n","==================================================\n","==================================================\n","== EPOCH NO 3 ==\n","==================================================\n","step=0, loss=62.7092, reward=6.2259\n","val step=0, BLEU=0.4200; best-ever=0.4200\n","test step=0, BLEU=0.5000\n","==================================================\n","step=1, loss=48.5758, reward=5.9256\n","step=50, loss=64.1603, reward=5.7643\n","val step=50, BLEU=0.0900; best-ever=0.4200\n","test step=50, BLEU=0.1300\n","==================================================\n","step=100, loss=57.1242, reward=5.5661\n","val step=100, BLEU=0.0900; best-ever=0.4200\n","test step=100, BLEU=0.1300\n","==================================================\n","step=150, loss=60.1585, reward=5.3732\n","val step=150, BLEU=0.1200; best-ever=0.4200\n","test step=150, BLEU=0.1000\n","==================================================\n","step=200, loss=49.0580, reward=5.1686\n","val step=200, BLEU=0.0900; best-ever=0.4200\n","test step=200, BLEU=0.1100\n","==================================================\n","step=250, loss=57.5856, reward=5.3825\n","val step=250, BLEU=0.1200; best-ever=0.4200\n","test step=250, BLEU=0.0900\n","==================================================\n","step=300, loss=63.3885, reward=5.6768\n","val step=300, BLEU=0.1000; best-ever=0.4200\n","test step=300, BLEU=0.1300\n","==================================================\n","==================================================\n","== EPOCH NO 4 ==\n","==================================================\n","step=0, loss=72.8124, reward=6.0795\n","val step=0, BLEU=0.1300; best-ever=0.1300\n","test step=0, BLEU=0.1300\n","==================================================\n","step=1, loss=69.8360, reward=5.9508\n","step=50, loss=81.9250, reward=6.3566\n","val step=50, BLEU=0.1200; best-ever=0.1300\n","test step=50, BLEU=0.1200\n","==================================================\n","step=100, loss=93.1386, reward=6.7036\n","val step=100, BLEU=0.1600; best-ever=0.1600\n","test step=100, BLEU=0.1300\n","==================================================\n","step=150, loss=110.8245, reward=7.5207\n","val step=150, BLEU=0.1600; best-ever=0.1600\n","test step=150, BLEU=0.1600\n","==================================================\n","step=200, loss=115.8251, reward=7.6792\n","val step=200, BLEU=0.1900; best-ever=0.1900\n","test step=200, BLEU=0.2300\n","==================================================\n","step=250, loss=131.1340, reward=7.6648\n","val step=250, BLEU=0.1800; best-ever=0.1900\n","test step=250, BLEU=0.1700\n","==================================================\n","step=300, loss=133.7383, reward=7.7504\n","val step=300, BLEU=0.1700; best-ever=0.1900\n","test step=300, BLEU=0.1500\n","==================================================\n"],"name":"stdout"}]}]}