{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Different branches of RL, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frameworks to train faster the RL algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GORILA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1507.04296.pdf)\n",
    "\n",
    "General Reinforcement Learning Architecture\n",
    "\n",
    "Scalable architecture for DQN like RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1pte5VawZYKyuwdf8ftBtGiwPAzIu2eRe\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1OaJdfQDCArd9o2JrVPfVVezdql6YqY2Q\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### IMPALA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1802.01561.pdf)\n",
    "\n",
    "Importance Weighted Actor-Learner Architecture\n",
    "\n",
    "Scalable architecture for actor-critic like algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1MZZD1xuzP3H8bpLdUNKSsAQFSuZIchzx\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Main idea:\n",
    "\n",
    "* learns an actor (policy) and a critic (V)\n",
    "* decouples the learning from acting\n",
    "* there is a policy lag due to the parallel actors, they propose V-trace as a solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Policy distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1511.06295.pdf)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1ENOIXhN3MgM2BHSOGfdCqh001-mizIXj\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This method is good for compressing the knowledge of the DQN agent into a smaller network, which is more efficient to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1707.01495.pdf)\n",
    "\n",
    "Hindsight Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"We demonstrate our approach on the task of manipulating objects with a robotic\n",
    "arm. In particular, we run experiments on three different tasks: pushing, sliding,\n",
    "and pick-and-place, in each case using only binary rewards indicating whether or\n",
    "not the task is completed. Our ablation studies show that Hindsight Experience\n",
    "Replay is a crucial ingredient which makes training possible in these challenging\n",
    "environments. We show that our policies trained on a physics simulation can\n",
    "be deployed on a physical robot and successfully complete the task. The video\n",
    "presenting our experiments is available at [link](https://goo.gl/SMrQnI).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This method is usefule when you have explicit goals. Then the goals are also included in the experiences and saved in the replay buffer. Traning with noisy observations can help to transfer the policy into the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other areas of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Multitask learning\n",
    "* Multiagent learning\n",
    "* inverse reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "During the course we have covered:\n",
    "\n",
    "* Bellman-formalizm as a mathematical framework\n",
    "* Value-based RL (classic, linear-approximators, dnn)\n",
    "* Policy-based RL (policy gradient theorems, actor-critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
