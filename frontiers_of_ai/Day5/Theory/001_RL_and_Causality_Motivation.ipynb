{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinfocement Learning and Causality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.1. Motivation\n",
    "### 1.1.1 Challenges in RL\n",
    "\n",
    "\n",
    "__Many real word problems in reinforcement learning share some of the following characteristics__\n",
    "\n",
    "* Environment partially observed (e.g. POMDP)-> unobserved confounders, mismatch between model and the true environment\n",
    "* Costly to conduct trials (on policy) -> data efficiency essential, infinite data convergence proofs not good enough\n",
    "* RL data use inefficient -> higher efficiency needed (e.g. buffer replay)\n",
    "* Observational data (based on other policies) -> off policy learning\n",
    "* RL often does not generalize well (if the tasks slightly change)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1.2 Relevance of causality \n",
    "\n",
    "__Back to the basics: the RL environment__\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1w1YCEYSUSYXkpH2i8wjuKzeL3vh5lLaD\" width=50%>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Source**, [here](https://www.youtube.com/watch?v=QRTgLWfFBMM). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Back to the basics: RL Environment and Causality__\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1-Bb-toVaiiSV7rvhuejOZq1zIq_lq54_\" width=50%>\n",
    "\n",
    "\n",
    "**Source**, [here](https://www.youtube.com/watch?v=QRTgLWfFBMM). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1.3 A counterfactual missing data problem \n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1w28BxgOSCIEz7tnFlK7t_EulxEe4b0aN\" width=50%>\n",
    "\n",
    "\n",
    "\n",
    "**Source**, [here](https://www.youtube.com/watch?v=GXjc-tomqpo&t). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1euJXZDfgWJQBPTrxZmVuWOgleokZLiXO\" width=50%>\n",
    "\n",
    "\n",
    "**Source**, [here](https://www.youtube.com/watch?v=GXjc-tomqpo&t). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1iq-oEP5nwayHEgnfeHpOkRtPT-y5Wppj\" width=50%>\n",
    "\n",
    "\n",
    "**Source**, [here](https://www.youtube.com/watch?v=GXjc-tomqpo&t). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1dzeHf4T7iIx7XUWZtc_SW2jKpY7AFTEI\" width=50%>\n",
    "\n",
    "\n",
    "\n",
    "**Source**, [here](https://www.youtube.com/watch?v=GXjc-tomqpo&t). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1EqRnXKlY6A4rZET2eOT0zICvNG3NYp7u\" width=50%>\n",
    "\n",
    "\n",
    "\n",
    "**Source**, [here](https://www.youtube.com/watch?v=GXjc-tomqpo&t). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=16p_7ESC1wEKoAWlsYdqjK7S8cLKFVV6T\" width=50%>\n",
    "\n",
    "\n",
    "\n",
    "But it has very high variance (remember that the total error is bias + variance)\n",
    "\n",
    "\n",
    "**Source**, [here](https://www.youtube.com/watch?v=GXjc-tomqpo&t). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1fCTUxfaAflE2a0SsA1iosZ8r6VeBvbYW\" width=50%>\n",
    "\n",
    "\n",
    "\n",
    "**Source**, [here](https://www.youtube.com/watch?v=GXjc-tomqpo&t). \n",
    "\n",
    "\n",
    "\n",
    "Applying a doubly robust estimator to reinforcement learning\n",
    "\n",
    "\n",
    "**Jiang & Li (2016). Doubly Robust Off-policy Value Evaluation for Reinforcement Learning**, [here](http://proceedings.mlr.press/v48/jiang16.pdf). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Building a model to infer the missing data__\n",
    "\n",
    "* Alternative is to build a model to infer the observed data\n",
    "\n",
    "\n",
    "* But based on a particular behavioural policy -> lower variance but biased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Take home messages__\n",
    "\n",
    "* RL Missing data problem (compare to causality)\n",
    "\n",
    "\n",
    "* The actions not taken and rewards for these actions (missing data) are counterfactuals\n",
    "\n",
    "\n",
    "* Missing data problem either addressed by correcting for __sampling error__ (at best - policy perfectly known) __high variance o__r by __uncorrected model - high bias__\n",
    "\n",
    "\n",
    "* Historically, naive approach to address the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1.4 Reinforcement learning as identifying relevant causal relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Judea Pearl (winner of the Turing Award):__\n",
    "\n",
    "_\"Is RL an exercise in Causal inference? Of course! Albeit a restricted one. By deploying interventions in training, RL allows us to __infer consequences__ of those __interventions__ but __ONLY those interventions__. A __causal model__ is needed to go __BEYOND, i.e. to actions to used in the training.__\"_\n",
    "\n",
    "_\"The relation between RL and Causal inference has been a topic of some debate. It can be resolved, I believe, by understanding the limits of each.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Bernhard Sch√∂lkopf Director Max Planck for Intelligent Systems__\n",
    "\n",
    "*_\"__Question 1__: why is RL on the original high-dimensional Atari games harder than on a downsampled versions?\"_\n",
    "\n",
    "*_\"__Question 2__: why is RL easier if we permute the replayed data?\"_\n",
    "\n",
    "_\"RL is closer to causality research than the machine learning mainstream in that it sometimes effectively directly estimates __do probabilities__ (on policy learning). However, as soon as off-policy learning is considered, in particular in the batch (or observational setting), issues of causality become subtle.\"_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Proof: knowing the causal structure leads to orders of magnitude more effective RL__\n",
    "\n",
    "**Zhang & Bareinboim, 2020. \"Designing Optimal Dynamic Treatment Regimes:\n",
    "A Causal Reinforcement Learning Approach\"**, [here](http://proceedings.mlr.press/v119/zhang20a/zhang20a.pdf).\n",
    "\n",
    "* Reinforcement learning algorithms for finding the optimal DTR (Dynamic Treatment Regimes) in online settings will suffer $\\Omega\\left(\\sqrt{\\left|\\mathcal{D}_{X \\cup S}\\right| T}\\right)$ regret on some environments\n",
    "\n",
    "* Where $T$ is the number of experiments and $\\mathcal{D}_{X \\cup S}$ is the domains of the treatments $X$ and covariates $S .$\n",
    "\n",
    "* And $f=\\tilde{\\mathcal{O}}(g) \\text { if and only if } \\exists k \\text { such that } f=\\mathcal{O}\\left(g \\log ^{k}(g)\\right)\n",
    "$ is a lower bound for $\\Omega$\n",
    "* Implies that $T=\\Omega\\left(\\left|\\mathcal{D}_{\\boldsymbol{X} \\cup S}\\right|\\right)$ trials will be required to generate an optimal DTR. \n",
    "* In many applications, the domains of $X$ and $S$ could be enormous, which means that the time required to ensure appropriate learning may be unattainable. \n",
    "* Paper shows show that, if the causal diagram of the underlying environment is provided, one could achieve regret that is exponentially smaller than $\\mathcal{D}_{X \\cup S} .$ \n",
    "* Develops two online algorithms that satisfy such regret bounds by exploiting the causal structure underlying the DTR; one is the based on the principle of optimism in the face of uncertainty (OFU-DTR), and the other uses the posterior sampling learning (PS-DTR). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Take home message:__\n",
    "\n",
    "\n",
    "__Knowing the causal structure can make sampling efficiency exponentially smaller__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Concrete Example of RL without causality and its bias in the case of unobserved confounders__\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1ngGXYCqhR80niXOLnJ5ZvTklMWW-5uJl\" width=30%>\n",
    "\n",
    "\n",
    "\n",
    "**Ashton (2020), \"Causal Campbell-Goodhart‚Äôs law and Reinforcement Learning\"**, [here](https://arxiv.org/pdf/2011.01010.pdf). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Experiment $1,$ pressure has no auto-correlation:\n",
    "$\\rho_{L L}=\\rho_{H H}=0.5 .$ This makes the strategy of waiting for high pressure or a high barometer reading the most efficient. We denote this strategy $\\Pi_{n w}$. Table 5 shows the results of the different training methods. When Pressure is visible, the optimal strategy is recovered by both algos though some of the time $\\mathrm{A} 2 \\mathrm{C}$ converges on $\\Pi_{n c}$ - wear or don't wear coat according to barometer. The A2C algorithm successfully finds the optimal strategy $\\Pi_{n c}$ on every occasion. The DQN algorithm always converges on $\\Pi_{n b}$; the naive strategy that involves pressing the barometer button if the barometer is initially low and going outside without a coat if the barometer is high.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1VXLSmJROi2IcXzng-ubBBsjNhI4wzrwD\" width=50%>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ashton (2020), \"Causal Campbell-Goodhart‚Äôs law and Reinforcement Learning\"**, [here](https://arxiv.org/pdf/2011.01010.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Take home message__\n",
    "\n",
    "* In the presence of unobserved confounders RL can be highly biased\n",
    "* In real world situations we rarely observe all relevant variables\n",
    "* POMDP will have unobserved confounders\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall take home message:\n",
    "* RL is essentially a __missing data and a causal inference problem__\n",
    "* The __missing data__ are the __counterfactuals__ of the causal interventions that could not be observed (vs. the one that was observed)\n",
    "* Not taking this into account leads either to estimating results with __high variance__ (given that the policy is known, or __highly biased__ model estimation, if the sampling imbalances are not taken into account\n",
    "* This leads at best to __exponentially less effective sampling__ and at __worst to incorrect (biased)__ results e.g. in the case of hidden confounders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two main closely related causal approaches:\n",
    "* Neymen-Ruben Causal Model - estimating effects of particular treatments\n",
    "* Structural Causal Model - SCM (Pearl)   _see separate notebook_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember some key points of causality:\n",
    "    \n",
    "* Balanced distributions\n",
    "* Local similarity\n",
    "* Statistical twins (finding a statistical twin that is very similar except for the treatment)\n",
    "* If we have a non-treatment the statistical twin becomes data imputation where we want to find the closest neighbour to the missing data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
