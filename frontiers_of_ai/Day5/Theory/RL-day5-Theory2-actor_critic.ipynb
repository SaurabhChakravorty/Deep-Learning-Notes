{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Actor-critic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taxonomy of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1Gz0WBOtTxYrZ91uAidFE_Tuw0RBSfl9O\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1Is1w5j2e-3WOSSl9qya7BwKW0Hh1hIJL\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The actor-critic framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The actor critic framework consists of an actor and a critic. The actor is the policy (the behavior of the agent) we want to learn. The critic's task is to help the training of the actor. Technically it is used to calculate the policy gradient. A critic can be:\n",
    "\n",
    "* Q-function\n",
    "* V-function\n",
    "* A-function (advantage: Q-V)\n",
    "* other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The $\\rho$ objective function has different formulations:\n",
    "\n",
    "**Start-state formulation:** The start-state formulation uses the following function for measuring the performance of a policy:\n",
    "\n",
    "$$\\rho(\\pi_\\theta) = E_\\tau\\left[ \\left. \\sum_{t=0}^\\infty{\\gamma^t r_t} \\right| s_0, \\pi_\\theta \\right]$$\n",
    "\n",
    "**Average-reward fomrulation:**  The average-reward formulation uses the following function for measuring the performance of a policy:\n",
    "\n",
    "$$\\rho(\\pi_\\theta) = \\lim_{n \\rightarrow \\infty} \\frac{1}{n}E_\\tau\\left[ \\left. \\sum_{t=0}^n{r_t} \\right| \\pi_\\theta \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Theorem 1:** In both the start-state formulation and the average-reward formulation case the following formula is true for the gradient:\n",
    "\n",
    "$$\\frac{\\partial \\rho(\\pi_\\theta)}{\\partial \\theta} = E_\\tau \\left[ \\left. \\frac{\\partial \\log \\pi_\\theta(s, a)}{\\partial \\theta} \\cdot Q^{\\pi_\\theta}(s, a) \\right| \\pi_\\theta \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here the actor is defined by the policy: $\\pi_\\theta$. The critic is defined by the $Q^{\\pi_\\theta}$. The problem with using the $Q$-function that the gradient will be **too noisy**. In case of policy-based RL, the noisy gradient is always an issue. There is a commonly used technique to reduce the variance (noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It can be proven that by subtracting any arbitrary function $B(s)$ from the $Q(s, a)$, the gradient will not change but with a careful choice the **variance can be reduced significantly**. It turns out that the best choice for $B(s)$ is the state-value function itself, $V^{\\pi_\\theta}(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First let us see why the subtraction of $B(s)$ does not change the expected value:\n",
    "\n",
    "$$E\\left[ \\frac{\\partial \\log \\pi_\\theta(s, a)}{\\partial \\theta} B(s) \\right] = \\sum_s{d^{\\pi_\\theta}(s) \\sum_a{\\frac{\\partial \\pi_\\theta(s, a)}{\\partial \\theta} B(s)} } = \\sum_s{d^{\\pi_\\theta}(s) B(s) \\frac{\\partial}{\\partial \\theta} \\underbrace{\\sum_a{\\pi_\\theta(s, a)}}_{1}} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have already seen that the difference between $Q$ and $V$ is the so called advantage:\n",
    "\n",
    "$$A(s, a) = Q(s, a) - V(s)$$\n",
    "\n",
    "The usage of **advantage** reduces the variance significantly, indeed. But how can we estimate it efficiently? It is not a good idea to estimate both the $Q$ and the $V$ function because none of them will be accurate enough and we have to maintain three neural networks ($Q$, $V$ and $\\pi$). Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fortunately, we observe the **advantage can be estimated by the TD-error**:\n",
    "\n",
    "$$A^{\\pi_\\theta}(s, a) = E\\left[ \\delta^{\\pi_\\theta} | s, a \\right]$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\delta^{\\pi_\\theta} = r + \\gamma V^{\\pi_\\theta}(s') - V^{\\pi_\\theta}(s)$$\n",
    "\n",
    "That means, it is enough to estimate the $V$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarity to GANs\n",
    "\n",
    "In GANs there are generators and discriminators. The goal of the generator is to create images (or voices, or other artifacts) which is indistinguishable from the real ones according to the discriminator.\n",
    "\n",
    "In Actor-Critic algorithms, the actor (analogy: generator) tries to behave in a way that the critic (discriminator) can not criticize it (the policy gradient becomes 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Actor-critic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From now we will look into the details of different actor-critic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A3C (and A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1602.01783.pdf)\n",
    "\n",
    "Stands for Asynchronous Advantage Actor Critic.\n",
    "\n",
    "The TD-error is calculated with n-step return (we saw it earlier):\n",
    "\n",
    "$$\\sum_{i=0}^{k-1}{\\gamma^i r_{t+i} + \\gamma^k V(s_{t+k}; \\theta_v)} - V(s_t; \\theta_v).$$\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1QDvdX5cu2JJ6wuX8EFyJehn5_XmRWqBr\" >\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1lSf_vAVT5BWjSBBlNy-IMxtH11qSvP1m\" >\n",
    "\n",
    "Several agents learn parallel and they share the knowledge with each other (the networks in the threads are synchronized with a central network). This algorithm learns faster on CPUs then DQN on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Torch video](https://youtu.be/0xo1Ldx3L5Q)\n",
    "\n",
    "[Labyrinth video](https://youtu.be/nMR5mjCFZCw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "\n",
    "This algorithm is applicable for continuous actions, like actions in robotics. The policy (actor) is deterministic.\n",
    "\n",
    "**Why continuous action space is challenging?**\n",
    "\n",
    "One approach would be the discretization of a continuous space but this results in high dimension. For instance, if we have actions $(-k, 0, k)$, and 7 degrees of freedom, then we have $3^7 = 2187$ states. However, in reality after discretization we will have several 100 actions.\n",
    "\n",
    "The maximum operator also computationally inefficient when the $Q$-function is continuous:\n",
    "\n",
    "$$\\pi(s) = \\arg\\max_a Q(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=19_QJ8nt5mTrr531QajPEh9kNTU8C_c_v\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Deterministic policy gradient theorem:**\n",
    "\n",
    "$$\\frac{\\partial \\rho(\\pi_\\theta)}{\\partial \\theta} = E\\left[ \\frac{\\partial \\pi_\\theta(s)}{\\partial \\theta} \\frac{\\partial Q(s, a)}{\\partial a} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[video](https://www.youtube.com/watch?v=tJBIqkC1wWM&feature=youtu.be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/abs/1801.01290)\n",
    "\n",
    "Off-policy, actor-critic like DDPG but it is more robust and has better convergence guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/abs/1502.05477)\n",
    "\n",
    "Trusted region policy optimization.\n",
    "\n",
    "Uses natural gradients for searching the next policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1fpkg2tgbhGi7-lkpSr8uObrVrcqPsxZe\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1Rs6OBsrO9vYBMODGCbWvdwu0jC69iZF0\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* difficult to solve the constraint optimization\n",
    "* can learn high quality locomotion controllers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://arxiv.org/pdf/1707.06347.pdf)\n",
    "\n",
    "Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Constraint is expressed with KL divergence:**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1lkbVd04tv8L7yhVRdyHrEjx6TeuNdMh9\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=download&id=1hUfS4t_2dcw3Lt8QwwcpEXqFbpCdMjH4\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Constrant is expressed with clipping:**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1AcbJVnReKjNqDovugTL2HxqUgfo5hPEX\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1zf5o6ykK1pA3JGn9dT7-6rJM5q4i0oec\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dota2 with PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[paper](https://cdn.openai.com/dota-2.pdf)\n",
    "\n",
    "The policy for controlling OpenAI Five was trained with a scaled up version of PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Rapid:**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1TzDjjxwU2TbTE-Ij0w6bgI0AR54bkZXn\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Architecture:**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1Gdnd9lIM-Hap5wqDRJKg7PatQCZ1oX92\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The algorithm is quite simple but efficient:\n",
    "\n",
    "* it achieves long term horizon\n",
    "* collaboration among the agents emerges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
