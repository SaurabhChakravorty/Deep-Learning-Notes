{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"RL-day3-gridworld-handout.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"e9kAqthnRUBu","executionInfo":{"status":"ok","timestamp":1610315293980,"user_tz":-60,"elapsed":481,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["# importing the necessary libraries\n","from matplotlib import pyplot as plt\n","from matplotlib import cm\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6PXeqAsPRUB-"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=18q7KL4aV6McMtaid_1Let2aGkw6d4QYn\" width=45%>"]},{"cell_type":"code","metadata":{"id":"OitVVzh9RUCA","executionInfo":{"status":"error","timestamp":1610315321478,"user_tz":-60,"elapsed":701,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}},"outputId":"a47a6f11-6810-464e-be05-a6a755820b77","colab":{"base_uri":"https://localhost:8080/","height":131}},"source":["class GridWorld:\n","    \"\"\"\n","    The gridworls is a frequently used demo environment in reinforcement learning\n","    to try and test ideas.\n","    Today, we will use it to understand the concepts so far.\n","    \n","    The environment: (see image)\n","    * cells: the agent can step on a cell. There is exactly one cell to start from.\n","    This is the top left corner. There is one terminal cell where the walking ends, \n","    the agent can not leave it (blue).\n","    * obstacles: there are cells where the agent can not step. (gray)\n","    * agent: it can move from one cell to an other neighboring cell. \n","    Possible directions: up, down, left, right. Each transition happens with probability 1.\n","    * reward: after each transition the agent receives -1 point. In the terminal cell, no reward\n","    received anymore.\n","    \n","    Implement the environment below!\n","    \"\"\"\n","    def __init__(self, size, start_cell, obstacles, terminating_state):\n","        self.size = size\n","        self.start = start_cell\n","        self.obstacles = obstacles\n","        self.termin = terminating_state\n","        self.current_cell = self.start\n","    \n","    def reset(self):\n","        # ----- reset the current cell to the start cell to start again -----\n","    \n","   def transition(self, cell, action):\n","        # ----- IMPLEMENT FUNCTION -----\n","        # cell = (row, column) indices\n","        # action: 0 left, 1 up, 2 right, 3 down\n","        # returns: What will be the next state\n","        # Take care of the borders of the grid!\n","        \n","        # ....\n","        \n","        return (r_next, c_next)\n","\n","    def reward(self, cell, action):\n","        # ----- RETURN REWARD -----\n","        # -1 if not in the terminal state\n","    \n","    def in_terminal(self):\n","        return self.current_cell == self.termin"],"execution_count":4,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-7c7c37d73a4f>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    def transition(self, cell, action):\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"]}]},{"cell_type":"code","metadata":{"id":"puCL_72_RUCB","executionInfo":{"status":"aborted","timestamp":1610315294324,"user_tz":-60,"elapsed":800,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["class DPsolver:\n","    \"\"\"\n","    This solver is based on the Bellman-equation and it is \n","    solved by iteratively.\n","    The action-value is used to represent the utility of the \n","    actions and states.\n","    \"\"\"\n","\n","    def __init__(self, gridworld, gamma, iterations):\n","        # setting parametes according to the input parameters\n","        self.gridworld = gridworld\n","        self.gamma = gamma\n","        self.iterations = iterations\n","        size = gridworld.size\n","        # initialize accumulaters\n","        self.cntr = 0\n","        self.sum_rewards = []\n","        self.path = []\n","        # ----- initialize the table for Q-function -----\n","        self.q_table = \n","\n","    def step(self):\n","        # ----- WRITE THE CODE BELOW -----\n","        # implement one step in the value iteration\n","        rows, cols = self.gridworld.size  # ask for the size of the grid\n","        # ----- cycle over the rows -----\n","        \n","            # ----- cycle over the columns -----\n","\n","                # ----- cycle over the actions -----\n","\n","                    # ----- get the reward -----\n","                    reward = ... \n","                    # ----- calculate the corresponding next step (what would happen) -----\n","                    cell_next = ... \n","                    r2, c2 = cell_next\n","                    self.q_table[act, r, c] = ... # ----- update the q-table -----\n","        # increase the counter\n","        self.cntr += 1\n","        # add the return to the sum_rewards list\n","        self.sum_rewards.append(self.trajectory())\n","\n","    def trajectory(self):\n","        # ----- IMPLEMENT THE FUNCTION -----\n","        # reset the gridworld\n","        \n","        # calculate the return along a trajectory followed by the current policy\n","        # when started from the start_cell\n","        sum_rewards = 0\n","        \n","        # TODO\n","        \n","        return sum_rewards\n","\n","    def is_learning_finished(self):\n","        # ----- IMPLEMENT THIS FUNCTION -----\n","        # check whether learning has finished, return it\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6OHppZXRUCC","executionInfo":{"status":"aborted","timestamp":1610315294331,"user_tz":-60,"elapsed":801,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["def plot_learning_curve(ql):\n","    values = ql.sum_rewards\n","    x = list(range(len(values)))\n","    y = values\n","    plt.plot(x, y, 'ro')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oj7tNBKERUCD","executionInfo":{"status":"aborted","timestamp":1610315294345,"user_tz":-60,"elapsed":809,"user":{"displayName":"Saurabh Chakravorty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4i8aVesE-zqnMJttjboEDqonbdcW85GPg1Yfj5g=s64","userId":"01575942328847603880"}}},"source":["# grid world parameters\n","size = (6, 6)\n","start_cell = (0, 0)\n","obstacles = [(3, 3)]\n","terminating_state = (3, 5)\n","# q learning parameters\n","gamma = 0.9\n","# ----- What is the minimum required number of iterations? -----\n","iterations = 500\n","\n","gw = GridWorld(size, start_cell, obstacles, terminating_state)\n","solver = DPsolver(gw, gamma, iterations)\n","\n","while not solver.is_learning_finished():\n","    solver.step()\n","    sum_rewards = solver.sum_rewards[-1]\n","    print(sum_rewards)\n","\n","sum_rewards = solver.trajectory()\n","print(sum_rewards)\n","plot_learning_curve(solver)"],"execution_count":null,"outputs":[]}]}