{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"RL-day3-bandits-handout.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"8F5ot6_VRN5B"},"source":["# Multi-armed bandits"]},{"cell_type":"markdown","metadata":{"id":"xnPg2PMgRN5S"},"source":["In this task you are going to implement ETC and UCB. Then you will play around the parameters in order to draw some more conclusion.\n","\n","Outline:\n","* Simulator writing\n","* ETC and experiments\n","* UCB and experiments"]},{"cell_type":"markdown","metadata":{"id":"fpyQFVhNRN5T"},"source":["## Bulding a simulator for the bandit"]},{"cell_type":"code","metadata":{"id":"YqCypZ_MRN5U"},"source":["from matplotlib import pyplot as plt\n","import numpy as np  # we can use this library for working with matrices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SeXBsW9FRN5V"},"source":["class Bandit:\n","    def __init__(self, k, means, rounds):\n","        # we assume Gaussian distributions with sigma=1\n","        self.k = k  # number of arms\n","        self.means = means\n","        self.rounds = rounds  # number of available rounds\n","        \n","        # ----- chose the optimal reward -----\n","        self.optimal_reward = \n","        self.counter = 0\n","        # gather the empirical regret so far\n","        self.empirical_regret = 0\n","        self.emp_regrets = []\n","        # gather the expected regret so far\n","        self.expected_regret = 0\n","        self.exp_regrets = []\n","    \n","    def play_arm(self, arm):\n","        # ----- sample the appropriate reward -----\n","        \n","        # ----- calculate the regret so far and save it -----\n","        \n","        \n","        return reward\n","    \n","    def finished(self):\n","        # ----- return if there is no more round remained -----\n","        return \n","    \n","    def plot_regret(self):\n","        plt.plot(list(range(self.counter)), self.emp_regrets, 'b+', list(range(self.counter)), self.exp_regrets, 'ro')\n","        plt.xlabel(\"iteration\")\n","        plt.ylabel(\"regret\")\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NsnCjQ1LRN5X"},"source":["## ETC algorithm"]},{"cell_type":"code","metadata":{"id":"a5tZSMR0RN5X"},"source":["class ETCsolver:\n","    def __init__(self, k, m, bandit):\n","        self.k = k  # number of arms\n","        self.m = m  # number of exploration rounds for each arm\n","        self.bandit = bandit\n","        \n","        # ----- create a cache storing the number of trials and the average rewards -----\n","        # for each action\n","        self.cache = \n","    \n","    def _exploration_phase(self):\n","        counter = 0\n","        while counter < self.k * self.m:\n","            # ----- implement the exploration part -----\n","            # play the bandit and update cache\n","    \n","    def _choose_best_action(self):\n","        # ----- we calculate the average reward of each arm -----\n","        # return the best arm\n","        \n","        return \n","    \n","    def run(self):\n","        self._exploration_phase()\n","        # after exploration we choose the best action\n","        optimal_arm = self._choose_best_action()\n","        # ----- play until finished -----\n","        \n","    \n","    def get_regret(self):\n","        return self.bandit.regrets\n","    \n","    def best_action(self):\n","        return self._choose_best_action() + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U083iTGHRN5Y"},"source":["def experiment_etc(k, mu, m, rounds):\n","    bandit = Bandit(k, mu, rounds)\n","    etc = ETCsolver(k, m, bandit)\n","\n","    etc.run()\n","    etc.bandit.plot_regret()\n","    print(\"Optimal action: {}\".format(etc.best_action()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ff8H1LZ_RN5Z"},"source":["## UCB algorithm"]},{"cell_type":"code","metadata":{"id":"q81AX3J4RN5a"},"source":["class UCBsolver:\n","    def __init__(self, k, delta, bandit):\n","        self.k = k  # number of actions\n","        self.delta = delta  # error probability\n","        self.bandit = bandit\n","\n","        self.cache = np.zeros((k, 2))  # stores the number and rewards so far\n","        self.actions = []\n","    \n","    def _init_phase(self):\n","        # at the very beginning each arm is equally good\n","        # unexplored arms are always the best\n","        # ----- pick each arm once to initialize the cache -----\n","        # we want to avoid division by zero\n","        for arm in range(self.k):\n","            # TODO\n","    \n","    def _choose_best_action(self):\n","        # this implements the score for ucb\n","        # ----- first is the average reward term -----\n","        \n","        # ----- second is the exploration term -----\n","\n","        return \n","    \n","    def run(self):\n","        self._init_phase()\n","        # ----- while not finished -----\n","\n","            # ----- chooe optimal arm -----\n","\n","            # ----- storing the actions so far -----\n","\n","            # ----- playing the chosen arm -----\n","\n","            # ----- update cache -----\n","    \n","    def plot_actions(self):\n","        plt.plot(list(range(len(self.actions))), self.actions, 'r+')\n","        plt.xlabel(\"iteration\")\n","        plt.ylabel(\"chosen action\")\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sg_O1WwoRN5h"},"source":["def experiment_ucb(k, mu, delta, rounds):\n","    bandit = Bandit(k, mu, rounds)\n","    ucb = UCBsolver(k, delta, bandit)\n","\n","    ucb.run()\n","    ucb.bandit.plot_regret()\n","    ucb.plot_actions()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W_cTjzbPRN5i"},"source":["## Experiments"]},{"cell_type":"markdown","metadata":{"id":"4Ld1VmyERN5i"},"source":["### ETC use-cases"]},{"cell_type":"code","metadata":{"id":"CaVc17c8RN5i"},"source":["# Try it with two arms and small difference between the mean values\n","# What are the required number of rounds to find the best action?\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"586h0eKERN5j"},"source":["# Try it with two arms and big difference between the mean values\n","# What are the required number of rounds to find the best action?\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vr-rbTntRN5k"},"source":["# Try it with five arms and small differences among the mean values\n","# What are the required number of rounds to find the best action?\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VbFIxMBZRN5k"},"source":["# Try it with fifty arms and small differences among the mean values\n","# What are the required number of rounds to find the best action?\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUAKg9RlRN5l"},"source":["### UCB use-cases"]},{"cell_type":"code","metadata":{"id":"ZmLn9sG9RN5l"},"source":["# Try it with two arms and small difference between the mean values\n","# What are the required number of rounds to find the best action?\n","# How the result changes with delta? \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhvLlS0sRN5l"},"source":["# Try it with two arms and big difference between the mean values\n","# What are the required number of rounds to find the best action?\n","# How the result changes with delta? \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_k3VYelvRN5m"},"source":["# Try it with five arms and small differences among the mean values\n","# What are the required number of rounds to find the best action?\n","# How the result changes with delta? \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lukE7dpkRN5m"},"source":["# Try it with fifty arms and small differences among the mean values\n","# What are the required number of rounds to find the best action?\n","# How the result changes with delta?\n","\n"],"execution_count":null,"outputs":[]}]}