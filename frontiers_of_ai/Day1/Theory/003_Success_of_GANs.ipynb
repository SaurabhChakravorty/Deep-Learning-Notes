{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History\n",
    "\n",
    "The limitations of VAEs and the advancements in noise contrastive estimation, together with some decent stroke of genius (during a pub night, as [the author attests](https://youtu.be/Z6rxFNMGdn0?t=1600)) lead to the elaborartion of the Generative Adversarial models by [Ian Goodfellow](https://en.wikipedia.org/wiki/Ian_Goodfellow) in his [seminal paper](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) in 2014.\n",
    "\n",
    "It is safe to say, that it sparked a kind of revolution, and a huge resurgence of interest in unsupervised learning. (eg. because of huge scaling and computational advantages over deep Bolzmann machines)\n",
    "\n",
    "\"Generative adversarial networks (GANs) have become a hot research topic recently. Yann LeCun, a legend in deep learning, said in a Quora post “GANs are the most interesting idea in the last 10 years in machine learning.” There are a large number of papers related to GANs according to Google scholar. For example, there are about 11,800 papers related to GANs in 2018. That is to say, there are about 32 papers everyday and more than one paper every hour related to GANs in 2018.\" - States [A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications](https://arxiv.org/abs/2001.06937) in early 2020.\n",
    "\n",
    "Even in the now infrequently updated [GAN Zoo](https://github.com/hindupuravinash/the-gan-zoo), the collection of GANs with \"own names\" there are over 500 entries!\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/media/C_9S0DvUwAAkS8Z.jpg\" width=55%>\n",
    "\n",
    "(Attention, cumulative chart!)\n",
    "\n",
    "Some, like LeCun even venture to say, that after the deep learning revolution, \"the adversarial revolution\" can be considered an event of it's own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN variants\n",
    "\n",
    "In getting an overview about the huge area of GANs, [A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications](https://arxiv.org/abs/2001.06937) is a great resource. It presents key variants of the default paradigm with their contextual connections, covering:\n",
    "\n",
    "**Different variants categorized by main properties:**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1cqwKFBwNvRxd0UbaNY9b43xWIEHH20JH\" width=65%>\n",
    "\n",
    "**Different variants for specific tasks and domains:**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1uY7NnlY5Yu5OhldIuTHZdJafhrYr0TsV\" width=65%>\n",
    "\n",
    "Just to get a flavor of the different structures\n",
    "<img src=\"https://www.researchgate.net/profile/Kunfeng_Wang/publication/319869547/figure/fig3/AS:660911190716417@1534584915656/Computation-procedures-and-structures-of-some-GAN-variants-a-GAN-1-WGAN-31.png\" width=55%>\n",
    "\n",
    "([source](https://www.researchgate.net/publication/319869547_Generative_Adversarial_Networks_Introduction_and_Outlook))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main avenues of progress\n",
    "\n",
    "It is outright impossible to detail each version of GAN that happened to be produced since 2014, but a very good summary of progress can be gained from [here](https://blog.floydhub.com/gans-story-so-far/).\n",
    "\n",
    "An \"island map\" for the tour on GAN variants: \n",
    "\n",
    "<img src=\"https://paper-attachments.dropbox.com/s_D85DDA7D01FD04AEE96825C4B90F1126BC7D080CA4F2947D4A5DEC07FAD6122C_1559841150851_Map.png\" width=55%>\n",
    "\n",
    "Also a very good, more scientific summary can be found [here](https://arxiv.org/pdf/1807.04720v1.pdf), or a more recent one [here](https://arxiv.org/pdf/2001.06937.pdf).\n",
    "\n",
    "Some of the structural variants we already discussed, or will do so later on, but one main topic was of considerable interest, that of the **different loss functions**, that allow for a more stabile training and good quality results.\n",
    "\n",
    "### Distance metrics\n",
    "\n",
    "One of the key areas of innovation lies in the application of theoretically well motivated \"distance metrics\", that provide the basis for potent lossfunctions, enabling detailed feedback and more efficient learning for GAN networks.\n",
    "\n",
    "#### Wasserstein distance\n",
    "\n",
    "Maybe the most widespread innovation of metrics is the application of the [Wasserstein distance](https://en.wikipedia.org/wiki/Wasserstein_metric) (also known as: Kantorovich–Rubinstein metric, earth mover's distance) introduced by the paper [Wasserstein GAN](https://arxiv.org/abs/1701.07875) or WGAN in short in 2017.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Soheil_Kolouri2/publication/324246144/figure/fig2/AS:612260313567233@1522985642521/The-Wasserstein-distance-for-one-dimensional-probability-distributions-p-X-and-p-Y-top.png\" width=55%>\n",
    "\n",
    "The paper claims, that:\n",
    "\n",
    "\"In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches.\"\n",
    "\n",
    "This is done by replacing the goal for the discriminator to simply do a binary classification over the generator's output as being fake, towards a **continuous ranking function about \"realisticness\"**.\n",
    "\n",
    "Or as an [introductory blogpost](https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/) (quoting the paper) puts it:\n",
    "\n",
    "\"Instead of using a discriminator to classify or predict the probability of generated images as being real or fake, the WGAN changes or replaces the discriminator model with a critic that scores the realness or fakeness of a given image.  This change is motivated by a mathematical argument that training the generator should seek a minimization of the distance between the distribution of the data observed in the training dataset and the distribution observed in generated examples. The argument contrasts different distribution distance measures, such as Kullback-Leibler (KL) divergence, Jensen-Shannon (JS) divergence, and the Earth-Mover (EM) distance, referred to as Wasserstein distance.  \n",
    "\n",
    "_'The most fundamental difference between such distances is their impact on the convergence of sequences of probability distributions.'_\n",
    "\n",
    "They demonstrate that a critic neural network can be trained to approximate the Wasserstein distance, and, in turn, used to effectively train a generator model.\n",
    "\n",
    "_'… we define a form of GAN called Wasserstein-GAN that minimizes a reasonable and efficient approximation of the EM distance, and we theoretically show that the corresponding optimization problem is sound.'_\n",
    "\n",
    "Importantly, the Wasserstein distance has the properties that it is continuous and differentiable and **continues to provide a linear gradient, even after the critic is well trained**. \n",
    "\n",
    "_'The fact that the EM distance is continuous and differentiable a.e. means that we can (and should) train the critic till optimality. […] the more we train the critic, the more reliable gradient of the Wasserstein we get, which is actually useful by the fact that Wasserstein is differentiable almost everywhere.'_\n",
    "\n",
    "**This is unlike the discriminator model that, once trained, may fail to provide useful gradient information for updating the generator model.**\n",
    "\n",
    "_'The discriminator learns very quickly to distinguish between fake and real, and as expected provides no reliable gradient information. The critic, however, can’t saturate, and converges to a linear function that gives remarkably clean gradients everywhere.'_\n",
    "\n",
    "The benefit of the WGAN is that **the training process is more stable and less sensitive to model architecture and choice of hyperparameter configurations**.\n",
    "\n",
    "_'… training WGANs does not require maintaining a careful balance in training of the discriminator and the generator, and does not require a careful design of the network architecture either. The mode dropping phenomenon that is typical in GANs is also drastically reduced.'_\n",
    "\n",
    "Perhaps most importantly, the loss of the discriminator appears to relate to the quality of images created by the generator.  \n",
    "\n",
    "Specifically, the lower the loss of the critic when evaluating generated images, the higher the expected quality of the generated images. This is important as **unlike other GANs that seek stability in terms of finding an equilibrium between two models, the WGAN seeks convergence, lowering generator loss**.  \n",
    "\n",
    "_'To our knowledge, this is the first time in GAN literature that such a property is shown, where the loss of the GAN shows properties of convergence. This property is extremely useful when doing research in adversarial networks as one does not need to stare at the generated samples to figure out failure modes and to gain information on which models are doing better over others._\"\n",
    "\n",
    "##### Implementation details\n",
    "\n",
    "Again, based on the blogpost above:\n",
    "\n",
    "\"\n",
    "- Use a linear activation function in the output layer of the critic model (instead of sigmoid).\n",
    "- Use Wasserstein loss to train the critic and generator models that promote larger difference between scores for real and generated images.\n",
    "- Constrain critic model weights to a limited range after each mini batch update (e.g. [-0.01,0.01]).\n",
    "- Update the critic model more times than the generator each iteration (e.g. 5).\n",
    "- Use the RMSProp version of gradient descent with small learning rate and no momentum (e.g. 0.00005).\" (This maybe can be changed with the advent on new optiomization methods...\n",
    "\n",
    "And finally the loss function itself:\n",
    "\n",
    "\"The Wasserstein loss function seeks to increase the gap between the scores for real and generated images.\n",
    "\n",
    "We can summarize the function as it is described in the paper as follows:\n",
    "\n",
    "_Critic Loss_ = (average critic score on real images) – (average critic score on fake images)\n",
    "_Generator Loss_ = -(average critic score on fake images)\n",
    "\n",
    "Where the average scores are calculated across a mini-batch of samples.\"\n",
    "\n",
    "#### Other distance metrics\n",
    "\n",
    "Naturally, there are a multitude of other loss functions being experimented with, eg.:\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2019/06/Summary-of-Different-GAN-Loss-Functions.png\" width=65%>\n",
    "\n",
    "More on these [here](https://towardsdatascience.com/gan-objective-functions-gans-and-their-variations-ad77340bce3c).\n",
    "\n",
    "### Stabilizing and enhancing training\n",
    "\n",
    "Some early work was examining the \"fragility\" of the training, and some solutions, in the form of [regularization functions](https://arxiv.org/abs/1705.09367) have been proposed and widely used. \n",
    "\n",
    "None the less, the most successful approach came from a quasi architectural standpoint:  \n",
    "\n",
    "#### Progressively growing GANs\n",
    "\n",
    "One of the major steps in stabilizing the GAN training process came from the understanding, that the training of large, overparametrized networks from the \"get go\" is not a feasible task, it is not feasible to try to approximate high resolution images at once. Instead, in their paper, [Progressive growing of GANs for improved quality, stability and variation](https://arxiv.org/pdf/1710.10196.pdf), the researchers at NVIDIA propose a different solution, namely starting from small capacity, and gradually increasing it over the course of the training.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*tUhgr3m54Qc80GU2BkaOiQ.gif\" width=65%>\n",
    "\n",
    "As the empirical results show: **Progressive training stabilizes GANs and speeds up the training process.**\n",
    "\n",
    "\"Most of the iterations are done at lower resolutions, and training is **2–6 times faster** with comparable image quality using other approaches. In short, it produces higher resolution images with better image quality.\" \n",
    "\n",
    "[source](https://medium.com/@jonathan_hui/gan-progressive-growing-of-gans-f9e4f91edf33)\n",
    "\n",
    "This approach was so succefful, it became a quasi standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning to the extreme\n",
    "\n",
    "As already noted in case of VAEs, the manipulation or \"conditioning\" of the representations is of strong interest, because it means direct control over the generative process.\n",
    "\n",
    "In case of ACGAN, the conditioning class label can be embedded, and the resulting vector can be easily multiplied with the noise. If this is true, and we do know, how to transform input data, for example **text into dense semantic vectors**, how about pushing this conditioning to a kind of extreme, by **generating images conditioned on a text description**. That is exactly what [StackGAN](https://arxiv.org/abs/1612.03242) did already in 2016, with remarkable results:\n",
    "\n",
    "<img src=\"https://cstwiki.wtb.tue.nl/images/Group3-stackgan-fig5.png\" width=65%>\n",
    "\n",
    "(Nice introduction in [this video](https://www.youtube.com/watch?v=rAbhypxs1qQ).)\n",
    "\n",
    "So the input here is the description, and the result is the picture, NOT the other way around, like image captioning (which is in itself quite remarkable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating style and content\n",
    "\n",
    "Not long after the introcustion of GANs the idea of \"style transfer\", or \"image to image translation\" also appeared, that is, people tried to separate the stylistic elements of a certain (say works of art) from the content, and apply the style in a different context:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/767/1*B5zSHvNBUP6gaoOtaIy4wg.jpeg\" width=65%>\n",
    "\n",
    "This technique became quite popular, but was only enabled in massive scale by a neat trick (very much in parallel with [\"backtranslation\"](https://arxiv.org/abs/1808.09381) in NLP) is the idea of **CycleGAN**, which **gets rid of the necessary pairing of input and output images**, instead uses a to-and-back translation approach.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*oZsw1JaGkKPxWKKvVUWlyg.png\" width=45%>\n",
    "\n",
    "This necessiates the introduction of strong structural modifications:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2692/1*_KxtJIVtZjVaxxl-Yl1vJg.png\" width=65%>\n",
    "\n",
    "The two main innovations of CycleGAN are:\n",
    "\n",
    "1. **Using two generator - discriminator pairs**\n",
    "\n",
    "$F$ for mapping between domain $Y$ and $X$, and it's counterpart $D_X$, to discriminate it's output from real images coming from $X$.\n",
    "\n",
    "$G$ for mapping between domain $X$ and $Y$, and it's counterpart $D_Y$, to discriminate it's output from real images coming from $Y$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. **Using \"cycle consistency loss\"**\n",
    "\n",
    "Which means, enforcing the constraints:\n",
    "\n",
    "$F(G(x)) \\approx x, x \\in X$\n",
    "\n",
    "and\n",
    "\n",
    "$G(F(y)) \\approx y, y \\in Y$\n",
    "\n",
    "Or visually:\n",
    "\n",
    "<img src=\"https://www.andrewszot.com/img/voice_conversion/cyclegan.png\" width=65%>\n",
    "\n",
    "The fact, that the transformation through one style transfer generator and back through the other one should result in (approximately) the original image. This ensures, that the $F$ and $G$ networks focus on style and become \"counterparts\" of each other.\n",
    "\n",
    "(More on CycleGAN can be found [here](https://medium.com/@jonathan_hui/gan-cyclegan-6a50e7600d7) and [here](https://blog.floydhub.com/gans-story-so-far/), as well as a nice video illustration [here](https://www.youtube.com/watch?v=D4C1dB9UheQ).)\n",
    "\n",
    "\n",
    "The maybe most extreme case of this research direction is **\"pix2pix\"**, which is basically image translation but on hand drawings as input:\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/media/DjIG9O4VAAE8-En.png\" width=45%>\n",
    "\n",
    "(Nice video about this method coming from 2017 can be found [here](https://www.youtube.com/watch?v=u7kQ5lNfUfg).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-of-the-art in GANs: StyleGAN(2)\n",
    "\n",
    "Though the search for more efficient GAN models is far from over, and there is considerable effort being undertaken in fields of objective functions and training procedures, one of the major breakthroughs in the are was presented by **radical architectural innovation** form the authors of [StyleGAN](https://arxiv.org/abs/1812.04948), and later it's refinement [StyleGAN2](https://arxiv.org/abs/1912.04958) at NVIDIA.\n",
    "\n",
    "The main aim of this research is to capitalize on previous training procedures (like progressive growing), and to propose some theoretically motivated **radical architectural restructuring** to improve efficiency by a large margin.\n",
    "\n",
    "The StyleGAN model amazes with the extremely realistic, high resolution (1024x1024) images it can generate, as well as the lower occurence of artefacts:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2604/1*6EyiiBQPvH5GqwdaulcCWA.png\" width=55%>\n",
    "\n",
    "\n",
    "## Main changes in architecture\n",
    "\n",
    "As we can see in the illustration below, there are some important structural changes \n",
    "\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Summary-of-the-StyleGAN-Generator-Model-Architecture.png\" width=45%>\n",
    "\n",
    "The discriminator was not changed in any way.\n",
    "\n",
    "\n",
    "### Z vector gets non-linearly transformed and added at multiple layers\n",
    "\n",
    "The burden of adding variance at all layers of a network from a single noise vector is quite high. Basically what the paper states is, that up till this point, some of the non linearities inside the generator network itself had the task of **generating pseudo-random projections**, so as to apply certain parts of the variance on certain levels. This was a difficult task, as well as a **cause for entanglement**.\n",
    "\n",
    "The StyleGAN approach thus introduces a separate preprocessing subnetwork from $z$ to $w$, which can \"entangle\" the variability in $z$ in an appropriate manner (light skin and light hair is often correlated,...) but the original $z$ can remain pretty much linear and disentangled. The **entangled $w$ is then used with learned affine transformations $A$ at each layer to influence elements of \"style\"**. \n",
    "\n",
    "### Generator input is a constant, not noise\n",
    "\n",
    "The above approach of generating a complex projection of $z$ to $w$, thus disentangled \"conditions\" for styles, and their application to individual layers makes it unnecessary to add $z$ as an input, instead the **generator network starts from a learned constant as input**. \n",
    "\n",
    "### Noise is added at each layer\n",
    "\n",
    "Since now the network starts from a constant, and $z$ is used only for style based variations, the question is, how we can introduce variance on the more fine grained level, that is, to introduce something that does not change the fact that it is a male on the picture with light hair, but makes the curls realisticly varied?\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*ZfbAn45eLjFaKVx8EQdgjA.png\" width=55%>\n",
    "\n",
    "For this to be achievable, the StyleGAN architecture introduices layerwise noises, that only have localized effects to a certain level, enabling variance inside a style, instead of amongst them.\n",
    "\n",
    "### Normalization enhances disentanglement\n",
    "\n",
    "One of the additional goals of the architecture design is to **localize the effect** of the incoming \"style\" image (generated from the $z$ noise transformed by the non-linearities), so that they **only effect one layer at a time**, hence allowing for the control of certain aspects of the image, **independently from other things**.\n",
    "\n",
    "For this the paper uses the already proposed [AdaIN layer](https://arxiv.org/abs/1703.06868).\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3858/1*tmzVhW0gs0KQnSX-RDPbUg.png\" width=35%>\n",
    "\n",
    "AdaIN can be considered as a distant batchnorm variant, but it normalizes the distribution of a given layer in a way, that:\n",
    "\n",
    "1. it first normalizes it to zero mean and unit variance\n",
    "2. then it scales and shifts the distribution towards a mean and variance that is a learned projection of the processed $z$ vector (named $w$) for a specific layer. \n",
    "\n",
    "This latter approach means, that a learned \"subset\" of the \"style\" gets applied to a certain layer. \n",
    "\n",
    "Please observe, that this is only having a **local effect on the block between two AdaINs**, since the normalization at the beginning is destroying all skewedness from the previous AdaIN. This **strongly aids the disentangled usage of styles**.\n",
    "\n",
    "\n",
    "## Side effect: Smooth interpolation\n",
    "\n",
    "One of the side effects of the above mentioned disentanglement (and \"linearity) of the style space is the possibility of very smooth interpolation between points in the latent space.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1GSVLsyvRh2V2j_MDLPMvV2RQpeT5Gaw_\" width=55%>\n",
    "\n",
    "(The full video is worth watching [here](https://www.youtube.com/watch?v=6E1_dgYlifc).) \n",
    "\n",
    "This indicates, that the different properties of the image (here \"styles\") are nicely projected into a quasi linear space, thus they \"cover\" a great area of plausible pictures (or alternatively, the interpolation between two of them will also be a plausible picture). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The improvements in StyleGAN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.youtube.com/embed/c-NJtV9Jvp0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x2b7b9372c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo('c-NJtV9Jvp0', width=800, height=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The main improvement of StyleGAN2, according to the paper itself is:\n",
    "\n",
    "\"We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.\"\n",
    "\n",
    "Read the paper [here](https://research.nvidia.com/publication/2019-12_Analyzing-and-Improving)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to measure the \"goodness\" of fellow GANs?\n",
    "\n",
    "With all these variants circulating, it is of great importance, that we find some objective measurements / metrics we can observe to evaluate the quality of the GANs and their representations. This turns out to be trickier than expected.\n",
    "\n",
    "## Basic problem\n",
    "\n",
    "Let's state the obvious:\n",
    "\n",
    "Even [Salimans and Goodfellow](https://arxiv.org/abs/1606.03498) argue, that\n",
    "\n",
    "\"**Generative adversarial networks lack an objective function**, which makes it difficult to compare performance of different models.\"\n",
    "\n",
    "Or to put it more precisely, the objective function for the discriminator that we measure is really ill suited to represent the \"goodness\" of the whole network (remember, the discriminator's loss is maximal, if the whole system works well, but can also be huge if the discriminator is a complete failure...)\n",
    "\n",
    "Further complicating the situation is the fact, that we do not have a good measure for the representation itself, since if we would, we could omit adversarial training altogether, so we are **forced to evaluate the generated images themselves**.\n",
    "\n",
    "There are two competing expectations towards good generated images:\n",
    "\n",
    "- Images have to be **realistic**, that is a given image should be the closest to reality as possible\n",
    "- Images have to be **diverse**, we would not like to have mode collapse, eg. only one nice picture, or even one per class\n",
    "\n",
    "\n",
    "\n",
    "## Evaluation methods\n",
    "\n",
    "In parallel with the development of GAN models, the evaluation criteria got also more and more sophisticated, and quite numerous ([this paper](https://arxiv.org/pdf/1802.03446.pdf) lists over 20 of them) but all of them have to meet some criteria.\n",
    "\n",
    "Or to be more detailed, based on the paper:\n",
    "\n",
    "\"\n",
    "1. favor models that generate high fidelity samples (i.e. ability to distinguish generated samples from real ones; discriminability),\n",
    "2. favor models that generate diverse samples (and thus is sensitive to overfitting, mode collapse and mode drop, and can undermine trivial models such as the memory GAN),\n",
    "3. favor models with disentangled latent spaces as well as space continuity (a.k.a controllable sampling),\n",
    "4. have well-defined bounds (lower, upper, and chance),\n",
    "5. be sensitive to image distortions and transformations. GANs are often applied to image datasets where certain transformations to the input do not change semantic meanings. Thus, an ideal measure should be invariant to such transformations. For instance, score of a generator trained on CelebA face dataset should not change much if its generated faces are shifted by a few pixels or rotated by a small angle.\n",
    "6. agree with human perceptual judgments and human rankings of models, and\n",
    "7. have low sample and computational complexity.\"\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1dbJBxJPrOOG0trEjrF3tz6aOL7i6BExS\" width=95%>\n",
    "\n",
    "\n",
    "### Manual inspection\n",
    "\n",
    "Surprisingly the baseline for evaluation in the earliest days was done pretty manually. This sounds quite suspicious, and rightly so, but bear in mind, that the earliest generative models were so radically bad, that if one could discern anything at all on the pictures, it was already a great sign of progress.\n",
    "\n",
    "Thus said, it is quite obvious, that manual inspection is not scalable, and suffers from subjective bias from the evaluator's side, so some more robust measurements are needed.\n",
    "\n",
    "None the less, GANs are still a field, where looking at the results carefully with your own eyes is still essential.\n",
    "\n",
    "### Inception score\n",
    "\n",
    "One of the most widespread metrics for GAN evaluation comes from the 2016 paper [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498), \n",
    "\n",
    "\"As an alternative to human annotators, we propose an automatic method to evaluate samples, which we find to correlate well with human evaluation: We apply the Inception model1 to every generated image to get the conditional label distribution $p(y|x)$. Images that contain meaningful objects should have a conditional label distribution $p(y|x)$ with low entropy. Moreover, we expect the model to generate varied images, so the marginal $\\int p(y|x = G(z))dz$ should have high entropy. Combining these two requirements, the metric that we propose is: $exp(E_xKL(p(y|x)||p(y)))$, where we exponentiate results so the values are easier to compare.\"\n",
    "\n",
    "Or with other words: The authors **use a pre-trained Inception model (from [here](https://arxiv.org/abs/1512.00567)) to produce predictions over the generated images, and use the predicted class distribution for evaluation of quality**.\n",
    "\n",
    "A \"good\" generated image has low entorpy:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2976/1*X29oOi1Tzch2j6MuG9XS1Q.png\" width=55%>\n",
    "\n",
    "While a not so good one has higher:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2812/1*t8lE_W4UKQ8jKgzxCAbHTA.png\" width=55%>\n",
    "\n",
    "Remember, Inception fires on known objects, so if there is a known realistic object on the image, entropy has to be low.\n",
    "\n",
    "This metric correlated well with human judgement, and was deemed plausible.\n",
    "\n",
    "The only drawback was, that though some thought was given to diversity, Inception Score is still gamed by the system, if it only produces just one image from every class.\n",
    "\n",
    "### Fréchet Inception Distance\n",
    "\n",
    "To mitigate the shortcoming of IS, and to further approach human judgement, Fréchet Inception Score was proposed.\n",
    "\n",
    "As the [post](https://machinelearningmastery.com/how-to-evaluate-generative-adversarial-networks/) elaborates:\n",
    "\n",
    "\"The Frechet Inception Distance, or FID, score was proposed and used by Martin Heusel, et al. in their 2017 paper titled [“GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.”](https://arxiv.org/abs/1706.08500) The score was proposed as an improvement over the existing Inception Score.\n",
    "\n",
    "FID performs well in terms of discriminability, robustness and computational efficiency. […] It has been shown that FID is consistent with human judgments and is more robust to noise than IS.\n",
    "\n",
    "— Pros and Cons of GAN Evaluation Measures, 2018.\n",
    "\n",
    "Like the inception score, the FID score uses the inception v3 model. Specifically, the coding layer of the model (the last pooling layer prior to the output classification of images) is used to capture computer vision specific features of an input image. These activations are calculated for a collection of real and generated images.\n",
    "\n",
    "The activations for each real and generated image are summarized as a multivariate Gaussian and the distance between these two distributions is then calculated using the Frechet distance, also called the Wasserstein-2 distance.\n",
    "\n",
    "A lower FID score indicates more realistic images that match the statistical properties of real images.\"\n",
    "\n",
    "Though as stated above, a great variety of metrics exist, these are some very widespread baselines to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections to reinforcement learning\n",
    "\n",
    "It is well worth noting, that a kind of \"cross pollination\" exists between the field of GANs and Reinforcement Learning. \n",
    "\n",
    "From the GAN perspective, the competing agent's play is in a sense a game scenario, so it only took so much time, till people tried to use explicit reinforcement learning techniques to facilitate GAN training, like in the case of [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473), **not incidentally to a natural language generation task** (we will talk about this topic later in detail).\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1800/0*FUwClIx3rko7vbFG\" width=55%>\n",
    "\n",
    "\n",
    "On the other hand, from the reinforcement learning side, the concepts of self play and especially actor-critic approaches have some resemblance to the Adversarial setting. (More on this later on again.)\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/335242067/figure/fig1/AS:793659691778048@1566234624454/Actor-critic-RL-architecture.png\" width=35%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some GAN additional areas of interest and application:\n",
    "\n",
    "Some GAN applications: https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900 \n",
    "\n",
    "What are GANs good for:\n",
    "https://github.com/LiDan456/MAD-GANs \n",
    "\n",
    "Example \"Taylor GAN\":\n",
    "https://www.profillic.com/paper/arxiv:2001.06427 \n",
    "\n",
    "Example: \"Film restoration\":\n",
    "https://www.youtube.com/watch?v=EjVzjxihGvU&feature=em-uploademail \n",
    "\n",
    "Semi supervised learning with GANs https://arxiv.org/abs/1606.03498 (\"MNIST on 100 examples\")\n",
    "\n",
    "Privacy with GANs https://www.biorxiv.org/content/10.1101/159756v4\n",
    "\n",
    "Domain adversarial training https://arxiv.org/abs/1505.07818\n",
    "\n",
    "Goodfellow's AAAI talk about adversarial training in general. https://www.youtube.com/watch?v=AJJRWFVfNPg\n",
    "\n",
    "The list could be continiued, as well as the research..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
