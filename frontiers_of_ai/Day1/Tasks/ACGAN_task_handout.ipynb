{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aPeeOCJ6NKu"
   },
   "source": [
    "# GAN - first tries\n",
    "\n",
    "We are implementing a simple feed-forward GAN architecture on MNIST - just to get the feeling right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ToOlMYj6NK7"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "s4z2pdw06NK_",
    "outputId": "781ee561-2e71-4be3-9100-f874d09f3a3a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4lLQUX26NLS"
   },
   "outputs": [],
   "source": [
    "(train_X, train_y),(test_X,test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POtCtKQm6NLf"
   },
   "outputs": [],
   "source": [
    "# The more the merrier :-)\n",
    "# Remember, this is unsupervised learning, so \"holdout\" and such makes less sense\n",
    "X = np.concatenate((train_X,test_X),axis=0)\n",
    "y = np.concatenate((train_y,test_y),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K_0M86x06NLu",
    "outputId": "5597d1e4-2eac-4aef-ad45-817bced8bfe7"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0Iakww8woGSQ",
    "outputId": "ae87cbee-8000-4514-ec4d-58ebd74d8b8e"
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJvkyD2J6NMD"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example_count = 5\n",
    "\n",
    "for ex in range(example_count):\n",
    "    plt.subplot(5, example_count//5, ex+1)\n",
    "    plt.imshow(data[ex], interpolation=\"nearest\", cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOYjr23B6NMP"
   },
   "outputs": [],
   "source": [
    "# Normalization betwenn -1 and 1 !!!!\n",
    "quasi_mean = X.max()/2 # Max is always 255, so this works ok.\n",
    "X = (X.astype(np.float32)-quasi_mean)/quasi_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPqwfkZw6NMp",
    "outputId": "d434c258-02d1-4fc4-ef36-7496b2964213"
   },
   "outputs": [],
   "source": [
    "# NOT Flattening of the image vectors!!!!\n",
    "# This is a convolutional model, so it works well with 2D data\n",
    "# X = X.reshape(X.shape[0],-1)\n",
    "\n",
    "# Instead, we add a new \"channel\" axis to the data. \n",
    "# Since this is grayscale, that is only 1 channel.\n",
    "\n",
    "X = X[...,np.newaxis]\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HpZFqr6c6NM0"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x675qgOy6NM4"
   },
   "source": [
    "### Training parameters\n",
    "\n",
    "We will be forced to do manual batching here, so we have to calcculate the number of batches manually, and iterate on a per batch basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7WqMOVw6NM8"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 200\n",
    "HALF_BATCH = BATCH_SIZE // 2\n",
    "BATCH_NUM = (X.shape[0] // BATCH_SIZE)\n",
    "if X.shape[0] % BATCH_SIZE:\n",
    "    BATCH_NUM+=1\n",
    "Z_DIM = 100\n",
    "\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kpjkPEdS6NNU"
   },
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snAXlrvr6NNY"
   },
   "outputs": [],
   "source": [
    "GENERATOR_INITIAL_IMAGE_SIZE = 7\n",
    "GENERATOR_INITIAL_IMAGE_CHANNELS = 128\n",
    "GENERATOR_L1_DIM = GENERATOR_INITIAL_IMAGE_SIZE*GENERATOR_INITIAL_IMAGE_SIZE*GENERATOR_INITIAL_IMAGE_CHANNELS \n",
    "# eg. 7*7 image, 128 channels it will be, and we go DOWN with the channels from there\n",
    "\n",
    "# We have to tkae care, that the final shape of all generator convolutions results in 28*28*1, \n",
    "# so it is a kind of balancing act\n",
    "GENERATOR_L2_DIM = 64\n",
    "GENERATOR_L2_KERNEL_SIZE = (5,5)\n",
    "GENERATOR_OUTPUT_DIM = 1 # Nuber of output CHANNELS!!!!\n",
    "GENERATOR_OUTPUT_KERNEL_SIZE = (5,5)\n",
    "\n",
    "GENERATOR_L3_DIM = 1024\n",
    "\n",
    "DISCRIMINATOR_L1_DIM = 64\n",
    "DISCRIMINATOR_L1_KERNEL_SIZE = (5,5)\n",
    "DISCRIMINATOR_L2_DIM = 128\n",
    "DISCRIMINATOR_L2_KERNEL_SIZE = (5,5)\n",
    "\n",
    "LEAKY_ALPHA = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7E4_OMC6NNj"
   },
   "source": [
    "## Model building\n",
    "\n",
    "Please remember th ACGAN architecture:\n",
    "\n",
    "<img src=\"https://programming.vip/images/doc/7337a1370ac02af82912e32d852fabeb.jpg\" width=25%>\n",
    "\n",
    "Which is convolutional in this case, so DCGAN is relevant here, for the convolutions:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/760/1*B7y91tLgeWE-EuuFP-1XwA.png\" width=65%>\n",
    "\n",
    "As well as the calculation of the resulting image sizes in case of a convolution:\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-d4023fe66cac95238a76ea1b5bc21d84\" wudth=45%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNWBYT8o6NNn"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Some empirically set values. \n",
    "# It might well be worth experimenting with newer optimizers / settings\n",
    "optimizer = Adam(lr=2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7FDJpwtE6NNw"
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0oh5Av66NNz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Conv2D, Flatten, Reshape\n",
    "from tensorflow.keras.layers import BatchNormalization, UpSampling2D, Embedding, multiply\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "X-TPOT6D6NN5",
    "outputId": "db68d22d-a876-49b7-ce5d-a7f3c2edee81"
   },
   "outputs": [],
   "source": [
    "### Define the generator!\n",
    "#########################\n",
    "\n",
    "# We use FUNCTIONAL API!\n",
    "\n",
    "# The generator always gets a noise vector as input\n",
    "noise_input = ....\n",
    "# AND a real as a single value. (Please observe, even a single value should be an 1 long vector!)\n",
    "label_input = ....\n",
    "\n",
    "# Now we have to make sure, that the labels are embedded to a space which has the same dimensions as the noise input\n",
    "# For this, ve use an embedding, that transforms the class input (which has a range defined in a comnstant above)\n",
    "# to something, that matches the dimensions of the noise input.\n",
    "embedded_label_input = ....\n",
    "# This is a technical step to ensure, it is a vector\n",
    "embedded_label_input = Flatten()(embedded_label_input)\n",
    "\n",
    "# And then we simply multiply the noise with the embedded class \n",
    "# For this, we already imported the function above.\n",
    "# Please observe, that the function takes in a LIST of things\n",
    "combined_input = ....\n",
    "# Note, other operations can make sense here, it is an empirical thing to try...\n",
    "\n",
    "# Define the first layer of the fully connected network, without activation!\n",
    "# Use the parameters defined with capital letter constants in the cells above for node counts!\n",
    "g_layer_1 = ....\n",
    "\n",
    "# But we do have to ensure that the input for anything convolutional is 2D + channel, so reshape is in order\n",
    "# remeber, it is image szie * image size * channels\n",
    "# ant this is the initial \"image\"\n",
    "# USE THE DEFINED CONSTANTS FROM ABOVE, and define a reshape layer!\n",
    "reshaped_layer = ....\n",
    "\n",
    "# Define a non-linearity, namely leaky relu on this layer!\n",
    "# We use LeakyReLU for avoiding sparsity - other options are viable also, just not normal relu\n",
    "# use the alpha value defined in constants above!\n",
    "g_layer_1_nonlin = ....\n",
    "\n",
    "# For stability, we add a batch normalization layer - no extra settings.\n",
    "g_layer_1_batchnorm = ....\n",
    "\n",
    "# Now we use Upsampling to gradually get the image resolution up, by doubling\n",
    "# upsampling layer does this without any extra parameters.\n",
    "g_layer_2_upsample = ....\n",
    "# As a result we have 14*14*128 - with the above default settings\n",
    "\n",
    "# Furthermore we use convolutions to get the number of channels down\n",
    "# Define a convolutional layer to get down to the layer 2 dimension of generator (number of filters)\n",
    "# Use the defined constant from above for layer 2 kernel size, and use _\"same\"_ padding\n",
    "g_layer_2_conv = ....\n",
    "# As a result we get 14*14*64 - with the above default settings\n",
    "\n",
    "# And again a non linearity as above, please...\n",
    "g_layer_2_nonlin = ....\n",
    "# And a batch normalization, as above, please...\n",
    "g_layer_2_batchnorm = ....\n",
    "\n",
    "# Now we again use Upsampling to gradually get the image resolution up, by doubling\n",
    "g_layer_3_upsample = ....\n",
    "# As a result we have 28*28*64 - with the above default settings\n",
    "\n",
    "# Furthermore we use Conv2D to get the number of channels down\n",
    "# Define a convolutional layer to get down to the output layer dimension of generator (number of filters)\n",
    "# Use the defined constant from above for output layer kernel size, and use _\"same\"_ padding\n",
    "# USE A NON-LINEARITY TO PROJECT BETWEEN -1 and 1!!! \n",
    "# Remember, the images are normalized!\n",
    "g_output_layer = ....\n",
    "# As a result we get 28*28*1 - with the above default settings\n",
    "# And please notice, that we used an activation, so our pixels get between -1 and 1 again\n",
    "# This should now be indistinguishable from an input image - hopefully\n",
    "\n",
    "# Please instantiate the model!\n",
    "generator = ....\n",
    "\n",
    "# Please remeber, that the loss for the discriminator will be a binary loss, so this applies here also\n",
    "# Use the appropriate loss measure!\n",
    "generator.compile(loss=...., optimizer=optimizer)\n",
    "# Think about this carefully, please!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDS7ZWQs6NOA"
   },
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "0BsR4uc-6NOB",
    "outputId": "8cc795e9-cb2d-4bde-eece-2334fcb4c653"
   },
   "outputs": [],
   "source": [
    "### Define the discriminator!\n",
    "#############################\n",
    "\n",
    "# We still use FUNCTIONAL API!\n",
    "\n",
    "# The discriminator always gets original sized images, so 28x28x1 (channel) as inputs\n",
    "image_input = ....\n",
    "\n",
    "# Define the first convolutional layer of the discriminator!\n",
    "# Use the above defined constants for  filter number and kernel size!\n",
    "# We use strides of 2,2 instead of pooling, which is a sparse operator, and _\"same\"_ padding\n",
    "d_layer_1 = ....\n",
    "# And add a non-linearity, as in the discriminator, please...\n",
    "d_layer_1_nonlin = ....\n",
    "\n",
    "# Repet the block again, please!\n",
    "d_layer_2 = ....\n",
    "d_layer_2_nonlin = ....\n",
    "\n",
    "# Please use an appropriate operation \n",
    "# to make the output of the previos conv compatible with a fully connected layer!\n",
    "d_layer_2_flattened = ....\n",
    "\n",
    "# Please implement ONE OF the output layers of the discriminator!\n",
    "# One output of the discriminator is a single binary decision, \n",
    "# so one use an appropriate activation and dimensionality!\n",
    "validity = \n",
    "\n",
    "# Please implement THE OTHER output layer of the discriminator!\n",
    "# The other output of the discriminator is the predicted class of the input (+1 for the fake class!!!!)\n",
    "# Please observe, thisis a multiclass classification! Use appropriate nonlinearity!\n",
    "predicted_class = ....\n",
    "\n",
    "# Please instantiate the model!\n",
    "discriminator = Model(inputs=...., outputs=[....])\n",
    "# Observe syntax for multi output!!!\n",
    "\n",
    "# Please remeber, that ONE loss for the discriminator will be a binary loss (validity)\n",
    "# And the other will be something for the multi-class classification (including +1 class for fake)\n",
    "# This is NOT one hot encoded class label, so only one kind of builtin lossfunction works here!\n",
    "# Think and ask, if unsure!\n",
    "discriminator.compile(loss=[....], optimizer=optimizer)\n",
    "# Observe syntax for multi output!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0ZgKpmC6NOL"
   },
   "source": [
    "### Joint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhgWdhGl6NON"
   },
   "outputs": [],
   "source": [
    "### Define the GAN itself!\n",
    "##########################\n",
    "\n",
    "\n",
    "# STOP!!!!!!\n",
    "# This is a crucial line, since in the joint model, discriminator will be frozen, so no weight update!\n",
    "discriminator.trainable = False\n",
    "\n",
    "....\n",
    "# Remove this .... if you understand, why the above line is here!\n",
    "\n",
    "# What is the input for the whole GAN?\n",
    "# For one, there is noise:\n",
    "noise_input = ....\n",
    "# And for two: there is the class label, as a single number (vector)\n",
    "label_input = ....\n",
    "\n",
    "# Use the generator as a function on the input!\n",
    "# Please remeber, there is a list of TWO things that has to go into the generator!\n",
    "generated_image = ....\n",
    "\n",
    "# Use the discriminator as a function on the fake images!\n",
    "validity, target_label = ....\n",
    "# Observe, that two things come out of the discriminator!\n",
    "\n",
    "# Instantiate the joint model, appropriate inputS and outputS! (Plural is not an accident! :-)\n",
    "# Use lists!\n",
    "joint_model = ....\n",
    "\n",
    "# Please think about, why need TWO losses!\n",
    "joint_model.compile(loss=[....], optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0bxzVNkh6NOb"
   },
   "source": [
    "## Helper functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ickSx5Ed6NOd"
   },
   "outputs": [],
   "source": [
    "# Nothing to see here! :-P\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "def get_example_images(epoch, example_count=25, num_classes=10):\n",
    "    input_noise = np.random.normal(0,1, size=(example_count,Z_DIM))\n",
    "    input_classes = []\n",
    "    class_num = 0\n",
    "    while len(input_classes)<example_count:\n",
    "        input_classes.append(class_num)\n",
    "        class_num+=1\n",
    "        if class_num > num_classes:\n",
    "            class_num = 0\n",
    "    input_classes=np.array(input_classes).reshape(-1,1)\n",
    "    \n",
    "    generated_images = generator.predict([input_noise,input_classes])\n",
    "    generated_images = generated_images.reshape(example_count, 28, 28) #,_\n",
    "    \n",
    "    plt.figure(figsize = (5, example_count // 5))\n",
    "    for ex in range(example_count):\n",
    "        plt.subplot(5, example_count//5, ex+1)\n",
    "        plt.imshow(generated_images[ex], interpolation=\"nearest\", cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ACGAN_example_images_epoch_num_{0}.png\".format(epoch))\n",
    "\n",
    "def show_image_for_epoch(epoch):\n",
    "    imgname = \"ACGAN_example_images_epoch_num_\"+str(epoch)+\".png\"\n",
    "    img = mpimg.imread(imgname)\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tfFKGOR6NOn"
   },
   "source": [
    "## Training\n",
    "\n",
    "Sadly, we can not use simple `fit()`, but have to construct the main training loop ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Wt2FjLP86NOq",
    "outputId": "b1ced919-d60b-43f3-e29b-83a80c0e2a0f"
   },
   "outputs": [],
   "source": [
    "# To see some progress, we use tqdm as a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Since we do NOT use fit\n",
    "# sadly, we have to do this ourselves manually\n",
    "history = {\"discriminator\":[],\"generator\":[]}\n",
    "\n",
    "# Main trainign loop\n",
    "for epoch_num in range(EPOCHS):\n",
    "    epoch_discriminator_loss = 0\n",
    "    epoch_generator_loss = 0\n",
    "    \n",
    "    for batch in tqdm(range(BATCH_NUM)):\n",
    "        # We select randomly a half batch amount of real images from MNIST\n",
    "        # Use Numpy to choose, no replacement!\n",
    "        # To ensure, that we have image and class PAIRS, and don't mess up, we use a binary mask.\n",
    "        # Use choice from numpy to generate a mask for the whole dataset length!\n",
    "        choice_mask = ....\n",
    "        real_images = X[choice_mask]\n",
    "        real_labels = y[choice_mask]\n",
    "        \n",
    "        \n",
    "        # We generate a half batch amount of fake images\n",
    "        # By first generating a half batch worth of Gaussian noise with zero mean, unit variance\n",
    "        # and appropriate noise dimensions\n",
    "        input_noise = ....\n",
    "        \n",
    "        # And we don't forget to generate the fake labels for them!\n",
    "        # Which are random integers between 0 and the class number, reshaped to be 1D matrices\n",
    "        generator_labels = ....\n",
    "        \n",
    "        # And then using the fixed generator, to output some images from it\n",
    "        # Using the predict method of the generator!\n",
    "        # Warning, the generator needs a list of two things!\n",
    "        generated_images = ....\n",
    "        \n",
    "        ....\n",
    "        # STOP, and thik through, WHY predict?!\n",
    "        # Then you can remove the ....\n",
    "      \n",
    "        # We generate our \"validity\" class\n",
    "        # Remember one sided label smoothing for the positive class!\n",
    "        # Let's say with 0.9...\n",
    "        # So please, generate a half batch sized, one dimensional matrix with ones, using numpy\n",
    "        # and multiuply it by 0.9\n",
    "        real_validity = ....\n",
    "        # And generate a half batch worth of zeroes, again one dimensional matrix\n",
    "        generated_validity = ....\n",
    "\n",
    "        # And we are making available a vector, with the new \"fake\" class for the discriminator to find\n",
    "        # Typically, if we had classes 0-9, we use 10 as the new class label that the discriminator has to find.\n",
    "        # Happens to be, that NUM_CLASSES is 10 (0-9), so we can use it as the 11th class label\n",
    "        # Practically: generate a half batch of ones, reshape it to be 1D matrix \n",
    "        # and multiply by the number of clases\n",
    "        labeled_as_fake = \n",
    "        \n",
    "        ### Do the actual training!\n",
    "        \n",
    "        # First for the discriminator on the real data\n",
    "        discriminator_loss_real = discriminator.train_on_batch(real_images, [real_validity, real_labels])\n",
    "        \n",
    "        # Then on the fake data\n",
    "        discriminator_loss_generated = discriminator.train_on_batch(generated_images, [generated_validity, labeled_as_fake])\n",
    "        \n",
    "        # We have now two losses for real and two for fake\n",
    "        # We take the mean of them se two again\n",
    "        discriminator_loss = 0.5 * (np.array(discriminator_loss_real) + np.array(discriminator_loss_generated))\n",
    "        epoch_discriminator_loss += np.mean(discriminator_loss)\n",
    "        \n",
    "        ### We then update the generator\n",
    "        # We use the discriminator that was trained a line above, and is frozen, as defined in the joint model\n",
    "        \n",
    "        # Please generate a new set of input noise, notice, it is a full batch!\n",
    "        # Again, using numpy, normal distribution, zero mean, unit variance\n",
    "        new_input_noise = ....\n",
    "        \n",
    "        # And we generate the new random labels for the generator again\n",
    "        # Between 0 and number of classes, a full batch worth, reshaped as an 1D matrix\n",
    "        generator_labels = ....\n",
    " \n",
    "        # We try to convince the discriminator, that this is real data - which is not\n",
    "        # So please generate a batch worth of one dimensional matrix filled with ones \n",
    "        convincing_y = .... \n",
    "        # Notice, no label smoothing!\n",
    "\n",
    "        # Remember, the joint model takes in noise plus target labels, \n",
    "        # does the generation, the discrimination, whereby outputting validity and a predicted label \n",
    "        # (10, if it finds out, that the sample is fake) then computes loss\n",
    "        # But the discriminator is frozen, so only the generator will get updated\n",
    "        # It is \"successful\" if the discriminator predicts \"real\" - hence the convincing_y\n",
    "        # and some label differnt from 10 - ideally the one we gave in as input for the generator\n",
    "        generator_loss = joint_model.train_on_batch([new_input_noise, generator_labels], [convincing_y, generator_labels])\n",
    "        # Same as above, we take the mean of the losses\n",
    "        epoch_generator_loss += np.mean(generator_loss)\n",
    "        \n",
    "    # Loss printout in every epoch, averaged over the batches\n",
    "    print(\"Epoch number:\",epoch_num,\"discriminator_loss:\",epoch_discriminator_loss / BATCH_NUM, \"generator_loss:\", epoch_generator_loss / BATCH_NUM)\n",
    "    \n",
    "    # Save it for the future\n",
    "    history[\"discriminator\"].append(epoch_discriminator_loss / BATCH_NUM)\n",
    "    history[\"generator\"].append(epoch_generator_loss / BATCH_NUM)\n",
    "    \n",
    "    #Save model - optional\n",
    "    #generator.save(\"ACGAN_generator.h5\")\n",
    "    \n",
    "    #Save images\n",
    "    get_example_images(epoch_num)\n",
    "    \n",
    "    # Show epoch example\n",
    "    show_image_for_epoch(epoch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo7-eaJf6NO0"
   },
   "source": [
    "## Visualization of training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "pkY1ixWj6NO2",
    "outputId": "f03fe71c-f80b-41a4-98af-e5628f6a3ad5"
   },
   "outputs": [],
   "source": [
    "plt.plot(history[\"discriminator\"], color='blue', linewidth=2, label=\"Discriminator\")\n",
    "plt.plot(history[\"generator\"],  color='red', linewidth=2, label=\"Generator\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend();\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ACGAN_try.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
