{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saurabh_Assignment_handout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDEnfPD1LOed"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Snr_CjcABfhV",
        "outputId": "e4378248-2ac4-4447-d85c-bd22b3a7f6c8"
      },
      "source": [
        "!pip install rpy2==2.9.6b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rpy2==2.9.6b\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/eb/2f7115990ee54eaa075f6c10d06a6225531b2a643b5779d30dda601cdbed/rpy2-2.9.6b0.tar.gz (194kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 7.1MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rpy2==2.9.6b) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from rpy2==2.9.6b) (2.11.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->rpy2==2.9.6b) (1.1.1)\n",
            "Building wheels for collected packages: rpy2\n",
            "  Building wheel for rpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rpy2: filename=rpy2-2.9.6b0-cp36-cp36m-linux_x86_64.whl size=316071 sha256=aabe73bde01339e23ebea953b263719b3a30b6592874611b9a0358bf91e8edb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/d6/dc/8c3faafb8cb7165a30d67f899ff7e88766e20260dda41ace88\n",
            "Successfully built rpy2\n",
            "Installing collected packages: rpy2\n",
            "  Found existing installation: rpy2 3.2.7\n",
            "    Uninstalling rpy2-3.2.7:\n",
            "      Successfully uninstalled rpy2-3.2.7\n",
            "Successfully installed rpy2-2.9.6b0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR84hEDqUbUz",
        "outputId": "2c608899-17a8-4eaa-dbb4-4605bf6cba39"
      },
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/11/2dc62c5263d9eb322f2f028f7b56cd9d096bb8988fcf82d65fa2e4057afe/pip-20.3.1-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mVRUQreMkgc",
        "outputId": "65ca6c69-4e87-48c7-b544-6d8e22c58023"
      },
      "source": [
        "%load_ext rpy2.ipython"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
            "  from pandas.core.index import Index as PandasIndex\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gGjI1cMNsCX",
        "outputId": "3007131e-a710-4cb3-958b-e48bea180dd0"
      },
      "source": [
        "%%R\n",
        "install.packages(\"grf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: also installing the dependencies ‘zoo’, ‘DiceKriging’, ‘lmtest’, ‘sandwich’, ‘RcppEigen’\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/zoo_1.8-8.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Content type 'application/x-gzip'\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 849487 bytes (829 KB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: =\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: \n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 829 KB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/DiceKriging_1.5.8.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 95896 bytes (93 KB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 93 KB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/lmtest_0.9-38.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 227052 bytes (221 KB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 221 KB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/sandwich_3.0-0.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 1445320 bytes (1.4 MB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 1.4 MB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/RcppEigen_0.3.3.7.0.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 1643103 bytes (1.6 MB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 1.6 MB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: trying URL 'https://cran.rstudio.com/src/contrib/grf_1.2.0.tar.gz'\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning:  length 136352 bytes (133 KB)\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: downloaded 133 KB\n",
            "\n",
            "\n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: \n",
            "  warnings.warn(x, RRuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: The downloaded source packages are in\n",
            "\t‘/tmp/RtmpzuUkX0/downloaded_packages’\n",
            "  warnings.warn(x, RRuntimeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2WB2i7qbusi",
        "outputId": "c9b710cf-f316-47f9-a311-c6ec0c832b7a"
      },
      "source": [
        "!pip install justcause==0.3.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting justcause==0.3.2\n",
            "  Downloading JustCause-0.3.2-py2.py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.0 MB/s  eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (0.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.1.5)\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (2.9.6b0)\n",
            "Collecting causalml\n",
            "  Downloading causalml-0.9.0.tar.gz (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (50.3.2)\n",
            "Requirement already satisfied: pip>=10.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (20.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (0.22.2.post1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.10.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.11.0)\n",
            "Requirement already satisfied: Cython>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.29.21)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.90)\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.3.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (20.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.4.3)\n",
            "Requirement already satisfied: tensorflow>=1.15.2 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause==0.3.2) (3.13)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause==0.3.2) (2.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->justcause==0.3.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (2.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (2.4.7)\n",
            "Collecting pygam\n",
            "  Downloading pygam-0.8.0-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 33.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from pygam->justcause==0.3.2) (3.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pygam->justcause==0.3.2) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->pygam->justcause==0.3.2) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from rpy2->justcause==0.3.2) (2.11.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->rpy2->justcause==0.3.2) (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->justcause==0.3.2) (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (3.2.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Collecting shap\n",
            "  Downloading shap-0.37.0.tar.gz (326 kB)\n",
            "\u001b[K     |████████████████████████████████| 326 kB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (4.41.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from shap->causalml->justcause==0.3.2) (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (50.3.2)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->shap->causalml->justcause==0.3.2) (0.31.0)\n",
            "Collecting slicer==0.0.3\n",
            "  Downloading slicer-0.0.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.9.0->causalml->justcause==0.3.2) (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.1.5)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.12.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause==0.3.2) (2.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.34.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.36.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.36.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (50.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.36.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (50.3.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.17.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.34.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (50.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras->causalml->justcause==0.3.2) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.3.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.17.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.10)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.4.8)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n",
            "Building wheels for collected packages: causalml, shap\n",
            "  Building wheel for causalml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causalml: filename=causalml-0.9.0-cp36-cp36m-linux_x86_64.whl size=482241 sha256=171e4352686ebc8e7a2267513329b8d533e9d30a58d72f0817425df67c013f90\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/a6/03/2b84df7887db775e47e4079749cad2f1733812ac49e7d57279\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.37.0-cp36-cp36m-linux_x86_64.whl size=463946 sha256=4eb970db2109e263288698cffdd019dfc6efaca8e413a824f3ddc07a6ddfdcff\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/81/55d1fce3ccceacb520abc45e88f3d2de2959cd079f712b5f85\n",
            "Successfully built causalml shap\n",
            "Installing collected packages: slicer, shap, pygam, causalml, justcause\n",
            "Successfully installed causalml-0.9.0 justcause-0.3.2 pygam-0.8.0 shap-0.37.0 slicer-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPzAPlfebvbX"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4UpBS6mNOCW"
      },
      "source": [
        "from justcause.data import Col\n",
        "from justcause.data.sets import load_ihdp\n",
        "from justcause.metrics import pehe_score, mean_absolute\n",
        "from justcause.evaluation import calc_scores, summarize_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CfvwSmoLOe-",
        "outputId": "8297ec9c-9bc1-4ba4-ad72-bcfaa0da1f37"
      },
      "source": [
        "%load_ext autoreload\n",
        "\n",
        "%autoreload 2\n",
        "\n",
        "# Loading all required packages \n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jufTu9k5LOfL"
      },
      "source": [
        "#### Infant Health Development Program Data-Set used in this exercise\n",
        "\n",
        "- Original study constructed to study the effect of special child care for low birthweight, premature infants.\n",
        "- In total, six continuous and 19 binary pretreatment variables\n",
        "- Using the covariates of all instances in both treatment groups, the potential outcomes are generated synthetically\n",
        "- Finally, manipulation of observational study by omitting a non-random set of samples from the treatment group.\n",
        "- The way the subset is generated from the experimental data does not ensure complete overlap - latent confounder\n",
        "- Specifically, the observational subset is created by throwing away the set of all children with nonwhite mothers from the treatment group\n",
        "- Following data generation process used for potentia outcomes\n",
        "- After the adaptions from Hill, we are left with 139 instances in the treated group and 608 instances in the control group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLQd0vO_LOfP"
      },
      "source": [
        "# 1. Running the causal models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg5b4PoxLOfU"
      },
      "source": [
        "# We Import the IHDP data-set \n",
        "# There are 1000 replications in this data-set, each with a different individual treament effect\n",
        "# produced from an underlying generative function. \n",
        "# Check out https://justcause.readthedocs.io/en/latest/\n",
        "\n",
        "\n",
        "# We load 100 of the 1000 data-sets\n",
        "replications = load_ihdp(select_rep=np.arange(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyTH9JV_LOfh"
      },
      "source": [
        "# Defining global parameters\n",
        "train_size = 0.8        # Size of the training data-set \n",
        "random_state = 42        # Setting the random state\n",
        "\n",
        "n = 0       # number of the data-sets we look at \n",
        "\n",
        "metrics = [pehe_score, mean_absolute]    ## Defining the metrics that will be calculated below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQWUbE6HLOfw"
      },
      "source": [
        "## 1.1 S-Learner Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT9UjEovLOf0"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import SLearner\n",
        "\n",
        "\n",
        "# Defining the S-Learner function that returns the ITE\n",
        "# We define a function that takes the data, splits it up and returns individual treatment effect accuracies for the train and the test data-set\n",
        "# The function takes each, the train and test data separately and selects the relevant variables and coverts them into np arrays\n",
        "# The relevant variables have the followings names: x (the covariates), t (the treatment), y (the outcome)\n",
        "# Note that the treatment needs to be explicitly defined\n",
        "\n",
        "\n",
        "\n",
        "def basic_slearner(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    slearner = model     # Select linear regression as a method to find the ITE for the S-Learner\n",
        "    slearner.fit(train_X, train_t, train_y)      # Fitting the s-learner with linear regression\n",
        "    return (\n",
        "        slearner.predict_ite(train_X, train_t, train_y),   # Returning the predicting values for ITE for train\n",
        "        slearner.predict_ite(test_X, test_t, test_y)       # Returning the prediction values for ITE for test\n",
        "    )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxwuoQxwLOgD"
      },
      "source": [
        "results_df = list()     # We define the list that contains the results\n",
        "test_scores = list()    # Storing the test scores in a list\n",
        "train_scores = list()   # Storing the train scores in a list\n",
        "\n",
        "\n",
        "# Here we define the model that is going to be used for the S-learner\n",
        "# Please instantiate linear regression for the simple learner\n",
        "\n",
        "\n",
        "##-----------------Question------------------###\n",
        "# Pass a LinearRegression Model into the S-Learner\n",
        "# No particular parameter-settings necessary\n",
        "\n",
        "## Creating a object of linear regression. We need to create an instance of each model  ##\n",
        "linear = LinearRegression()\n",
        "\n",
        "# pass the linear model\n",
        "model=SLearner(linear)\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = basic_slearner(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'S-Learner LR', 'train': True})\n",
        "test_result.update({'method': 'S-Learner LR', 'train': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Gb37xoDuLOgQ",
        "scrolled": true,
        "outputId": "3d06cc98-ad2a-4c51-b269-1296274b0b4d"
      },
      "source": [
        "df_S_learner_LR=pd.DataFrame([train_result, test_result])\n",
        "df_S_learner_LR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.891075</td>\n",
              "      <td>0.891075</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035169</td>\n",
              "      <td>0.035169</td>\n",
              "      <td>0.0</td>\n",
              "      <td>S-Learner LR</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.723163</td>\n",
              "      <td>0.723163</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.048408</td>\n",
              "      <td>0.048408</td>\n",
              "      <td>0.0</td>\n",
              "      <td>S-Learner LR</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0         0.891075           0.891075  ...  S-Learner LR   True\n",
              "1         0.723163           0.723163  ...  S-Learner LR  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfhFVRd4LOgk"
      },
      "source": [
        "### 1.1.1 S-Learner Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbbouLxaLOgo"
      },
      "source": [
        "# We run the same analysis again but only on an indvidual run of the data\n",
        "# The reason is that the data generating process is varied every time,... \n",
        "# ...so we can only look at individual runs of the ITE effect\n",
        "results_df = list()    # We define the list that contains the results\n",
        "test_scores = list()   # Storing the test scores in a list\n",
        "train_scores = list()  # Storing the train scores in a list\n",
        "\n",
        "\n",
        "#for rep in replications:\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state     # Use train_test_split  to split the data-set (replications[n]) \n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = basic_slearner(train, test, model)         # using the pre-defined basic learner function to return train, test\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))    # Using the just cause API to calcualte the scores from the estimate ITE for the training data\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))     # Using the just cause API to calcualte the scores from the estimate ITE for the test data\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)   #summary of the scores \n",
        "train_result.update({'method': 'S-Learner LR', 'train': True})\n",
        "test_result.update({'method': 'S-Learner LR', 'train': False})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "AsxWY_9mLOg2",
        "outputId": "e76ff2b1-475a-4f9c-cec7-d21951dea871"
      },
      "source": [
        "# Importing Matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.show()\n",
        "\n",
        "# If the treatment effect is perfectly represented there should be a 45 degree line!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSiTTv7KLOhI"
      },
      "source": [
        "## QUESTION 1\n",
        "\n",
        "IS THE S-LEARNER WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnYcRuZwvR6s"
      },
      "source": [
        "\r\n",
        "\r\n",
        "<div class=\"alert alert-block alert-warning\">\r\n",
        "\r\n",
        "**'NO'** the S-LEARNER with linear regression is not estimating well with individual treatments as we can clearly see there is no direct linear relationship between them.\r\n",
        "\r\n",
        "*Why so?*\r\n",
        "\r\n",
        "Here is the reason:\r\n",
        "\r\n",
        "\"The linear regression doesn't captures **'heterogenous treatment effect'** as it is heavily dependent on the feature\"\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJe9Cnj6LOhL"
      },
      "source": [
        "## 1.2 Propensity Score Weighing with Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7TpRNPdLOhP"
      },
      "source": [
        "# Importing the relevant PSWEstimator\n",
        "\n",
        "from justcause.learners import PSWEstimator\n",
        "\n",
        "\n",
        "#Defining the Propoensity Score weighing function that returns the ITE\n",
        "\n",
        "def propensity_score_weighing(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    pswestimator = model\n",
        "\n",
        "    return (\n",
        "        pswestimator.estimate_ate(train_X, train_t, train_y),\n",
        "        pswestimator.estimate_ate(test_X, test_t, test_y)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNoIPNfgLOhb"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = PSWEstimator(propensity_learner=None, delta=0.001)\n",
        "\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = propensity_score_weighing(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'PSW', 'train': True})\n",
        "test_result.update({'method': 'PSW', 'train': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "caopoaQrLOhm",
        "outputId": "0d0ea92b-a30b-4a8c-f705-d9cea37ad9e9"
      },
      "source": [
        "df_PSW_LR=pd.DataFrame([train_result, test_result])\n",
        "df_PSW_LR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.595322</td>\n",
              "      <td>2.537818</td>\n",
              "      <td>8.244302</td>\n",
              "      <td>0.412006</td>\n",
              "      <td>0.284332</td>\n",
              "      <td>0.457697</td>\n",
              "      <td>PSW</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.837997</td>\n",
              "      <td>3.484394</td>\n",
              "      <td>8.323623</td>\n",
              "      <td>3.783440</td>\n",
              "      <td>2.649187</td>\n",
              "      <td>3.225824</td>\n",
              "      <td>PSW</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...  method  train\n",
              "0         5.595322           2.537818  ...     PSW   True\n",
              "1         6.837997           3.484394  ...     PSW  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAcuhZhKLOhy"
      },
      "source": [
        "## 1.3 S-Learner Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtiDtGbyLOh3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "\n",
        "# Importing the relevant S-Learner estimator\n",
        "from justcause.learners import SLearner\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def basic_slearner(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    slearner = model  \n",
        "    slearner.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        slearner.predict_ite(train_X, train_t, train_y),\n",
        "        slearner.predict_ite(test_X, test_t, test_y)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW0XLJ0ELOiG"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "#---------------------------Question--------------------------------#\n",
        "# Pass a RandomForestRegressor into the S-learner\n",
        "\n",
        "# Making instance of the 'RandomForestRgressor' class\"\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "model = SLearner(rf)\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = basic_slearner(train, test, model )\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'S-Learner RF', 'train': True})\n",
        "test_result.update({'method': 'S-Learner RF', 'train': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "QgIkGA9MLOiR",
        "scrolled": true,
        "outputId": "bb945789-e14c-4857-e1b7-8c070be31719"
      },
      "source": [
        "df_S_learner_RF=pd.DataFrame([train_result, test_result])\n",
        "df_S_learner_RF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.120911</td>\n",
              "      <td>1.038537</td>\n",
              "      <td>4.849793</td>\n",
              "      <td>0.509454</td>\n",
              "      <td>0.122667</td>\n",
              "      <td>0.961678</td>\n",
              "      <td>S-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.306203</td>\n",
              "      <td>1.253123</td>\n",
              "      <td>5.149171</td>\n",
              "      <td>0.453604</td>\n",
              "      <td>0.125523</td>\n",
              "      <td>1.045012</td>\n",
              "      <td>S-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0         3.120911           1.038537  ...  S-Learner RF   True\n",
              "1         3.306203           1.253123  ...  S-Learner RF  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyU9_8rcLOid"
      },
      "source": [
        "### 1.3.1 Random Forest Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meb4UZjELOig"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "\n",
        "# Importing the relevant SLearner module\n",
        "from justcause.learners import SLearner\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def basic_slearner(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    slearner = model\n",
        "    slearner.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        slearner.predict_ite(train_X, train_t, train_y),\n",
        "        slearner.predict_ite(test_X, test_t, test_y)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "eF2mk_AxLOin",
        "scrolled": true,
        "outputId": "e0bf5601-6c0a-4609-e38f-51061b1b3b29"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0t4K4CfLOit"
      },
      "source": [
        "## QUESTION 2\n",
        "\n",
        "IS THE S-LEARNER WITH RANDOM FOREST ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnSbjKXr_E0X"
      },
      "source": [
        "\r\n",
        "\r\n",
        "<div class=\"alert alert-block alert-warning\">\r\n",
        "\r\n",
        "**'YES'** the S-LEARNER with RandomForestRegressor is estimating well with individual treatments on individual treatment effect due to following causation:\r\n",
        "\r\n",
        "- The random forest is showing a direct relatonship between real and estimated treatment effect.\r\n",
        "\r\n",
        "- We somewhat get a **heterogeneous relationship** in this mapping\r\n",
        "\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vowzj2teLOiw"
      },
      "source": [
        "## 1.4 T-Learner Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "811XNGgLLOix"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import TLearner\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def basic_tlearner(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    tlearner = model\n",
        "    tlearner.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        tlearner.predict_ite(train_X, train_t, train_y),\n",
        "        tlearner.predict_ite(test_X, test_t, test_y)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_GCgPWtLOi2"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "#------------------Question------------------------------#\n",
        "# Pass linear regression into the T-Learner\n",
        "\n",
        "# Passing \"linear\" instance\n",
        "linear = LinearRegression()\n",
        "tlearner = TLearner(linear)\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = basic_tlearner(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner LR', 'train': True})\n",
        "test_result.update({'method': 'T-Learner LR', 'train': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "tMeJsu4zLOi6",
        "scrolled": true,
        "outputId": "d37af7e5-f41a-4b09-d5d7-71d292a9ecfb"
      },
      "source": [
        "df_T_learner_LR=pd.DataFrame([train_result, test_result])\n",
        "df_T_learner_LR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.159059</td>\n",
              "      <td>1.910384</td>\n",
              "      <td>6.298658</td>\n",
              "      <td>0.455613</td>\n",
              "      <td>0.202166</td>\n",
              "      <td>0.761666</td>\n",
              "      <td>T-Learner LR</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.372969</td>\n",
              "      <td>1.805614</td>\n",
              "      <td>6.591175</td>\n",
              "      <td>0.683354</td>\n",
              "      <td>0.250850</td>\n",
              "      <td>1.274274</td>\n",
              "      <td>T-Learner LR</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0         4.159059           1.910384  ...  T-Learner LR   True\n",
              "1         4.372969           1.805614  ...  T-Learner LR  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k49nGyQLOi9"
      },
      "source": [
        "### 1.4.1 T-Learner Linear Regression Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u2DMEWTLOi-"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = basic_tlearner(train, test, model)\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner LR', 'train': True})\n",
        "test_result.update({'method': 'T-Learner LR', 'train': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "VtFgO5PZLOjC",
        "scrolled": true,
        "outputId": "d38bbf4b-b559-4975-fd3c-8f2df4549204"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61rEzJoLLOjF"
      },
      "source": [
        "## QUESTION 3\n",
        "\n",
        "IS THE T-LEARNER WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_z_EayBCdyo"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\r\n",
        "\r\n",
        "**'NO'** the T-LEARNER with linear regression isn't estimating well with individual treatments on individual treatment effect due to the fact that real treatment is trained on treated instances and the estimated on control instances.\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO36FBY1LOjG"
      },
      "source": [
        "## 1.5 T-Learner Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjA-HNpCLOjH"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import TLearner\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def basic_tlearner(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    tlearner = model\n",
        "    tlearner.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        tlearner.predict_ite(train_X, train_t, train_y),\n",
        "        tlearner.predict_ite(test_X, test_t, test_y)\n",
        "    )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMXoNfexLOjL"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "#--------------------Question----------------------------------#\n",
        "# Pass a Random Forest into the T-Learner\n",
        "\n",
        "# Passing the earlier 'rf' instance\n",
        "rf = RandomForestRegressor()\n",
        "model = TLearner(rf)\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = basic_tlearner(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner RF', 'train': True})\n",
        "test_result.update({'method': 'T-Learner RF', 'train': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "psT80zSaLOjP",
        "scrolled": true,
        "outputId": "e600f4b7-3a27-4811-bf0f-226cc13f2e50"
      },
      "source": [
        "df_T_learner_RF=pd.DataFrame([train_result, test_result])\n",
        "df_T_learner_RF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.734244</td>\n",
              "      <td>0.950921</td>\n",
              "      <td>2.126502</td>\n",
              "      <td>0.128307</td>\n",
              "      <td>0.113694</td>\n",
              "      <td>0.108579</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.615614</td>\n",
              "      <td>1.111719</td>\n",
              "      <td>3.821031</td>\n",
              "      <td>0.230886</td>\n",
              "      <td>0.122041</td>\n",
              "      <td>0.397752</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0         1.734244           0.950921  ...  T-Learner RF   True\n",
              "1         2.615614           1.111719  ...  T-Learner RF  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSU2Kyj9LOjT"
      },
      "source": [
        "### 1.5.1 T-Learner with Random Forest Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKNP1x2KLOjU"
      },
      "source": [
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = basic_tlearner(train, test, model)\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner RF', 'train': True})\n",
        "test_result.update({'method': 'T-Learner RF', 'train': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Ha8GizLtLOjY",
        "outputId": "9ef21b8e-d7f3-44f3-978a-48f6838b0d2d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x_BryEaLOjd"
      },
      "source": [
        "## QUESTION 4\n",
        "\n",
        "IS THE T-LEARNER WITH RANDOM FOREST ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLQaymB0JkDr"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\r\n",
        "\r\n",
        "- **YES** the T-LEARNER with random forest is estimating well with individual treatments on individual treatment effect.\r\n",
        "\r\n",
        "- The mapping is even better than S-LEARNER method due to following reason that since we use different methods to estimate mean(µ1) and\r\n",
        "mean(µ0) we adapt to the structure of the data if necessary.\r\n",
        "\r\n",
        "- Here it takes into account nearby observations as well for better estimation\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh0sdhdKLOjf"
      },
      "source": [
        "## 1.6 Causal Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GWzTUTlLOjg"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import CausalForest\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def causal_forest(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    causalforest = model\n",
        "    causalforest.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        causalforest.predict_ite(train_X, train_t, train_y),\n",
        "        causalforest.predict_ite(test_X, test_t, test_y)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY-qvwfHLOjk"
      },
      "source": [
        "random_state = 1\n",
        "\n",
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "model = CausalForest(random_state=random_state)\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = causal_forest(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner RF', 'train': True})\n",
        "test_result.update({'method': 'T-Learner RF', 'train': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "B78davrGLOjn",
        "outputId": "f67956ba-41f2-4e0a-ac30-296cb508bf99"
      },
      "source": [
        "df_causal_forest=pd.DataFrame([train_result, test_result])\n",
        "df_causal_forest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.148852</td>\n",
              "      <td>1.939224</td>\n",
              "      <td>6.273968</td>\n",
              "      <td>0.435427</td>\n",
              "      <td>0.202059</td>\n",
              "      <td>0.737229</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.360059</td>\n",
              "      <td>1.822446</td>\n",
              "      <td>6.550383</td>\n",
              "      <td>0.672660</td>\n",
              "      <td>0.249457</td>\n",
              "      <td>1.289471</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0         4.148852           1.939224  ...  T-Learner RF   True\n",
              "1         4.360059           1.822446  ...  T-Learner RF  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfQBHXUTLOjt"
      },
      "source": [
        "### 1.6.1 Causal Forest Visualization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4c3Y-XDLOju"
      },
      "source": [
        "random_state = 1\n",
        "\n",
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = causal_forest(train, test, model)\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'T-Learner RF', 'train': True})\n",
        "test_result.update({'method': 'T-Learner RF', 'train': False})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "A3mIANgDLOjw",
        "outputId": "0593887c-0507-4a8d-f310-01d170467fe3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ingDVHQLOj0"
      },
      "source": [
        "## QUESTION 5\n",
        "\n",
        "IS THE CAUSAL FOREST WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4c26cUDNwob"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\r\n",
        "\r\n",
        "**'NO'** it's suprising the effect of causal forest is not perfroming well with ITE. The fundamental concept of minimising squared error loss is not working in this case which can be verified by the metric \"pehe_score\"\r\n",
        "\r\n",
        "This can be due to following reasons:\r\n",
        "\r\n",
        "- The data distribution is non-uniform due to existence of some perfect outliers which can be understood by error distribution and large difference between pehe_score-mean and pehe_score-median\r\n",
        "\r\n",
        "- Causal forest takes into account \"homogeneity\" as a standard but here we don't notice any sufficient improvement due to it\r\n",
        "\r\n",
        "- Linear regression as an estimator is not a right choice as a modelling parameter due to non-heterogeneity effect\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnFLlHMKLOj1"
      },
      "source": [
        "## 1.7 Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXE4Usi1LOj2"
      },
      "source": [
        "# Importing the relevant SLearner module\n",
        "\n",
        "from justcause.learners import DragonNet\n",
        "\n",
        "\n",
        "#Defining the S-Learner function that returns the ITE\n",
        "\n",
        "def causal_forest(train, test, model):\n",
        "    \"\"\" \"\"\"\n",
        "    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n",
        "    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n",
        "\n",
        "    dragonnet = model\n",
        "    dragonnet.fit(train_X, train_t, train_y)\n",
        "    return (\n",
        "        dragonnet.predict_ite(train_X, train_t, train_y),\n",
        "        dragonnet.predict_ite(test_X, test_t, test_y)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeJ5_9NBLOj5",
        "outputId": "397f45d4-8cdd-4c30-fd69-ea8dc12c0523"
      },
      "source": [
        "random_state = 1\n",
        "\n",
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "#---------------------------Question----------------------------#\n",
        "# Set the model to the DragonNet neural network from JustCause\n",
        "\n",
        "model = DragonNet()\n",
        "\n",
        "\n",
        "for rep in replications:\n",
        "\n",
        "    train, test = train_test_split(\n",
        "        rep, train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # REPLACE this with the function you implemented and want to evaluate\n",
        "    train_ite, test_ite = causal_forest(train, test, model)\n",
        "\n",
        "    # Calculate the scores and append them to a dataframe\n",
        "    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'Dragonnet', 'train': True})\n",
        "test_result.update({'method': 'Dragonnet', 'train': False})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 170153.7188 - regression_loss: 46473.5742 - val_loss: 16899.1660 - val_regression_loss: 8405.6514\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 145459.9062 - regression_loss: 39618.9219 - val_loss: 13764.1162 - val_regression_loss: 6842.7705\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 118699.7109 - regression_loss: 32142.6094 - val_loss: 9896.2705 - val_regression_loss: 4914.6016\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 85343.7031 - regression_loss: 22885.3203 - val_loss: 5950.8604 - val_regression_loss: 2949.0684\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 49214.8398 - regression_loss: 13306.9863 - val_loss: 3539.4993 - val_regression_loss: 1751.6499\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 26797.2930 - regression_loss: 7218.9824 - val_loss: 4266.5757 - val_regression_loss: 2122.3716\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 29381.0566 - regression_loss: 8047.5132 - val_loss: 4415.1362 - val_regression_loss: 2197.4407\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 29658.3906 - regression_loss: 8049.9365 - val_loss: 2784.0691 - val_regression_loss: 1378.6815\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 17891.5039 - regression_loss: 4749.2910 - val_loss: 1495.3198 - val_regression_loss: 729.9699\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 9186.1592 - regression_loss: 2424.1423 - val_loss: 1291.0073 - val_regression_loss: 624.1578\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 9419.7559 - regression_loss: 2448.3081 - val_loss: 1606.5284 - val_regression_loss: 779.9149\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 12727.5088 - regression_loss: 3431.5579 - val_loss: 1787.1671 - val_regression_loss: 869.9803\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 205ms/step - loss: 14609.5781 - regression_loss: 3892.8149 - val_loss: 1647.2821 - val_regression_loss: 801.1010\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 13274.6455 - regression_loss: 3513.5242 - val_loss: 1334.0653 - val_regression_loss: 646.2787\n",
            "Epoch 15/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 10773.3838 - regression_loss: 5218.2012\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 10287.5996 - regression_loss: 2688.0325 - val_loss: 1067.1664 - val_regression_loss: 514.8480\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7483.1982 - regression_loss: 1964.1838 - val_loss: 1000.1712 - val_regression_loss: 482.3202\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6667.6421 - regression_loss: 1749.6224 - val_loss: 982.1746 - val_regression_loss: 474.2044\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6100.6353 - regression_loss: 1648.6029 - val_loss: 997.2037 - val_regression_loss: 482.4573\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 6285.9272 - regression_loss: 1646.0146 - val_loss: 1020.0878 - val_regression_loss: 494.4638\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6344.8940 - regression_loss: 1668.8154 - val_loss: 1028.1714 - val_regression_loss: 498.8710\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6381.4648 - regression_loss: 1673.1804 - val_loss: 1011.2856 - val_regression_loss: 490.5883\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 6203.5996 - regression_loss: 1634.1624 - val_loss: 977.4076 - val_regression_loss: 473.6359\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 6026.5039 - regression_loss: 1567.3306 - val_loss: 936.9024 - val_regression_loss: 453.2228\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5731.3936 - regression_loss: 1493.2286 - val_loss: 900.1509 - val_regression_loss: 434.5868\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5397.7847 - regression_loss: 1426.0276 - val_loss: 870.8539 - val_regression_loss: 419.6357\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 5385.3706 - regression_loss: 1381.3994 - val_loss: 846.4559 - val_regression_loss: 407.1275\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 5276.3438 - regression_loss: 1344.9296 - val_loss: 823.5331 - val_regression_loss: 395.3841\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 5155.9849 - regression_loss: 1307.5609 - val_loss: 800.2932 - val_regression_loss: 383.5398\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4866.7905 - regression_loss: 1265.9670 - val_loss: 777.8458 - val_regression_loss: 372.1507\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4756.9160 - regression_loss: 1224.7698 - val_loss: 758.0270 - val_regression_loss: 362.1358\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4528.4141 - regression_loss: 1184.0653 - val_loss: 742.4567 - val_regression_loss: 354.2971\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4516.4204 - regression_loss: 1153.4584 - val_loss: 729.5042 - val_regression_loss: 347.8102\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4332.5942 - regression_loss: 1125.4733 - val_loss: 716.8971 - val_regression_loss: 341.5263\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4202.6172 - regression_loss: 1097.1315 - val_loss: 703.1575 - val_regression_loss: 334.6915\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4121.3931 - regression_loss: 1065.8221 - val_loss: 689.0937 - val_regression_loss: 327.6978\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4002.3845 - regression_loss: 1033.7434 - val_loss: 676.3115 - val_regression_loss: 321.3424\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3887.3049 - regression_loss: 1003.6987 - val_loss: 665.8171 - val_regression_loss: 316.1238\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3824.8511 - regression_loss: 976.4624 - val_loss: 657.2525 - val_regression_loss: 311.8701\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3765.3718 - regression_loss: 952.6116 - val_loss: 649.4565 - val_regression_loss: 308.0036\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3679.9338 - regression_loss: 930.2906 - val_loss: 642.1436 - val_regression_loss: 304.3807\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3435.8210 - regression_loss: 906.6041 - val_loss: 634.5812 - val_regression_loss: 300.6293\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3488.6162 - regression_loss: 883.5873 - val_loss: 627.5354 - val_regression_loss: 297.1293\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3409.7302 - regression_loss: 860.9323 - val_loss: 620.2647 - val_regression_loss: 293.4934\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3396.3689 - regression_loss: 839.6435 - val_loss: 612.5219 - val_regression_loss: 289.6002\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3215.5481 - regression_loss: 818.6099 - val_loss: 604.0798 - val_regression_loss: 285.3368\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3198.6255 - regression_loss: 798.1958 - val_loss: 595.4243 - val_regression_loss: 280.9551\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3095.7866 - regression_loss: 778.4009 - val_loss: 587.0209 - val_regression_loss: 276.7049\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3009.7070 - regression_loss: 759.5529 - val_loss: 578.9263 - val_regression_loss: 272.6149\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2923.9888 - regression_loss: 741.8394 - val_loss: 571.9174 - val_regression_loss: 269.0899\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2877.0132 - regression_loss: 725.3057 - val_loss: 565.4394 - val_regression_loss: 265.8566\n",
            "***************************** elapsed_time is:  6.244769811630249\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 210ms/step - loss: 852308.9375 - regression_loss: 234141.5625 - val_loss: 94817.3672 - val_regression_loss: 47381.1719\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 809603.1250 - regression_loss: 220639.2656 - val_loss: 88591.7422 - val_regression_loss: 44271.0391\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 763929.9375 - regression_loss: 206651.5312 - val_loss: 80168.1250 - val_regression_loss: 40061.9062\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 686824.6875 - regression_loss: 187564.7969 - val_loss: 69133.7656 - val_regression_loss: 34547.1367\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 585510.1250 - regression_loss: 162449.6250 - val_loss: 55760.2383 - val_regression_loss: 27862.0273\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 485559.5000 - regression_loss: 132404.5000 - val_loss: 41038.3164 - val_regression_loss: 20501.5410\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 368246.0000 - regression_loss: 99289.1016 - val_loss: 26773.0273 - val_regression_loss: 13367.9941\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 243603.0469 - regression_loss: 66754.3750 - val_loss: 15716.4248 - val_regression_loss: 7837.9707\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 157476.3125 - regression_loss: 41896.6328 - val_loss: 11828.4385 - val_regression_loss: 5893.0195\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 121077.0234 - regression_loss: 32718.1914 - val_loss: 16415.1230 - val_regression_loss: 8187.2476\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 152636.3125 - regression_loss: 42091.9648 - val_loss: 18785.9688 - val_regression_loss: 9374.4150\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 174962.6875 - regression_loss: 48324.5703 - val_loss: 14919.6143 - val_regression_loss: 7442.3110\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 154900.2656 - regression_loss: 41828.1406 - val_loss: 10272.1338 - val_regression_loss: 5118.9614\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 122584.5938 - regression_loss: 32927.1211 - val_loss: 8306.0439 - val_regression_loss: 4135.8115\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 108858.6953 - regression_loss: 29356.0586 - val_loss: 8541.1562 - val_regression_loss: 4252.9849\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 110702.7734 - regression_loss: 30045.2949 - val_loss: 9310.8408 - val_regression_loss: 4637.3794\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 115778.8281 - regression_loss: 31357.7891 - val_loss: 9566.2412 - val_regression_loss: 4764.7231\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 115772.0781 - regression_loss: 31451.7754 - val_loss: 9152.9971 - val_regression_loss: 4557.8872\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 110403.8359 - regression_loss: 30019.4375 - val_loss: 8362.4863 - val_regression_loss: 4162.5200\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 104453.2188 - regression_loss: 27924.5000 - val_loss: 7620.3413 - val_regression_loss: 3791.3813\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 96848.6406 - regression_loss: 25916.7637 - val_loss: 7235.0049 - val_regression_loss: 3598.6240\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 90752.9688 - regression_loss: 24597.2090 - val_loss: 7225.0205 - val_regression_loss: 3593.4917\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 83894.1641 - regression_loss: 24073.6426 - val_loss: 7273.6792 - val_regression_loss: 3617.7009\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 88054.7188 - regression_loss: 23755.3457 - val_loss: 7081.4697 - val_regression_loss: 3521.5752\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 84380.3438 - regression_loss: 22733.4336 - val_loss: 6606.2886 - val_regression_loss: 3284.1501\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 78938.6953 - regression_loss: 21164.2930 - val_loss: 6155.0415 - val_regression_loss: 3058.7971\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 72620.8594 - regression_loss: 19598.9160 - val_loss: 5914.7041 - val_regression_loss: 2938.9026\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 69342.6172 - regression_loss: 18510.2852 - val_loss: 5807.2593 - val_regression_loss: 2885.3755\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 65913.9609 - regression_loss: 17714.2520 - val_loss: 5644.7627 - val_regression_loss: 2804.2122\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 59205.4023 - regression_loss: 16792.8145 - val_loss: 5356.5796 - val_regression_loss: 2660.1074\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 58961.1758 - regression_loss: 15623.4248 - val_loss: 5026.6978 - val_regression_loss: 2495.0786\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 51584.2383 - regression_loss: 14208.2256 - val_loss: 4763.7266 - val_regression_loss: 2363.5061\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 48859.1523 - regression_loss: 13055.2803 - val_loss: 4573.4688 - val_regression_loss: 2268.3276\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 45137.9609 - regression_loss: 12075.5527 - val_loss: 4314.6143 - val_regression_loss: 2138.9578\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 41962.7109 - regression_loss: 11129.5674 - val_loss: 3915.5154 - val_regression_loss: 1939.5703\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 37408.3320 - regression_loss: 10122.8027 - val_loss: 3499.1189 - val_regression_loss: 1731.5811\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 33645.2305 - regression_loss: 9263.0381 - val_loss: 3178.9287 - val_regression_loss: 1571.6577\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 32519.6582 - regression_loss: 8611.3955 - val_loss: 2966.0955 - val_regression_loss: 1465.2701\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 29902.6250 - regression_loss: 7976.6084 - val_loss: 2831.7444 - val_regression_loss: 1398.0050\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 27782.2207 - regression_loss: 7347.3779 - val_loss: 2755.6362 - val_regression_loss: 1359.8270\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 25508.6406 - regression_loss: 6825.4790 - val_loss: 2609.5928 - val_regression_loss: 1286.7715\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 23627.8555 - regression_loss: 6365.6675 - val_loss: 2326.6096 - val_regression_loss: 1145.3734\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 22031.0410 - regression_loss: 5879.9014 - val_loss: 2062.7070 - val_regression_loss: 1013.5148\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 20727.5801 - regression_loss: 5469.0918 - val_loss: 1867.5026 - val_regression_loss: 915.9454\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 19118.2031 - regression_loss: 5076.9888 - val_loss: 1749.4513 - val_regression_loss: 856.8427\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 17832.2402 - regression_loss: 4690.1333 - val_loss: 1684.9777 - val_regression_loss: 824.4993\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 15173.6289 - regression_loss: 4354.5806 - val_loss: 1579.1858 - val_regression_loss: 771.5781\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 15257.8105 - regression_loss: 4068.2327 - val_loss: 1472.2515 - val_regression_loss: 718.1210\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14341.5518 - regression_loss: 3813.4971 - val_loss: 1390.4531 - val_regression_loss: 677.2383\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12666.7705 - regression_loss: 3595.8914 - val_loss: 1357.8726 - val_regression_loss: 660.9087\n",
            "***************************** elapsed_time is:  5.91606593132019\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 183431.1562 - regression_loss: 49851.5703 - val_loss: 23471.3301 - val_regression_loss: 11691.8945\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 159661.6562 - regression_loss: 44239.8789 - val_loss: 20454.3691 - val_regression_loss: 10191.7275\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 136212.1562 - regression_loss: 38086.9492 - val_loss: 16538.4199 - val_regression_loss: 8242.9404\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 110924.8359 - regression_loss: 30317.7773 - val_loss: 12022.3730 - val_regression_loss: 5994.3447\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 80567.9062 - regression_loss: 21746.7715 - val_loss: 7886.2563 - val_regression_loss: 3934.7524\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 49267.9727 - regression_loss: 14461.4287 - val_loss: 5675.9858 - val_regression_loss: 2834.6316\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 43705.2188 - regression_loss: 12019.5479 - val_loss: 5469.0400 - val_regression_loss: 2726.7319\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 49321.5664 - regression_loss: 13272.1201 - val_loss: 4785.0483 - val_regression_loss: 2373.5715\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 45060.0430 - regression_loss: 12096.1387 - val_loss: 3836.2263 - val_regression_loss: 1893.5797\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 34989.4258 - regression_loss: 9483.3721 - val_loss: 3613.9531 - val_regression_loss: 1781.7419\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 31345.8887 - regression_loss: 8261.4619 - val_loss: 4042.8726 - val_regression_loss: 1996.1344\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 31794.3613 - regression_loss: 8554.0078 - val_loss: 4327.3032 - val_regression_loss: 2138.5774\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 32361.8730 - regression_loss: 8799.3125 - val_loss: 4010.9426 - val_regression_loss: 1981.5410\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 30813.7051 - regression_loss: 8113.2769 - val_loss: 3300.9939 - val_regression_loss: 1628.7125\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 25826.0898 - regression_loss: 6840.7808 - val_loss: 2688.6311 - val_regression_loss: 1325.0950\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 21045.2969 - regression_loss: 5958.9795 - val_loss: 2448.6355 - val_regression_loss: 1207.3984\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 22172.3652 - regression_loss: 5916.1333 - val_loss: 2402.6135 - val_regression_loss: 1185.7350\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 21325.8984 - regression_loss: 6045.9546 - val_loss: 2235.3928 - val_regression_loss: 1102.0359\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 20668.4941 - regression_loss: 5621.6846 - val_loss: 2011.1523 - val_regression_loss: 988.6134\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 18650.6973 - regression_loss: 4958.2202 - val_loss: 1919.4894 - val_regression_loss: 940.9584\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 16923.5879 - regression_loss: 4549.9424 - val_loss: 1858.8905 - val_regression_loss: 909.4158\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 16321.8496 - regression_loss: 4361.7832 - val_loss: 1662.0972 - val_regression_loss: 810.8500\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 14863.7314 - regression_loss: 3980.5054 - val_loss: 1382.8557 - val_regression_loss: 671.9440\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 13363.5508 - regression_loss: 3528.6111 - val_loss: 1198.0100 - val_regression_loss: 580.6304\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 12354.7295 - regression_loss: 3246.1301 - val_loss: 1118.1027 - val_regression_loss: 541.3378\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 11853.5684 - regression_loss: 3120.2429 - val_loss: 1012.0884 - val_regression_loss: 488.3069\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 10665.3789 - regression_loss: 2855.2673 - val_loss: 908.3878 - val_regression_loss: 435.9655\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9876.8213 - regression_loss: 2568.7744 - val_loss: 850.8049 - val_regression_loss: 406.9102\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 9082.4180 - regression_loss: 2406.7727 - val_loss: 759.5884 - val_regression_loss: 361.7391\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8591.9551 - regression_loss: 2219.8386 - val_loss: 646.0676 - val_regression_loss: 305.9265\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 7297.0854 - regression_loss: 1990.1742 - val_loss: 579.9416 - val_regression_loss: 273.7928\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 7168.8062 - regression_loss: 1852.7019 - val_loss: 536.6514 - val_regression_loss: 252.5663\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6626.7676 - regression_loss: 1728.7614 - val_loss: 467.6509 - val_regression_loss: 217.7516\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5487.1709 - regression_loss: 1565.7842 - val_loss: 420.6159 - val_regression_loss: 193.7399\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4237.7617 - regression_loss: 1447.1272 - val_loss: 390.3466 - val_regression_loss: 178.5161\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5340.5742 - regression_loss: 1359.6124 - val_loss: 378.0708 - val_regression_loss: 172.8782\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4823.0698 - regression_loss: 1243.5239 - val_loss: 405.3745 - val_regression_loss: 186.9358\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 4234.2490 - regression_loss: 1178.6235 - val_loss: 380.2580 - val_regression_loss: 174.2663\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4318.7676 - regression_loss: 1097.2640 - val_loss: 355.0823 - val_regression_loss: 161.5267\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 4156.8657 - regression_loss: 1043.1058 - val_loss: 350.2976 - val_regression_loss: 159.1988\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3904.9707 - regression_loss: 1006.4085 - val_loss: 371.9217 - val_regression_loss: 170.4059\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3798.8262 - regression_loss: 966.3179 - val_loss: 407.6221 - val_regression_loss: 188.6261\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3526.2776 - regression_loss: 945.1777 - val_loss: 406.0066 - val_regression_loss: 187.8549\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3434.7217 - regression_loss: 914.7620 - val_loss: 359.2731 - val_regression_loss: 164.1583\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3492.3313 - regression_loss: 882.7864 - val_loss: 344.2393 - val_regression_loss: 156.5159\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3152.2693 - regression_loss: 855.4664 - val_loss: 350.9537 - val_regression_loss: 159.9729\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3254.5640 - regression_loss: 827.1647 - val_loss: 374.8951 - val_regression_loss: 172.1060\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3232.7444 - regression_loss: 808.4299 - val_loss: 343.6789 - val_regression_loss: 156.2962\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3153.0029 - regression_loss: 778.8245 - val_loss: 310.2007 - val_regression_loss: 139.2899\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2939.1399 - regression_loss: 762.7628 - val_loss: 315.9323 - val_regression_loss: 142.2600\n",
            "***************************** elapsed_time is:  5.783000946044922\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 218ms/step - loss: 18744.4258 - regression_loss: 5002.5815 - val_loss: 1331.5594 - val_regression_loss: 641.5764\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 12341.3662 - regression_loss: 3263.8232 - val_loss: 885.4719 - val_regression_loss: 417.2740\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7773.6685 - regression_loss: 2021.9785 - val_loss: 724.7835 - val_regression_loss: 335.9475\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 5679.0098 - regression_loss: 1427.2852 - val_loss: 516.7537 - val_regression_loss: 233.0473\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3869.1606 - regression_loss: 942.1223 - val_loss: 297.1011 - val_regression_loss: 125.3860\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2321.2256 - regression_loss: 545.4825 - val_loss: 368.9047 - val_regression_loss: 163.2093\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3033.0125 - regression_loss: 754.5806 - val_loss: 385.6455 - val_regression_loss: 172.6741\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3222.7156 - regression_loss: 798.6449 - val_loss: 325.1261 - val_regression_loss: 143.0092\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2680.3203 - regression_loss: 650.0035 - val_loss: 296.2509 - val_regression_loss: 128.9632\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2300.1235 - regression_loss: 567.0830 - val_loss: 255.0000 - val_regression_loss: 108.6835\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2002.0901 - regression_loss: 470.1487 - val_loss: 248.9588 - val_regression_loss: 105.9814\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1927.6250 - regression_loss: 461.7775 - val_loss: 267.8816 - val_regression_loss: 115.7317\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2087.7620 - regression_loss: 508.2503 - val_loss: 259.0933 - val_regression_loss: 111.6125\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2082.7273 - regression_loss: 489.9280 - val_loss: 241.3167 - val_regression_loss: 102.9943\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1896.3875 - regression_loss: 446.3335 - val_loss: 231.5003 - val_regression_loss: 98.3648\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1797.3645 - regression_loss: 423.8346 - val_loss: 215.3314 - val_regression_loss: 90.5383\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1652.9618 - regression_loss: 391.1527 - val_loss: 205.3407 - val_regression_loss: 85.7419\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1605.6814 - regression_loss: 373.5447 - val_loss: 212.9489 - val_regression_loss: 89.6622\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1659.2180 - regression_loss: 385.8554 - val_loss: 216.6278 - val_regression_loss: 91.5444\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1667.2573 - regression_loss: 384.4462 - val_loss: 215.7759 - val_regression_loss: 91.1078\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1610.4597 - regression_loss: 372.0009 - val_loss: 212.8396 - val_regression_loss: 89.6179\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1589.1274 - regression_loss: 363.6269 - val_loss: 204.5604 - val_regression_loss: 85.4647\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1505.0842 - regression_loss: 346.5540 - val_loss: 203.2281 - val_regression_loss: 84.7905\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1497.3933 - regression_loss: 345.7249 - val_loss: 203.3753 - val_regression_loss: 84.8473\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1481.3344 - regression_loss: 344.8715 - val_loss: 203.2922 - val_regression_loss: 84.7856\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1493.0139 - regression_loss: 339.2463 - val_loss: 203.7649 - val_regression_loss: 85.0263\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1469.4147 - regression_loss: 335.2656 - val_loss: 201.4169 - val_regression_loss: 83.8872\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1464.1884 - regression_loss: 326.7186 - val_loss: 201.0888 - val_regression_loss: 83.7656\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1432.7113 - regression_loss: 324.2215 - val_loss: 200.1353 - val_regression_loss: 83.3121\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1422.2141 - regression_loss: 322.0066 - val_loss: 198.3127 - val_regression_loss: 82.3969\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1362.8851 - regression_loss: 316.5432 - val_loss: 195.6848 - val_regression_loss: 81.0848\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1361.7792 - regression_loss: 312.5398 - val_loss: 192.8134 - val_regression_loss: 79.6473\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1381.4789 - regression_loss: 307.6960 - val_loss: 191.9020 - val_regression_loss: 79.1793\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1326.9839 - regression_loss: 304.8749 - val_loss: 192.2246 - val_regression_loss: 79.3138\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1327.0779 - regression_loss: 300.7249 - val_loss: 193.4796 - val_regression_loss: 79.9165\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1308.6068 - regression_loss: 297.7802 - val_loss: 192.7316 - val_regression_loss: 79.5424\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1284.3918 - regression_loss: 294.3997 - val_loss: 192.0814 - val_regression_loss: 79.2304\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1298.8079 - regression_loss: 291.3875 - val_loss: 191.1749 - val_regression_loss: 78.7876\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1290.3470 - regression_loss: 288.1185 - val_loss: 191.0524 - val_regression_loss: 78.7261\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1291.5430 - regression_loss: 285.5966 - val_loss: 191.3568 - val_regression_loss: 78.8732\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1282.1512 - regression_loss: 283.2387 - val_loss: 191.2664 - val_regression_loss: 78.8193\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1242.3242 - regression_loss: 280.7895 - val_loss: 191.5130 - val_regression_loss: 78.9302\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1258.7913 - regression_loss: 278.7189 - val_loss: 191.8403 - val_regression_loss: 79.0754\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1229.2975 - regression_loss: 276.9475 - val_loss: 192.5081 - val_regression_loss: 79.3888\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1239.2150 - regression_loss: 275.2278 - val_loss: 192.0778 - val_regression_loss: 79.1756\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1219.8011 - regression_loss: 273.6800 - val_loss: 192.2727 - val_regression_loss: 79.2785\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1224.6561 - regression_loss: 272.4430 - val_loss: 193.3780 - val_regression_loss: 79.8230\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1215.3438 - regression_loss: 271.2242 - val_loss: 192.9678 - val_regression_loss: 79.6157\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1213.4587 - regression_loss: 269.9765 - val_loss: 192.1216 - val_regression_loss: 79.1976\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1216.6396 - regression_loss: 269.2346 - val_loss: 191.8369 - val_regression_loss: 79.0486\n",
            "***************************** elapsed_time is:  5.852035284042358\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 7776.3506 - regression_loss: 2075.1758 - val_loss: 746.7016 - val_regression_loss: 347.9163\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5315.5054 - regression_loss: 1323.0948 - val_loss: 501.2674 - val_regression_loss: 221.9634\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3602.7690 - regression_loss: 850.9801 - val_loss: 322.2895 - val_regression_loss: 131.7112\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2365.0344 - regression_loss: 513.9614 - val_loss: 205.6159 - val_regression_loss: 77.5579\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1461.0935 - regression_loss: 292.2140 - val_loss: 260.6571 - val_regression_loss: 111.4914\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1906.0651 - regression_loss: 440.1481 - val_loss: 240.0923 - val_regression_loss: 102.2257\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1911.7628 - regression_loss: 456.3875 - val_loss: 180.3123 - val_regression_loss: 71.5791\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1531.7693 - regression_loss: 342.7372 - val_loss: 161.2144 - val_regression_loss: 61.4214\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1269.1666 - regression_loss: 272.2475 - val_loss: 186.7123 - val_regression_loss: 73.9414\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1344.7498 - regression_loss: 286.7125 - val_loss: 207.1919 - val_regression_loss: 83.8872\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1462.4312 - regression_loss: 317.3770 - val_loss: 191.6721 - val_regression_loss: 75.9681\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1398.5280 - regression_loss: 298.6992 - val_loss: 168.9009 - val_regression_loss: 65.1460\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1289.7650 - regression_loss: 272.4992 - val_loss: 156.8463 - val_regression_loss: 60.5193\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1205.0151 - regression_loss: 257.0071 - val_loss: 163.0529 - val_regression_loss: 65.2368\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1224.3046 - regression_loss: 268.2809 - val_loss: 168.9027 - val_regression_loss: 69.0328\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1252.2153 - regression_loss: 284.9865 - val_loss: 159.2518 - val_regression_loss: 63.9855\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1235.6151 - regression_loss: 278.2351 - val_loss: 150.9161 - val_regression_loss: 58.9619\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1196.3774 - regression_loss: 264.2626 - val_loss: 154.1274 - val_regression_loss: 59.7242\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1181.4934 - regression_loss: 252.5878 - val_loss: 162.6111 - val_regression_loss: 63.3721\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1169.4309 - regression_loss: 252.8064 - val_loss: 165.2566 - val_regression_loss: 64.3989\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1177.5569 - regression_loss: 254.4173 - val_loss: 158.8218 - val_regression_loss: 61.1633\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1145.8101 - regression_loss: 252.0818 - val_loss: 153.7611 - val_regression_loss: 59.0271\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1157.8563 - regression_loss: 250.2751 - val_loss: 155.5601 - val_regression_loss: 60.6529\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1143.4858 - regression_loss: 249.9597 - val_loss: 155.6617 - val_regression_loss: 61.2015\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1159.4208 - regression_loss: 253.3377 - val_loss: 151.5345 - val_regression_loss: 59.2346\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1157.9265 - regression_loss: 253.8910 - val_loss: 149.8333 - val_regression_loss: 58.2192\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1121.3547 - regression_loss: 251.9825 - val_loss: 153.0103 - val_regression_loss: 59.5469\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1149.5966 - regression_loss: 248.3096 - val_loss: 157.0883 - val_regression_loss: 61.3217\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1129.4365 - regression_loss: 246.8252 - val_loss: 156.6506 - val_regression_loss: 60.8560\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1129.1683 - regression_loss: 246.1175 - val_loss: 153.7632 - val_regression_loss: 59.2973\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1124.6689 - regression_loss: 245.7557 - val_loss: 156.2727 - val_regression_loss: 60.7640\n",
            "Epoch 32/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1180.4003 - regression_loss: 468.1003\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1128.1730 - regression_loss: 245.2827 - val_loss: 156.5037 - val_regression_loss: 61.1580\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1108.5525 - regression_loss: 245.6874 - val_loss: 155.3967 - val_regression_loss: 60.6746\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1125.5358 - regression_loss: 245.5211 - val_loss: 152.6776 - val_regression_loss: 59.2916\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1114.3663 - regression_loss: 245.2980 - val_loss: 151.1780 - val_regression_loss: 58.5119\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1121.1409 - regression_loss: 245.2983 - val_loss: 152.4422 - val_regression_loss: 59.1408\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1095.8785 - regression_loss: 244.7683 - val_loss: 154.4495 - val_regression_loss: 60.1324\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1113.0469 - regression_loss: 244.5614 - val_loss: 155.2166 - val_regression_loss: 60.4497\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1101.5157 - regression_loss: 244.1697 - val_loss: 154.3676 - val_regression_loss: 59.9461\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1111.2924 - regression_loss: 243.5659 - val_loss: 153.5251 - val_regression_loss: 59.4636\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1105.4041 - regression_loss: 243.2185 - val_loss: 153.9176 - val_regression_loss: 59.6525\n",
            "Epoch 42/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1163.3413 - regression_loss: 463.0188\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1111.9523 - regression_loss: 242.9526 - val_loss: 153.7283 - val_regression_loss: 59.5697\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1110.6813 - regression_loss: 242.7462 - val_loss: 153.8478 - val_regression_loss: 59.6441\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1105.4337 - regression_loss: 242.7084 - val_loss: 154.1256 - val_regression_loss: 59.8073\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1103.1758 - regression_loss: 242.7445 - val_loss: 154.5724 - val_regression_loss: 60.0516\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1116.5278 - regression_loss: 242.7411 - val_loss: 154.4024 - val_regression_loss: 59.9676\n",
            "Epoch 47/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1160.3145 - regression_loss: 463.2038\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1109.0220 - regression_loss: 242.6805 - val_loss: 153.9715 - val_regression_loss: 59.7446\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1106.0643 - regression_loss: 242.5178 - val_loss: 154.0399 - val_regression_loss: 59.7783\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1096.9324 - regression_loss: 242.5096 - val_loss: 154.2601 - val_regression_loss: 59.8844\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1113.2878 - regression_loss: 242.4363 - val_loss: 154.1839 - val_regression_loss: 59.8377\n",
            "***************************** elapsed_time is:  6.2209553718566895\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 18962.5605 - regression_loss: 5052.8755 - val_loss: 1722.8754 - val_regression_loss: 836.2561\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12563.2793 - regression_loss: 3356.1755 - val_loss: 1093.2837 - val_regression_loss: 524.9674\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 7646.6069 - regression_loss: 2074.6699 - val_loss: 688.3187 - val_regression_loss: 326.2379\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 5297.3052 - regression_loss: 1361.9404 - val_loss: 485.4213 - val_regression_loss: 225.8444\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3680.8867 - regression_loss: 937.7226 - val_loss: 300.7672 - val_regression_loss: 131.4279\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2014.3771 - regression_loss: 472.7410 - val_loss: 352.2839 - val_regression_loss: 153.9762\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2108.4128 - regression_loss: 498.4172 - val_loss: 496.8670 - val_regression_loss: 224.7833\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3093.7654 - regression_loss: 742.8922 - val_loss: 448.5850 - val_regression_loss: 201.6660\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2718.7410 - regression_loss: 642.4136 - val_loss: 322.2708 - val_regression_loss: 140.6808\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1852.1410 - regression_loss: 424.1828 - val_loss: 253.8729 - val_regression_loss: 108.7153\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1506.2166 - regression_loss: 346.3218 - val_loss: 253.2089 - val_regression_loss: 110.0519\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1660.6987 - regression_loss: 391.0784 - val_loss: 268.0767 - val_regression_loss: 118.3181\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1766.4871 - regression_loss: 434.9851 - val_loss: 274.8151 - val_regression_loss: 121.8354\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1811.7136 - regression_loss: 429.6445 - val_loss: 273.1454 - val_regression_loss: 120.7209\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1663.1393 - regression_loss: 399.9453 - val_loss: 264.3304 - val_regression_loss: 115.8099\n",
            "Epoch 16/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1602.2925 - regression_loss: 679.1776\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1532.3345 - regression_loss: 360.4414 - val_loss: 248.9449 - val_regression_loss: 107.4457\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1434.9988 - regression_loss: 325.9769 - val_loss: 242.4455 - val_regression_loss: 103.8248\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1407.3798 - regression_loss: 316.7904 - val_loss: 237.9046 - val_regression_loss: 101.2297\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1403.5918 - regression_loss: 315.7367 - val_loss: 234.7442 - val_regression_loss: 99.4349\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1425.5957 - regression_loss: 317.0993 - val_loss: 231.7203 - val_regression_loss: 97.8634\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1412.9790 - regression_loss: 315.9656 - val_loss: 228.5936 - val_regression_loss: 96.3829\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1393.4576 - regression_loss: 311.4565 - val_loss: 225.9059 - val_regression_loss: 95.2168\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1374.7551 - regression_loss: 306.5644 - val_loss: 223.9424 - val_regression_loss: 94.4514\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1344.7629 - regression_loss: 302.5944 - val_loss: 221.8456 - val_regression_loss: 93.6080\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1344.5079 - regression_loss: 300.9398 - val_loss: 219.3062 - val_regression_loss: 92.5121\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1327.4381 - regression_loss: 299.7694 - val_loss: 216.1460 - val_regression_loss: 91.0651\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1302.5620 - regression_loss: 298.7267 - val_loss: 213.3123 - val_regression_loss: 89.7202\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1328.2025 - regression_loss: 298.0905 - val_loss: 211.2448 - val_regression_loss: 88.7004\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1331.0425 - regression_loss: 297.0696 - val_loss: 209.9840 - val_regression_loss: 88.0272\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1313.2052 - regression_loss: 294.9218 - val_loss: 209.9343 - val_regression_loss: 87.9198\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1316.7579 - regression_loss: 292.1022 - val_loss: 210.4406 - val_regression_loss: 88.0690\n",
            "Epoch 32/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1372.4164 - regression_loss: 559.4294\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1311.0697 - regression_loss: 289.5222 - val_loss: 210.5922 - val_regression_loss: 88.0428\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1294.9395 - regression_loss: 287.7180 - val_loss: 210.4150 - val_regression_loss: 87.9156\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1305.0092 - regression_loss: 287.0137 - val_loss: 209.6829 - val_regression_loss: 87.5274\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1283.8177 - regression_loss: 286.3266 - val_loss: 208.6489 - val_regression_loss: 87.0008\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1257.0449 - regression_loss: 285.6727 - val_loss: 207.6454 - val_regression_loss: 86.5004\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1269.0710 - regression_loss: 285.0807 - val_loss: 206.7176 - val_regression_loss: 86.0484\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1283.1187 - regression_loss: 284.6029 - val_loss: 205.9243 - val_regression_loss: 85.6689\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1274.2627 - regression_loss: 284.1656 - val_loss: 205.3188 - val_regression_loss: 85.3886\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1284.7347 - regression_loss: 283.7086 - val_loss: 205.0520 - val_regression_loss: 85.2769\n",
            "Epoch 41/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1323.4224 - regression_loss: 537.6461\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1265.4838 - regression_loss: 283.2198 - val_loss: 205.0035 - val_regression_loss: 85.2687\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1283.3627 - regression_loss: 282.7704 - val_loss: 204.9507 - val_regression_loss: 85.2526\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1260.7996 - regression_loss: 282.5761 - val_loss: 204.9272 - val_regression_loss: 85.2484\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1275.5198 - regression_loss: 282.3708 - val_loss: 204.7637 - val_regression_loss: 85.1757\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1272.9176 - regression_loss: 282.1865 - val_loss: 204.6390 - val_regression_loss: 85.1207\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1250.6127 - regression_loss: 281.9714 - val_loss: 204.4164 - val_regression_loss: 85.0166\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1273.3302 - regression_loss: 281.7619 - val_loss: 204.1580 - val_regression_loss: 84.8932\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1262.6362 - regression_loss: 281.5377 - val_loss: 204.0150 - val_regression_loss: 84.8241\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1266.8568 - regression_loss: 281.3090 - val_loss: 203.8113 - val_regression_loss: 84.7270\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1260.5596 - regression_loss: 281.0850 - val_loss: 203.6900 - val_regression_loss: 84.6691\n",
            "***************************** elapsed_time is:  5.870985507965088\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 203351.5000 - regression_loss: 55309.9727 - val_loss: 21964.2520 - val_regression_loss: 10964.5996\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 176915.5469 - regression_loss: 48047.4922 - val_loss: 18305.0020 - val_regression_loss: 9135.4658\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 146291.1719 - regression_loss: 39843.4375 - val_loss: 13494.2314 - val_regression_loss: 6729.8604\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 107938.3594 - regression_loss: 29168.9219 - val_loss: 8013.0049 - val_regression_loss: 3988.0942\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 63350.7266 - regression_loss: 17045.9102 - val_loss: 3503.0693 - val_regression_loss: 1730.9839\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 27341.9492 - regression_loss: 7224.4492 - val_loss: 2780.5757 - val_regression_loss: 1367.2094\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 20913.4102 - regression_loss: 5754.0205 - val_loss: 4364.5752 - val_regression_loss: 2159.3882\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 33361.1875 - regression_loss: 9001.9248 - val_loss: 3391.2163 - val_regression_loss: 1675.4274\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 25303.2207 - regression_loss: 6726.8999 - val_loss: 1806.2238 - val_regression_loss: 885.6208\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 12712.9766 - regression_loss: 3348.4036 - val_loss: 1227.4108 - val_regression_loss: 597.9738\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 8121.0176 - regression_loss: 2180.5461 - val_loss: 1486.3881 - val_regression_loss: 728.3856\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 10152.5068 - regression_loss: 2814.1179 - val_loss: 1875.5382 - val_regression_loss: 923.2900\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 13081.8525 - regression_loss: 3666.5444 - val_loss: 1978.9126 - val_regression_loss: 974.8901\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 14363.4619 - regression_loss: 3873.8228 - val_loss: 1752.3505 - val_regression_loss: 861.2175\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 12568.7891 - regression_loss: 3368.6807 - val_loss: 1367.2343 - val_regression_loss: 668.0200\n",
            "Epoch 16/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 9932.1709 - regression_loss: 4846.9023\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9488.2412 - regression_loss: 2518.3750 - val_loss: 1046.4481 - val_regression_loss: 506.8375\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6913.1274 - regression_loss: 1811.2242 - val_loss: 965.7701 - val_regression_loss: 466.0959\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 6276.0630 - regression_loss: 1631.2671 - val_loss: 943.0084 - val_regression_loss: 454.3207\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6110.9233 - regression_loss: 1574.3082 - val_loss: 960.4527 - val_regression_loss: 462.6802\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6200.0952 - regression_loss: 1608.4414 - val_loss: 987.4719 - val_regression_loss: 475.8735\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6342.5234 - regression_loss: 1661.7740 - val_loss: 998.1751 - val_regression_loss: 480.9915\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6473.7310 - regression_loss: 1680.5248 - val_loss: 982.1077 - val_regression_loss: 472.8223\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6225.8662 - regression_loss: 1644.9926 - val_loss: 947.9508 - val_regression_loss: 455.7236\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5924.6851 - regression_loss: 1572.7822 - val_loss: 910.6927 - val_regression_loss: 437.1823\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5673.5088 - regression_loss: 1497.4154 - val_loss: 881.6020 - val_regression_loss: 422.8007\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5510.4092 - regression_loss: 1443.2686 - val_loss: 864.1463 - val_regression_loss: 414.2752\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 5369.9009 - regression_loss: 1411.0330 - val_loss: 855.4940 - val_regression_loss: 410.1525\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5385.7505 - regression_loss: 1395.1141 - val_loss: 849.8875 - val_regression_loss: 407.5336\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5360.6821 - regression_loss: 1383.7959 - val_loss: 843.7180 - val_regression_loss: 404.6012\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5275.8857 - regression_loss: 1365.9487 - val_loss: 836.1381 - val_regression_loss: 400.9211\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5139.1089 - regression_loss: 1341.8651 - val_loss: 827.7352 - val_regression_loss: 396.7882\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5015.5742 - regression_loss: 1314.2277 - val_loss: 819.9081 - val_regression_loss: 392.9073\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 4912.4038 - regression_loss: 1287.6710 - val_loss: 812.3956 - val_regression_loss: 389.1414\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4867.5171 - regression_loss: 1264.4924 - val_loss: 804.7402 - val_regression_loss: 385.2675\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4771.0454 - regression_loss: 1241.9031 - val_loss: 795.5744 - val_regression_loss: 380.6117\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 4614.8872 - regression_loss: 1219.7952 - val_loss: 784.1934 - val_regression_loss: 374.8290\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4503.4077 - regression_loss: 1197.7283 - val_loss: 770.9305 - val_regression_loss: 368.0956\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4572.1792 - regression_loss: 1174.3890 - val_loss: 757.5684 - val_regression_loss: 361.3211\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4485.5879 - regression_loss: 1151.6942 - val_loss: 744.5853 - val_regression_loss: 354.7550\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4343.6978 - regression_loss: 1130.6864 - val_loss: 732.4622 - val_regression_loss: 348.6581\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 4270.9873 - regression_loss: 1111.2010 - val_loss: 721.0606 - val_regression_loss: 342.9462\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 4298.1313 - regression_loss: 1092.4717 - val_loss: 710.5064 - val_regression_loss: 337.6883\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 4160.2554 - regression_loss: 1072.8241 - val_loss: 700.3582 - val_regression_loss: 332.6582\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4168.3657 - regression_loss: 1053.3563 - val_loss: 690.8126 - val_regression_loss: 327.9370\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3950.3269 - regression_loss: 1033.7299 - val_loss: 681.9300 - val_regression_loss: 323.5500\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3982.8003 - regression_loss: 1015.2139 - val_loss: 673.5427 - val_regression_loss: 319.4040\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3814.5051 - regression_loss: 996.9616 - val_loss: 664.7377 - val_regression_loss: 315.0399\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3809.1970 - regression_loss: 980.0752 - val_loss: 655.5098 - val_regression_loss: 310.4406\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3749.5706 - regression_loss: 963.2683 - val_loss: 645.6522 - val_regression_loss: 305.5142\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3690.1389 - regression_loss: 947.1763 - val_loss: 634.8325 - val_regression_loss: 300.0881\n",
            "***************************** elapsed_time is:  5.863900184631348\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 23374.0957 - regression_loss: 6234.2676 - val_loss: 1796.8928 - val_regression_loss: 873.8856\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 16050.2793 - regression_loss: 4309.8315 - val_loss: 1125.2441 - val_regression_loss: 541.4568\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 10707.4863 - regression_loss: 2797.5103 - val_loss: 727.6440 - val_regression_loss: 346.5575\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 7207.0098 - regression_loss: 1891.7771 - val_loss: 628.5339 - val_regression_loss: 299.0264\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6053.6655 - regression_loss: 1569.4219 - val_loss: 352.1451 - val_regression_loss: 159.4754\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3248.7273 - regression_loss: 814.5994 - val_loss: 247.8532 - val_regression_loss: 104.4013\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2014.5765 - regression_loss: 476.5438 - val_loss: 413.7023 - val_regression_loss: 184.7795\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3154.4573 - regression_loss: 771.4803 - val_loss: 464.9485 - val_regression_loss: 209.9326\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3564.8293 - regression_loss: 870.0560 - val_loss: 364.8031 - val_regression_loss: 161.0525\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2786.2373 - regression_loss: 664.7399 - val_loss: 269.9922 - val_regression_loss: 115.4925\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2075.0781 - regression_loss: 485.5401 - val_loss: 225.8977 - val_regression_loss: 95.0998\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1806.4318 - regression_loss: 433.7109 - val_loss: 213.5204 - val_regression_loss: 89.9569\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1903.6887 - regression_loss: 450.4659 - val_loss: 218.7832 - val_regression_loss: 93.0547\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1983.4169 - regression_loss: 489.6274 - val_loss: 222.9393 - val_regression_loss: 95.2246\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1993.8650 - regression_loss: 498.3311 - val_loss: 215.8635 - val_regression_loss: 91.5435\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1906.4835 - regression_loss: 458.3389 - val_loss: 207.7532 - val_regression_loss: 87.0634\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1726.3861 - regression_loss: 401.0618 - val_loss: 212.1089 - val_regression_loss: 88.4983\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1557.4167 - regression_loss: 368.0955 - val_loss: 221.9669 - val_regression_loss: 92.5338\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1560.3140 - regression_loss: 358.4846 - val_loss: 228.4953 - val_regression_loss: 95.0955\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1585.6543 - regression_loss: 358.0026 - val_loss: 232.3269 - val_regression_loss: 96.7345\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1578.1987 - regression_loss: 361.9215 - val_loss: 231.1350 - val_regression_loss: 96.2520\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1581.4207 - regression_loss: 356.1262 - val_loss: 225.7023 - val_regression_loss: 93.9517\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1494.4630 - regression_loss: 338.7336 - val_loss: 225.1184 - val_regression_loss: 94.1969\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1473.8634 - regression_loss: 331.0999 - val_loss: 225.3478 - val_regression_loss: 94.7972\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1468.8459 - regression_loss: 332.6667 - val_loss: 222.9462 - val_regression_loss: 93.8807\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1482.7262 - regression_loss: 333.2473 - val_loss: 222.3832 - val_regression_loss: 93.6963\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1454.6732 - regression_loss: 334.4177 - val_loss: 223.2082 - val_regression_loss: 94.1035\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1455.8978 - regression_loss: 330.6402 - val_loss: 226.4317 - val_regression_loss: 95.6459\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1413.5342 - regression_loss: 323.3496 - val_loss: 229.2384 - val_regression_loss: 96.8600\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1412.3917 - regression_loss: 319.1579 - val_loss: 229.3012 - val_regression_loss: 96.6527\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1407.2277 - regression_loss: 315.1191 - val_loss: 230.1022 - val_regression_loss: 96.8822\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1416.5264 - regression_loss: 314.1713 - val_loss: 232.3882 - val_regression_loss: 97.9889\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1414.1473 - regression_loss: 312.5327 - val_loss: 235.4743 - val_regression_loss: 99.6419\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1394.0168 - regression_loss: 310.0393 - val_loss: 236.9897 - val_regression_loss: 100.5610\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1377.1086 - regression_loss: 308.4692 - val_loss: 235.3470 - val_regression_loss: 99.8801\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1389.3248 - regression_loss: 307.0336 - val_loss: 233.9026 - val_regression_loss: 99.2715\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1377.7954 - regression_loss: 306.6503 - val_loss: 232.9674 - val_regression_loss: 98.8867\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1352.7881 - regression_loss: 305.4732 - val_loss: 232.6201 - val_regression_loss: 98.7538\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1343.4470 - regression_loss: 303.4681 - val_loss: 230.9823 - val_regression_loss: 97.9066\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1354.6039 - regression_loss: 301.8498 - val_loss: 228.4925 - val_regression_loss: 96.5845\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1323.0806 - regression_loss: 299.5814 - val_loss: 229.8522 - val_regression_loss: 97.2618\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1320.9659 - regression_loss: 298.0078 - val_loss: 231.4066 - val_regression_loss: 98.0340\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1329.7412 - regression_loss: 296.6519 - val_loss: 231.6206 - val_regression_loss: 98.1576\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1312.9888 - regression_loss: 295.0392 - val_loss: 231.4822 - val_regression_loss: 98.0907\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1324.5455 - regression_loss: 293.7704 - val_loss: 233.0799 - val_regression_loss: 98.9436\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1302.8529 - regression_loss: 292.4201 - val_loss: 234.1366 - val_regression_loss: 99.5191\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1293.4742 - regression_loss: 291.4781 - val_loss: 233.4322 - val_regression_loss: 99.1773\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1300.6226 - regression_loss: 290.1769 - val_loss: 232.3390 - val_regression_loss: 98.6647\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1300.8563 - regression_loss: 288.9914 - val_loss: 231.3669 - val_regression_loss: 98.2065\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1294.0188 - regression_loss: 287.7931 - val_loss: 231.0858 - val_regression_loss: 98.0642\n",
            "***************************** elapsed_time is:  5.871152877807617\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 780050.6875 - regression_loss: 211999.3750 - val_loss: 98192.1797 - val_regression_loss: 48996.9648\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 730480.0000 - regression_loss: 199381.0000 - val_loss: 91218.4609 - val_regression_loss: 45519.6016\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 675047.3125 - regression_loss: 184126.1250 - val_loss: 81304.1016 - val_regression_loss: 40575.4961\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 591920.0000 - regression_loss: 162506.0469 - val_loss: 68134.8203 - val_regression_loss: 34007.0898\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 489451.6250 - regression_loss: 134059.0469 - val_loss: 52408.9727 - val_regression_loss: 26162.4023\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 364815.1562 - regression_loss: 100449.0078 - val_loss: 35766.8945 - val_regression_loss: 17859.7031\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 238249.5156 - regression_loss: 65560.1172 - val_loss: 20935.7285 - val_regression_loss: 10459.8389\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 129994.8438 - regression_loss: 36099.4688 - val_loss: 12247.8740 - val_regression_loss: 6122.2051\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 81848.3125 - regression_loss: 22376.8828 - val_loss: 13818.7725 - val_regression_loss: 6883.5464\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 113693.1406 - regression_loss: 31049.3379 - val_loss: 17066.6113 - val_regression_loss: 8462.6455\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 148584.4062 - regression_loss: 40167.8164 - val_loss: 14360.2500 - val_regression_loss: 7110.7310\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 120840.7812 - regression_loss: 32936.9375 - val_loss: 10579.4980 - val_regression_loss: 5243.6050\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 83270.3906 - regression_loss: 22372.2461 - val_loss: 9501.9043 - val_regression_loss: 4720.1392\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 63161.3203 - regression_loss: 17647.6270 - val_loss: 10664.8467 - val_regression_loss: 5307.4512\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 68643.9531 - regression_loss: 18340.4688 - val_loss: 12211.9736 - val_regression_loss: 6082.2812\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 75420.7188 - regression_loss: 20819.1387 - val_loss: 12948.2324 - val_regression_loss: 6450.4048\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 82650.6250 - regression_loss: 22025.8691 - val_loss: 12476.9424 - val_regression_loss: 6214.9492\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 72634.5469 - regression_loss: 21086.5430 - val_loss: 11149.9307 - val_regression_loss: 5552.1650\n",
            "Epoch 19/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 74488.7031 - regression_loss: 37102.9844\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 71088.9453 - regression_loss: 18913.5703 - val_loss: 9532.5166 - val_regression_loss: 4744.7861\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 60762.2656 - regression_loss: 16503.5078 - val_loss: 8830.6152 - val_regression_loss: 4394.6240\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 58014.6367 - regression_loss: 15632.6943 - val_loss: 8276.1152 - val_regression_loss: 4118.1758\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 55306.6289 - regression_loss: 15081.4209 - val_loss: 7899.9722 - val_regression_loss: 3930.8394\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 54766.9492 - regression_loss: 14885.4043 - val_loss: 7673.3276 - val_regression_loss: 3818.0869\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 54949.8789 - regression_loss: 14905.8203 - val_loss: 7533.3618 - val_regression_loss: 3748.4727\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 53875.5039 - regression_loss: 14913.0811 - val_loss: 7428.7583 - val_regression_loss: 3696.2778\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 53697.8516 - regression_loss: 14800.9834 - val_loss: 7333.4541 - val_regression_loss: 3648.4990\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 53048.1602 - regression_loss: 14508.0059 - val_loss: 7251.3613 - val_regression_loss: 3607.1528\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 52151.5156 - regression_loss: 14117.3701 - val_loss: 7201.9429 - val_regression_loss: 3582.0403\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 49664.5117 - regression_loss: 13713.7773 - val_loss: 7191.6313 - val_regression_loss: 3576.4517\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 49266.4180 - regression_loss: 13363.6514 - val_loss: 7202.9722 - val_regression_loss: 3581.7358\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 48055.8320 - regression_loss: 13115.0723 - val_loss: 7214.3037 - val_regression_loss: 3587.0952\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 47804.8359 - regression_loss: 12907.3604 - val_loss: 7187.7803 - val_regression_loss: 3573.6470\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 44121.3320 - regression_loss: 12712.1504 - val_loss: 7110.8574 - val_regression_loss: 3535.1265\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 46928.3398 - regression_loss: 12495.9219 - val_loss: 6970.5186 - val_regression_loss: 3465.0369\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 46372.6875 - regression_loss: 12240.5312 - val_loss: 6780.1274 - val_regression_loss: 3370.0410\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 44744.7773 - regression_loss: 11948.5293 - val_loss: 6563.7710 - val_regression_loss: 3262.1279\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 42744.2773 - regression_loss: 11636.3545 - val_loss: 6351.1826 - val_regression_loss: 3156.1106\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 41035.3125 - regression_loss: 11351.1182 - val_loss: 6150.8618 - val_regression_loss: 3056.2139\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 39950.6133 - regression_loss: 11086.4121 - val_loss: 5966.8081 - val_regression_loss: 2964.3999\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 40262.9844 - regression_loss: 10827.6777 - val_loss: 5809.2769 - val_regression_loss: 2885.7722\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 39624.3086 - regression_loss: 10565.3057 - val_loss: 5675.7979 - val_regression_loss: 2819.0938\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 38146.0117 - regression_loss: 10291.3115 - val_loss: 5560.8799 - val_regression_loss: 2761.6384\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 37199.6445 - regression_loss: 10011.5811 - val_loss: 5460.1719 - val_regression_loss: 2711.2383\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 36527.5742 - regression_loss: 9727.2842 - val_loss: 5370.9619 - val_regression_loss: 2666.5520\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 35151.3945 - regression_loss: 9443.8672 - val_loss: 5274.8882 - val_regression_loss: 2618.4658\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 34564.3086 - regression_loss: 9172.0381 - val_loss: 5174.7002 - val_regression_loss: 2568.3467\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 31895.1699 - regression_loss: 8894.4375 - val_loss: 5065.9888 - val_regression_loss: 2513.9915\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 32324.5371 - regression_loss: 8632.4121 - val_loss: 4941.5493 - val_regression_loss: 2451.8125\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 29998.1562 - regression_loss: 8349.5996 - val_loss: 4810.4097 - val_regression_loss: 2386.3203\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 29727.7988 - regression_loss: 8079.9468 - val_loss: 4664.5234 - val_regression_loss: 2313.4812\n",
            "***************************** elapsed_time is:  6.239571571350098\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 18863.4023 - regression_loss: 5122.3208 - val_loss: 1614.0464 - val_regression_loss: 785.2592\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 13458.2773 - regression_loss: 3540.4763 - val_loss: 1020.2411 - val_regression_loss: 488.5936\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8560.5400 - regression_loss: 2210.1357 - val_loss: 644.0826 - val_regression_loss: 300.8239\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5097.3613 - regression_loss: 1351.1929 - val_loss: 541.6055 - val_regression_loss: 250.6309\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4407.8164 - regression_loss: 1114.6266 - val_loss: 342.1457 - val_regression_loss: 152.3365\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2821.6733 - regression_loss: 695.2745 - val_loss: 260.1480 - val_regression_loss: 112.3932\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2412.9707 - regression_loss: 581.0754 - val_loss: 326.2840 - val_regression_loss: 145.9208\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2890.6304 - regression_loss: 732.1539 - val_loss: 357.3549 - val_regression_loss: 161.5636\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3076.9001 - regression_loss: 762.2894 - val_loss: 303.5473 - val_regression_loss: 134.6617\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2429.7397 - regression_loss: 608.4515 - val_loss: 238.0994 - val_regression_loss: 101.9129\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1871.7753 - regression_loss: 451.5442 - val_loss: 212.9724 - val_regression_loss: 89.3081\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1679.1372 - regression_loss: 396.9417 - val_loss: 234.1200 - val_regression_loss: 99.8294\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1862.7238 - regression_loss: 442.9899 - val_loss: 254.4178 - val_regression_loss: 109.9637\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2001.0603 - regression_loss: 476.4019 - val_loss: 246.6938 - val_regression_loss: 106.1596\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1840.3417 - regression_loss: 439.6420 - val_loss: 235.8706 - val_regression_loss: 100.8597\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1694.4497 - regression_loss: 392.0401 - val_loss: 238.8015 - val_regression_loss: 102.4622\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1604.0156 - regression_loss: 374.5283 - val_loss: 244.4392 - val_regression_loss: 105.4246\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1598.5073 - regression_loss: 372.1021 - val_loss: 245.4782 - val_regression_loss: 106.0751\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1588.2753 - regression_loss: 369.8460 - val_loss: 243.2651 - val_regression_loss: 105.0585\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1598.7286 - regression_loss: 368.9879 - val_loss: 240.3477 - val_regression_loss: 103.6257\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1552.3290 - regression_loss: 362.7744 - val_loss: 236.7984 - val_regression_loss: 101.8153\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1501.7983 - regression_loss: 347.0148 - val_loss: 237.1273 - val_regression_loss: 101.9091\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1468.3107 - regression_loss: 332.6803 - val_loss: 243.2316 - val_regression_loss: 104.9003\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1448.3997 - regression_loss: 330.1368 - val_loss: 249.4461 - val_regression_loss: 107.9845\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1445.4825 - regression_loss: 329.7118 - val_loss: 253.0671 - val_regression_loss: 109.8238\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1440.8434 - regression_loss: 324.3543 - val_loss: 254.1675 - val_regression_loss: 110.4358\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1389.7251 - regression_loss: 317.4179 - val_loss: 254.0963 - val_regression_loss: 110.4712\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1371.3973 - regression_loss: 311.5434 - val_loss: 254.6897 - val_regression_loss: 110.8263\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1340.3253 - regression_loss: 306.8808 - val_loss: 255.0022 - val_regression_loss: 111.0164\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1336.5510 - regression_loss: 305.1726 - val_loss: 254.0085 - val_regression_loss: 110.5399\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1355.2261 - regression_loss: 303.6236 - val_loss: 249.8924 - val_regression_loss: 108.4754\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1349.2687 - regression_loss: 299.8047 - val_loss: 245.4617 - val_regression_loss: 106.2384\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1309.5253 - regression_loss: 296.2542 - val_loss: 242.8653 - val_regression_loss: 104.9111\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1340.6890 - regression_loss: 294.2322 - val_loss: 242.7929 - val_regression_loss: 104.8539\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1301.8274 - regression_loss: 292.0743 - val_loss: 241.5602 - val_regression_loss: 104.2287\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1298.3295 - regression_loss: 290.2939 - val_loss: 238.0724 - val_regression_loss: 102.4848\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1301.3955 - regression_loss: 288.1283 - val_loss: 233.1026 - val_regression_loss: 100.0038\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1284.8505 - regression_loss: 285.7004 - val_loss: 227.9679 - val_regression_loss: 97.4367\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1258.3326 - regression_loss: 283.9212 - val_loss: 223.4843 - val_regression_loss: 95.1840\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1254.3635 - regression_loss: 282.7790 - val_loss: 218.8906 - val_regression_loss: 92.8659\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1277.8453 - regression_loss: 280.8574 - val_loss: 214.0640 - val_regression_loss: 90.4171\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1237.0304 - regression_loss: 279.2062 - val_loss: 209.5051 - val_regression_loss: 88.0972\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1239.3339 - regression_loss: 277.6907 - val_loss: 205.0207 - val_regression_loss: 85.8123\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1233.2596 - regression_loss: 276.3175 - val_loss: 200.3825 - val_regression_loss: 83.4595\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1233.1188 - regression_loss: 275.1429 - val_loss: 196.2686 - val_regression_loss: 81.3678\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1229.2500 - regression_loss: 273.9971 - val_loss: 192.9524 - val_regression_loss: 79.6824\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1218.8120 - regression_loss: 272.9490 - val_loss: 190.5839 - val_regression_loss: 78.4689\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1217.5610 - regression_loss: 271.8876 - val_loss: 188.4259 - val_regression_loss: 77.3558\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1229.6584 - regression_loss: 271.0553 - val_loss: 186.4490 - val_regression_loss: 76.3194\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1202.3073 - regression_loss: 270.1100 - val_loss: 184.7516 - val_regression_loss: 75.4196\n",
            "***************************** elapsed_time is:  5.913635969161987\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 31593.3008 - regression_loss: 8503.3838 - val_loss: 2649.0540 - val_regression_loss: 1312.2803\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 23249.4844 - regression_loss: 6279.1362 - val_loss: 1857.8806 - val_regression_loss: 910.7594\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 16334.4170 - regression_loss: 4313.3511 - val_loss: 1080.0894 - val_regression_loss: 513.2310\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8978.2100 - regression_loss: 2355.7117 - val_loss: 698.1277 - val_regression_loss: 311.3714\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5323.6553 - regression_loss: 1350.3467 - val_loss: 761.0740 - val_regression_loss: 339.9756\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6106.2778 - regression_loss: 1485.6565 - val_loss: 618.5174 - val_regression_loss: 277.9547\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5154.3677 - regression_loss: 1275.0959 - val_loss: 433.2400 - val_regression_loss: 197.4288\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3896.1790 - regression_loss: 988.9272 - val_loss: 483.4678 - val_regression_loss: 230.7261\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4310.8423 - regression_loss: 1158.3536 - val_loss: 484.8638 - val_regression_loss: 231.5291\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4432.0137 - regression_loss: 1169.5763 - val_loss: 369.2504 - val_regression_loss: 169.0034\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3463.5867 - regression_loss: 881.2873 - val_loss: 310.5905 - val_regression_loss: 134.0645\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2730.9775 - regression_loss: 699.6811 - val_loss: 349.6522 - val_regression_loss: 149.6547\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3036.9177 - regression_loss: 734.1924 - val_loss: 379.8295 - val_regression_loss: 163.5316\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3197.1318 - regression_loss: 770.5496 - val_loss: 344.6517 - val_regression_loss: 147.1591\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 2890.1121 - regression_loss: 701.3528 - val_loss: 297.5200 - val_regression_loss: 126.0737\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2464.2429 - regression_loss: 632.1660 - val_loss: 278.3069 - val_regression_loss: 119.0972\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2590.8516 - regression_loss: 626.6331 - val_loss: 273.8875 - val_regression_loss: 118.8966\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2563.0247 - regression_loss: 637.6039 - val_loss: 263.2961 - val_regression_loss: 114.7164\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2480.8718 - regression_loss: 615.4731 - val_loss: 249.8229 - val_regression_loss: 108.1251\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2219.6013 - regression_loss: 562.0448 - val_loss: 249.2224 - val_regression_loss: 107.1316\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2093.0007 - regression_loss: 530.5156 - val_loss: 259.1398 - val_regression_loss: 110.9194\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2167.4521 - regression_loss: 521.5763 - val_loss: 262.5138 - val_regression_loss: 111.5368\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2005.1853 - regression_loss: 505.9081 - val_loss: 253.1819 - val_regression_loss: 106.3637\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1983.9818 - regression_loss: 478.4298 - val_loss: 245.5635 - val_regression_loss: 102.5343\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1949.3307 - regression_loss: 463.5747 - val_loss: 241.4822 - val_regression_loss: 100.6760\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1888.5991 - regression_loss: 456.0775 - val_loss: 237.3038 - val_regression_loss: 98.7429\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1857.1481 - regression_loss: 440.2390 - val_loss: 237.1026 - val_regression_loss: 98.7072\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1804.1672 - regression_loss: 426.7108 - val_loss: 240.7893 - val_regression_loss: 100.6338\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1770.7087 - regression_loss: 421.7447 - val_loss: 238.4715 - val_regression_loss: 99.6690\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1739.4933 - regression_loss: 413.3895 - val_loss: 228.8789 - val_regression_loss: 95.1187\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1629.9135 - regression_loss: 397.6971 - val_loss: 221.7079 - val_regression_loss: 91.6787\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1656.6970 - regression_loss: 388.6660 - val_loss: 219.8473 - val_regression_loss: 90.6428\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1642.9905 - regression_loss: 380.2639 - val_loss: 220.5255 - val_regression_loss: 90.7401\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1524.5138 - regression_loss: 368.7061 - val_loss: 222.5949 - val_regression_loss: 91.6052\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1506.7710 - regression_loss: 359.3188 - val_loss: 223.2142 - val_regression_loss: 91.9860\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1526.7269 - regression_loss: 351.6753 - val_loss: 221.6816 - val_regression_loss: 91.4532\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1437.7262 - regression_loss: 345.4542 - val_loss: 219.0788 - val_regression_loss: 90.4582\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1467.4282 - regression_loss: 340.3819 - val_loss: 219.3870 - val_regression_loss: 90.7080\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1413.6407 - regression_loss: 333.7288 - val_loss: 220.2961 - val_regression_loss: 91.0712\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1416.9390 - regression_loss: 326.9106 - val_loss: 223.2306 - val_regression_loss: 92.3438\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1422.6104 - regression_loss: 321.2745 - val_loss: 225.2187 - val_regression_loss: 93.2282\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1331.3781 - regression_loss: 316.7046 - val_loss: 224.6046 - val_regression_loss: 92.9896\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1383.8422 - regression_loss: 313.1392 - val_loss: 225.5638 - val_regression_loss: 93.4952\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1370.8020 - regression_loss: 310.0655 - val_loss: 226.8405 - val_regression_loss: 94.1944\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1393.5031 - regression_loss: 307.4259 - val_loss: 228.3145 - val_regression_loss: 94.9312\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1337.9360 - regression_loss: 304.8347 - val_loss: 230.1188 - val_regression_loss: 95.8025\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1326.4435 - regression_loss: 302.6458 - val_loss: 229.3732 - val_regression_loss: 95.4843\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1351.6967 - regression_loss: 300.5369 - val_loss: 230.1146 - val_regression_loss: 95.8012\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1342.5697 - regression_loss: 298.2757 - val_loss: 229.5030 - val_regression_loss: 95.5006\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1325.6079 - regression_loss: 296.6953 - val_loss: 227.1150 - val_regression_loss: 94.4501\n",
            "***************************** elapsed_time is:  5.795040130615234\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 1s 402ms/step - loss: 49318.3359 - regression_loss: 13328.8789 - val_loss: 5064.5894 - val_regression_loss: 2513.3750\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 40672.7383 - regression_loss: 11052.5859 - val_loss: 3952.9387 - val_regression_loss: 1956.5818\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 32380.9727 - regression_loss: 8694.4648 - val_loss: 2657.7188 - val_regression_loss: 1307.0234\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 22248.7109 - regression_loss: 5977.7134 - val_loss: 1564.6669 - val_regression_loss: 757.3537\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14300.0352 - regression_loss: 3781.5073 - val_loss: 1268.6759 - val_regression_loss: 606.2825\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 12391.3516 - regression_loss: 3302.1477 - val_loss: 1062.4752 - val_regression_loss: 504.3171\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 10681.1221 - regression_loss: 2812.7410 - val_loss: 688.3049 - val_regression_loss: 320.9449\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 7372.6313 - regression_loss: 1903.0815 - val_loss: 643.3061 - val_regression_loss: 302.2760\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6377.3242 - regression_loss: 1695.5634 - val_loss: 868.9943 - val_regression_loss: 417.6988\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7605.3481 - regression_loss: 2064.2700 - val_loss: 946.8619 - val_regression_loss: 457.6627\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7635.5513 - regression_loss: 2160.6191 - val_loss: 787.6447 - val_regression_loss: 378.0227\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7019.4146 - regression_loss: 1819.3229 - val_loss: 576.6556 - val_regression_loss: 271.8992\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5472.0459 - regression_loss: 1415.2726 - val_loss: 477.8679 - val_regression_loss: 221.6408\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4972.8848 - regression_loss: 1285.8571 - val_loss: 472.6798 - val_regression_loss: 218.2936\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 5106.3364 - regression_loss: 1333.2732 - val_loss: 455.0445 - val_regression_loss: 209.1469\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5113.4302 - regression_loss: 1295.2128 - val_loss: 433.8294 - val_regression_loss: 198.6790\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4599.4238 - regression_loss: 1195.0225 - val_loss: 453.7595 - val_regression_loss: 209.0672\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4603.7783 - regression_loss: 1162.3108 - val_loss: 471.1825 - val_regression_loss: 218.2247\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4445.6357 - regression_loss: 1134.4026 - val_loss: 433.5479 - val_regression_loss: 199.7353\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3730.1272 - regression_loss: 1029.2311 - val_loss: 378.8806 - val_regression_loss: 172.6476\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3669.2544 - regression_loss: 927.7350 - val_loss: 359.4858 - val_regression_loss: 163.1891\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3502.5081 - regression_loss: 905.0668 - val_loss: 360.2634 - val_regression_loss: 163.8425\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3439.2051 - regression_loss: 892.7782 - val_loss: 355.7920 - val_regression_loss: 161.8512\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3316.7175 - regression_loss: 837.0927 - val_loss: 359.5852 - val_regression_loss: 163.8732\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3175.3772 - regression_loss: 795.1755 - val_loss: 359.0630 - val_regression_loss: 163.5380\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 3076.6470 - regression_loss: 767.7770 - val_loss: 332.5158 - val_regression_loss: 149.9631\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2921.2954 - regression_loss: 712.3245 - val_loss: 307.6107 - val_regression_loss: 137.0803\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2676.2444 - regression_loss: 666.1562 - val_loss: 301.4902 - val_regression_loss: 133.6823\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2453.5630 - regression_loss: 647.6833 - val_loss: 294.1069 - val_regression_loss: 129.9105\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2570.2593 - regression_loss: 618.2518 - val_loss: 291.3634 - val_regression_loss: 128.6419\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2473.5479 - regression_loss: 593.4016 - val_loss: 291.0107 - val_regression_loss: 128.5648\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2375.4343 - regression_loss: 574.7725 - val_loss: 285.7074 - val_regression_loss: 125.9040\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2267.0513 - regression_loss: 545.4705 - val_loss: 286.9550 - val_regression_loss: 126.5074\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2171.5015 - regression_loss: 526.2859 - val_loss: 282.0070 - val_regression_loss: 124.1295\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1925.9993 - regression_loss: 508.6940 - val_loss: 267.5193 - val_regression_loss: 117.0867\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1999.7032 - regression_loss: 487.0357 - val_loss: 261.1620 - val_regression_loss: 113.9434\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1981.6301 - regression_loss: 471.9043 - val_loss: 258.5022 - val_regression_loss: 112.4812\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1934.3412 - regression_loss: 451.6923 - val_loss: 260.5915 - val_regression_loss: 113.3419\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1853.7434 - regression_loss: 436.6163 - val_loss: 254.9799 - val_regression_loss: 110.4367\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1815.2571 - regression_loss: 421.1647 - val_loss: 248.8245 - val_regression_loss: 107.2939\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1741.6288 - regression_loss: 408.6918 - val_loss: 245.6258 - val_regression_loss: 105.6038\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1723.4652 - regression_loss: 398.3373 - val_loss: 247.1568 - val_regression_loss: 106.2203\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1671.8807 - regression_loss: 387.9436 - val_loss: 244.0197 - val_regression_loss: 104.6141\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1647.8430 - regression_loss: 379.7824 - val_loss: 238.6917 - val_regression_loss: 101.9862\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1595.5778 - regression_loss: 372.1293 - val_loss: 231.4618 - val_regression_loss: 98.4674\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1605.1942 - regression_loss: 365.8549 - val_loss: 230.0090 - val_regression_loss: 97.7535\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1566.3470 - regression_loss: 359.2877 - val_loss: 230.3691 - val_regression_loss: 97.9070\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1520.9264 - regression_loss: 353.4207 - val_loss: 227.4929 - val_regression_loss: 96.4862\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1490.6226 - regression_loss: 348.7719 - val_loss: 219.5649 - val_regression_loss: 92.5901\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1494.7451 - regression_loss: 343.9391 - val_loss: 219.6049 - val_regression_loss: 92.5395\n",
            "***************************** elapsed_time is:  6.251651287078857\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 59536.5273 - regression_loss: 16217.1445 - val_loss: 6174.6226 - val_regression_loss: 3084.7544\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 48340.3633 - regression_loss: 13079.9287 - val_loss: 4868.8438 - val_regression_loss: 2426.8188\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 36451.7852 - regression_loss: 9872.1143 - val_loss: 3390.7329 - val_regression_loss: 1678.7717\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 22983.7598 - regression_loss: 6205.2988 - val_loss: 2242.3391 - val_regression_loss: 1088.4681\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 12403.2900 - regression_loss: 3248.8147 - val_loss: 2255.2610 - val_regression_loss: 1072.8147\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11103.9561 - regression_loss: 2856.5015 - val_loss: 2374.4321 - val_regression_loss: 1130.1967\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 12420.8389 - regression_loss: 3163.8889 - val_loss: 1815.3173 - val_regression_loss: 865.4097\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 9134.0449 - regression_loss: 2305.1489 - val_loss: 1297.8906 - val_regression_loss: 622.6035\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 6050.7285 - regression_loss: 1561.9812 - val_loss: 1219.8630 - val_regression_loss: 593.9318\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 6224.5449 - regression_loss: 1641.3267 - val_loss: 1296.6937 - val_regression_loss: 635.9926\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 7160.4941 - regression_loss: 1892.9055 - val_loss: 1208.2000 - val_regression_loss: 590.3026\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6103.5059 - regression_loss: 1652.1029 - val_loss: 1024.4513 - val_regression_loss: 494.3267\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4607.1631 - regression_loss: 1164.6978 - val_loss: 956.7286 - val_regression_loss: 455.7315\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3656.8315 - regression_loss: 904.0957 - val_loss: 1038.8638 - val_regression_loss: 493.4866\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3952.8506 - regression_loss: 1015.1929 - val_loss: 1070.7499 - val_regression_loss: 508.8998\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4374.2422 - regression_loss: 1081.6368 - val_loss: 971.2264 - val_regression_loss: 461.2116\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 3460.8953 - regression_loss: 901.3115 - val_loss: 883.0508 - val_regression_loss: 420.5845\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3098.3792 - regression_loss: 760.4243 - val_loss: 879.8059 - val_regression_loss: 422.1500\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3183.4070 - regression_loss: 800.1656 - val_loss: 874.1262 - val_regression_loss: 421.0867\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3092.0496 - regression_loss: 816.7847 - val_loss: 828.1199 - val_regression_loss: 398.2807\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2925.5627 - regression_loss: 726.1490 - val_loss: 797.1979 - val_regression_loss: 382.1190\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2574.2708 - regression_loss: 650.5688 - val_loss: 799.8373 - val_regression_loss: 382.5294\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2638.5630 - regression_loss: 649.3943 - val_loss: 793.6373 - val_regression_loss: 378.6623\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2460.1880 - regression_loss: 634.7377 - val_loss: 762.1182 - val_regression_loss: 362.5475\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2333.6035 - regression_loss: 579.4692 - val_loss: 732.0421 - val_regression_loss: 347.5590\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2197.5261 - regression_loss: 543.4303 - val_loss: 716.4799 - val_regression_loss: 339.9276\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1993.9038 - regression_loss: 543.1240 - val_loss: 701.1887 - val_regression_loss: 332.2422\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2182.2112 - regression_loss: 535.4974 - val_loss: 680.9529 - val_regression_loss: 321.8176\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2126.7783 - regression_loss: 505.5767 - val_loss: 669.9070 - val_regression_loss: 315.9644\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2034.1060 - regression_loss: 483.2655 - val_loss: 666.5797 - val_regression_loss: 314.3954\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1870.2327 - regression_loss: 478.3604 - val_loss: 657.9066 - val_regression_loss: 310.6405\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1979.5978 - regression_loss: 467.7235 - val_loss: 646.4796 - val_regression_loss: 305.6298\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1893.3008 - regression_loss: 453.1623 - val_loss: 639.1679 - val_regression_loss: 302.3899\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1875.2363 - regression_loss: 446.9030 - val_loss: 632.6387 - val_regression_loss: 298.9771\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1805.9698 - regression_loss: 436.9074 - val_loss: 623.0267 - val_regression_loss: 293.5355\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1619.0205 - regression_loss: 419.6649 - val_loss: 613.9491 - val_regression_loss: 288.3719\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1779.2662 - regression_loss: 409.4991 - val_loss: 605.0329 - val_regression_loss: 283.6084\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1723.0106 - regression_loss: 402.6983 - val_loss: 591.9799 - val_regression_loss: 277.4203\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1542.7880 - regression_loss: 393.1035 - val_loss: 579.3336 - val_regression_loss: 271.7143\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1681.9076 - regression_loss: 387.6169 - val_loss: 567.9366 - val_regression_loss: 266.4936\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1638.2614 - regression_loss: 383.6817 - val_loss: 556.2971 - val_regression_loss: 260.7242\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1582.1986 - regression_loss: 376.1460 - val_loss: 545.5743 - val_regression_loss: 255.0676\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1530.1770 - regression_loss: 367.1264 - val_loss: 535.6081 - val_regression_loss: 249.8587\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1546.5328 - regression_loss: 360.3937 - val_loss: 523.3324 - val_regression_loss: 243.7325\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1519.8750 - regression_loss: 353.1893 - val_loss: 511.7376 - val_regression_loss: 238.0850\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1489.5830 - regression_loss: 347.8758 - val_loss: 502.1947 - val_regression_loss: 233.3719\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1475.4797 - regression_loss: 342.8973 - val_loss: 493.3969 - val_regression_loss: 228.8381\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1438.8292 - regression_loss: 335.9457 - val_loss: 486.7024 - val_regression_loss: 225.3737\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1455.2057 - regression_loss: 330.3654 - val_loss: 479.0573 - val_regression_loss: 221.6460\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1437.4846 - regression_loss: 326.1688 - val_loss: 469.7212 - val_regression_loss: 217.2288\n",
            "***************************** elapsed_time is:  5.82218599319458\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 227ms/step - loss: 103654.2891 - regression_loss: 28098.2695 - val_loss: 10654.2656 - val_regression_loss: 5264.6001\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 86846.1719 - regression_loss: 23995.0508 - val_loss: 8846.0225 - val_regression_loss: 4373.2920\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 75003.8750 - regression_loss: 20034.8457 - val_loss: 6642.6938 - val_regression_loss: 3286.3545\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 54876.3984 - regression_loss: 15136.6465 - val_loss: 4285.7437 - val_regression_loss: 2124.4744\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 36868.5312 - regression_loss: 10017.1787 - val_loss: 2540.7314 - val_regression_loss: 1267.8036\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 23543.2090 - regression_loss: 6362.0430 - val_loss: 2303.9553 - val_regression_loss: 1158.0100\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 21794.9336 - regression_loss: 6070.7031 - val_loss: 2427.0491 - val_regression_loss: 1211.0985\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 24229.0879 - regression_loss: 6509.1274 - val_loss: 1830.3984 - val_regression_loss: 897.5698\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 19541.4707 - regression_loss: 5362.8486 - val_loss: 1334.7267 - val_regression_loss: 640.1544\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 15566.1738 - regression_loss: 4247.6177 - val_loss: 1421.1562 - val_regression_loss: 679.1426\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 15737.8379 - regression_loss: 4192.2393 - val_loss: 1677.6630 - val_regression_loss: 806.4578\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 17259.1406 - regression_loss: 4489.9097 - val_loss: 1639.5510 - val_regression_loss: 789.5099\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 15010.0381 - regression_loss: 4284.4658 - val_loss: 1333.3718 - val_regression_loss: 640.9306\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 14176.2090 - regression_loss: 3701.2988 - val_loss: 1004.4938 - val_regression_loss: 482.4709\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 12058.2676 - regression_loss: 3159.7437 - val_loss: 861.7080 - val_regression_loss: 416.8095\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 11310.9258 - regression_loss: 3020.7559 - val_loss: 853.2737 - val_regression_loss: 416.2830\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11726.1934 - regression_loss: 3122.9631 - val_loss: 806.2556 - val_regression_loss: 393.2397\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 11332.3213 - regression_loss: 3004.3154 - val_loss: 702.1663 - val_regression_loss: 338.4309\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9794.8350 - regression_loss: 2601.1682 - val_loss: 667.1269 - val_regression_loss: 316.2865\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8721.6699 - regression_loss: 2281.7422 - val_loss: 730.8359 - val_regression_loss: 343.6772\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8301.2324 - regression_loss: 2161.0100 - val_loss: 752.9982 - val_regression_loss: 352.5879\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 8033.3315 - regression_loss: 2060.0588 - val_loss: 660.1951 - val_regression_loss: 306.8920\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6883.0156 - regression_loss: 1819.4673 - val_loss: 557.9915 - val_regression_loss: 258.2142\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6377.3486 - regression_loss: 1638.1841 - val_loss: 522.1617 - val_regression_loss: 242.5355\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 6160.8486 - regression_loss: 1582.3356 - val_loss: 484.8540 - val_regression_loss: 224.3743\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5620.9263 - regression_loss: 1474.8311 - val_loss: 426.6141 - val_regression_loss: 193.8701\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5115.9746 - regression_loss: 1291.7053 - val_loss: 407.4888 - val_regression_loss: 182.9266\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4749.1060 - regression_loss: 1204.7405 - val_loss: 396.2097 - val_regression_loss: 177.4771\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4564.6089 - regression_loss: 1152.3452 - val_loss: 367.3625 - val_regression_loss: 165.0395\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4065.0718 - regression_loss: 1051.3271 - val_loss: 361.6356 - val_regression_loss: 164.7292\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3914.0334 - regression_loss: 996.3265 - val_loss: 371.7333 - val_regression_loss: 171.2094\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3577.3010 - regression_loss: 957.5549 - val_loss: 345.3412 - val_regression_loss: 157.5366\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3444.8320 - regression_loss: 875.2410 - val_loss: 317.6609 - val_regression_loss: 142.5342\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3240.0288 - regression_loss: 807.4307 - val_loss: 303.3265 - val_regression_loss: 134.9896\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3023.4375 - regression_loss: 763.6124 - val_loss: 296.4799 - val_regression_loss: 132.2829\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2831.2913 - regression_loss: 714.2096 - val_loss: 308.6949 - val_regression_loss: 139.6753\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2765.9170 - regression_loss: 686.3265 - val_loss: 319.3138 - val_regression_loss: 145.7418\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2736.3018 - regression_loss: 666.2699 - val_loss: 303.8584 - val_regression_loss: 137.9116\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2574.1060 - regression_loss: 632.0023 - val_loss: 293.2769 - val_regression_loss: 132.4846\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2457.0095 - regression_loss: 614.2011 - val_loss: 292.9792 - val_regression_loss: 132.6127\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2403.3936 - regression_loss: 601.1741 - val_loss: 302.2677 - val_regression_loss: 138.0330\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2354.8225 - regression_loss: 586.7012 - val_loss: 316.8180 - val_regression_loss: 146.0203\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2362.2842 - regression_loss: 578.9682 - val_loss: 310.0928 - val_regression_loss: 142.4034\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2297.8008 - regression_loss: 561.8821 - val_loss: 292.5917 - val_regression_loss: 132.7967\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2082.4287 - regression_loss: 543.6647 - val_loss: 282.9101 - val_regression_loss: 127.3162\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2213.3638 - regression_loss: 532.6862 - val_loss: 287.2367 - val_regression_loss: 129.7131\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2139.9556 - regression_loss: 518.3292 - val_loss: 297.1145 - val_regression_loss: 135.0688\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2115.0310 - regression_loss: 511.1167 - val_loss: 290.7799 - val_regression_loss: 131.6498\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2040.5901 - regression_loss: 497.2434 - val_loss: 280.6305 - val_regression_loss: 125.9969\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2050.3389 - regression_loss: 486.3553 - val_loss: 278.6307 - val_regression_loss: 124.8439\n",
            "***************************** elapsed_time is:  5.9700846672058105\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 28311.9883 - regression_loss: 7557.0327 - val_loss: 2598.8577 - val_regression_loss: 1275.2361\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 19986.8750 - regression_loss: 5356.3345 - val_loss: 1724.3416 - val_regression_loss: 837.8364\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 13376.9014 - regression_loss: 3545.5583 - val_loss: 1094.1445 - val_regression_loss: 522.4710\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9210.7393 - regression_loss: 2365.0659 - val_loss: 903.8535 - val_regression_loss: 427.2909\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 7835.5249 - regression_loss: 2027.0184 - val_loss: 600.3625 - val_regression_loss: 275.8810\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 4914.8569 - regression_loss: 1223.1177 - val_loss: 440.5521 - val_regression_loss: 196.3088\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2815.3096 - regression_loss: 670.4659 - val_loss: 625.7395 - val_regression_loss: 289.3360\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3566.6389 - regression_loss: 896.9871 - val_loss: 749.6761 - val_regression_loss: 352.2706\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 4433.1274 - regression_loss: 1110.5955 - val_loss: 642.9029 - val_regression_loss: 300.1310\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3744.4973 - regression_loss: 931.9796 - val_loss: 502.7436 - val_regression_loss: 231.2239\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2763.7292 - regression_loss: 716.7618 - val_loss: 419.1938 - val_regression_loss: 190.3448\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2583.3853 - regression_loss: 630.1380 - val_loss: 370.8040 - val_regression_loss: 166.7211\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2470.9155 - regression_loss: 596.1303 - val_loss: 353.3079 - val_regression_loss: 158.3247\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 2485.8777 - regression_loss: 598.3530 - val_loss: 358.9962 - val_regression_loss: 161.4357\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2533.2593 - regression_loss: 621.7096 - val_loss: 358.6120 - val_regression_loss: 161.4990\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2485.0781 - regression_loss: 607.9320 - val_loss: 341.6472 - val_regression_loss: 153.2606\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2283.0791 - regression_loss: 555.1594 - val_loss: 324.4532 - val_regression_loss: 144.8376\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2114.0261 - regression_loss: 506.8853 - val_loss: 317.1934 - val_regression_loss: 141.2641\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1930.1605 - regression_loss: 478.3607 - val_loss: 317.6764 - val_regression_loss: 141.4553\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1930.6613 - regression_loss: 458.0677 - val_loss: 324.7527 - val_regression_loss: 144.9111\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1907.6133 - regression_loss: 450.3997 - val_loss: 329.6933 - val_regression_loss: 147.3522\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1907.2512 - regression_loss: 451.1734 - val_loss: 315.4246 - val_regression_loss: 140.2852\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1858.7635 - regression_loss: 434.5061 - val_loss: 293.8772 - val_regression_loss: 129.6491\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1726.7552 - regression_loss: 414.7172 - val_loss: 281.4098 - val_regression_loss: 123.5583\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1699.2423 - regression_loss: 402.7075 - val_loss: 275.4550 - val_regression_loss: 120.6850\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1700.4281 - regression_loss: 391.9584 - val_loss: 276.5778 - val_regression_loss: 121.3045\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1651.1667 - regression_loss: 387.0306 - val_loss: 270.9626 - val_regression_loss: 118.5495\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1667.3829 - regression_loss: 382.5842 - val_loss: 260.6612 - val_regression_loss: 113.4422\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1630.3615 - regression_loss: 375.7062 - val_loss: 255.7673 - val_regression_loss: 111.0094\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1555.7997 - regression_loss: 369.9224 - val_loss: 258.2696 - val_regression_loss: 112.2428\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1560.0964 - regression_loss: 361.8656 - val_loss: 262.3545 - val_regression_loss: 114.2575\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1556.3575 - regression_loss: 356.7399 - val_loss: 259.6095 - val_regression_loss: 112.8722\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1509.0186 - regression_loss: 352.2693 - val_loss: 251.5144 - val_regression_loss: 108.8263\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1532.4730 - regression_loss: 348.9037 - val_loss: 244.5358 - val_regression_loss: 105.3378\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1491.6603 - regression_loss: 344.5040 - val_loss: 240.2800 - val_regression_loss: 103.2082\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1508.0205 - regression_loss: 340.0452 - val_loss: 235.6393 - val_regression_loss: 100.8957\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1460.9691 - regression_loss: 335.8273 - val_loss: 230.2727 - val_regression_loss: 98.2235\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1472.8571 - regression_loss: 332.3406 - val_loss: 224.7522 - val_regression_loss: 95.4642\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1395.5349 - regression_loss: 328.8847 - val_loss: 220.1127 - val_regression_loss: 93.1325\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1433.9261 - regression_loss: 325.6288 - val_loss: 215.7746 - val_regression_loss: 90.9548\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1432.4657 - regression_loss: 322.2108 - val_loss: 211.5170 - val_regression_loss: 88.8158\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1418.0393 - regression_loss: 318.7540 - val_loss: 209.8772 - val_regression_loss: 87.9837\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1379.2822 - regression_loss: 315.7721 - val_loss: 204.2790 - val_regression_loss: 85.1797\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1378.3708 - regression_loss: 312.7119 - val_loss: 201.0567 - val_regression_loss: 83.5542\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1338.4054 - regression_loss: 309.5708 - val_loss: 193.8361 - val_regression_loss: 79.9409\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1351.0679 - regression_loss: 306.6983 - val_loss: 190.1830 - val_regression_loss: 78.1115\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1318.6779 - regression_loss: 304.1502 - val_loss: 184.8037 - val_regression_loss: 75.4246\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1344.8558 - regression_loss: 301.6732 - val_loss: 179.0689 - val_regression_loss: 72.5564\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1330.0878 - regression_loss: 299.4471 - val_loss: 176.5762 - val_regression_loss: 71.2932\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1307.9784 - regression_loss: 297.1432 - val_loss: 172.3106 - val_regression_loss: 69.1523\n",
            "***************************** elapsed_time is:  6.017229318618774\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 19725.0449 - regression_loss: 5189.2954 - val_loss: 1598.9844 - val_regression_loss: 776.6474\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 13798.5928 - regression_loss: 3664.5078 - val_loss: 1082.8396 - val_regression_loss: 518.2048\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 9227.1592 - regression_loss: 2413.1150 - val_loss: 754.9144 - val_regression_loss: 353.8463\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6147.7476 - regression_loss: 1614.4308 - val_loss: 681.6657 - val_regression_loss: 317.5386\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 5430.3472 - regression_loss: 1389.2715 - val_loss: 403.8600 - val_regression_loss: 179.6838\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3229.6018 - regression_loss: 785.0307 - val_loss: 225.0030 - val_regression_loss: 91.2101\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1939.0950 - regression_loss: 445.9694 - val_loss: 277.8093 - val_regression_loss: 118.3960\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2560.3064 - regression_loss: 621.0368 - val_loss: 336.6807 - val_regression_loss: 148.6446\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3084.2124 - regression_loss: 771.3236 - val_loss: 291.5214 - val_regression_loss: 126.8224\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2700.7754 - regression_loss: 656.5558 - val_loss: 224.5564 - val_regression_loss: 93.9145\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2012.2395 - regression_loss: 476.2034 - val_loss: 198.1618 - val_regression_loss: 81.0539\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1639.8641 - regression_loss: 390.5149 - val_loss: 207.3807 - val_regression_loss: 85.8214\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1702.4875 - regression_loss: 399.0301 - val_loss: 220.1759 - val_regression_loss: 92.3008\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1814.9027 - regression_loss: 422.8592 - val_loss: 219.7865 - val_regression_loss: 92.2039\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1799.0804 - regression_loss: 424.1362 - val_loss: 208.6661 - val_regression_loss: 86.7870\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1759.3356 - regression_loss: 401.7465 - val_loss: 191.4103 - val_regression_loss: 78.3343\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1579.2917 - regression_loss: 365.5365 - val_loss: 176.3731 - val_regression_loss: 70.9785\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1487.0588 - regression_loss: 340.0486 - val_loss: 170.1158 - val_regression_loss: 67.9565\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1496.8391 - regression_loss: 333.5644 - val_loss: 169.2617 - val_regression_loss: 67.5583\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1505.8853 - regression_loss: 338.3773 - val_loss: 168.8253 - val_regression_loss: 67.3147\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1443.4171 - regression_loss: 336.2132 - val_loss: 169.2873 - val_regression_loss: 67.5035\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1464.1212 - regression_loss: 330.0748 - val_loss: 170.1351 - val_regression_loss: 67.9010\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1420.0670 - regression_loss: 321.5861 - val_loss: 170.8869 - val_regression_loss: 68.2793\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1403.2872 - regression_loss: 312.7152 - val_loss: 173.0463 - val_regression_loss: 69.3888\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1361.3468 - regression_loss: 307.8572 - val_loss: 175.5537 - val_regression_loss: 70.6694\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1383.0565 - regression_loss: 309.4177 - val_loss: 175.6920 - val_regression_loss: 70.7615\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1383.7679 - regression_loss: 308.1223 - val_loss: 173.8339 - val_regression_loss: 69.8398\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1341.8353 - regression_loss: 304.6419 - val_loss: 171.0209 - val_regression_loss: 68.4396\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1330.0060 - regression_loss: 299.9523 - val_loss: 169.1462 - val_regression_loss: 67.5122\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1356.5851 - regression_loss: 295.6694 - val_loss: 168.8329 - val_regression_loss: 67.3709\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1343.7677 - regression_loss: 294.9051 - val_loss: 168.3389 - val_regression_loss: 67.1379\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1321.6464 - regression_loss: 294.2545 - val_loss: 166.9874 - val_regression_loss: 66.4813\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1292.8032 - regression_loss: 292.0836 - val_loss: 165.5171 - val_regression_loss: 65.7713\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1307.3601 - regression_loss: 289.5534 - val_loss: 165.1345 - val_regression_loss: 65.5997\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1299.0647 - regression_loss: 287.2855 - val_loss: 165.2954 - val_regression_loss: 65.6900\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1287.2286 - regression_loss: 286.0726 - val_loss: 166.0586 - val_regression_loss: 66.0709\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1285.3801 - regression_loss: 284.9922 - val_loss: 166.1649 - val_regression_loss: 66.1217\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1256.6836 - regression_loss: 283.4907 - val_loss: 166.5114 - val_regression_loss: 66.3016\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1278.3862 - regression_loss: 282.0199 - val_loss: 165.9068 - val_regression_loss: 66.0283\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1262.7533 - regression_loss: 280.0392 - val_loss: 164.1168 - val_regression_loss: 65.1630\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1243.2029 - regression_loss: 278.2307 - val_loss: 163.0306 - val_regression_loss: 64.6406\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1256.3632 - regression_loss: 277.0185 - val_loss: 162.4920 - val_regression_loss: 64.3862\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1258.9310 - regression_loss: 275.7811 - val_loss: 162.3201 - val_regression_loss: 64.3164\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1236.1787 - regression_loss: 274.3072 - val_loss: 162.1017 - val_regression_loss: 64.2158\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1233.1499 - regression_loss: 272.8759 - val_loss: 161.4371 - val_regression_loss: 63.8928\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1228.6309 - regression_loss: 271.6891 - val_loss: 161.3390 - val_regression_loss: 63.8545\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1196.6053 - regression_loss: 270.5900 - val_loss: 160.8189 - val_regression_loss: 63.6106\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1214.4209 - regression_loss: 269.4827 - val_loss: 160.7762 - val_regression_loss: 63.6085\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1193.0725 - regression_loss: 268.4821 - val_loss: 159.7083 - val_regression_loss: 63.0903\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1219.2455 - regression_loss: 266.9552 - val_loss: 159.8297 - val_regression_loss: 63.1620\n",
            "***************************** elapsed_time is:  6.217289447784424\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 3189681.2500 - regression_loss: 870896.3750 - val_loss: 349706.6562 - val_regression_loss: 174818.6406\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3077376.5000 - regression_loss: 841516.9375 - val_loss: 335162.5312 - val_regression_loss: 167552.5312\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2938179.5000 - regression_loss: 807888.3750 - val_loss: 314452.0938 - val_regression_loss: 157202.5781\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2772077.2500 - regression_loss: 760161.3750 - val_loss: 286212.6562 - val_regression_loss: 143087.1719\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2551390.7500 - regression_loss: 695641.6875 - val_loss: 250682.1094 - val_regression_loss: 125324.7969\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2259070.0000 - regression_loss: 614559.6875 - val_loss: 208856.2500 - val_regression_loss: 104412.8672\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1886072.1250 - regression_loss: 518843.8750 - val_loss: 161990.4531 - val_regression_loss: 80979.1641\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1520886.6250 - regression_loss: 411985.7188 - val_loss: 112322.5312 - val_regression_loss: 56143.5312\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1103489.0000 - regression_loss: 297870.5312 - val_loss: 65167.5586 - val_regression_loss: 32564.1094\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 673837.2500 - regression_loss: 187377.1406 - val_loss: 32808.9961 - val_regression_loss: 16382.0625\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 411642.8125 - regression_loss: 111381.2891 - val_loss: 35471.0898 - val_regression_loss: 17708.0801\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 387945.7812 - regression_loss: 108507.1016 - val_loss: 60822.0391 - val_regression_loss: 30381.7930\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 579805.5625 - regression_loss: 157833.5000 - val_loss: 58109.9805 - val_regression_loss: 29030.5547\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 556520.4375 - regression_loss: 151281.3125 - val_loss: 37070.6211 - val_regression_loss: 18516.1406\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 403686.2500 - regression_loss: 108885.9219 - val_loss: 22023.2930 - val_regression_loss: 10995.0498\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 298511.2500 - regression_loss: 79938.7734 - val_loss: 18732.6562 - val_regression_loss: 9349.4844\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 240329.6250 - regression_loss: 76848.4766 - val_loss: 21289.5898 - val_regression_loss: 10625.9170\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 292119.1875 - regression_loss: 84043.9453 - val_loss: 23709.4512 - val_regression_loss: 11833.4277\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 334303.7812 - regression_loss: 89832.1641 - val_loss: 23564.0957 - val_regression_loss: 11759.0605\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 325835.3438 - regression_loss: 88326.7500 - val_loss: 21113.8711 - val_regression_loss: 10533.6924\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 290062.7812 - regression_loss: 80244.2344 - val_loss: 18035.0293 - val_regression_loss: 8995.3232\n",
            "Epoch 22/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 274507.4062 - regression_loss: 137093.6406\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 262002.2969 - regression_loss: 70015.3906 - val_loss: 16520.1953 - val_regression_loss: 8239.6816\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 222018.5469 - regression_loss: 61102.6016 - val_loss: 16849.1660 - val_regression_loss: 8405.0664\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 218435.3906 - regression_loss: 58777.1602 - val_loss: 17838.6758 - val_regression_loss: 8900.5508\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 214420.5000 - regression_loss: 57790.7266 - val_loss: 19119.0859 - val_regression_loss: 9541.2764\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 214294.5938 - regression_loss: 57704.1289 - val_loss: 20230.2793 - val_regression_loss: 10097.0977\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 215277.8281 - regression_loss: 57674.8398 - val_loss: 20824.0645 - val_regression_loss: 10393.8506\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 209489.4375 - regression_loss: 57120.8125 - val_loss: 20725.1562 - val_regression_loss: 10343.8730\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 208411.2344 - regression_loss: 55987.1641 - val_loss: 20166.3301 - val_regression_loss: 10063.5996\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 199001.1562 - regression_loss: 54583.5508 - val_loss: 19357.3457 - val_regression_loss: 9657.9492\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 193445.5625 - regression_loss: 53225.4922 - val_loss: 18600.7637 - val_regression_loss: 9278.4844\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 190022.1562 - regression_loss: 52157.9219 - val_loss: 17932.0918 - val_regression_loss: 8943.1377\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 192273.5312 - regression_loss: 51407.2305 - val_loss: 17389.4199 - val_regression_loss: 8671.2109\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 185749.2969 - regression_loss: 50834.0547 - val_loss: 16980.1934 - val_regression_loss: 8466.6289\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 183597.6406 - regression_loss: 50193.5156 - val_loss: 16662.2598 - val_regression_loss: 8308.2393\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 183726.0000 - regression_loss: 49446.0781 - val_loss: 16462.1484 - val_regression_loss: 8209.2168\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 177411.7500 - regression_loss: 48544.8867 - val_loss: 16379.9346 - val_regression_loss: 8169.3643\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 163108.1406 - regression_loss: 47553.1523 - val_loss: 16379.1113 - val_regression_loss: 8170.3379\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 175122.0156 - regression_loss: 46700.6484 - val_loss: 16485.2480 - val_regression_loss: 8224.7490\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 167363.8438 - regression_loss: 45810.3867 - val_loss: 16596.2227 - val_regression_loss: 8281.4424\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 162180.6719 - regression_loss: 45011.6992 - val_loss: 16616.2383 - val_regression_loss: 8292.3896\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 162379.9219 - regression_loss: 44253.7070 - val_loss: 16554.1875 - val_regression_loss: 8261.9199\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 160670.8906 - regression_loss: 43456.7852 - val_loss: 16351.7998 - val_regression_loss: 8160.8877\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 157496.5625 - regression_loss: 42627.5703 - val_loss: 16023.3154 - val_regression_loss: 7996.2007\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 154130.4062 - regression_loss: 41773.6641 - val_loss: 15633.9990 - val_regression_loss: 7800.6138\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 145799.1875 - regression_loss: 40912.6016 - val_loss: 15219.2861 - val_regression_loss: 7591.8613\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 139434.2188 - regression_loss: 40072.1641 - val_loss: 14849.9277 - val_regression_loss: 7405.9722\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 137289.0156 - regression_loss: 39258.6602 - val_loss: 14544.5293 - val_regression_loss: 7252.6221\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 140743.7500 - regression_loss: 38487.3477 - val_loss: 14350.8535 - val_regression_loss: 7156.0215\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 141499.4062 - regression_loss: 37681.7656 - val_loss: 14211.9307 - val_regression_loss: 7087.5723\n",
            "***************************** elapsed_time is:  5.966772079467773\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 237ms/step - loss: 23681.5898 - regression_loss: 6451.1470 - val_loss: 1583.1635 - val_regression_loss: 771.2258\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 16324.3486 - regression_loss: 4372.5166 - val_loss: 991.2320 - val_regression_loss: 471.4744\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 10314.3506 - regression_loss: 2722.0103 - val_loss: 780.3938 - val_regression_loss: 361.0522\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7294.0122 - regression_loss: 1853.5983 - val_loss: 816.8000 - val_regression_loss: 377.8037\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6541.5327 - regression_loss: 1647.7954 - val_loss: 473.2745 - val_regression_loss: 209.9715\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3791.6113 - regression_loss: 926.5676 - val_loss: 230.7519 - val_regression_loss: 94.0334\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2361.0276 - regression_loss: 561.6276 - val_loss: 281.5488 - val_regression_loss: 123.5829\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3028.2244 - regression_loss: 793.1020 - val_loss: 325.3757 - val_regression_loss: 146.9288\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3473.8530 - regression_loss: 871.4573 - val_loss: 263.3795 - val_regression_loss: 115.3451\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2605.6152 - regression_loss: 641.9502 - val_loss: 226.3233 - val_regression_loss: 95.4180\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1961.0267 - regression_loss: 466.7855 - val_loss: 234.6730 - val_regression_loss: 98.3722\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1921.1415 - regression_loss: 450.5090 - val_loss: 226.9021 - val_regression_loss: 93.9945\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1937.6936 - regression_loss: 462.7170 - val_loss: 216.8005 - val_regression_loss: 89.1167\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2021.4443 - regression_loss: 482.5667 - val_loss: 209.3170 - val_regression_loss: 85.8287\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1979.6697 - regression_loss: 477.2295 - val_loss: 189.7056 - val_regression_loss: 76.5498\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1787.2555 - regression_loss: 414.9917 - val_loss: 182.9785 - val_regression_loss: 73.8718\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1565.5718 - regression_loss: 357.4725 - val_loss: 196.3274 - val_regression_loss: 81.3936\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1546.2733 - regression_loss: 353.4619 - val_loss: 205.8812 - val_regression_loss: 86.9606\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1569.2279 - regression_loss: 363.3166 - val_loss: 200.2920 - val_regression_loss: 84.6543\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1538.5978 - regression_loss: 363.9720 - val_loss: 188.6668 - val_regression_loss: 78.9287\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1541.5746 - regression_loss: 356.8510 - val_loss: 179.5180 - val_regression_loss: 74.1051\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1478.5244 - regression_loss: 337.8405 - val_loss: 177.0465 - val_regression_loss: 72.4103\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1395.5626 - regression_loss: 313.9203 - val_loss: 184.4849 - val_regression_loss: 75.6486\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1352.1921 - regression_loss: 303.6331 - val_loss: 192.2053 - val_regression_loss: 79.1999\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1369.6079 - regression_loss: 304.3374 - val_loss: 188.7402 - val_regression_loss: 77.4656\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1355.4904 - regression_loss: 301.0160 - val_loss: 182.2296 - val_regression_loss: 74.4411\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1342.9651 - regression_loss: 297.7754 - val_loss: 181.4890 - val_regression_loss: 74.3593\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1309.5552 - regression_loss: 292.5039 - val_loss: 186.6674 - val_regression_loss: 77.1823\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1281.3767 - regression_loss: 286.8685 - val_loss: 193.1464 - val_regression_loss: 80.5957\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1274.2913 - regression_loss: 286.7284 - val_loss: 191.7435 - val_regression_loss: 79.9878\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1294.6572 - regression_loss: 285.6029 - val_loss: 184.4620 - val_regression_loss: 76.3199\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1248.2439 - regression_loss: 282.6053 - val_loss: 180.3879 - val_regression_loss: 74.0969\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1256.1321 - regression_loss: 278.6600 - val_loss: 181.5154 - val_regression_loss: 74.4101\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1234.4613 - regression_loss: 274.1746 - val_loss: 183.7019 - val_regression_loss: 75.2992\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1227.8109 - regression_loss: 271.9416 - val_loss: 182.1455 - val_regression_loss: 74.4650\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1182.1130 - regression_loss: 269.7909 - val_loss: 181.0029 - val_regression_loss: 73.9465\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1231.4751 - regression_loss: 268.5012 - val_loss: 180.2715 - val_regression_loss: 73.7088\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1206.9183 - regression_loss: 266.8949 - val_loss: 183.6005 - val_regression_loss: 75.4742\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1216.5654 - regression_loss: 265.2559 - val_loss: 184.4853 - val_regression_loss: 76.0244\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1188.0503 - regression_loss: 264.4456 - val_loss: 182.1894 - val_regression_loss: 74.9421\n",
            "Epoch 41/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1269.9795 - regression_loss: 512.0436\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1212.9476 - regression_loss: 263.4362 - val_loss: 180.7369 - val_regression_loss: 74.1898\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1173.1373 - regression_loss: 262.1548 - val_loss: 180.7166 - val_regression_loss: 74.1314\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1201.5626 - regression_loss: 260.9212 - val_loss: 180.9772 - val_regression_loss: 74.1918\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1191.1493 - regression_loss: 259.9426 - val_loss: 180.9068 - val_regression_loss: 74.0968\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1204.5681 - regression_loss: 259.2248 - val_loss: 180.6773 - val_regression_loss: 73.9426\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1171.5538 - regression_loss: 258.7885 - val_loss: 179.0167 - val_regression_loss: 73.1125\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1166.3977 - regression_loss: 258.1108 - val_loss: 178.9802 - val_regression_loss: 73.0923\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1183.5077 - regression_loss: 257.6890 - val_loss: 179.9880 - val_regression_loss: 73.6016\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1152.0149 - regression_loss: 257.1596 - val_loss: 180.4807 - val_regression_loss: 73.8657\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1181.8274 - regression_loss: 256.6921 - val_loss: 180.9104 - val_regression_loss: 74.1024\n",
            "***************************** elapsed_time is:  5.94106125831604\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 220ms/step - loss: 9598.7666 - regression_loss: 2487.3154 - val_loss: 712.7867 - val_regression_loss: 332.8987\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6240.3784 - regression_loss: 1585.7670 - val_loss: 471.1728 - val_regression_loss: 210.7289\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4191.9009 - regression_loss: 1010.2905 - val_loss: 330.4596 - val_regression_loss: 140.3191\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2754.3677 - regression_loss: 648.6205 - val_loss: 220.6786 - val_regression_loss: 87.9168\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1690.4142 - regression_loss: 365.8010 - val_loss: 241.7018 - val_regression_loss: 101.7958\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1713.0109 - regression_loss: 399.9859 - val_loss: 307.0089 - val_regression_loss: 136.4914\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2232.8706 - regression_loss: 540.9854 - val_loss: 244.7563 - val_regression_loss: 105.5685\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1757.2683 - regression_loss: 410.2889 - val_loss: 179.7215 - val_regression_loss: 72.4989\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1318.0817 - regression_loss: 292.0844 - val_loss: 167.3626 - val_regression_loss: 65.6884\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1314.1053 - regression_loss: 293.3535 - val_loss: 176.0280 - val_regression_loss: 69.6921\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1485.0752 - regression_loss: 331.6870 - val_loss: 175.1311 - val_regression_loss: 69.3405\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1486.8169 - regression_loss: 333.7217 - val_loss: 165.1744 - val_regression_loss: 64.8114\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1372.0883 - regression_loss: 302.2885 - val_loss: 158.9574 - val_regression_loss: 62.3627\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1262.2773 - regression_loss: 273.8127 - val_loss: 164.1378 - val_regression_loss: 65.6662\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1241.7173 - regression_loss: 268.8776 - val_loss: 176.5218 - val_regression_loss: 72.4026\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1286.2394 - regression_loss: 284.3603 - val_loss: 180.8331 - val_regression_loss: 74.7341\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1296.0443 - regression_loss: 290.5122 - val_loss: 172.1992 - val_regression_loss: 70.2006\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1254.4729 - regression_loss: 278.1363 - val_loss: 160.1345 - val_regression_loss: 63.7376\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1201.4167 - regression_loss: 263.8303 - val_loss: 153.8834 - val_regression_loss: 60.1970\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1210.2443 - regression_loss: 261.0242 - val_loss: 153.7327 - val_regression_loss: 59.8521\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1179.5096 - regression_loss: 264.0641 - val_loss: 156.7406 - val_regression_loss: 61.2907\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1217.2302 - regression_loss: 265.0058 - val_loss: 158.8903 - val_regression_loss: 62.4688\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1204.0024 - regression_loss: 261.7479 - val_loss: 160.0129 - val_regression_loss: 63.2428\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1176.1217 - regression_loss: 258.3925 - val_loss: 161.2025 - val_regression_loss: 64.0638\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1176.0864 - regression_loss: 258.1736 - val_loss: 162.2668 - val_regression_loss: 64.7493\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1201.9546 - regression_loss: 260.0455 - val_loss: 162.6468 - val_regression_loss: 64.9796\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1173.2013 - regression_loss: 260.5267 - val_loss: 160.7851 - val_regression_loss: 63.9701\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1185.3438 - regression_loss: 258.6498 - val_loss: 158.5460 - val_regression_loss: 62.6971\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1179.3704 - regression_loss: 256.3301 - val_loss: 156.3078 - val_regression_loss: 61.4243\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1168.9558 - regression_loss: 255.4142 - val_loss: 155.6229 - val_regression_loss: 60.9864\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1142.0463 - regression_loss: 255.4940 - val_loss: 156.4855 - val_regression_loss: 61.4081\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1170.4874 - regression_loss: 255.0797 - val_loss: 157.8292 - val_regression_loss: 62.1343\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1173.1217 - regression_loss: 254.4369 - val_loss: 159.0871 - val_regression_loss: 62.8520\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1168.5659 - regression_loss: 254.0859 - val_loss: 158.6545 - val_regression_loss: 62.7002\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1132.3342 - regression_loss: 254.0801 - val_loss: 157.7675 - val_regression_loss: 62.2754\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1148.8976 - regression_loss: 254.1615 - val_loss: 156.7538 - val_regression_loss: 61.7182\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1155.5988 - regression_loss: 253.3839 - val_loss: 156.0354 - val_regression_loss: 61.2789\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1144.8140 - regression_loss: 252.7222 - val_loss: 154.2815 - val_regression_loss: 60.3169\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1156.6866 - regression_loss: 251.9892 - val_loss: 154.4790 - val_regression_loss: 60.3714\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1131.6976 - regression_loss: 251.5135 - val_loss: 155.8565 - val_regression_loss: 61.0515\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1141.2062 - regression_loss: 251.2160 - val_loss: 156.9654 - val_regression_loss: 61.6262\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1138.5900 - regression_loss: 251.0713 - val_loss: 156.0437 - val_regression_loss: 61.1820\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1141.1451 - regression_loss: 250.7407 - val_loss: 155.4212 - val_regression_loss: 60.8661\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1124.0234 - regression_loss: 250.5105 - val_loss: 154.9450 - val_regression_loss: 60.5950\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1140.4062 - regression_loss: 250.0845 - val_loss: 154.5284 - val_regression_loss: 60.3266\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1128.6562 - regression_loss: 249.6959 - val_loss: 153.6297 - val_regression_loss: 59.8171\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1134.2633 - regression_loss: 249.1715 - val_loss: 154.4161 - val_regression_loss: 60.1511\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1141.9733 - regression_loss: 248.7297 - val_loss: 154.6024 - val_regression_loss: 60.2134\n",
            "Epoch 49/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1190.9484 - regression_loss: 478.8419\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1137.7354 - regression_loss: 248.4398 - val_loss: 155.1746 - val_regression_loss: 60.4705\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1130.1390 - regression_loss: 248.1188 - val_loss: 155.1203 - val_regression_loss: 60.4364\n",
            "***************************** elapsed_time is:  6.218482494354248\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 213ms/step - loss: 273516.5312 - regression_loss: 73495.5000 - val_loss: 30306.3262 - val_regression_loss: 15117.2285\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 246800.7500 - regression_loss: 67133.4219 - val_loss: 26838.6211 - val_regression_loss: 13387.9941\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 217016.9062 - regression_loss: 59619.0508 - val_loss: 22117.2695 - val_regression_loss: 11032.2373\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 180340.6250 - regression_loss: 49458.9023 - val_loss: 16364.0996 - val_regression_loss: 8160.6367\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 138901.5781 - regression_loss: 37311.0430 - val_loss: 10665.0088 - val_regression_loss: 5315.7510\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 92246.8125 - regression_loss: 25326.4219 - val_loss: 6892.3506 - val_regression_loss: 3433.2363\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 66115.8203 - regression_loss: 17714.2363 - val_loss: 6532.5381 - val_regression_loss: 3255.3462\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 63508.7227 - regression_loss: 17233.9375 - val_loss: 6616.9224 - val_regression_loss: 3296.4287\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 62129.2383 - regression_loss: 17032.7441 - val_loss: 5143.7944 - val_regression_loss: 2556.4648\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 48411.2656 - regression_loss: 13500.6240 - val_loss: 3813.4304 - val_regression_loss: 1887.7625\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 38625.3867 - regression_loss: 10443.9795 - val_loss: 3538.6240 - val_regression_loss: 1748.3464\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 37320.8672 - regression_loss: 10037.4170 - val_loss: 3918.0210 - val_regression_loss: 1937.4694\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 40772.3164 - regression_loss: 10948.6836 - val_loss: 4168.7334 - val_regression_loss: 2063.0667\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 42447.5312 - regression_loss: 11430.3682 - val_loss: 3952.5664 - val_regression_loss: 1955.6553\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 41198.0391 - regression_loss: 10841.4287 - val_loss: 3418.5281 - val_regression_loss: 1689.5698\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 36357.2852 - regression_loss: 9545.0703 - val_loss: 2905.4998 - val_regression_loss: 1434.0780\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 31038.8594 - regression_loss: 8263.8574 - val_loss: 2680.3162 - val_regression_loss: 1322.4375\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 28554.2949 - regression_loss: 7606.0210 - val_loss: 2754.3555 - val_regression_loss: 1360.1853\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 27464.1094 - regression_loss: 7533.6646 - val_loss: 2862.1167 - val_regression_loss: 1414.4132\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 28337.4629 - regression_loss: 7564.7744 - val_loss: 2759.7031 - val_regression_loss: 1363.1289\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 26583.3906 - regression_loss: 7194.2080 - val_loss: 2499.2761 - val_regression_loss: 1232.4984\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 24867.9609 - regression_loss: 6607.1514 - val_loss: 2258.0833 - val_regression_loss: 1111.3301\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 21953.1270 - regression_loss: 6133.9849 - val_loss: 2102.3015 - val_regression_loss: 1032.9285\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 22174.8145 - regression_loss: 5861.5122 - val_loss: 1970.5306 - val_regression_loss: 966.7896\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 21147.9102 - regression_loss: 5540.8726 - val_loss: 1843.8844 - val_regression_loss: 903.4646\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 18175.0078 - regression_loss: 5081.1846 - val_loss: 1782.7169 - val_regression_loss: 873.0355\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 17760.9648 - regression_loss: 4713.1260 - val_loss: 1785.8345 - val_regression_loss: 874.7810\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 16749.5801 - regression_loss: 4452.9238 - val_loss: 1707.1595 - val_regression_loss: 835.5505\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 15676.1113 - regression_loss: 4159.2549 - val_loss: 1495.9290 - val_regression_loss: 729.9333\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14040.7480 - regression_loss: 3753.4768 - val_loss: 1287.9369 - val_regression_loss: 625.9006\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13006.4453 - regression_loss: 3427.8298 - val_loss: 1154.0011 - val_regression_loss: 559.0088\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 12096.2754 - regression_loss: 3189.2944 - val_loss: 1059.4631 - val_regression_loss: 511.9854\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11296.4297 - regression_loss: 2923.6804 - val_loss: 1013.4345 - val_regression_loss: 489.3031\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10166.8779 - regression_loss: 2671.2278 - val_loss: 991.0049 - val_regression_loss: 478.3068\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9677.3906 - regression_loss: 2499.8518 - val_loss: 911.6481 - val_regression_loss: 438.6216\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8613.2900 - regression_loss: 2301.1511 - val_loss: 796.2079 - val_regression_loss: 380.7113\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6833.5020 - regression_loss: 2106.3289 - val_loss: 729.8629 - val_regression_loss: 347.3749\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7529.9062 - regression_loss: 1982.1738 - val_loss: 711.7764 - val_regression_loss: 338.3420\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 7000.8013 - regression_loss: 1830.8829 - val_loss: 723.0307 - val_regression_loss: 344.0431\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5790.2686 - regression_loss: 1710.2151 - val_loss: 698.3427 - val_regression_loss: 331.6964\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6176.4829 - regression_loss: 1613.6279 - val_loss: 650.2675 - val_regression_loss: 307.6300\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5830.7646 - regression_loss: 1508.1815 - val_loss: 592.6269 - val_regression_loss: 278.7664\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 5439.5747 - regression_loss: 1420.1744 - val_loss: 569.5443 - val_regression_loss: 267.2963\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5296.1396 - regression_loss: 1343.0038 - val_loss: 577.6936 - val_regression_loss: 271.5071\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 4662.5850 - regression_loss: 1267.0540 - val_loss: 570.2070 - val_regression_loss: 267.8267\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 4698.1816 - regression_loss: 1212.9106 - val_loss: 567.7809 - val_regression_loss: 266.6519\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3837.5549 - regression_loss: 1153.7762 - val_loss: 534.6663 - val_regression_loss: 250.0502\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 4380.1338 - regression_loss: 1106.8871 - val_loss: 520.7073 - val_regression_loss: 243.0715\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 4123.1514 - regression_loss: 1064.1348 - val_loss: 534.9551 - val_regression_loss: 250.2603\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3624.4263 - regression_loss: 1021.5264 - val_loss: 529.5218 - val_regression_loss: 247.5619\n",
            "***************************** elapsed_time is:  5.849602460861206\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 6444.5815 - regression_loss: 1670.2458 - val_loss: 586.8091 - val_regression_loss: 269.8434\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4069.1160 - regression_loss: 986.9937 - val_loss: 407.6465 - val_regression_loss: 180.5648\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2451.9856 - regression_loss: 589.7683 - val_loss: 247.3269 - val_regression_loss: 102.2324\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1520.9928 - regression_loss: 330.7615 - val_loss: 207.0448 - val_regression_loss: 84.3915\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1710.6572 - regression_loss: 389.2358 - val_loss: 217.2377 - val_regression_loss: 90.7263\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1947.3096 - regression_loss: 457.5996 - val_loss: 174.7690 - val_regression_loss: 69.6001\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1494.1444 - regression_loss: 333.9180 - val_loss: 171.8852 - val_regression_loss: 67.8597\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1282.1603 - regression_loss: 278.7029 - val_loss: 198.6072 - val_regression_loss: 80.9819\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1351.5460 - regression_loss: 306.7726 - val_loss: 211.7769 - val_regression_loss: 87.5777\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1430.2968 - regression_loss: 328.0025 - val_loss: 199.3429 - val_regression_loss: 81.6039\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1400.3746 - regression_loss: 310.5689 - val_loss: 173.6577 - val_regression_loss: 69.1552\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1273.5922 - regression_loss: 278.2266 - val_loss: 155.9367 - val_regression_loss: 60.7069\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1217.7557 - regression_loss: 270.4648 - val_loss: 152.2413 - val_regression_loss: 59.1516\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1290.0970 - regression_loss: 284.8396 - val_loss: 153.2688 - val_regression_loss: 59.7338\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1280.6237 - regression_loss: 290.5930 - val_loss: 153.4973 - val_regression_loss: 59.7119\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1251.8990 - regression_loss: 276.9427 - val_loss: 156.5312 - val_regression_loss: 61.0114\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1203.7081 - regression_loss: 265.6122 - val_loss: 162.3575 - val_regression_loss: 63.7289\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1186.1642 - regression_loss: 266.3972 - val_loss: 166.4572 - val_regression_loss: 65.6672\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1216.1708 - regression_loss: 270.3053 - val_loss: 164.9268 - val_regression_loss: 64.9102\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1211.2086 - regression_loss: 269.3553 - val_loss: 159.4259 - val_regression_loss: 62.2587\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1194.3777 - regression_loss: 264.1687 - val_loss: 154.4031 - val_regression_loss: 59.8698\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1162.4895 - regression_loss: 262.7730 - val_loss: 151.2769 - val_regression_loss: 58.4193\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1201.5612 - regression_loss: 264.0081 - val_loss: 150.7502 - val_regression_loss: 58.1845\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1193.5834 - regression_loss: 265.1125 - val_loss: 151.5073 - val_regression_loss: 58.5113\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1168.6100 - regression_loss: 264.0429 - val_loss: 153.0859 - val_regression_loss: 59.1737\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1180.1676 - regression_loss: 261.3547 - val_loss: 154.6365 - val_regression_loss: 59.8220\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1161.2238 - regression_loss: 260.9531 - val_loss: 155.5821 - val_regression_loss: 60.2110\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1178.2148 - regression_loss: 261.0232 - val_loss: 154.7854 - val_regression_loss: 59.8013\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1197.0419 - regression_loss: 260.6924 - val_loss: 152.8532 - val_regression_loss: 58.8726\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1176.2866 - regression_loss: 259.9123 - val_loss: 150.9491 - val_regression_loss: 57.9695\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1158.9913 - regression_loss: 259.5745 - val_loss: 150.3815 - val_regression_loss: 57.7097\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1161.9634 - regression_loss: 259.7894 - val_loss: 150.4106 - val_regression_loss: 57.7141\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1175.2732 - regression_loss: 259.4916 - val_loss: 151.5249 - val_regression_loss: 58.2125\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1142.1328 - regression_loss: 258.9893 - val_loss: 152.1352 - val_regression_loss: 58.4368\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1155.5580 - regression_loss: 258.1609 - val_loss: 152.3574 - val_regression_loss: 58.4898\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1151.5671 - regression_loss: 258.0577 - val_loss: 152.0654 - val_regression_loss: 58.3228\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1175.0138 - regression_loss: 257.5677 - val_loss: 151.9722 - val_regression_loss: 58.2725\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1164.3455 - regression_loss: 257.2061 - val_loss: 151.8710 - val_regression_loss: 58.2215\n",
            "Epoch 39/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1203.9761 - regression_loss: 489.1814\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1151.0331 - regression_loss: 257.0961 - val_loss: 151.4312 - val_regression_loss: 57.9915\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1149.4968 - regression_loss: 256.6809 - val_loss: 151.4411 - val_regression_loss: 57.9804\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1149.5518 - regression_loss: 256.5054 - val_loss: 151.4412 - val_regression_loss: 57.9578\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1137.1071 - regression_loss: 256.1251 - val_loss: 151.5288 - val_regression_loss: 57.9795\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1138.1947 - regression_loss: 256.0310 - val_loss: 151.6896 - val_regression_loss: 58.0226\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1136.6969 - regression_loss: 255.7369 - val_loss: 151.9672 - val_regression_loss: 58.1348\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1139.6348 - regression_loss: 255.3932 - val_loss: 152.0627 - val_regression_loss: 58.1626\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1147.9117 - regression_loss: 255.2845 - val_loss: 151.9310 - val_regression_loss: 58.0904\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1146.1820 - regression_loss: 255.1701 - val_loss: 152.0384 - val_regression_loss: 58.1410\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1147.6515 - regression_loss: 255.0177 - val_loss: 152.0057 - val_regression_loss: 58.1285\n",
            "Epoch 49/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1190.8655 - regression_loss: 484.3191\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1138.5646 - regression_loss: 254.7460 - val_loss: 151.8557 - val_regression_loss: 58.0473\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1129.4141 - regression_loss: 254.5977 - val_loss: 151.7503 - val_regression_loss: 57.9872\n",
            "***************************** elapsed_time is:  6.003032684326172\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 39929.8945 - regression_loss: 10856.1260 - val_loss: 3403.1917 - val_regression_loss: 1689.9100\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 29292.9473 - regression_loss: 7897.5127 - val_loss: 2214.8489 - val_regression_loss: 1090.2207\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 19038.4004 - regression_loss: 5100.4902 - val_loss: 1120.3480 - val_regression_loss: 533.0952\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 9582.9316 - regression_loss: 2499.9199 - val_loss: 798.7829 - val_regression_loss: 357.8406\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6410.6851 - regression_loss: 1578.9943 - val_loss: 928.5817 - val_regression_loss: 418.9690\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 7064.3706 - regression_loss: 1760.1929 - val_loss: 630.9702 - val_regression_loss: 279.7035\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4858.9121 - regression_loss: 1172.8917 - val_loss: 406.5666 - val_regression_loss: 178.9459\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3227.4158 - regression_loss: 785.9246 - val_loss: 454.2394 - val_regression_loss: 210.2978\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3591.4009 - regression_loss: 928.1985 - val_loss: 543.1484 - val_regression_loss: 257.4973\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4330.0957 - regression_loss: 1124.9161 - val_loss: 503.2630 - val_regression_loss: 237.1684\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 4001.7751 - regression_loss: 1034.2812 - val_loss: 372.9850 - val_regression_loss: 170.1402\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3019.8760 - regression_loss: 755.3536 - val_loss: 262.0056 - val_regression_loss: 112.3002\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2203.0295 - regression_loss: 524.6519 - val_loss: 242.9193 - val_regression_loss: 100.7479\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2113.8999 - regression_loss: 504.2224 - val_loss: 282.7506 - val_regression_loss: 119.5739\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2466.8723 - regression_loss: 601.9110 - val_loss: 291.2147 - val_regression_loss: 123.8880\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2618.6538 - regression_loss: 618.4346 - val_loss: 254.7365 - val_regression_loss: 106.5876\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2273.0664 - regression_loss: 529.2276 - val_loss: 227.0585 - val_regression_loss: 94.0655\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1956.3806 - regression_loss: 460.0508 - val_loss: 229.3358 - val_regression_loss: 96.5269\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1950.2562 - regression_loss: 458.2569 - val_loss: 239.3503 - val_regression_loss: 102.6737\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1967.1163 - regression_loss: 475.9082 - val_loss: 239.3089 - val_regression_loss: 103.5381\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1957.8562 - regression_loss: 471.4559 - val_loss: 232.1568 - val_regression_loss: 100.5045\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1841.4663 - regression_loss: 452.0293 - val_loss: 227.0396 - val_regression_loss: 98.0403\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1744.3643 - regression_loss: 437.9954 - val_loss: 222.3142 - val_regression_loss: 95.2679\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1799.0288 - regression_loss: 424.9857 - val_loss: 213.8967 - val_regression_loss: 90.2502\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1713.2678 - regression_loss: 404.9124 - val_loss: 206.0055 - val_regression_loss: 85.4286\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1639.2188 - regression_loss: 388.0226 - val_loss: 202.6813 - val_regression_loss: 83.1601\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1640.2751 - regression_loss: 381.7765 - val_loss: 201.6098 - val_regression_loss: 82.4481\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1643.1102 - regression_loss: 379.7165 - val_loss: 199.0938 - val_regression_loss: 81.4450\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1631.9642 - regression_loss: 371.4975 - val_loss: 196.0432 - val_regression_loss: 80.4581\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1547.7914 - regression_loss: 361.2454 - val_loss: 195.6809 - val_regression_loss: 80.9081\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1535.8757 - regression_loss: 356.8907 - val_loss: 197.0861 - val_regression_loss: 82.0999\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1556.6661 - regression_loss: 357.3290 - val_loss: 195.7206 - val_regression_loss: 81.6501\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1518.2100 - regression_loss: 354.9893 - val_loss: 191.6042 - val_regression_loss: 79.5448\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1515.7968 - regression_loss: 347.6922 - val_loss: 186.5067 - val_regression_loss: 76.7618\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1477.8203 - regression_loss: 340.8218 - val_loss: 181.8521 - val_regression_loss: 74.1676\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1464.5201 - regression_loss: 335.0836 - val_loss: 179.0629 - val_regression_loss: 72.5856\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1475.1538 - regression_loss: 330.3231 - val_loss: 177.7900 - val_regression_loss: 71.8854\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1408.9121 - regression_loss: 326.3081 - val_loss: 177.0226 - val_regression_loss: 71.6006\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1416.0262 - regression_loss: 322.7729 - val_loss: 176.6870 - val_regression_loss: 71.6652\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1397.7937 - regression_loss: 319.0848 - val_loss: 175.5890 - val_regression_loss: 71.3597\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1395.8857 - regression_loss: 316.2672 - val_loss: 173.8683 - val_regression_loss: 70.6744\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1381.7772 - regression_loss: 314.0608 - val_loss: 171.5110 - val_regression_loss: 69.5630\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1369.8213 - regression_loss: 310.7661 - val_loss: 169.3886 - val_regression_loss: 68.4921\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1365.7753 - regression_loss: 307.3658 - val_loss: 167.6397 - val_regression_loss: 67.5379\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1355.2555 - regression_loss: 303.5963 - val_loss: 166.3707 - val_regression_loss: 66.8728\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1352.8472 - regression_loss: 300.6256 - val_loss: 164.9270 - val_regression_loss: 66.1633\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1310.1259 - regression_loss: 297.8882 - val_loss: 163.2657 - val_regression_loss: 65.4274\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1312.8959 - regression_loss: 295.6592 - val_loss: 162.1824 - val_regression_loss: 64.9557\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1280.5098 - regression_loss: 293.7059 - val_loss: 161.4987 - val_regression_loss: 64.6996\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1287.0623 - regression_loss: 291.8910 - val_loss: 161.8686 - val_regression_loss: 64.9733\n",
            "***************************** elapsed_time is:  5.817589282989502\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 13052.0703 - regression_loss: 3436.9114 - val_loss: 809.4247 - val_regression_loss: 381.6776\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8538.6641 - regression_loss: 2223.3955 - val_loss: 486.2524 - val_regression_loss: 218.1003\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5202.2529 - regression_loss: 1323.7272 - val_loss: 366.3881 - val_regression_loss: 157.4360\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3696.9307 - regression_loss: 893.3918 - val_loss: 260.3432 - val_regression_loss: 107.7750\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2321.8113 - regression_loss: 545.0983 - val_loss: 241.0316 - val_regression_loss: 102.8178\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1898.4062 - regression_loss: 457.9811 - val_loss: 340.1370 - val_regression_loss: 155.5004\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2640.4299 - regression_loss: 668.1732 - val_loss: 301.9594 - val_regression_loss: 136.4960\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2399.9719 - regression_loss: 594.5976 - val_loss: 200.5741 - val_regression_loss: 84.5044\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1697.3676 - regression_loss: 400.0192 - val_loss: 158.6438 - val_regression_loss: 62.0496\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1497.9675 - regression_loss: 339.4902 - val_loss: 165.6878 - val_regression_loss: 64.5245\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1623.8821 - regression_loss: 381.2969 - val_loss: 171.1395 - val_regression_loss: 66.8878\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1724.6646 - regression_loss: 404.6447 - val_loss: 163.8506 - val_regression_loss: 63.5016\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1716.7745 - regression_loss: 388.1152 - val_loss: 155.9595 - val_regression_loss: 60.2096\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1582.3689 - regression_loss: 357.0316 - val_loss: 153.4693 - val_regression_loss: 59.8029\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1446.2213 - regression_loss: 330.0980 - val_loss: 160.4541 - val_regression_loss: 64.1577\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1424.6926 - regression_loss: 322.6653 - val_loss: 174.0315 - val_regression_loss: 71.6124\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1458.3369 - regression_loss: 336.4829 - val_loss: 179.8121 - val_regression_loss: 74.7420\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1468.2473 - regression_loss: 340.2328 - val_loss: 174.1201 - val_regression_loss: 71.6868\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1406.4166 - regression_loss: 327.2355 - val_loss: 165.9360 - val_regression_loss: 67.1408\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1387.1523 - regression_loss: 314.4532 - val_loss: 159.8767 - val_regression_loss: 63.6222\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1374.2412 - regression_loss: 306.4183 - val_loss: 157.2225 - val_regression_loss: 61.9252\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1377.5911 - regression_loss: 303.6299 - val_loss: 157.9827 - val_regression_loss: 62.1180\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1364.8029 - regression_loss: 304.7887 - val_loss: 157.4961 - val_regression_loss: 61.9019\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1361.1638 - regression_loss: 301.8663 - val_loss: 157.3672 - val_regression_loss: 62.0305\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1323.1943 - regression_loss: 298.2526 - val_loss: 159.3744 - val_regression_loss: 63.2958\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1323.2214 - regression_loss: 296.3438 - val_loss: 162.5143 - val_regression_loss: 65.0873\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1292.2761 - regression_loss: 295.5663 - val_loss: 166.1143 - val_regression_loss: 67.0224\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1298.1307 - regression_loss: 295.9166 - val_loss: 167.4955 - val_regression_loss: 67.7292\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1293.9614 - regression_loss: 295.2968 - val_loss: 165.4536 - val_regression_loss: 66.6152\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1291.1255 - regression_loss: 292.3853 - val_loss: 162.8222 - val_regression_loss: 65.1616\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1301.7388 - regression_loss: 290.0219 - val_loss: 160.0965 - val_regression_loss: 63.6510\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1301.5779 - regression_loss: 288.0443 - val_loss: 158.6366 - val_regression_loss: 62.8153\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1279.6248 - regression_loss: 286.9336 - val_loss: 157.8833 - val_regression_loss: 62.4115\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1262.3976 - regression_loss: 286.0771 - val_loss: 157.6561 - val_regression_loss: 62.3674\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1270.7028 - regression_loss: 284.6829 - val_loss: 158.4863 - val_regression_loss: 62.8765\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1264.1304 - regression_loss: 284.3239 - val_loss: 159.8192 - val_regression_loss: 63.6103\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1273.8088 - regression_loss: 283.6169 - val_loss: 160.5922 - val_regression_loss: 64.0199\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1270.9235 - regression_loss: 283.0821 - val_loss: 160.5562 - val_regression_loss: 63.9947\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1258.6110 - regression_loss: 282.2274 - val_loss: 159.7617 - val_regression_loss: 63.5509\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1237.0524 - regression_loss: 281.2054 - val_loss: 158.7966 - val_regression_loss: 63.0404\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1241.1338 - regression_loss: 280.5526 - val_loss: 157.6644 - val_regression_loss: 62.4028\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1262.3879 - regression_loss: 279.4011 - val_loss: 157.4650 - val_regression_loss: 62.2709\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1238.7888 - regression_loss: 278.7968 - val_loss: 157.7896 - val_regression_loss: 62.4741\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1239.3065 - regression_loss: 278.0912 - val_loss: 158.3492 - val_regression_loss: 62.7851\n",
            "Epoch 45/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1315.0820 - regression_loss: 537.7797\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1256.0867 - regression_loss: 277.6236 - val_loss: 159.3601 - val_regression_loss: 63.3368\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1247.3663 - regression_loss: 277.2622 - val_loss: 159.7441 - val_regression_loss: 63.5412\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1207.5867 - regression_loss: 277.1201 - val_loss: 159.7771 - val_regression_loss: 63.5456\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1229.2836 - regression_loss: 276.6927 - val_loss: 159.3960 - val_regression_loss: 63.3287\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1228.3313 - regression_loss: 276.3740 - val_loss: 158.9517 - val_regression_loss: 63.0857\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1243.0367 - regression_loss: 275.9703 - val_loss: 158.6814 - val_regression_loss: 62.9333\n",
            "***************************** elapsed_time is:  6.310288906097412\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 224ms/step - loss: 9388.6543 - regression_loss: 2411.3486 - val_loss: 708.4813 - val_regression_loss: 333.2374\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 6220.2759 - regression_loss: 1590.5706 - val_loss: 483.0705 - val_regression_loss: 226.1703\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3973.9158 - regression_loss: 1038.0239 - val_loss: 358.6230 - val_regression_loss: 168.3373\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2861.6345 - regression_loss: 736.2772 - val_loss: 239.8362 - val_regression_loss: 105.1175\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1751.9764 - regression_loss: 420.8707 - val_loss: 229.0123 - val_regression_loss: 91.6682\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1676.4790 - regression_loss: 362.2134 - val_loss: 297.1324 - val_regression_loss: 120.9532\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2276.0735 - regression_loss: 498.1877 - val_loss: 253.0862 - val_regression_loss: 100.9477\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1891.7256 - regression_loss: 411.0303 - val_loss: 192.1669 - val_regression_loss: 75.0674\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1371.9489 - regression_loss: 297.0170 - val_loss: 177.4383 - val_regression_loss: 71.8372\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1335.7936 - regression_loss: 300.9256 - val_loss: 187.1695 - val_regression_loss: 79.3729\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1440.8967 - regression_loss: 348.9324 - val_loss: 192.0020 - val_regression_loss: 82.7577\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1527.8093 - regression_loss: 371.1649 - val_loss: 185.2193 - val_regression_loss: 79.0130\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1465.0114 - regression_loss: 352.3955 - val_loss: 174.3285 - val_regression_loss: 72.3054\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1362.7460 - regression_loss: 313.5961 - val_loss: 169.1162 - val_regression_loss: 67.7490\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1270.1095 - regression_loss: 284.0519 - val_loss: 173.7365 - val_regression_loss: 67.7310\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1287.0486 - regression_loss: 275.6054 - val_loss: 180.7586 - val_regression_loss: 69.5797\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1302.0363 - regression_loss: 279.7110 - val_loss: 181.7149 - val_regression_loss: 69.8802\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1329.5657 - regression_loss: 277.9443 - val_loss: 177.0238 - val_regression_loss: 68.6862\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1269.3789 - regression_loss: 272.8032 - val_loss: 172.8383 - val_regression_loss: 68.1205\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1255.7800 - regression_loss: 272.7601 - val_loss: 171.2942 - val_regression_loss: 68.6555\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1231.7750 - regression_loss: 278.1960 - val_loss: 171.9049 - val_regression_loss: 69.7987\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1219.9708 - regression_loss: 284.3860 - val_loss: 172.2292 - val_regression_loss: 70.1960\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1242.1306 - regression_loss: 285.3346 - val_loss: 171.2627 - val_regression_loss: 69.4009\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1252.6166 - regression_loss: 280.3799 - val_loss: 170.8765 - val_regression_loss: 68.4967\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1223.6445 - regression_loss: 273.3165 - val_loss: 172.5856 - val_regression_loss: 68.5947\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1237.0483 - regression_loss: 268.8197 - val_loss: 174.7156 - val_regression_loss: 69.1077\n",
            "Epoch 27/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1282.8951 - regression_loss: 508.3029\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1226.1204 - regression_loss: 267.2751 - val_loss: 175.6160 - val_regression_loss: 69.4294\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1237.0620 - regression_loss: 266.6660 - val_loss: 175.1678 - val_regression_loss: 69.3079\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1249.0521 - regression_loss: 266.3783 - val_loss: 174.0457 - val_regression_loss: 68.9921\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1227.6813 - regression_loss: 266.2473 - val_loss: 172.8054 - val_regression_loss: 68.6612\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1215.7800 - regression_loss: 266.4875 - val_loss: 171.9261 - val_regression_loss: 68.4807\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1224.3269 - regression_loss: 267.2948 - val_loss: 171.3712 - val_regression_loss: 68.4102\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1209.1134 - regression_loss: 268.3260 - val_loss: 171.1907 - val_regression_loss: 68.4492\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1214.1500 - regression_loss: 268.9081 - val_loss: 171.3104 - val_regression_loss: 68.5322\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1181.0609 - regression_loss: 268.8583 - val_loss: 171.3596 - val_regression_loss: 68.4563\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1188.2151 - regression_loss: 268.1334 - val_loss: 171.4879 - val_regression_loss: 68.3589\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1196.0945 - regression_loss: 266.8612 - val_loss: 171.7744 - val_regression_loss: 68.3214\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1173.7554 - regression_loss: 265.7347 - val_loss: 172.3224 - val_regression_loss: 68.4354\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1201.2346 - regression_loss: 264.8338 - val_loss: 172.7632 - val_regression_loss: 68.5643\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1196.8781 - regression_loss: 264.3642 - val_loss: 172.9902 - val_regression_loss: 68.6194\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1197.5686 - regression_loss: 263.9170 - val_loss: 172.9048 - val_regression_loss: 68.6228\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1194.8193 - regression_loss: 263.8120 - val_loss: 172.7685 - val_regression_loss: 68.6449\n",
            "Epoch 43/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1272.1033 - regression_loss: 515.3754\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1214.7256 - regression_loss: 263.7894 - val_loss: 172.6614 - val_regression_loss: 68.6668\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1178.4563 - regression_loss: 263.8518 - val_loss: 172.6250 - val_regression_loss: 68.6803\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1179.5017 - regression_loss: 263.8905 - val_loss: 172.6475 - val_regression_loss: 68.7116\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1176.0743 - regression_loss: 263.8835 - val_loss: 172.6489 - val_regression_loss: 68.7273\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1180.9778 - regression_loss: 263.8508 - val_loss: 172.6955 - val_regression_loss: 68.7446\n",
            "Epoch 48/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1239.9692 - regression_loss: 502.8799\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1185.3776 - regression_loss: 263.7847 - val_loss: 172.8133 - val_regression_loss: 68.7848\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1185.8297 - regression_loss: 263.6257 - val_loss: 172.8195 - val_regression_loss: 68.7754\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1181.2184 - regression_loss: 263.5248 - val_loss: 172.8884 - val_regression_loss: 68.7871\n",
            "***************************** elapsed_time is:  5.945566892623901\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 226ms/step - loss: 455004.2500 - regression_loss: 124630.4609 - val_loss: 43737.7969 - val_regression_loss: 21978.4160\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 414581.4375 - regression_loss: 113723.2891 - val_loss: 38563.6211 - val_regression_loss: 19393.7012\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 368824.4062 - regression_loss: 100915.1797 - val_loss: 31623.6172 - val_regression_loss: 15921.6738\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 302791.4062 - regression_loss: 83578.3984 - val_loss: 23002.9980 - val_regression_loss: 11601.0449\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 223217.8750 - regression_loss: 61882.0039 - val_loss: 13847.7588 - val_regression_loss: 6993.5024\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 141201.2344 - regression_loss: 38426.5312 - val_loss: 6809.1362 - val_regression_loss: 3387.3613\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 67375.8359 - regression_loss: 18657.4453 - val_loss: 6109.4404 - val_regression_loss: 2777.3809\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 46513.2891 - regression_loss: 12011.8896 - val_loss: 11135.7559 - val_regression_loss: 4944.4268\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 73989.1953 - regression_loss: 18167.7793 - val_loss: 10854.7988 - val_regression_loss: 4931.0361\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 70142.5547 - regression_loss: 18042.3242 - val_loss: 6934.1841 - val_regression_loss: 3215.4490\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 47937.9609 - regression_loss: 12145.8408 - val_loss: 3742.9946 - val_regression_loss: 1763.7629\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 30528.9160 - regression_loss: 7945.6802 - val_loss: 2555.5674 - val_regression_loss: 1236.5071\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 27151.1348 - regression_loss: 7402.0620 - val_loss: 2658.0046 - val_regression_loss: 1314.7905\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 32017.0762 - regression_loss: 8874.3242 - val_loss: 3008.7261 - val_regression_loss: 1498.1932\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 36946.6953 - regression_loss: 10142.4600 - val_loss: 3058.1436 - val_regression_loss: 1521.2720\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 37139.8594 - regression_loss: 10148.1943 - val_loss: 2772.1829 - val_regression_loss: 1371.6760\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 28565.7012 - regression_loss: 8926.7959 - val_loss: 2411.2600 - val_regression_loss: 1182.2930\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 27125.3242 - regression_loss: 7321.8208 - val_loss: 2294.6213 - val_regression_loss: 1114.3601\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 22375.0605 - regression_loss: 6009.2188 - val_loss: 2584.1108 - val_regression_loss: 1250.4695\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 20960.3242 - regression_loss: 5605.2100 - val_loss: 3113.0100 - val_regression_loss: 1508.4231\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 22108.3398 - regression_loss: 5886.2759 - val_loss: 3464.8325 - val_regression_loss: 1680.5977\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 23250.0684 - regression_loss: 6149.3984 - val_loss: 3393.1636 - val_regression_loss: 1643.8529\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 21894.1973 - regression_loss: 5914.5615 - val_loss: 3011.7058 - val_regression_loss: 1454.7844\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 17394.8359 - regression_loss: 5383.3672 - val_loss: 2600.6440 - val_regression_loss: 1252.9521\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 18650.3730 - regression_loss: 4967.0342 - val_loss: 2334.2021 - val_regression_loss: 1124.1611\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 18684.3828 - regression_loss: 4889.2573 - val_loss: 2160.7119 - val_regression_loss: 1041.9199\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 17485.1133 - regression_loss: 4916.1421 - val_loss: 2019.7234 - val_regression_loss: 975.2388\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 18397.7793 - regression_loss: 4866.3340 - val_loss: 1906.4164 - val_regression_loss: 921.1199\n",
            "Epoch 29/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 18713.1367 - regression_loss: 9237.4873\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 17855.2441 - regression_loss: 4686.4688 - val_loss: 1843.5314 - val_regression_loss: 890.9318\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 16806.1387 - regression_loss: 4493.2153 - val_loss: 1842.5894 - val_regression_loss: 890.4194\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 16097.9219 - regression_loss: 4413.2119 - val_loss: 1854.4846 - val_regression_loss: 895.7783\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 16337.7988 - regression_loss: 4349.7856 - val_loss: 1871.5864 - val_regression_loss: 903.2620\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 16067.2012 - regression_loss: 4283.8647 - val_loss: 1882.4636 - val_regression_loss: 907.4169\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 15407.8311 - regression_loss: 4213.9058 - val_loss: 1880.4937 - val_regression_loss: 905.0876\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 15576.7559 - regression_loss: 4140.7109 - val_loss: 1868.4685 - val_regression_loss: 897.8326\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 15407.5303 - regression_loss: 4070.2524 - val_loss: 1846.6343 - val_regression_loss: 885.8788\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 15239.7510 - regression_loss: 3999.2454 - val_loss: 1814.0070 - val_regression_loss: 868.9503\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14223.3496 - regression_loss: 3934.7256 - val_loss: 1775.7010 - val_regression_loss: 849.6263\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 14727.4902 - regression_loss: 3876.7266 - val_loss: 1735.0796 - val_regression_loss: 829.5381\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14367.4102 - regression_loss: 3821.3801 - val_loss: 1692.4895 - val_regression_loss: 808.7941\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 14003.9492 - regression_loss: 3762.3123 - val_loss: 1646.0527 - val_regression_loss: 786.3802\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11369.2520 - regression_loss: 3697.7747 - val_loss: 1600.1376 - val_regression_loss: 764.3260\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13571.2686 - regression_loss: 3637.5435 - val_loss: 1559.9928 - val_regression_loss: 745.1323\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13493.1826 - regression_loss: 3573.0742 - val_loss: 1519.1343 - val_regression_loss: 725.4790\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 13230.2500 - regression_loss: 3513.5447 - val_loss: 1480.3235 - val_regression_loss: 706.7079\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 13115.6455 - regression_loss: 3452.7766 - val_loss: 1436.5695 - val_regression_loss: 685.3077\n",
            "Epoch 47/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 13414.7236 - regression_loss: 6570.1074\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12810.7051 - regression_loss: 3390.3022 - val_loss: 1391.9705 - val_regression_loss: 663.2556\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12503.8516 - regression_loss: 3327.3274 - val_loss: 1369.0568 - val_regression_loss: 651.8141\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 12378.4199 - regression_loss: 3293.7019 - val_loss: 1345.1471 - val_regression_loss: 639.8065\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12375.0312 - regression_loss: 3259.0544 - val_loss: 1322.7670 - val_regression_loss: 628.5051\n",
            "***************************** elapsed_time is:  6.0012335777282715\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 35292.4922 - regression_loss: 9504.7295 - val_loss: 3178.4326 - val_regression_loss: 1560.2161\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 24646.0098 - regression_loss: 6561.3286 - val_loss: 1981.0200 - val_regression_loss: 964.8752\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 15192.3643 - regression_loss: 3981.8230 - val_loss: 954.3212 - val_regression_loss: 455.4288\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7121.4678 - regression_loss: 1844.6379 - val_loss: 691.7397 - val_regression_loss: 328.1721\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5513.0981 - regression_loss: 1426.8741 - val_loss: 679.2684 - val_regression_loss: 322.1521\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 5671.5327 - regression_loss: 1472.3711 - val_loss: 411.5086 - val_regression_loss: 185.7765\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3690.6443 - regression_loss: 921.8470 - val_loss: 287.8786 - val_regression_loss: 121.0567\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2590.2976 - regression_loss: 628.1415 - val_loss: 383.8722 - val_regression_loss: 167.3254\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3167.1135 - regression_loss: 761.8427 - val_loss: 465.6016 - val_regression_loss: 208.3312\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3606.3315 - regression_loss: 891.0715 - val_loss: 416.0598 - val_regression_loss: 185.0572\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3238.1201 - regression_loss: 780.4547 - val_loss: 300.3587 - val_regression_loss: 129.2592\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2355.6824 - regression_loss: 559.5777 - val_loss: 216.6528 - val_regression_loss: 89.3983\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1826.1295 - regression_loss: 420.1240 - val_loss: 204.9928 - val_regression_loss: 85.1215\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1817.2987 - regression_loss: 436.7556 - val_loss: 229.1263 - val_regression_loss: 98.1124\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2089.3704 - regression_loss: 509.0107 - val_loss: 230.7490 - val_regression_loss: 99.2228\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2105.5942 - regression_loss: 513.5064 - val_loss: 207.2791 - val_regression_loss: 87.3251\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1879.4298 - regression_loss: 448.0948 - val_loss: 192.2287 - val_regression_loss: 79.3777\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1670.9138 - regression_loss: 396.0034 - val_loss: 195.7995 - val_regression_loss: 80.6407\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1631.4589 - regression_loss: 390.1297 - val_loss: 203.2681 - val_regression_loss: 83.8331\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1709.3815 - regression_loss: 394.5798 - val_loss: 201.3413 - val_regression_loss: 82.3716\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1675.0244 - regression_loss: 386.6978 - val_loss: 194.0496 - val_regression_loss: 78.3388\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1640.9789 - regression_loss: 374.8013 - val_loss: 187.5490 - val_regression_loss: 74.9109\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1602.4152 - regression_loss: 367.5553 - val_loss: 182.7273 - val_regression_loss: 72.5796\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1587.5178 - regression_loss: 364.3777 - val_loss: 176.7557 - val_regression_loss: 69.9249\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1550.2390 - regression_loss: 357.5592 - val_loss: 172.3046 - val_regression_loss: 68.1345\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1516.3187 - regression_loss: 350.0167 - val_loss: 172.9462 - val_regression_loss: 68.8109\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1522.8423 - regression_loss: 349.0836 - val_loss: 176.1078 - val_regression_loss: 70.5732\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1507.2510 - regression_loss: 352.6366 - val_loss: 176.7151 - val_regression_loss: 70.8938\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1537.8079 - regression_loss: 350.5465 - val_loss: 173.2616 - val_regression_loss: 69.0433\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1493.1327 - regression_loss: 342.2558 - val_loss: 169.5166 - val_regression_loss: 66.9700\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1469.3236 - regression_loss: 335.2726 - val_loss: 167.6679 - val_regression_loss: 65.8315\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1470.8424 - regression_loss: 332.3633 - val_loss: 167.2739 - val_regression_loss: 65.4852\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1444.1930 - regression_loss: 331.0402 - val_loss: 167.4063 - val_regression_loss: 65.5162\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1445.3724 - regression_loss: 328.0496 - val_loss: 167.5411 - val_regression_loss: 65.6549\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1409.1693 - regression_loss: 325.0430 - val_loss: 167.3341 - val_regression_loss: 65.6816\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1424.2268 - regression_loss: 322.7983 - val_loss: 165.3082 - val_regression_loss: 64.8377\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1407.3058 - regression_loss: 320.1263 - val_loss: 162.5198 - val_regression_loss: 63.5744\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1399.6920 - regression_loss: 318.0572 - val_loss: 160.2874 - val_regression_loss: 62.5595\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1376.7961 - regression_loss: 316.6577 - val_loss: 159.5570 - val_regression_loss: 62.2452\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1393.5736 - regression_loss: 314.7292 - val_loss: 158.6518 - val_regression_loss: 61.7734\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1354.4055 - regression_loss: 312.1664 - val_loss: 159.0831 - val_regression_loss: 61.9371\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1366.9675 - regression_loss: 309.4199 - val_loss: 159.5490 - val_regression_loss: 62.1230\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1361.8165 - regression_loss: 307.1497 - val_loss: 160.1048 - val_regression_loss: 62.3710\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1358.0223 - regression_loss: 305.1150 - val_loss: 159.9539 - val_regression_loss: 62.3211\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1348.7972 - regression_loss: 303.1339 - val_loss: 158.9928 - val_regression_loss: 61.8936\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1350.8059 - regression_loss: 301.3414 - val_loss: 158.5486 - val_regression_loss: 61.7330\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1331.4962 - regression_loss: 299.7392 - val_loss: 158.3417 - val_regression_loss: 61.6910\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1303.3112 - regression_loss: 298.2240 - val_loss: 158.7473 - val_regression_loss: 61.9410\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1327.3533 - regression_loss: 296.7007 - val_loss: 159.4564 - val_regression_loss: 62.3092\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1318.6613 - regression_loss: 295.1793 - val_loss: 159.2473 - val_regression_loss: 62.2031\n",
            "***************************** elapsed_time is:  6.209998369216919\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 222ms/step - loss: 16877.4414 - regression_loss: 4451.6816 - val_loss: 1497.6183 - val_regression_loss: 727.2252\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 12112.2500 - regression_loss: 3176.1760 - val_loss: 1008.5579 - val_regression_loss: 481.4966\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7914.9561 - regression_loss: 2095.9858 - val_loss: 681.8171 - val_regression_loss: 316.9652\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5576.4277 - regression_loss: 1433.8945 - val_loss: 516.7031 - val_regression_loss: 235.3432\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4324.3408 - regression_loss: 1079.7598 - val_loss: 282.1399 - val_regression_loss: 120.7047\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2316.2175 - regression_loss: 548.0367 - val_loss: 195.7333 - val_regression_loss: 80.3750\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1578.8595 - regression_loss: 373.6449 - val_loss: 306.4697 - val_regression_loss: 137.7877\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2484.4668 - regression_loss: 629.7517 - val_loss: 330.3306 - val_regression_loss: 150.1938\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2701.3623 - regression_loss: 679.1316 - val_loss: 244.4458 - val_regression_loss: 106.7184\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2072.4961 - regression_loss: 494.9417 - val_loss: 173.4704 - val_regression_loss: 70.2765\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1504.6605 - regression_loss: 348.5924 - val_loss: 158.3253 - val_regression_loss: 61.7490\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1475.7823 - regression_loss: 332.9922 - val_loss: 171.2491 - val_regression_loss: 67.5327\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1615.0238 - regression_loss: 375.0962 - val_loss: 178.9810 - val_regression_loss: 71.1533\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1694.3689 - regression_loss: 393.6116 - val_loss: 174.3331 - val_regression_loss: 68.9851\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1644.5526 - regression_loss: 374.2063 - val_loss: 164.9427 - val_regression_loss: 64.7021\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1477.1390 - regression_loss: 339.6506 - val_loss: 158.4035 - val_regression_loss: 61.9418\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1375.8712 - regression_loss: 315.7275 - val_loss: 158.1389 - val_regression_loss: 62.3127\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1377.3572 - regression_loss: 310.8201 - val_loss: 163.3843 - val_regression_loss: 65.3174\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 1423.2494 - regression_loss: 321.8065 - val_loss: 166.0081 - val_regression_loss: 66.7829\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1454.3490 - regression_loss: 329.4435 - val_loss: 160.6902 - val_regression_loss: 64.0499\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1403.8428 - regression_loss: 321.7380 - val_loss: 151.4251 - val_regression_loss: 59.1879\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1341.0620 - regression_loss: 308.4389 - val_loss: 143.5260 - val_regression_loss: 54.9747\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1339.8737 - regression_loss: 300.3762 - val_loss: 138.3101 - val_regression_loss: 52.1569\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1328.9244 - regression_loss: 298.0045 - val_loss: 135.8244 - val_regression_loss: 50.7861\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1330.2539 - regression_loss: 297.9526 - val_loss: 135.2719 - val_regression_loss: 50.4748\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1339.7377 - regression_loss: 298.0168 - val_loss: 134.8877 - val_regression_loss: 50.3354\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 1317.4807 - regression_loss: 294.9236 - val_loss: 134.7619 - val_regression_loss: 50.3732\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1294.4525 - regression_loss: 291.1104 - val_loss: 135.4435 - val_regression_loss: 50.8295\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1295.8766 - regression_loss: 289.0298 - val_loss: 136.4911 - val_regression_loss: 51.4541\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1305.6981 - regression_loss: 288.9695 - val_loss: 137.0900 - val_regression_loss: 51.8166\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1262.2181 - regression_loss: 288.4268 - val_loss: 137.0781 - val_regression_loss: 51.8216\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1285.7013 - regression_loss: 288.1529 - val_loss: 135.1965 - val_regression_loss: 50.8300\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1284.3527 - regression_loss: 285.8126 - val_loss: 132.8222 - val_regression_loss: 49.5734\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1271.7086 - regression_loss: 283.8207 - val_loss: 130.8321 - val_regression_loss: 48.5036\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1272.8448 - regression_loss: 282.6205 - val_loss: 129.4389 - val_regression_loss: 47.7569\n",
            "Epoch 36/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1327.1101 - regression_loss: 541.1722\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1268.0996 - regression_loss: 281.7634 - val_loss: 128.2538 - val_regression_loss: 47.1495\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1242.4633 - regression_loss: 280.7232 - val_loss: 127.8804 - val_regression_loss: 46.9721\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1270.7727 - regression_loss: 280.0132 - val_loss: 127.5801 - val_regression_loss: 46.8396\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1257.8950 - regression_loss: 279.3836 - val_loss: 127.5763 - val_regression_loss: 46.8627\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1263.3024 - regression_loss: 278.8660 - val_loss: 127.6902 - val_regression_loss: 46.9392\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1271.4937 - regression_loss: 278.4261 - val_loss: 127.8698 - val_regression_loss: 47.0453\n",
            "Epoch 42/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1304.8982 - regression_loss: 533.2031\n",
            "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1247.1251 - regression_loss: 278.2438 - val_loss: 128.0496 - val_regression_loss: 47.1477\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1246.2051 - regression_loss: 277.8327 - val_loss: 127.9800 - val_regression_loss: 47.1106\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1251.3126 - regression_loss: 277.6050 - val_loss: 127.8973 - val_regression_loss: 47.0637\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1260.8347 - regression_loss: 277.3401 - val_loss: 127.7287 - val_regression_loss: 46.9693\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1204.5782 - regression_loss: 277.0295 - val_loss: 127.5437 - val_regression_loss: 46.8669\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1245.7771 - regression_loss: 276.7892 - val_loss: 127.2951 - val_regression_loss: 46.7272\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1234.3110 - regression_loss: 276.4548 - val_loss: 127.0704 - val_regression_loss: 46.6019\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1188.2709 - regression_loss: 276.1682 - val_loss: 126.8511 - val_regression_loss: 46.4809\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1238.7871 - regression_loss: 275.9119 - val_loss: 126.6150 - val_regression_loss: 46.3541\n",
            "***************************** elapsed_time is:  6.084754705429077\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 20054.6875 - regression_loss: 5398.1724 - val_loss: 1750.5415 - val_regression_loss: 854.1224\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13681.6924 - regression_loss: 3558.2273 - val_loss: 1065.5116 - val_regression_loss: 509.6104\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8402.8135 - regression_loss: 2219.9988 - val_loss: 733.5505 - val_regression_loss: 341.9490\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6512.6348 - regression_loss: 1671.1293 - val_loss: 467.8500 - val_regression_loss: 210.8232\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 4273.7271 - regression_loss: 1052.6597 - val_loss: 244.8878 - val_regression_loss: 102.5403\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2009.1692 - regression_loss: 464.2645 - val_loss: 315.2141 - val_regression_loss: 140.7323\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2150.0039 - regression_loss: 540.2921 - val_loss: 436.8481 - val_regression_loss: 203.2578\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3197.3694 - regression_loss: 805.9628 - val_loss: 359.9942 - val_regression_loss: 165.0442\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2815.1545 - regression_loss: 705.4197 - val_loss: 238.3792 - val_regression_loss: 103.6246\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2023.8391 - regression_loss: 493.5705 - val_loss: 176.3802 - val_regression_loss: 71.7221\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1688.6637 - regression_loss: 393.9259 - val_loss: 168.8039 - val_regression_loss: 67.1017\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1636.1075 - regression_loss: 379.6604 - val_loss: 190.4597 - val_regression_loss: 77.4321\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1750.1276 - regression_loss: 412.0363 - val_loss: 210.2716 - val_regression_loss: 87.1820\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1868.0834 - regression_loss: 438.4598 - val_loss: 205.0499 - val_regression_loss: 84.6838\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1799.1926 - regression_loss: 418.0599 - val_loss: 181.8319 - val_regression_loss: 73.3920\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1633.2000 - regression_loss: 368.0338 - val_loss: 164.4685 - val_regression_loss: 65.1885\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1461.6766 - regression_loss: 332.7327 - val_loss: 162.7969 - val_regression_loss: 64.8873\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1444.8782 - regression_loss: 324.5641 - val_loss: 172.3714 - val_regression_loss: 70.1141\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1454.9298 - regression_loss: 329.9947 - val_loss: 184.9185 - val_regression_loss: 76.6230\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1482.8131 - regression_loss: 339.6966 - val_loss: 187.9583 - val_regression_loss: 78.1562\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1476.7290 - regression_loss: 338.9387 - val_loss: 177.4560 - val_regression_loss: 72.7191\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1428.9652 - regression_loss: 323.1652 - val_loss: 163.8286 - val_regression_loss: 65.6063\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1357.2554 - regression_loss: 307.2629 - val_loss: 156.8256 - val_regression_loss: 61.8061\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1352.3962 - regression_loss: 302.8403 - val_loss: 156.7389 - val_regression_loss: 61.5463\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1359.4066 - regression_loss: 303.2562 - val_loss: 160.2141 - val_regression_loss: 63.1921\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1380.5212 - regression_loss: 304.5199 - val_loss: 162.0649 - val_regression_loss: 64.1224\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1328.6844 - regression_loss: 303.0280 - val_loss: 161.1316 - val_regression_loss: 63.7257\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1362.5701 - regression_loss: 299.6161 - val_loss: 158.9459 - val_regression_loss: 62.7616\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1338.8583 - regression_loss: 295.4323 - val_loss: 158.2958 - val_regression_loss: 62.5821\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1326.5142 - regression_loss: 294.4093 - val_loss: 159.7733 - val_regression_loss: 63.4445\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1307.1823 - regression_loss: 294.7279 - val_loss: 160.8075 - val_regression_loss: 64.0248\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1287.5181 - regression_loss: 295.0213 - val_loss: 159.3574 - val_regression_loss: 63.2785\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1322.0822 - regression_loss: 293.5077 - val_loss: 157.1985 - val_regression_loss: 62.1223\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1309.1575 - regression_loss: 291.1354 - val_loss: 155.4890 - val_regression_loss: 61.1626\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1299.7871 - regression_loss: 288.8812 - val_loss: 154.7524 - val_regression_loss: 60.7004\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1275.0665 - regression_loss: 287.5757 - val_loss: 154.3506 - val_regression_loss: 60.4422\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1268.3177 - regression_loss: 286.9461 - val_loss: 153.4044 - val_regression_loss: 59.9515\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1289.4526 - regression_loss: 286.0883 - val_loss: 151.9949 - val_regression_loss: 59.2694\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1241.0785 - regression_loss: 285.0000 - val_loss: 150.2479 - val_regression_loss: 58.4282\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1268.1873 - regression_loss: 284.4154 - val_loss: 150.3122 - val_regression_loss: 58.5149\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1261.1183 - regression_loss: 283.6808 - val_loss: 149.8189 - val_regression_loss: 58.2779\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1220.4937 - regression_loss: 283.1835 - val_loss: 149.5914 - val_regression_loss: 58.1629\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1262.6982 - regression_loss: 282.4353 - val_loss: 147.4174 - val_regression_loss: 57.0348\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1261.7769 - regression_loss: 281.4120 - val_loss: 146.7368 - val_regression_loss: 56.6536\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1230.3563 - regression_loss: 280.3352 - val_loss: 146.7695 - val_regression_loss: 56.6383\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1229.7618 - regression_loss: 279.6909 - val_loss: 146.3347 - val_regression_loss: 56.3864\n",
            "Epoch 47/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1297.2264 - regression_loss: 526.1515\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1240.5406 - regression_loss: 279.1310 - val_loss: 143.5941 - val_regression_loss: 54.9657\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1258.2279 - regression_loss: 278.1850 - val_loss: 143.5238 - val_regression_loss: 54.9374\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1255.0059 - regression_loss: 277.6639 - val_loss: 144.0737 - val_regression_loss: 55.2322\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1255.8914 - regression_loss: 277.3082 - val_loss: 144.7265 - val_regression_loss: 55.5786\n",
            "***************************** elapsed_time is:  5.909247398376465\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 228ms/step - loss: 11953.7002 - regression_loss: 3119.2261 - val_loss: 931.2281 - val_regression_loss: 444.4346\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7129.9771 - regression_loss: 1863.9568 - val_loss: 568.9235 - val_regression_loss: 269.3224\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3911.0066 - regression_loss: 992.0073 - val_loss: 438.8984 - val_regression_loss: 209.0168\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2662.7119 - regression_loss: 682.6592 - val_loss: 341.4437 - val_regression_loss: 156.9260\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2329.2925 - regression_loss: 566.1232 - val_loss: 255.0625 - val_regression_loss: 107.6139\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2082.1150 - regression_loss: 479.0201 - val_loss: 236.9518 - val_regression_loss: 95.2134\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2112.1606 - regression_loss: 472.4035 - val_loss: 234.0027 - val_regression_loss: 94.7731\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2005.0889 - regression_loss: 448.2797 - val_loss: 212.6631 - val_regression_loss: 87.2934\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1682.5964 - regression_loss: 378.7521 - val_loss: 200.0335 - val_regression_loss: 84.3930\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1477.1667 - regression_loss: 335.1199 - val_loss: 213.7900 - val_regression_loss: 93.8730\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1477.9000 - regression_loss: 349.0372 - val_loss: 233.0762 - val_regression_loss: 104.6217\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1585.7856 - regression_loss: 385.1636 - val_loss: 225.7831 - val_regression_loss: 100.7000\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1548.1138 - regression_loss: 371.4883 - val_loss: 199.4749 - val_regression_loss: 86.4426\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1420.8888 - regression_loss: 324.4708 - val_loss: 181.4223 - val_regression_loss: 76.1261\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1328.8580 - regression_loss: 300.0637 - val_loss: 177.1175 - val_regression_loss: 72.8353\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1382.0339 - regression_loss: 302.9696 - val_loss: 175.9895 - val_regression_loss: 71.4349\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1386.4371 - regression_loss: 308.0994 - val_loss: 173.3485 - val_regression_loss: 69.7402\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1385.1317 - regression_loss: 303.6356 - val_loss: 170.7788 - val_regression_loss: 68.6385\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1340.3748 - regression_loss: 294.1430 - val_loss: 171.5763 - val_regression_loss: 69.7803\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1307.8989 - regression_loss: 288.7629 - val_loss: 175.4278 - val_regression_loss: 72.7486\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1273.7626 - regression_loss: 290.0659 - val_loss: 179.9494 - val_regression_loss: 75.9044\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1304.5559 - regression_loss: 293.6672 - val_loss: 182.1654 - val_regression_loss: 77.4510\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1313.2202 - regression_loss: 296.0980 - val_loss: 180.8098 - val_regression_loss: 76.7339\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1300.6766 - regression_loss: 294.0931 - val_loss: 175.8977 - val_regression_loss: 73.8404\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1270.1597 - regression_loss: 287.0868 - val_loss: 169.8732 - val_regression_loss: 70.2329\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1255.4684 - regression_loss: 280.5312 - val_loss: 165.2290 - val_regression_loss: 67.3188\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1249.8302 - regression_loss: 276.4223 - val_loss: 163.3098 - val_regression_loss: 66.0544\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1256.6571 - regression_loss: 275.3724 - val_loss: 163.1856 - val_regression_loss: 66.0474\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1248.0999 - regression_loss: 274.3111 - val_loss: 164.5062 - val_regression_loss: 67.0232\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1245.1099 - regression_loss: 273.4785 - val_loss: 167.0974 - val_regression_loss: 68.7410\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1229.7947 - regression_loss: 273.9858 - val_loss: 169.6093 - val_regression_loss: 70.3561\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1239.2836 - regression_loss: 274.8451 - val_loss: 170.3407 - val_regression_loss: 70.8537\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1231.4502 - regression_loss: 274.6121 - val_loss: 169.3728 - val_regression_loss: 70.3284\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1236.1514 - regression_loss: 273.1906 - val_loss: 166.9883 - val_regression_loss: 68.9016\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1217.1086 - regression_loss: 270.3873 - val_loss: 164.8937 - val_regression_loss: 67.6288\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1220.8445 - regression_loss: 268.5089 - val_loss: 163.6772 - val_regression_loss: 66.8623\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1198.9192 - regression_loss: 267.4219 - val_loss: 163.0667 - val_regression_loss: 66.5323\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1189.3235 - regression_loss: 266.8226 - val_loss: 163.2814 - val_regression_loss: 66.7516\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1210.7992 - regression_loss: 266.2782 - val_loss: 163.8730 - val_regression_loss: 67.2130\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1209.8783 - regression_loss: 266.4416 - val_loss: 164.3560 - val_regression_loss: 67.5630\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1212.8545 - regression_loss: 266.6938 - val_loss: 164.1425 - val_regression_loss: 67.4288\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1171.3313 - regression_loss: 266.2658 - val_loss: 163.2881 - val_regression_loss: 66.8378\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1185.9658 - regression_loss: 265.0587 - val_loss: 162.0396 - val_regression_loss: 65.9986\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1201.4044 - regression_loss: 263.8092 - val_loss: 161.3207 - val_regression_loss: 65.4927\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1204.6323 - regression_loss: 263.0175 - val_loss: 161.0820 - val_regression_loss: 65.3049\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1169.2367 - regression_loss: 262.6084 - val_loss: 161.0901 - val_regression_loss: 65.2870\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1185.8315 - regression_loss: 262.3187 - val_loss: 161.5116 - val_regression_loss: 65.5547\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1182.3862 - regression_loss: 262.2994 - val_loss: 162.0998 - val_regression_loss: 65.8749\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1191.2695 - regression_loss: 262.4287 - val_loss: 161.9704 - val_regression_loss: 65.7181\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1193.0834 - regression_loss: 261.9671 - val_loss: 161.6355 - val_regression_loss: 65.4416\n",
            "***************************** elapsed_time is:  5.825527667999268\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 214ms/step - loss: 2153969.5000 - regression_loss: 585004.5000 - val_loss: 269612.3438 - val_regression_loss: 134526.5000\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2093283.8750 - regression_loss: 565536.6250 - val_loss: 257816.1406 - val_regression_loss: 128675.6562\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2000719.3750 - regression_loss: 540855.4375 - val_loss: 240496.8906 - val_regression_loss: 120071.7656\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1789702.3750 - regression_loss: 504249.1250 - val_loss: 216117.8906 - val_regression_loss: 107948.6719\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1668728.1250 - regression_loss: 453861.8125 - val_loss: 184308.8906 - val_regression_loss: 92125.1953\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1424043.7500 - regression_loss: 388594.4688 - val_loss: 146038.7031 - val_regression_loss: 73098.0156\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1117017.2500 - regression_loss: 310980.7188 - val_loss: 103932.0156 - val_regression_loss: 52197.7344\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 827550.3750 - regression_loss: 227422.1094 - val_loss: 63482.5078 - val_regression_loss: 32122.5742\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 540176.3125 - regression_loss: 150185.8438 - val_loss: 35843.1875 - val_regression_loss: 18178.6445\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 344439.0625 - regression_loss: 101741.2422 - val_loss: 36437.7070 - val_regression_loss: 17866.0371\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 416479.0312 - regression_loss: 112969.1953 - val_loss: 49994.9023 - val_regression_loss: 24079.2891\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 546047.8750 - regression_loss: 142880.6719 - val_loss: 44828.6953 - val_regression_loss: 21466.4727\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 487143.3750 - regression_loss: 127203.1328 - val_loss: 33202.2969 - val_regression_loss: 15939.5166\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 376466.3125 - regression_loss: 99254.2891 - val_loss: 28476.6699 - val_regression_loss: 13918.3926\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 325310.0625 - regression_loss: 86108.9766 - val_loss: 30227.2676 - val_regression_loss: 15066.9814\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 322744.8438 - regression_loss: 88201.6719 - val_loss: 33208.2461 - val_regression_loss: 16735.5156\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 341106.2812 - regression_loss: 94020.2656 - val_loss: 34009.2773 - val_regression_loss: 17233.2051\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 357839.0938 - regression_loss: 95851.8203 - val_loss: 31914.0527 - val_regression_loss: 16219.5371\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 336967.9688 - regression_loss: 92149.4141 - val_loss: 27958.1152 - val_regression_loss: 14223.5127\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 313265.8750 - regression_loss: 84879.1875 - val_loss: 23945.9453 - val_regression_loss: 12155.2949\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 285488.0312 - regression_loss: 77262.1562 - val_loss: 21639.6387 - val_regression_loss: 10906.7676\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 270284.2500 - regression_loss: 72774.2188 - val_loss: 21518.9785 - val_regression_loss: 10740.6211\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 264490.9062 - regression_loss: 71773.6328 - val_loss: 22198.8301 - val_regression_loss: 10993.0010\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 267089.9062 - regression_loss: 71651.3672 - val_loss: 21826.2676 - val_regression_loss: 10762.3115\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 254331.5000 - regression_loss: 69150.3359 - val_loss: 20430.1699 - val_regression_loss: 10067.5498\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 245223.3125 - regression_loss: 65009.2070 - val_loss: 19233.7227 - val_regression_loss: 9500.5430\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 230009.0000 - regression_loss: 61581.5547 - val_loss: 18955.3301 - val_regression_loss: 9400.0293\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 225323.7188 - regression_loss: 59901.5508 - val_loss: 19258.0430 - val_regression_loss: 9582.5977\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 211270.8125 - regression_loss: 58980.6875 - val_loss: 19368.6074 - val_regression_loss: 9657.2520\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 211306.9375 - regression_loss: 57406.0547 - val_loss: 18881.5137 - val_regression_loss: 9420.0781\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 204762.8750 - regression_loss: 54906.9844 - val_loss: 18318.3945 - val_regression_loss: 9133.4883\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 189839.2344 - regression_loss: 51688.4180 - val_loss: 17998.7910 - val_regression_loss: 8969.1191\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 182685.5469 - regression_loss: 49572.9336 - val_loss: 17839.3965 - val_regression_loss: 8888.0508\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 177240.8125 - regression_loss: 47685.9961 - val_loss: 17178.6230 - val_regression_loss: 8566.0703\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 170296.1562 - regression_loss: 45574.2539 - val_loss: 16051.5088 - val_regression_loss: 8016.0557\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 161530.0938 - regression_loss: 43078.3320 - val_loss: 14807.6055 - val_regression_loss: 7406.5508\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 151526.1406 - regression_loss: 40669.1406 - val_loss: 13806.8535 - val_regression_loss: 6911.7422\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 141599.6406 - regression_loss: 38591.0625 - val_loss: 13058.9775 - val_regression_loss: 6534.9268\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 135547.7969 - regression_loss: 36540.2109 - val_loss: 12485.4482 - val_regression_loss: 6238.2158\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 127845.3359 - regression_loss: 34200.1484 - val_loss: 12175.3477 - val_regression_loss: 6069.1396\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 106314.5156 - regression_loss: 31760.1797 - val_loss: 11890.3369 - val_regression_loss: 5915.5186\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 107899.9141 - regression_loss: 29763.2402 - val_loss: 11417.2812 - val_regression_loss: 5673.4741\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 103195.0781 - regression_loss: 27892.4141 - val_loss: 10619.2900 - val_regression_loss: 5275.4194\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 96602.3750 - regression_loss: 26007.1855 - val_loss: 9558.0977 - val_regression_loss: 4748.7236\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 90527.9922 - regression_loss: 24274.0098 - val_loss: 8546.8145 - val_regression_loss: 4245.5410\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 85006.3984 - regression_loss: 22778.5020 - val_loss: 7814.3940 - val_regression_loss: 3878.0820\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 79460.8750 - regression_loss: 21398.1465 - val_loss: 7413.1982 - val_regression_loss: 3674.6545\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 75687.1797 - regression_loss: 20134.9746 - val_loss: 7354.4141 - val_regression_loss: 3640.3848\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 71924.5859 - regression_loss: 18996.8730 - val_loss: 7175.4927 - val_regression_loss: 3545.0181\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 68148.9375 - regression_loss: 18074.7422 - val_loss: 6752.0220 - val_regression_loss: 3326.4700\n",
            "***************************** elapsed_time is:  5.978200197219849\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 539152.8125 - regression_loss: 148863.6406 - val_loss: 57934.3477 - val_regression_loss: 29054.5664\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 499076.4062 - regression_loss: 139210.5469 - val_loss: 52938.8008 - val_regression_loss: 26550.1738\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 469785.9062 - regression_loss: 127455.6875 - val_loss: 46034.3203 - val_regression_loss: 23090.2852\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 411413.5000 - regression_loss: 111283.1406 - val_loss: 37363.3398 - val_regression_loss: 18746.2422\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 333865.2812 - regression_loss: 90997.2891 - val_loss: 27667.4453 - val_regression_loss: 13888.2471\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 249765.2656 - regression_loss: 68489.3828 - val_loss: 18219.8184 - val_regression_loss: 9149.5684\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 172631.0312 - regression_loss: 46489.4648 - val_loss: 10774.3896 - val_regression_loss: 5398.5933\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 101647.6719 - regression_loss: 28741.3203 - val_loss: 7618.3472 - val_regression_loss: 3758.8987\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 74806.7734 - regression_loss: 21023.8906 - val_loss: 10211.9150 - val_regression_loss: 4941.0146\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 95758.0391 - regression_loss: 25717.4316 - val_loss: 12649.6543 - val_regression_loss: 6122.5723\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 114375.9375 - regression_loss: 30516.6641 - val_loss: 10440.0713 - val_regression_loss: 5080.2876\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 96634.1562 - regression_loss: 26094.1367 - val_loss: 7091.6060 - val_regression_loss: 3471.8799\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 73992.9609 - regression_loss: 19559.4082 - val_loss: 5340.1455 - val_regression_loss: 2637.1316\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 61787.8047 - regression_loss: 16566.5254 - val_loss: 5331.7344 - val_regression_loss: 2655.0142\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 63722.6953 - regression_loss: 17173.6543 - val_loss: 5996.0010 - val_regression_loss: 2996.7251\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 70862.6875 - regression_loss: 18850.0117 - val_loss: 6157.3018 - val_regression_loss: 3077.6475\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 68841.5547 - regression_loss: 19048.9727 - val_loss: 5568.9106 - val_regression_loss: 2777.4734\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 64343.4805 - regression_loss: 17464.5547 - val_loss: 4725.6157 - val_regression_loss: 2346.4846\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 54686.4141 - regression_loss: 15391.7275 - val_loss: 4204.1191 - val_regression_loss: 2076.1016\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 50767.0430 - regression_loss: 14022.6416 - val_loss: 4169.4639 - val_regression_loss: 2051.7224\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 51835.5508 - regression_loss: 13845.4932 - val_loss: 4306.5015 - val_regression_loss: 2117.2524\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 52189.2188 - regression_loss: 14016.2363 - val_loss: 4209.9692 - val_regression_loss: 2070.7288\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 51594.8164 - regression_loss: 13670.3018 - val_loss: 3811.8552 - val_regression_loss: 1877.4265\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 47844.5703 - regression_loss: 12661.7188 - val_loss: 3428.9570 - val_regression_loss: 1693.5131\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 43366.5625 - regression_loss: 11640.3682 - val_loss: 3272.2981 - val_regression_loss: 1622.2573\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 38046.2852 - regression_loss: 11128.6055 - val_loss: 3254.3181 - val_regression_loss: 1618.2594\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 40249.4180 - regression_loss: 10906.3389 - val_loss: 3151.9568 - val_regression_loss: 1568.7456\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 39165.6680 - regression_loss: 10507.3193 - val_loss: 2888.8225 - val_regression_loss: 1435.5056\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 36893.8789 - regression_loss: 9797.7148 - val_loss: 2589.6650 - val_regression_loss: 1281.8804\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 34074.0586 - regression_loss: 9036.1211 - val_loss: 2398.8740 - val_regression_loss: 1181.6937\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 31064.4941 - regression_loss: 8455.2305 - val_loss: 2291.7017 - val_regression_loss: 1124.7074\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 30417.2227 - regression_loss: 8053.7480 - val_loss: 2151.5903 - val_regression_loss: 1053.8035\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 28077.3652 - regression_loss: 7522.1621 - val_loss: 1963.0802 - val_regression_loss: 961.3605\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 25933.1895 - regression_loss: 6906.2739 - val_loss: 1813.0028 - val_regression_loss: 889.2924\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 23602.0371 - regression_loss: 6410.9419 - val_loss: 1712.1366 - val_regression_loss: 841.0981\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 22645.3594 - regression_loss: 6039.0947 - val_loss: 1596.3785 - val_regression_loss: 783.3793\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 21114.7109 - regression_loss: 5569.4639 - val_loss: 1477.8956 - val_regression_loss: 722.3806\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 19169.3262 - regression_loss: 5039.5117 - val_loss: 1416.1212 - val_regression_loss: 689.0818\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 16956.7578 - regression_loss: 4576.8101 - val_loss: 1368.7189 - val_regression_loss: 664.3923\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 15518.9482 - regression_loss: 4187.2456 - val_loss: 1254.8784 - val_regression_loss: 608.8032\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14459.2197 - regression_loss: 3772.2676 - val_loss: 1144.6565 - val_regression_loss: 555.7418\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11386.4531 - regression_loss: 3410.0171 - val_loss: 1070.6655 - val_regression_loss: 520.1482\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 11913.0371 - regression_loss: 3170.0989 - val_loss: 1040.6589 - val_regression_loss: 504.2237\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10842.8486 - regression_loss: 2876.0547 - val_loss: 1060.3062 - val_regression_loss: 512.0172\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9661.7178 - regression_loss: 2636.2825 - val_loss: 1053.3530 - val_regression_loss: 507.9886\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8784.2822 - regression_loss: 2460.9470 - val_loss: 969.4861 - val_regression_loss: 467.3183\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8075.8652 - regression_loss: 2278.1370 - val_loss: 894.0533 - val_regression_loss: 430.6604\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8184.6377 - regression_loss: 2134.7087 - val_loss: 880.1846 - val_regression_loss: 422.7838\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7490.1172 - regression_loss: 1963.1213 - val_loss: 884.6182 - val_regression_loss: 423.7639\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 6525.7534 - regression_loss: 1816.2625 - val_loss: 821.7018 - val_regression_loss: 392.3508\n",
            "***************************** elapsed_time is:  5.869182348251343\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 261213.8281 - regression_loss: 70879.8438 - val_loss: 26401.9824 - val_regression_loss: 13207.0225\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 227715.9375 - regression_loss: 62553.8672 - val_loss: 22674.7793 - val_regression_loss: 11338.7402\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 198567.3281 - regression_loss: 53802.1641 - val_loss: 17927.4941 - val_regression_loss: 8958.7695\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 156667.5312 - regression_loss: 42651.2109 - val_loss: 12420.7246 - val_regression_loss: 6197.5332\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 110305.7109 - regression_loss: 29735.5156 - val_loss: 7030.0903 - val_regression_loss: 3493.7087\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 62899.0703 - regression_loss: 16984.0078 - val_loss: 3140.3252 - val_regression_loss: 1541.3961\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 28650.2812 - regression_loss: 7747.5396 - val_loss: 2336.4709 - val_regression_loss: 1135.8827\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 19627.6172 - regression_loss: 5412.1709 - val_loss: 4135.8364 - val_regression_loss: 2039.1273\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 33352.6172 - regression_loss: 9059.6406 - val_loss: 4735.2114 - val_regression_loss: 2346.1091\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 38590.9727 - regression_loss: 10444.6943 - val_loss: 3379.9915 - val_regression_loss: 1672.7581\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 28599.2891 - regression_loss: 7704.8633 - val_loss: 1857.0776 - val_regression_loss: 912.7983\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 17457.3398 - regression_loss: 4744.4189 - val_loss: 1231.4520 - val_regression_loss: 600.3191\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13408.7227 - regression_loss: 3684.7581 - val_loss: 1346.0637 - val_regression_loss: 657.4974\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 15415.8418 - regression_loss: 4185.6030 - val_loss: 1639.2081 - val_regression_loss: 803.4957\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 17849.8965 - regression_loss: 4936.0742 - val_loss: 1752.3884 - val_regression_loss: 859.0857\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 19101.8984 - regression_loss: 5157.7881 - val_loss: 1628.0520 - val_regression_loss: 795.6514\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 17651.3242 - regression_loss: 4751.9902 - val_loss: 1392.8673 - val_regression_loss: 676.8307\n",
            "Epoch 18/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 16190.2510 - regression_loss: 7963.2837\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 15452.1074 - regression_loss: 4061.1558 - val_loss: 1205.4733 - val_regression_loss: 582.2251\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12833.3516 - regression_loss: 3447.3877 - val_loss: 1165.2190 - val_regression_loss: 561.8681\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 12555.0049 - regression_loss: 3270.5281 - val_loss: 1163.0001 - val_regression_loss: 560.6870\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 11999.6953 - regression_loss: 3176.8550 - val_loss: 1188.6648 - val_regression_loss: 573.5709\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 11852.2305 - regression_loss: 3155.4670 - val_loss: 1222.3009 - val_regression_loss: 590.4986\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 11786.9609 - regression_loss: 3174.0710 - val_loss: 1245.0884 - val_regression_loss: 601.9814\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 11969.7559 - regression_loss: 3183.2517 - val_loss: 1244.8629 - val_regression_loss: 601.9045\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 11807.6074 - regression_loss: 3155.8840 - val_loss: 1220.1212 - val_regression_loss: 589.5104\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 11471.0752 - regression_loss: 3088.6277 - val_loss: 1180.2734 - val_regression_loss: 569.5205\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10793.8213 - regression_loss: 3001.8987 - val_loss: 1135.9025 - val_regression_loss: 547.2390\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10936.6270 - regression_loss: 2910.9822 - val_loss: 1098.1407 - val_regression_loss: 528.2769\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 10730.1729 - regression_loss: 2837.8423 - val_loss: 1070.5265 - val_regression_loss: 514.4043\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10649.4102 - regression_loss: 2785.1641 - val_loss: 1051.5404 - val_regression_loss: 504.8855\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10473.5020 - regression_loss: 2745.0317 - val_loss: 1037.5889 - val_regression_loss: 497.9132\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10325.6260 - regression_loss: 2708.6377 - val_loss: 1024.8967 - val_regression_loss: 491.5863\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10075.7646 - regression_loss: 2664.8789 - val_loss: 1011.9860 - val_regression_loss: 485.1555\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9883.2041 - regression_loss: 2611.9055 - val_loss: 999.4658 - val_regression_loss: 478.8796\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9604.8350 - regression_loss: 2550.6370 - val_loss: 988.5497 - val_regression_loss: 473.3769\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9132.7168 - regression_loss: 2485.0034 - val_loss: 980.1225 - val_regression_loss: 469.0924\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9034.4043 - regression_loss: 2424.4575 - val_loss: 973.8312 - val_regression_loss: 465.8355\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 8804.6133 - regression_loss: 2364.4807 - val_loss: 967.9504 - val_regression_loss: 462.7671\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 8614.3760 - regression_loss: 2307.2896 - val_loss: 960.6984 - val_regression_loss: 459.0010\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8343.6523 - regression_loss: 2250.8628 - val_loss: 950.3543 - val_regression_loss: 453.6852\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 7992.3071 - regression_loss: 2190.4404 - val_loss: 937.3542 - val_regression_loss: 447.0391\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 7868.5830 - regression_loss: 2128.6443 - val_loss: 922.5946 - val_regression_loss: 439.5349\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 7897.9385 - regression_loss: 2064.6150 - val_loss: 907.7535 - val_regression_loss: 432.0135\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7368.9751 - regression_loss: 2001.1409 - val_loss: 892.3731 - val_regression_loss: 424.2814\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 7460.4043 - regression_loss: 1940.0513 - val_loss: 878.8549 - val_regression_loss: 417.4948\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 7069.6021 - regression_loss: 1879.6606 - val_loss: 867.5099 - val_regression_loss: 411.8110\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6676.5249 - regression_loss: 1818.6271 - val_loss: 856.5736 - val_regression_loss: 406.3394\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 6731.4629 - regression_loss: 1760.1929 - val_loss: 848.0021 - val_regression_loss: 402.0291\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6113.0669 - regression_loss: 1698.1703 - val_loss: 840.7709 - val_regression_loss: 398.4009\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 6227.4932 - regression_loss: 1640.1139 - val_loss: 835.0665 - val_regression_loss: 395.5192\n",
            "***************************** elapsed_time is:  5.8889336585998535\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 216ms/step - loss: 586210.8125 - regression_loss: 159212.0625 - val_loss: 50136.9648 - val_regression_loss: 25076.0156\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 539632.0625 - regression_loss: 147001.3125 - val_loss: 44862.4844 - val_regression_loss: 22437.2500\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 500252.7188 - regression_loss: 133703.3906 - val_loss: 38177.5820 - val_regression_loss: 19091.9707\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 401194.1250 - regression_loss: 116249.5859 - val_loss: 30276.8340 - val_regression_loss: 15137.0977\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 345503.9375 - regression_loss: 96131.9453 - val_loss: 22029.2891 - val_regression_loss: 11006.7314\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 269137.0625 - regression_loss: 74742.3906 - val_loss: 14850.1523 - val_regression_loss: 7407.7598\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 205498.2188 - regression_loss: 55159.5781 - val_loss: 10528.6982 - val_regression_loss: 5233.0386\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 150666.7812 - regression_loss: 41290.0508 - val_loss: 11103.9463 - val_regression_loss: 5501.3130\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 137282.1094 - regression_loss: 37630.7070 - val_loss: 15494.3926 - val_regression_loss: 7685.4238\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 160189.9062 - regression_loss: 43503.4414 - val_loss: 16549.5488 - val_regression_loss: 8219.2285\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 166903.0938 - regression_loss: 44471.8242 - val_loss: 13192.5273 - val_regression_loss: 6553.2441\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 141299.2812 - regression_loss: 38166.8867 - val_loss: 9437.0762 - val_regression_loss: 4686.4092\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 113959.0391 - regression_loss: 32278.7480 - val_loss: 7367.8516 - val_regression_loss: 3659.2390\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 108059.4219 - regression_loss: 30427.2852 - val_loss: 6890.3564 - val_regression_loss: 3424.8538\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 112524.9141 - regression_loss: 31363.7383 - val_loss: 6986.9761 - val_regression_loss: 3475.0203\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 119090.5781 - regression_loss: 32062.3340 - val_loss: 6738.3623 - val_regression_loss: 3350.3242\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 115448.3438 - regression_loss: 30898.0391 - val_loss: 6182.4053 - val_regression_loss: 3070.7195\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 107027.9219 - regression_loss: 28416.1777 - val_loss: 5790.2412 - val_regression_loss: 2872.8391\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 96543.6406 - regression_loss: 25853.8340 - val_loss: 5880.6382 - val_regression_loss: 2917.0679\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 89079.0312 - regression_loss: 24228.5527 - val_loss: 6247.1006 - val_regression_loss: 3100.7742\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 84171.9766 - regression_loss: 23443.2266 - val_loss: 6295.6714 - val_regression_loss: 3127.1228\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 82978.0234 - regression_loss: 22574.8594 - val_loss: 5725.3394 - val_regression_loss: 2845.1228\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 76764.9219 - regression_loss: 20882.9102 - val_loss: 4810.9238 - val_regression_loss: 2391.6367\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 69876.8828 - regression_loss: 18842.6660 - val_loss: 4067.8955 - val_regression_loss: 2023.9924\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 65353.8164 - regression_loss: 17419.2070 - val_loss: 3653.0420 - val_regression_loss: 1820.0228\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 56450.1914 - regression_loss: 16499.7695 - val_loss: 3352.1262 - val_regression_loss: 1672.0131\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 58330.6797 - regression_loss: 15544.4033 - val_loss: 3053.0691 - val_regression_loss: 1523.6051\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 52588.3477 - regression_loss: 14098.5410 - val_loss: 2886.1260 - val_regression_loss: 1439.6429\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 47495.4961 - regression_loss: 12775.3203 - val_loss: 2831.3677 - val_regression_loss: 1409.8898\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 44905.7891 - regression_loss: 11958.4414 - val_loss: 2626.0784 - val_regression_loss: 1303.1986\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 41729.0117 - regression_loss: 11134.5391 - val_loss: 2192.2393 - val_regression_loss: 1081.2960\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 37834.0938 - regression_loss: 10114.5078 - val_loss: 1811.4672 - val_regression_loss: 885.5276\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 34511.2109 - regression_loss: 9305.2607 - val_loss: 1596.8353 - val_regression_loss: 771.9451\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 32816.4453 - regression_loss: 8698.5293 - val_loss: 1481.2610 - val_regression_loss: 706.6609\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 25267.3691 - regression_loss: 7907.1543 - val_loss: 1450.6141 - val_regression_loss: 683.5831\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 27346.6113 - regression_loss: 7294.9780 - val_loss: 1515.3571 - val_regression_loss: 709.3879\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 24723.3711 - regression_loss: 6755.3535 - val_loss: 1499.9027 - val_regression_loss: 698.4805\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 23431.2480 - regression_loss: 6221.1533 - val_loss: 1365.3099 - val_regression_loss: 631.7906\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 21111.1797 - regression_loss: 5646.5488 - val_loss: 1256.7545 - val_regression_loss: 580.0150\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 18298.9297 - regression_loss: 5217.1650 - val_loss: 1222.7003 - val_regression_loss: 565.5756\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 18236.0820 - regression_loss: 4838.1572 - val_loss: 1313.1750 - val_regression_loss: 612.2903\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 16543.2266 - regression_loss: 4385.1230 - val_loss: 1418.8097 - val_regression_loss: 667.6414\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 15414.7100 - regression_loss: 4066.7747 - val_loss: 1353.8917 - val_regression_loss: 639.5372\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 13138.8145 - regression_loss: 3726.7061 - val_loss: 1186.8052 - val_regression_loss: 560.7955\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12912.6709 - regression_loss: 3445.2236 - val_loss: 1136.7417 - val_regression_loss: 538.8011\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 11972.6992 - regression_loss: 3211.6873 - val_loss: 1187.1761 - val_regression_loss: 565.5038\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 11287.1094 - regression_loss: 2973.8391 - val_loss: 1249.8816 - val_regression_loss: 597.9986\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 10445.6689 - regression_loss: 2805.0439 - val_loss: 1167.6760 - val_regression_loss: 558.5599\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9768.5127 - regression_loss: 2626.2710 - val_loss: 1073.3245 - val_regression_loss: 512.8906\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8957.2617 - regression_loss: 2514.6072 - val_loss: 1049.1669 - val_regression_loss: 501.6508\n",
            "***************************** elapsed_time is:  6.34087061882019\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 208ms/step - loss: 1233000.3750 - regression_loss: 341249.3438 - val_loss: 84021.7969 - val_regression_loss: 41990.0000\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1173857.6250 - regression_loss: 325315.5938 - val_loss: 78315.6797 - val_regression_loss: 39139.2852\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1158424.3750 - regression_loss: 309282.3125 - val_loss: 70710.7812 - val_regression_loss: 35338.6641\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1001638.7500 - regression_loss: 287417.0000 - val_loss: 60832.2734 - val_regression_loss: 30400.5977\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 941626.1250 - regression_loss: 259543.6562 - val_loss: 49081.5430 - val_regression_loss: 24525.6211\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 803322.1875 - regression_loss: 225732.9688 - val_loss: 36458.9141 - val_regression_loss: 18213.7422\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 708926.0625 - regression_loss: 188858.9844 - val_loss: 24599.4980 - val_regression_loss: 12282.3193\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 553816.3750 - regression_loss: 150402.2812 - val_loss: 15994.3428 - val_regression_loss: 7976.6338\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 438236.2188 - regression_loss: 117409.7188 - val_loss: 14588.4492 - val_regression_loss: 7270.0718\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 354084.0938 - regression_loss: 96703.7500 - val_loss: 23978.6602 - val_regression_loss: 11962.7500\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 339266.5000 - regression_loss: 97407.6406 - val_loss: 34500.0977 - val_regression_loss: 17223.6914\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 397418.5000 - regression_loss: 107837.3984 - val_loss: 32668.6133 - val_regression_loss: 16309.9980\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 373129.7812 - regression_loss: 101106.9297 - val_loss: 23697.6816 - val_regression_loss: 11826.2598\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 316502.7812 - regression_loss: 85905.6719 - val_loss: 15573.6045 - val_regression_loss: 7764.6396\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 275462.2812 - regression_loss: 74968.9219 - val_loss: 11289.5938 - val_regression_loss: 5621.8579\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 260949.1562 - regression_loss: 71653.4219 - val_loss: 9954.3350 - val_regression_loss: 4952.9946\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 263079.7188 - regression_loss: 72321.7656 - val_loss: 10111.4316 - val_regression_loss: 5030.5635\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 264346.9688 - regression_loss: 72514.1562 - val_loss: 10433.6885 - val_regression_loss: 5191.0986\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 259182.9219 - regression_loss: 69547.5938 - val_loss: 10509.7871 - val_regression_loss: 5228.9180\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 235784.0156 - regression_loss: 63704.9141 - val_loss: 11166.6572 - val_regression_loss: 5557.7095\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 211327.3594 - regression_loss: 58554.4844 - val_loss: 12691.9355 - val_regression_loss: 6321.6514\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 206472.2188 - regression_loss: 56248.5742 - val_loss: 13824.5088 - val_regression_loss: 6890.0845\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 200346.9531 - regression_loss: 54885.0938 - val_loss: 12759.1855 - val_regression_loss: 6360.0122\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 186753.8125 - regression_loss: 51783.6523 - val_loss: 10063.3877 - val_regression_loss: 5014.8408\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 176554.7969 - regression_loss: 47288.0352 - val_loss: 7562.7793 - val_regression_loss: 3767.0867\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 160420.4062 - regression_loss: 43823.0391 - val_loss: 6082.8389 - val_regression_loss: 3029.2202\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 157392.0469 - regression_loss: 41864.9453 - val_loss: 5455.0562 - val_regression_loss: 2716.6826\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 148975.6250 - regression_loss: 39800.4531 - val_loss: 5335.5425 - val_regression_loss: 2657.2285\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 138617.5312 - regression_loss: 36907.5000 - val_loss: 5810.2495 - val_regression_loss: 2893.4509\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 127540.5469 - regression_loss: 33764.6992 - val_loss: 6729.2500 - val_regression_loss: 3350.2263\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 112882.5000 - regression_loss: 31530.5469 - val_loss: 7152.5503 - val_regression_loss: 3558.1804\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 109541.3984 - regression_loss: 29937.0078 - val_loss: 6731.9541 - val_regression_loss: 3344.1733\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 103721.6250 - regression_loss: 27766.7422 - val_loss: 5790.4214 - val_regression_loss: 2870.5491\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 95316.2812 - regression_loss: 25665.0820 - val_loss: 5139.0044 - val_regression_loss: 2542.3140\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 90127.1875 - regression_loss: 24152.4141 - val_loss: 4897.7666 - val_regression_loss: 2418.6125\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 77780.9219 - regression_loss: 22406.4805 - val_loss: 4982.0581 - val_regression_loss: 2457.1821\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 78103.2969 - regression_loss: 20693.9375 - val_loss: 5361.8481 - val_regression_loss: 2643.9092\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 72290.5156 - regression_loss: 19258.4453 - val_loss: 5253.3745 - val_regression_loss: 2589.0806\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 66393.4609 - regression_loss: 17921.0332 - val_loss: 4604.2944 - val_regression_loss: 2266.4534\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 60523.7266 - regression_loss: 16466.0508 - val_loss: 3939.0823 - val_regression_loss: 1936.8772\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 53388.5586 - regression_loss: 15273.8750 - val_loss: 3585.7935 - val_regression_loss: 1763.0394\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 50587.7539 - regression_loss: 14250.7344 - val_loss: 3823.7786 - val_regression_loss: 1883.4426\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 49165.8047 - regression_loss: 13004.8564 - val_loss: 3938.3113 - val_regression_loss: 1942.8376\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 44214.2031 - regression_loss: 12035.7627 - val_loss: 3549.6018 - val_regression_loss: 1751.0723\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 39893.2227 - regression_loss: 11053.2012 - val_loss: 3083.2834 - val_regression_loss: 1519.9080\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 38163.4531 - regression_loss: 10275.2627 - val_loss: 2922.7632 - val_regression_loss: 1440.5875\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 36042.7227 - regression_loss: 9505.6758 - val_loss: 2942.0388 - val_regression_loss: 1450.5675\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 33124.6445 - regression_loss: 8811.5078 - val_loss: 2932.3015 - val_regression_loss: 1445.9324\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 29454.4570 - regression_loss: 8226.7617 - val_loss: 2637.0105 - val_regression_loss: 1298.8027\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 29307.7070 - regression_loss: 7708.1719 - val_loss: 2425.7571 - val_regression_loss: 1193.5641\n",
            "***************************** elapsed_time is:  5.985640525817871\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 1868768.3750 - regression_loss: 512569.5625 - val_loss: 208335.7344 - val_regression_loss: 104332.6719\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1819740.2500 - regression_loss: 491397.9375 - val_loss: 197478.4219 - val_regression_loss: 98903.7656\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1704147.2500 - regression_loss: 466402.0312 - val_loss: 181947.2188 - val_regression_loss: 91141.0078\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1566321.3750 - regression_loss: 430997.2188 - val_loss: 160828.0938 - val_regression_loss: 80587.4062\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1401994.8750 - regression_loss: 383596.0000 - val_loss: 134648.0469 - val_regression_loss: 67506.5703\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1186627.8750 - regression_loss: 324758.9375 - val_loss: 104928.8672 - val_regression_loss: 52657.8086\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 944384.0000 - regression_loss: 258029.9844 - val_loss: 74022.1328 - val_regression_loss: 37209.8906\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 692654.1875 - regression_loss: 188633.5000 - val_loss: 45962.5898 - val_regression_loss: 23149.6895\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 450304.4688 - regression_loss: 124687.5938 - val_loss: 28293.1914 - val_regression_loss: 14127.8594\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 298853.2500 - regression_loss: 82025.9141 - val_loss: 31274.7285 - val_regression_loss: 14943.0400\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 299876.6562 - regression_loss: 81054.7891 - val_loss: 43233.0586 - val_regression_loss: 20531.1875\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 378345.5938 - regression_loss: 99694.5625 - val_loss: 39524.7383 - val_regression_loss: 19147.8281\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 347048.4062 - regression_loss: 92505.5859 - val_loss: 28587.0195 - val_regression_loss: 14047.9619\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 267788.8750 - regression_loss: 72821.8828 - val_loss: 20679.2324 - val_regression_loss: 10254.6279\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 222361.8438 - regression_loss: 59873.4102 - val_loss: 18190.2676 - val_regression_loss: 9075.3770\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 216653.1094 - regression_loss: 57708.6641 - val_loss: 18654.3145 - val_regression_loss: 9332.6113\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 226122.1406 - regression_loss: 60745.4570 - val_loss: 19255.1973 - val_regression_loss: 9639.1445\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 231362.3594 - regression_loss: 62904.2383 - val_loss: 18673.5000 - val_regression_loss: 9344.1475\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 226160.8750 - regression_loss: 61613.3750 - val_loss: 16940.2617 - val_regression_loss: 8467.7129\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 204827.5938 - regression_loss: 56825.8750 - val_loss: 14911.0869 - val_regression_loss: 7440.7090\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 186137.1250 - regression_loss: 50937.3047 - val_loss: 13651.9502 - val_regression_loss: 6798.3203\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 172885.5625 - regression_loss: 46134.6211 - val_loss: 13850.9736 - val_regression_loss: 6886.6689\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 161716.6406 - regression_loss: 43848.4141 - val_loss: 15098.9756 - val_regression_loss: 7502.9082\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 158180.0938 - regression_loss: 43837.3320 - val_loss: 15922.8447 - val_regression_loss: 7911.2305\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 161190.9219 - regression_loss: 43861.1367 - val_loss: 15435.9287 - val_regression_loss: 7667.9565\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 153176.1562 - regression_loss: 42171.5820 - val_loss: 13977.6377 - val_regression_loss: 6942.3062\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 144120.9531 - regression_loss: 39451.3906 - val_loss: 12704.7227 - val_regression_loss: 6311.3340\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 135938.6719 - regression_loss: 37570.4023 - val_loss: 11892.5586 - val_regression_loss: 5910.5894\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 134877.5938 - regression_loss: 36978.6328 - val_loss: 11369.7217 - val_regression_loss: 5653.0430\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 132688.2969 - regression_loss: 36504.1641 - val_loss: 10903.8154 - val_regression_loss: 5422.1650\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 128200.8672 - regression_loss: 35260.5547 - val_loss: 10485.3721 - val_regression_loss: 5213.6123\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 123936.8906 - regression_loss: 33438.0820 - val_loss: 10317.2080 - val_regression_loss: 5129.4131\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 118200.5781 - regression_loss: 31797.1895 - val_loss: 10416.9209 - val_regression_loss: 5179.2559\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 110737.0078 - regression_loss: 30520.9727 - val_loss: 10367.8066 - val_regression_loss: 5154.9727\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 107471.6094 - regression_loss: 29416.2148 - val_loss: 9916.5371 - val_regression_loss: 4929.6299\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 103991.8047 - regression_loss: 27965.9355 - val_loss: 9218.4902 - val_regression_loss: 4580.8735\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 94062.2812 - regression_loss: 26364.6484 - val_loss: 8495.3066 - val_regression_loss: 4219.7944\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 92620.1641 - regression_loss: 24888.0195 - val_loss: 7967.2886 - val_regression_loss: 3956.2944\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 85660.0312 - regression_loss: 23606.4746 - val_loss: 7545.7349 - val_regression_loss: 3745.9414\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 76821.3828 - regression_loss: 22223.3125 - val_loss: 7192.2456 - val_regression_loss: 3569.4109\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 71600.2422 - regression_loss: 20685.7031 - val_loss: 6871.4497 - val_regression_loss: 3409.5452\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 69408.7344 - regression_loss: 19199.0879 - val_loss: 6563.3623 - val_regression_loss: 3256.3215\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 63887.9180 - regression_loss: 17774.7930 - val_loss: 6051.6187 - val_regression_loss: 3002.4797\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 60064.1719 - regression_loss: 16420.6309 - val_loss: 5332.0332 - val_regression_loss: 2645.3655\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 54251.6914 - regression_loss: 14978.7568 - val_loss: 4611.0518 - val_regression_loss: 2287.7786\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 51450.0117 - regression_loss: 13778.1230 - val_loss: 4061.3230 - val_regression_loss: 2014.9365\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 46065.9336 - regression_loss: 12662.7109 - val_loss: 3683.1487 - val_regression_loss: 1826.7755\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 41500.7930 - regression_loss: 11627.3076 - val_loss: 3428.5347 - val_regression_loss: 1699.1310\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 38263.6758 - regression_loss: 10703.3389 - val_loss: 3237.2141 - val_regression_loss: 1602.1803\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 37048.5469 - regression_loss: 9990.0908 - val_loss: 2987.5886 - val_regression_loss: 1476.1261\n",
            "***************************** elapsed_time is:  5.948397636413574\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 56131.4023 - regression_loss: 15319.2959 - val_loss: 5769.9375 - val_regression_loss: 2861.7935\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 44192.6797 - regression_loss: 12029.4326 - val_loss: 4234.8384 - val_regression_loss: 2093.1965\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 32661.3398 - regression_loss: 8782.6084 - val_loss: 2626.2334 - val_regression_loss: 1286.7844\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 20116.5508 - regression_loss: 5452.6860 - val_loss: 1586.3895 - val_regression_loss: 763.7402\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 13357.2734 - regression_loss: 3564.8018 - val_loss: 1480.0964 - val_regression_loss: 708.9733\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13556.4453 - regression_loss: 3534.4736 - val_loss: 1127.3639 - val_regression_loss: 534.6085\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 10259.6973 - regression_loss: 2644.9768 - val_loss: 730.0656 - val_regression_loss: 339.3014\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6345.5400 - regression_loss: 1611.2780 - val_loss: 737.2644 - val_regression_loss: 345.9458\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5729.5942 - regression_loss: 1473.3563 - val_loss: 966.2657 - val_regression_loss: 462.6184\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7205.0161 - regression_loss: 1865.9220 - val_loss: 1014.4095 - val_regression_loss: 487.9656\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7565.8442 - regression_loss: 1955.7906 - val_loss: 846.8373 - val_regression_loss: 404.8387\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6371.3652 - regression_loss: 1647.8480 - val_loss: 625.7913 - val_regression_loss: 294.5983\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4669.8369 - regression_loss: 1252.8159 - val_loss: 485.1092 - val_regression_loss: 224.3059\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4100.7544 - regression_loss: 1035.7876 - val_loss: 456.4409 - val_regression_loss: 209.8841\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3930.8696 - regression_loss: 1021.8019 - val_loss: 477.2314 - val_regression_loss: 220.1624\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4144.7124 - regression_loss: 1069.4866 - val_loss: 482.5813 - val_regression_loss: 222.7937\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3923.5862 - regression_loss: 1042.0649 - val_loss: 478.9331 - val_regression_loss: 221.0600\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3910.3269 - regression_loss: 978.7876 - val_loss: 486.8242 - val_regression_loss: 225.1936\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3710.4712 - regression_loss: 948.1425 - val_loss: 487.3906 - val_regression_loss: 225.7156\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3637.2087 - regression_loss: 913.5059 - val_loss: 460.6093 - val_regression_loss: 212.5677\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3391.7178 - regression_loss: 847.0328 - val_loss: 426.2875 - val_regression_loss: 195.6326\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3217.0066 - regression_loss: 794.6747 - val_loss: 404.7332 - val_regression_loss: 185.0362\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3183.0925 - regression_loss: 786.2327 - val_loss: 384.9632 - val_regression_loss: 175.2560\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 3160.0710 - regression_loss: 774.6923 - val_loss: 362.8366 - val_regression_loss: 164.2226\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3022.5154 - regression_loss: 741.5186 - val_loss: 349.1608 - val_regression_loss: 157.3575\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2725.0203 - regression_loss: 715.7922 - val_loss: 343.3854 - val_regression_loss: 154.4058\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2549.5125 - regression_loss: 696.6913 - val_loss: 333.3366 - val_regression_loss: 149.2946\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2749.0791 - regression_loss: 672.9780 - val_loss: 324.3318 - val_regression_loss: 144.6930\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2400.3752 - regression_loss: 648.6646 - val_loss: 323.3415 - val_regression_loss: 144.1413\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2630.3274 - regression_loss: 641.6379 - val_loss: 320.3759 - val_regression_loss: 142.6683\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2545.0930 - regression_loss: 625.3132 - val_loss: 312.7547 - val_regression_loss: 138.9439\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2473.9048 - regression_loss: 606.4257 - val_loss: 307.5623 - val_regression_loss: 136.4622\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2441.2976 - regression_loss: 596.1301 - val_loss: 297.2779 - val_regression_loss: 131.3937\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2272.8704 - regression_loss: 577.8506 - val_loss: 287.6292 - val_regression_loss: 126.6312\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2345.4634 - regression_loss: 563.5220 - val_loss: 280.5481 - val_regression_loss: 123.1334\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2214.4827 - regression_loss: 553.1588 - val_loss: 271.6414 - val_regression_loss: 118.7415\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2239.8035 - regression_loss: 538.9702 - val_loss: 264.0714 - val_regression_loss: 114.9952\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2133.2849 - regression_loss: 527.4995 - val_loss: 256.7973 - val_regression_loss: 111.3480\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2127.2402 - regression_loss: 515.7955 - val_loss: 250.2031 - val_regression_loss: 108.0165\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2095.4998 - regression_loss: 504.1234 - val_loss: 244.6927 - val_regression_loss: 105.2514\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2056.6704 - regression_loss: 495.0973 - val_loss: 240.4881 - val_regression_loss: 103.1786\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2048.5425 - regression_loss: 484.6804 - val_loss: 237.7527 - val_regression_loss: 101.8494\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2009.9823 - regression_loss: 475.9716 - val_loss: 233.8353 - val_regression_loss: 99.9145\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1898.4637 - regression_loss: 466.8499 - val_loss: 227.9510 - val_regression_loss: 96.9941\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1931.9932 - regression_loss: 458.7529 - val_loss: 221.5011 - val_regression_loss: 93.7812\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1920.9285 - regression_loss: 450.7430 - val_loss: 215.5945 - val_regression_loss: 90.8685\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1853.5785 - regression_loss: 443.2200 - val_loss: 210.8954 - val_regression_loss: 88.5194\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1798.5238 - regression_loss: 436.0543 - val_loss: 207.5420 - val_regression_loss: 86.8093\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1829.8651 - regression_loss: 428.8083 - val_loss: 204.5789 - val_regression_loss: 85.3338\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1801.4172 - regression_loss: 421.7211 - val_loss: 200.6069 - val_regression_loss: 83.3552\n",
            "***************************** elapsed_time is:  6.322902202606201\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 7962.7051 - regression_loss: 2044.0134 - val_loss: 617.6179 - val_regression_loss: 287.8495\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5102.8213 - regression_loss: 1312.2856 - val_loss: 418.7006 - val_regression_loss: 191.0631\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3471.6450 - regression_loss: 885.2121 - val_loss: 272.4762 - val_regression_loss: 118.8105\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2294.8179 - regression_loss: 547.9895 - val_loss: 174.9708 - val_regression_loss: 69.1300\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1298.3752 - regression_loss: 287.8243 - val_loss: 238.7557 - val_regression_loss: 99.6224\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1782.7733 - regression_loss: 406.4692 - val_loss: 241.0577 - val_regression_loss: 101.1288\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1929.2443 - regression_loss: 446.6205 - val_loss: 197.1863 - val_regression_loss: 80.3597\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1589.4064 - regression_loss: 365.4565 - val_loss: 163.7274 - val_regression_loss: 64.6131\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1310.0874 - regression_loss: 288.7209 - val_loss: 166.2188 - val_regression_loss: 66.3546\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1246.3162 - regression_loss: 283.8578 - val_loss: 185.0454 - val_regression_loss: 76.1452\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1432.3429 - regression_loss: 325.0576 - val_loss: 179.7464 - val_regression_loss: 73.9675\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1376.3107 - regression_loss: 326.7979 - val_loss: 166.3978 - val_regression_loss: 67.5919\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1358.8802 - regression_loss: 310.2582 - val_loss: 157.2380 - val_regression_loss: 62.7368\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1287.1824 - regression_loss: 288.3805 - val_loss: 153.1445 - val_regression_loss: 59.9870\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1218.1769 - regression_loss: 266.2355 - val_loss: 159.8508 - val_regression_loss: 62.5911\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1221.1024 - regression_loss: 268.7520 - val_loss: 165.6483 - val_regression_loss: 65.0697\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1262.6029 - regression_loss: 276.6432 - val_loss: 160.8846 - val_regression_loss: 62.7832\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1220.7070 - regression_loss: 271.5951 - val_loss: 155.4799 - val_regression_loss: 60.4885\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1208.9276 - regression_loss: 266.4645 - val_loss: 153.2536 - val_regression_loss: 59.7740\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1188.3990 - regression_loss: 260.7914 - val_loss: 155.2121 - val_regression_loss: 60.9362\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1188.5847 - regression_loss: 261.2619 - val_loss: 158.9458 - val_regression_loss: 62.8316\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1200.8417 - regression_loss: 265.3779 - val_loss: 157.9759 - val_regression_loss: 62.3904\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1156.2787 - regression_loss: 265.5555 - val_loss: 155.3270 - val_regression_loss: 61.0978\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1194.1663 - regression_loss: 263.1287 - val_loss: 154.8433 - val_regression_loss: 60.7317\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1157.5967 - regression_loss: 260.8595 - val_loss: 157.4719 - val_regression_loss: 61.7757\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1153.2280 - regression_loss: 256.8083 - val_loss: 159.1030 - val_regression_loss: 62.4504\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1153.9045 - regression_loss: 257.4264 - val_loss: 158.0854 - val_regression_loss: 61.9395\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1151.6901 - regression_loss: 257.0364 - val_loss: 158.2503 - val_regression_loss: 62.0312\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1143.4667 - regression_loss: 256.4946 - val_loss: 158.6809 - val_regression_loss: 62.2990\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1136.1989 - regression_loss: 255.2516 - val_loss: 158.8420 - val_regression_loss: 62.4528\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1155.2832 - regression_loss: 255.1675 - val_loss: 159.1581 - val_regression_loss: 62.6726\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1147.4563 - regression_loss: 255.6151 - val_loss: 158.7702 - val_regression_loss: 62.5156\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1133.8812 - regression_loss: 255.8722 - val_loss: 158.5270 - val_regression_loss: 62.3619\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1138.8173 - regression_loss: 255.3162 - val_loss: 158.7575 - val_regression_loss: 62.3870\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1142.4103 - regression_loss: 254.0715 - val_loss: 159.6056 - val_regression_loss: 62.6894\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1111.0588 - regression_loss: 253.4550 - val_loss: 160.1507 - val_regression_loss: 62.8653\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1138.3822 - regression_loss: 253.0981 - val_loss: 160.2499 - val_regression_loss: 62.8629\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1133.0056 - regression_loss: 252.7822 - val_loss: 159.9642 - val_regression_loss: 62.7163\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1141.0178 - regression_loss: 252.6553 - val_loss: 159.6206 - val_regression_loss: 62.5651\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1131.4341 - regression_loss: 252.5348 - val_loss: 160.7421 - val_regression_loss: 63.0915\n",
            "Epoch 41/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1182.6670 - regression_loss: 480.3744\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1130.7069 - regression_loss: 252.1006 - val_loss: 160.7444 - val_regression_loss: 63.0819\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1129.9843 - regression_loss: 251.9465 - val_loss: 160.3934 - val_regression_loss: 62.8963\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1151.2445 - regression_loss: 251.7602 - val_loss: 160.2372 - val_regression_loss: 62.7976\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1138.4138 - regression_loss: 251.5994 - val_loss: 160.3260 - val_regression_loss: 62.8146\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1113.9731 - regression_loss: 251.5109 - val_loss: 160.8261 - val_regression_loss: 63.0164\n",
            "Epoch 46/50\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1177.8618 - regression_loss: 477.1604\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1126.0992 - regression_loss: 251.0471 - val_loss: 160.9732 - val_regression_loss: 63.0616\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1135.2821 - regression_loss: 250.8439 - val_loss: 161.0117 - val_regression_loss: 63.0720\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1102.9548 - regression_loss: 250.7770 - val_loss: 161.0040 - val_regression_loss: 63.0612\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1135.1617 - regression_loss: 250.6868 - val_loss: 160.7138 - val_regression_loss: 62.9209\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1117.4109 - regression_loss: 250.6234 - val_loss: 160.6824 - val_regression_loss: 62.9050\n",
            "***************************** elapsed_time is:  5.995509147644043\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 229ms/step - loss: 61494.1641 - regression_loss: 16700.2988 - val_loss: 6906.9316 - val_regression_loss: 3445.4043\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 49481.4219 - regression_loss: 13359.8242 - val_loss: 5286.8179 - val_regression_loss: 2631.4871\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 36234.2500 - regression_loss: 9908.7842 - val_loss: 3481.9194 - val_regression_loss: 1723.4043\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 23004.9844 - regression_loss: 6183.7471 - val_loss: 2074.4412 - val_regression_loss: 1011.7391\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 13230.8369 - regression_loss: 3513.2097 - val_loss: 1919.5200 - val_regression_loss: 927.2241\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 13586.3105 - regression_loss: 3546.1555 - val_loss: 1838.4369 - val_regression_loss: 889.4307\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12833.5684 - regression_loss: 3403.6611 - val_loss: 1375.0178 - val_regression_loss: 665.2458\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8910.0898 - regression_loss: 2296.0784 - val_loss: 1149.7349 - val_regression_loss: 559.5061\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 6395.4595 - regression_loss: 1669.6726 - val_loss: 1270.8054 - val_regression_loss: 624.1080\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 7202.9683 - regression_loss: 1909.2694 - val_loss: 1368.1671 - val_regression_loss: 673.8484\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 8098.5215 - regression_loss: 2156.9233 - val_loss: 1225.9119 - val_regression_loss: 601.6447\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7400.0264 - regression_loss: 1922.0242 - val_loss: 979.6874 - val_regression_loss: 476.2541\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5599.9829 - regression_loss: 1442.4082 - val_loss: 849.8104 - val_regression_loss: 408.6110\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4484.9688 - regression_loss: 1181.9658 - val_loss: 887.3668 - val_regression_loss: 424.9459\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 4980.2666 - regression_loss: 1269.8369 - val_loss: 931.9971 - val_regression_loss: 445.7022\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5296.4556 - regression_loss: 1365.5360 - val_loss: 872.8254 - val_regression_loss: 415.7719\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4801.6748 - regression_loss: 1231.4767 - val_loss: 785.6863 - val_regression_loss: 373.0363\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4126.4189 - regression_loss: 1045.3260 - val_loss: 757.8829 - val_regression_loss: 360.6396\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3888.7581 - regression_loss: 978.0350 - val_loss: 762.8314 - val_regression_loss: 364.6554\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3805.5544 - regression_loss: 978.2645 - val_loss: 739.8870 - val_regression_loss: 354.3341\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3660.3621 - regression_loss: 931.5402 - val_loss: 690.7980 - val_regression_loss: 330.3524\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3219.9014 - regression_loss: 844.8066 - val_loss: 652.6311 - val_regression_loss: 311.2100\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3162.7058 - regression_loss: 798.9907 - val_loss: 635.9648 - val_regression_loss: 302.2831\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3126.7729 - regression_loss: 782.9849 - val_loss: 618.6973 - val_regression_loss: 292.8046\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2937.9150 - regression_loss: 738.5242 - val_loss: 597.8428 - val_regression_loss: 281.6280\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2683.0093 - regression_loss: 668.0338 - val_loss: 591.8430 - val_regression_loss: 278.2195\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2596.3679 - regression_loss: 629.1198 - val_loss: 591.9150 - val_regression_loss: 278.1049\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2552.3467 - regression_loss: 619.7730 - val_loss: 576.0162 - val_regression_loss: 270.0883\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2383.1941 - regression_loss: 594.9048 - val_loss: 544.5042 - val_regression_loss: 254.2865\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2289.4351 - regression_loss: 553.6963 - val_loss: 520.1190 - val_regression_loss: 242.3061\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2151.4004 - regression_loss: 529.4490 - val_loss: 505.0385 - val_regression_loss: 235.3304\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2122.0398 - regression_loss: 513.2021 - val_loss: 495.4886 - val_regression_loss: 231.3315\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2058.3826 - regression_loss: 490.6465 - val_loss: 491.0719 - val_regression_loss: 229.6837\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1973.6981 - regression_loss: 474.4763 - val_loss: 483.1501 - val_regression_loss: 225.8296\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1910.1312 - regression_loss: 460.0540 - val_loss: 464.7933 - val_regression_loss: 216.2918\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1865.4598 - regression_loss: 437.6617 - val_loss: 446.3481 - val_regression_loss: 206.5431\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1755.7291 - regression_loss: 418.6302 - val_loss: 434.0196 - val_regression_loss: 200.0419\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1759.2893 - regression_loss: 406.6162 - val_loss: 425.9090 - val_regression_loss: 195.9899\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1707.3937 - regression_loss: 393.9682 - val_loss: 419.6304 - val_regression_loss: 193.1563\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1660.6366 - regression_loss: 382.7973 - val_loss: 411.8546 - val_regression_loss: 189.6196\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1633.4666 - regression_loss: 374.1768 - val_loss: 397.3486 - val_regression_loss: 182.5360\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1574.3380 - regression_loss: 363.6599 - val_loss: 381.9371 - val_regression_loss: 174.8819\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1557.1460 - regression_loss: 355.6143 - val_loss: 370.5250 - val_regression_loss: 169.2175\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1476.4834 - regression_loss: 348.4712 - val_loss: 361.7991 - val_regression_loss: 164.8784\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1413.2229 - regression_loss: 341.5226 - val_loss: 357.4990 - val_regression_loss: 162.7278\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1469.6735 - regression_loss: 334.7662 - val_loss: 348.9727 - val_regression_loss: 158.2603\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1449.1924 - regression_loss: 327.6020 - val_loss: 338.4897 - val_regression_loss: 152.7829\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1424.0613 - regression_loss: 321.8728 - val_loss: 330.3157 - val_regression_loss: 148.5776\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1424.6887 - regression_loss: 317.3778 - val_loss: 323.5450 - val_regression_loss: 145.3264\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1376.2310 - regression_loss: 313.5234 - val_loss: 319.0404 - val_regression_loss: 143.2827\n",
            "***************************** elapsed_time is:  6.0147786140441895\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 94351.7422 - regression_loss: 25482.5664 - val_loss: 8808.6748 - val_regression_loss: 4384.3467\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 77647.1172 - regression_loss: 20972.5293 - val_loss: 6809.5161 - val_regression_loss: 3384.5093\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 58860.0430 - regression_loss: 16085.9561 - val_loss: 4471.9609 - val_regression_loss: 2214.8643\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 38872.4922 - regression_loss: 10444.6885 - val_loss: 2368.7947 - val_regression_loss: 1161.7229\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 19789.1562 - regression_loss: 5227.2358 - val_loss: 1624.4073 - val_regression_loss: 787.6713\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 11290.5469 - regression_loss: 2987.3088 - val_loss: 2293.9348 - val_regression_loss: 1122.2141\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 15044.0410 - regression_loss: 4037.6357 - val_loss: 2070.8940 - val_regression_loss: 1012.9752\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14091.1143 - regression_loss: 3721.2048 - val_loss: 1212.2368 - val_regression_loss: 586.3372\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8847.6465 - regression_loss: 2294.7407 - val_loss: 600.3890 - val_regression_loss: 282.3432\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5344.7192 - regression_loss: 1389.4478 - val_loss: 467.0119 - val_regression_loss: 216.7025\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5256.2563 - regression_loss: 1402.3768 - val_loss: 582.3783 - val_regression_loss: 274.7984\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6556.8037 - regression_loss: 1794.5251 - val_loss: 664.5027 - val_regression_loss: 315.8647\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 7442.4385 - regression_loss: 1986.1660 - val_loss: 611.6505 - val_regression_loss: 289.2037\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6853.9751 - regression_loss: 1803.3447 - val_loss: 481.9577 - val_regression_loss: 223.9804\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5424.4673 - regression_loss: 1401.9431 - val_loss: 387.5716 - val_regression_loss: 176.3377\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4081.7864 - regression_loss: 1055.4585 - val_loss: 399.1865 - val_regression_loss: 181.6733\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 3631.2244 - regression_loss: 948.6567 - val_loss: 493.2993 - val_regression_loss: 228.2624\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3975.3835 - regression_loss: 1024.2214 - val_loss: 570.4279 - val_regression_loss: 266.4220\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4238.8540 - regression_loss: 1114.1279 - val_loss: 568.3713 - val_regression_loss: 265.1123\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4257.6450 - regression_loss: 1081.5251 - val_loss: 504.1744 - val_regression_loss: 232.9262\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3746.9829 - regression_loss: 958.5198 - val_loss: 435.9037 - val_regression_loss: 198.9329\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3402.9338 - regression_loss: 862.8840 - val_loss: 391.5855 - val_regression_loss: 177.0686\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3318.9460 - regression_loss: 829.4260 - val_loss: 363.7673 - val_regression_loss: 163.5247\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3276.9194 - regression_loss: 827.1749 - val_loss: 338.0271 - val_regression_loss: 151.0099\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3203.3916 - regression_loss: 808.2846 - val_loss: 314.7448 - val_regression_loss: 139.6460\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3118.4189 - regression_loss: 774.4978 - val_loss: 302.0771 - val_regression_loss: 133.4732\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2950.1421 - regression_loss: 740.4064 - val_loss: 302.8177 - val_regression_loss: 133.8545\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2831.1423 - regression_loss: 716.0629 - val_loss: 309.0294 - val_regression_loss: 136.8185\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2721.3293 - regression_loss: 690.3811 - val_loss: 312.0273 - val_regression_loss: 138.0673\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2628.4868 - regression_loss: 658.6537 - val_loss: 312.2212 - val_regression_loss: 137.8846\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2466.3267 - regression_loss: 624.0001 - val_loss: 312.6267 - val_regression_loss: 137.8687\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2466.9629 - regression_loss: 603.9536 - val_loss: 313.0462 - val_regression_loss: 137.9513\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2448.5925 - regression_loss: 592.8228 - val_loss: 307.3329 - val_regression_loss: 135.0792\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2350.0464 - regression_loss: 580.0190 - val_loss: 295.8398 - val_regression_loss: 129.3968\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2295.4163 - regression_loss: 562.1265 - val_loss: 282.0046 - val_regression_loss: 122.6042\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2246.0610 - regression_loss: 543.1675 - val_loss: 272.6294 - val_regression_loss: 118.0519\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2213.6011 - regression_loss: 531.9525 - val_loss: 265.8343 - val_regression_loss: 114.7672\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2172.5559 - regression_loss: 524.6078 - val_loss: 258.9456 - val_regression_loss: 111.3873\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2129.1011 - regression_loss: 515.6439 - val_loss: 252.2679 - val_regression_loss: 108.0526\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2089.1948 - regression_loss: 504.8443 - val_loss: 248.7091 - val_regression_loss: 106.2373\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2069.8289 - regression_loss: 493.7875 - val_loss: 243.9712 - val_regression_loss: 103.8111\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2004.0677 - regression_loss: 483.7388 - val_loss: 242.0898 - val_regression_loss: 102.8008\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1993.7020 - regression_loss: 472.7343 - val_loss: 242.5157 - val_regression_loss: 102.9690\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1943.6536 - regression_loss: 462.9796 - val_loss: 241.9736 - val_regression_loss: 102.6762\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1900.1090 - regression_loss: 453.5723 - val_loss: 238.6676 - val_regression_loss: 101.0447\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1847.7844 - regression_loss: 443.9190 - val_loss: 231.2830 - val_regression_loss: 97.4041\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1836.1713 - regression_loss: 435.1401 - val_loss: 223.6158 - val_regression_loss: 93.6265\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1815.0337 - regression_loss: 427.3563 - val_loss: 216.1648 - val_regression_loss: 89.9449\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1754.1531 - regression_loss: 419.6651 - val_loss: 213.2163 - val_regression_loss: 88.4884\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1723.6088 - regression_loss: 412.1929 - val_loss: 211.1463 - val_regression_loss: 87.4635\n",
            "***************************** elapsed_time is:  6.017762660980225\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 215ms/step - loss: 10113.5850 - regression_loss: 2707.5676 - val_loss: 948.3846 - val_regression_loss: 451.5056\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6703.1523 - regression_loss: 1729.7935 - val_loss: 642.2682 - val_regression_loss: 293.8604\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4502.0439 - regression_loss: 1108.9159 - val_loss: 461.8241 - val_regression_loss: 199.9549\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3445.1794 - regression_loss: 790.3898 - val_loss: 309.6855 - val_regression_loss: 126.8524\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2146.3254 - regression_loss: 458.2599 - val_loss: 280.3774 - val_regression_loss: 118.5810\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1669.1151 - regression_loss: 370.5771 - val_loss: 374.3215 - val_regression_loss: 170.2936\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2327.5535 - regression_loss: 567.3090 - val_loss: 331.3300 - val_regression_loss: 149.1373\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2094.8430 - regression_loss: 507.7131 - val_loss: 252.1248 - val_regression_loss: 108.3829\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1619.4893 - regression_loss: 367.9819 - val_loss: 217.4067 - val_regression_loss: 89.9707\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1415.3634 - regression_loss: 313.8747 - val_loss: 217.7894 - val_regression_loss: 89.6231\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1507.2493 - regression_loss: 328.4857 - val_loss: 226.5946 - val_regression_loss: 93.9444\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1588.1985 - regression_loss: 354.8920 - val_loss: 224.1690 - val_regression_loss: 93.0060\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1581.2356 - regression_loss: 350.2397 - val_loss: 210.3458 - val_regression_loss: 86.7629\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1452.6241 - regression_loss: 319.5774 - val_loss: 199.6174 - val_regression_loss: 82.4936\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1353.6854 - regression_loss: 296.2624 - val_loss: 198.8661 - val_regression_loss: 83.4478\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1323.5880 - regression_loss: 296.2350 - val_loss: 204.4565 - val_regression_loss: 87.3718\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1310.3313 - regression_loss: 308.4890 - val_loss: 206.4598 - val_regression_loss: 88.8512\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1358.5465 - regression_loss: 314.5755 - val_loss: 196.0161 - val_regression_loss: 83.2802\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1326.0612 - regression_loss: 302.9169 - val_loss: 181.5603 - val_regression_loss: 75.1661\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1268.1992 - regression_loss: 287.0889 - val_loss: 174.7041 - val_regression_loss: 70.8002\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1259.6541 - regression_loss: 280.7865 - val_loss: 175.9465 - val_regression_loss: 70.8133\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1269.5835 - regression_loss: 279.0596 - val_loss: 180.3459 - val_regression_loss: 72.8673\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1261.1010 - regression_loss: 279.5693 - val_loss: 182.7799 - val_regression_loss: 74.2480\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1266.1212 - regression_loss: 279.0593 - val_loss: 180.9957 - val_regression_loss: 73.7020\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1226.5703 - regression_loss: 275.5615 - val_loss: 178.9462 - val_regression_loss: 73.0953\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1247.1133 - regression_loss: 275.4325 - val_loss: 178.3991 - val_regression_loss: 73.1660\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1220.0967 - regression_loss: 276.4156 - val_loss: 178.5088 - val_regression_loss: 73.3672\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1247.2410 - regression_loss: 277.2955 - val_loss: 177.8410 - val_regression_loss: 72.9077\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1220.8225 - regression_loss: 274.9641 - val_loss: 176.6493 - val_regression_loss: 72.0407\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1230.9160 - regression_loss: 271.1705 - val_loss: 176.4579 - val_regression_loss: 71.6749\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1212.1378 - regression_loss: 269.2096 - val_loss: 178.3478 - val_regression_loss: 72.4811\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1227.3391 - regression_loss: 268.2425 - val_loss: 179.7768 - val_regression_loss: 73.2253\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1213.1523 - regression_loss: 267.9406 - val_loss: 178.4217 - val_regression_loss: 72.6526\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1214.9471 - regression_loss: 267.1991 - val_loss: 175.6589 - val_regression_loss: 71.4171\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1215.8749 - regression_loss: 266.1024 - val_loss: 175.0287 - val_regression_loss: 71.2113\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1205.1494 - regression_loss: 266.0573 - val_loss: 176.4083 - val_regression_loss: 71.9869\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1206.6112 - regression_loss: 265.8399 - val_loss: 175.9476 - val_regression_loss: 71.6936\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1180.4674 - regression_loss: 264.6938 - val_loss: 174.5127 - val_regression_loss: 70.8026\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1176.8810 - regression_loss: 263.0330 - val_loss: 175.2004 - val_regression_loss: 71.0600\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1164.1445 - regression_loss: 262.1127 - val_loss: 175.4898 - val_regression_loss: 71.1659\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1197.0323 - regression_loss: 261.0508 - val_loss: 174.3056 - val_regression_loss: 70.5744\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1183.6466 - regression_loss: 260.2159 - val_loss: 173.2763 - val_regression_loss: 70.1101\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1171.4032 - regression_loss: 259.6585 - val_loss: 173.8028 - val_regression_loss: 70.4714\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1153.7109 - regression_loss: 259.5488 - val_loss: 175.5992 - val_regression_loss: 71.4522\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1171.3812 - regression_loss: 259.2200 - val_loss: 174.2275 - val_regression_loss: 70.7400\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1171.9209 - regression_loss: 258.3699 - val_loss: 172.1738 - val_regression_loss: 69.6414\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1164.5913 - regression_loss: 257.3731 - val_loss: 172.0882 - val_regression_loss: 69.5730\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1172.7748 - regression_loss: 256.5434 - val_loss: 173.1486 - val_regression_loss: 70.0976\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1139.5776 - regression_loss: 256.0832 - val_loss: 172.5240 - val_regression_loss: 69.7453\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1165.9659 - regression_loss: 255.2926 - val_loss: 171.0322 - val_regression_loss: 68.9790\n",
            "***************************** elapsed_time is:  5.93858790397644\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 212ms/step - loss: 93395.5547 - regression_loss: 25902.5352 - val_loss: 9811.5908 - val_regression_loss: 4860.0352\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 79912.8281 - regression_loss: 21457.3965 - val_loss: 7682.9224 - val_regression_loss: 3800.1357\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 61838.0977 - regression_loss: 16617.5488 - val_loss: 5092.5122 - val_regression_loss: 2511.5066\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 40550.6289 - regression_loss: 10820.2891 - val_loss: 2624.0999 - val_regression_loss: 1285.7837\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 20208.7090 - regression_loss: 5421.4189 - val_loss: 1565.1074 - val_regression_loss: 765.6315\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 11684.5723 - regression_loss: 3231.2083 - val_loss: 2452.2375 - val_regression_loss: 1214.8949\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 19303.3945 - regression_loss: 5283.4941 - val_loss: 2512.1826 - val_regression_loss: 1243.2479\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 19692.4102 - regression_loss: 5331.1519 - val_loss: 1699.4227 - val_regression_loss: 833.3573\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 13266.6758 - regression_loss: 3529.7236 - val_loss: 1177.8730 - val_regression_loss: 569.5868\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 9242.3887 - regression_loss: 2433.5364 - val_loss: 1190.4364 - val_regression_loss: 574.1248\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 9630.0557 - regression_loss: 2549.2698 - val_loss: 1364.5565 - val_regression_loss: 660.7499\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11430.9014 - regression_loss: 3001.0471 - val_loss: 1385.8087 - val_regression_loss: 672.0214\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11855.7676 - regression_loss: 3089.1792 - val_loss: 1216.7065 - val_regression_loss: 588.8375\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10188.4746 - regression_loss: 2722.0808 - val_loss: 1001.3412 - val_regression_loss: 482.8233\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 8398.6006 - regression_loss: 2227.7510 - val_loss: 895.3839 - val_regression_loss: 431.3831\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7373.2661 - regression_loss: 1948.8810 - val_loss: 942.8627 - val_regression_loss: 456.2252\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 7482.6450 - regression_loss: 1987.9250 - val_loss: 1030.3479 - val_regression_loss: 500.5185\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8027.1509 - regression_loss: 2103.9822 - val_loss: 1015.1482 - val_regression_loss: 492.9598\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 7658.7368 - regression_loss: 2021.9932 - val_loss: 900.9084 - val_regression_loss: 435.5309\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6753.9858 - regression_loss: 1775.3433 - val_loss: 789.2316 - val_regression_loss: 379.0811\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5448.9800 - regression_loss: 1582.0991 - val_loss: 733.9060 - val_regression_loss: 350.6740\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5937.0151 - regression_loss: 1515.6052 - val_loss: 709.0192 - val_regression_loss: 337.6672\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 5733.8794 - regression_loss: 1511.1311 - val_loss: 669.1102 - val_regression_loss: 317.4882\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5122.1064 - regression_loss: 1442.7751 - val_loss: 614.0289 - val_regression_loss: 290.1447\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5135.5215 - regression_loss: 1314.5857 - val_loss: 578.9748 - val_regression_loss: 273.1621\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4625.7148 - regression_loss: 1199.0946 - val_loss: 581.7759 - val_regression_loss: 275.2022\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4441.2080 - regression_loss: 1152.5459 - val_loss: 588.9188 - val_regression_loss: 279.2214\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4373.9048 - regression_loss: 1124.6975 - val_loss: 558.5450 - val_regression_loss: 264.0542\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 4142.7881 - regression_loss: 1056.3167 - val_loss: 503.3541 - val_regression_loss: 236.1125\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3814.4954 - regression_loss: 976.1570 - val_loss: 458.6921 - val_regression_loss: 213.3017\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3722.6365 - regression_loss: 937.1777 - val_loss: 427.6290 - val_regression_loss: 197.5443\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3573.7849 - regression_loss: 909.0572 - val_loss: 406.4764 - val_regression_loss: 187.1615\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3455.4133 - regression_loss: 862.6158 - val_loss: 406.3869 - val_regression_loss: 187.5790\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3267.6775 - regression_loss: 817.1635 - val_loss: 415.5103 - val_regression_loss: 192.3950\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3098.7913 - regression_loss: 792.2658 - val_loss: 407.0694 - val_regression_loss: 188.0553\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3041.0193 - regression_loss: 761.9892 - val_loss: 381.2351 - val_regression_loss: 174.7819\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2909.0989 - regression_loss: 728.1673 - val_loss: 351.7708 - val_regression_loss: 159.6449\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2678.0505 - regression_loss: 700.9244 - val_loss: 330.9845 - val_regression_loss: 149.1128\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2636.5811 - regression_loss: 676.7549 - val_loss: 323.9640 - val_regression_loss: 145.8125\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2621.4519 - regression_loss: 650.1787 - val_loss: 325.0494 - val_regression_loss: 146.5992\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2483.8887 - regression_loss: 628.2908 - val_loss: 319.3832 - val_regression_loss: 143.7972\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2465.6355 - regression_loss: 606.1411 - val_loss: 302.7338 - val_regression_loss: 135.2948\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 2243.3296 - regression_loss: 583.6949 - val_loss: 284.0393 - val_regression_loss: 125.7211\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2341.5183 - regression_loss: 565.2592 - val_loss: 277.1827 - val_regression_loss: 122.2952\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2207.3047 - regression_loss: 544.6316 - val_loss: 275.4877 - val_regression_loss: 121.5426\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 2085.9233 - regression_loss: 528.3863 - val_loss: 272.7381 - val_regression_loss: 120.1825\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2102.8096 - regression_loss: 514.9589 - val_loss: 266.2014 - val_regression_loss: 116.8747\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2068.2493 - regression_loss: 499.0065 - val_loss: 257.0282 - val_regression_loss: 112.1940\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2016.7290 - regression_loss: 485.3555 - val_loss: 247.7302 - val_regression_loss: 107.4194\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1969.1266 - regression_loss: 473.1174 - val_loss: 244.4595 - val_regression_loss: 105.8011\n",
            "***************************** elapsed_time is:  5.945687294006348\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 203ms/step - loss: 798518.6250 - regression_loss: 223748.9375 - val_loss: 94794.2109 - val_regression_loss: 47566.6992\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 760827.3750 - regression_loss: 212771.4844 - val_loss: 88964.4297 - val_regression_loss: 44639.8867\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 733688.6250 - regression_loss: 200058.4688 - val_loss: 80632.1562 - val_regression_loss: 40459.1406\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 621772.1250 - regression_loss: 181657.0625 - val_loss: 69421.3047 - val_regression_loss: 34835.4180\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 558199.3750 - regression_loss: 157422.5000 - val_loss: 55848.7656 - val_regression_loss: 28026.0664\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 463215.7812 - regression_loss: 128324.9531 - val_loss: 41322.6797 - val_regression_loss: 20732.9316\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 352818.5625 - regression_loss: 97678.5469 - val_loss: 28126.5762 - val_regression_loss: 14090.0078\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 264139.5938 - regression_loss: 70515.2344 - val_loss: 19953.3105 - val_regression_loss: 9924.3359\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 206175.1094 - regression_loss: 55165.0742 - val_loss: 20512.3730 - val_regression_loss: 10078.1650\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 215937.4219 - regression_loss: 57557.5234 - val_loss: 22484.7402 - val_regression_loss: 11027.1289\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 211945.2344 - regression_loss: 62595.5898 - val_loss: 19655.3926 - val_regression_loss: 9682.2998\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 208927.5156 - regression_loss: 55847.9844 - val_loss: 16086.7852 - val_regression_loss: 7975.9971\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 175493.8750 - regression_loss: 46821.8906 - val_loss: 15016.1299 - val_regression_loss: 7493.2827\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 160889.4062 - regression_loss: 42560.6836 - val_loss: 15728.0146 - val_regression_loss: 7875.6499\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 152622.9688 - regression_loss: 42982.6953 - val_loss: 16531.8633 - val_regression_loss: 8286.9180\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 165953.0781 - regression_loss: 44013.4492 - val_loss: 16288.1738 - val_regression_loss: 8161.4688\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 162431.6406 - regression_loss: 43214.9922 - val_loss: 14989.6162 - val_regression_loss: 7498.8286\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 143742.6406 - regression_loss: 40118.2109 - val_loss: 13327.7363 - val_regression_loss: 6646.8311\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 133759.4219 - regression_loss: 36109.5195 - val_loss: 12269.7822 - val_regression_loss: 6092.3340\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 120224.8672 - regression_loss: 33215.2109 - val_loss: 12245.1426 - val_regression_loss: 6056.0078\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 97664.9844 - regression_loss: 32265.0938 - val_loss: 12511.4541 - val_regression_loss: 6175.7808\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 117923.7812 - regression_loss: 32183.2852 - val_loss: 12089.2607 - val_regression_loss: 5965.2803\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 110418.4062 - regression_loss: 30558.2207 - val_loss: 10875.5430 - val_regression_loss: 5371.9331\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 103594.5000 - regression_loss: 27797.9883 - val_loss: 9760.1221 - val_regression_loss: 4831.5776\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 93869.5703 - regression_loss: 25725.0078 - val_loss: 9085.0469 - val_regression_loss: 4507.4160\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 93417.1328 - regression_loss: 24667.2812 - val_loss: 8521.2891 - val_regression_loss: 4229.5508\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 86687.1797 - regression_loss: 23297.7148 - val_loss: 7930.3521 - val_regression_loss: 3929.4463\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 78501.4922 - regression_loss: 21223.1191 - val_loss: 7581.7959 - val_regression_loss: 3744.9143\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 72142.4844 - regression_loss: 19176.5410 - val_loss: 7477.9067 - val_regression_loss: 3683.4824\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 65963.8203 - regression_loss: 17684.7734 - val_loss: 7059.3638 - val_regression_loss: 3472.0557\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 57439.8477 - regression_loss: 16202.4492 - val_loss: 6055.9092 - val_regression_loss: 2977.0776\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 54650.9414 - regression_loss: 14461.1455 - val_loss: 5051.4395 - val_regression_loss: 2484.5845\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 49094.5195 - regression_loss: 13051.3301 - val_loss: 4355.5815 - val_regression_loss: 2143.5645\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 44881.3281 - regression_loss: 12001.1045 - val_loss: 3948.9253 - val_regression_loss: 1939.4246\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 40897.9609 - regression_loss: 10836.3818 - val_loss: 3820.5476 - val_regression_loss: 1869.7358\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 36170.4805 - regression_loss: 9768.7607 - val_loss: 3685.7339 - val_regression_loss: 1800.4375\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 32446.2500 - regression_loss: 9057.4199 - val_loss: 3285.2646 - val_regression_loss: 1604.4695\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 22252.0781 - regression_loss: 8370.2539 - val_loss: 2803.2327 - val_regression_loss: 1371.5289\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 29506.7988 - regression_loss: 7829.1709 - val_loss: 2569.3411 - val_regression_loss: 1256.4904\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 27398.8750 - regression_loss: 7325.9590 - val_loss: 2467.7073 - val_regression_loss: 1203.1649\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 25129.3730 - regression_loss: 6797.6689 - val_loss: 2379.9023 - val_regression_loss: 1156.7788\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 23989.0078 - regression_loss: 6349.0742 - val_loss: 2219.0352 - val_regression_loss: 1076.2865\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 22091.8848 - regression_loss: 5898.3765 - val_loss: 2006.8188 - val_regression_loss: 972.3213\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 20671.6387 - regression_loss: 5474.1274 - val_loss: 1860.0599 - val_regression_loss: 900.5560\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 19209.0039 - regression_loss: 5084.7920 - val_loss: 1779.4995 - val_regression_loss: 860.5470\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 18034.1250 - regression_loss: 4718.7427 - val_loss: 1783.5331 - val_regression_loss: 860.7151\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 16585.0605 - regression_loss: 4368.4438 - val_loss: 1755.1781 - val_regression_loss: 846.8278\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 14890.4932 - regression_loss: 4071.5085 - val_loss: 1682.9003 - val_regression_loss: 812.6383\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 14435.2285 - regression_loss: 3834.0361 - val_loss: 1641.1222 - val_regression_loss: 793.0929\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 13804.5029 - regression_loss: 3618.8296 - val_loss: 1650.8444 - val_regression_loss: 797.2495\n",
            "***************************** elapsed_time is:  5.879086017608643\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 221ms/step - loss: 41368.2461 - regression_loss: 11103.9678 - val_loss: 4150.9272 - val_regression_loss: 2034.0502\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 31229.6895 - regression_loss: 8273.6465 - val_loss: 2878.7424 - val_regression_loss: 1408.2723\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 20353.4785 - regression_loss: 5494.6050 - val_loss: 1574.5653 - val_regression_loss: 768.7062\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 10689.0449 - regression_loss: 2807.8132 - val_loss: 842.9871 - val_regression_loss: 415.8517\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 5559.1348 - regression_loss: 1491.8965 - val_loss: 938.5111 - val_regression_loss: 467.4981\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 7288.8892 - regression_loss: 1970.3097 - val_loss: 724.9378 - val_regression_loss: 352.5967\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6314.6885 - regression_loss: 1655.4553 - val_loss: 413.3023 - val_regression_loss: 187.9792\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3996.0684 - regression_loss: 993.3100 - val_loss: 362.8340 - val_regression_loss: 157.6338\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3392.8279 - regression_loss: 809.1422 - val_loss: 484.1473 - val_regression_loss: 216.6654\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4067.5288 - regression_loss: 1005.8224 - val_loss: 540.8011 - val_regression_loss: 245.9069\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 4454.8291 - regression_loss: 1096.1685 - val_loss: 467.5918 - val_regression_loss: 211.7950\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3827.0193 - regression_loss: 944.9527 - val_loss: 341.0325 - val_regression_loss: 151.8125\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2844.5447 - regression_loss: 691.6945 - val_loss: 264.3857 - val_regression_loss: 116.8318\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2266.8174 - regression_loss: 561.7245 - val_loss: 275.6653 - val_regression_loss: 125.2673\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2433.7542 - regression_loss: 606.5950 - val_loss: 317.0414 - val_regression_loss: 147.6397\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2728.1123 - regression_loss: 694.0106 - val_loss: 316.0249 - val_regression_loss: 147.5369\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2648.5771 - regression_loss: 669.0706 - val_loss: 282.4256 - val_regression_loss: 130.0720\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2285.6265 - regression_loss: 568.3755 - val_loss: 264.8919 - val_regression_loss: 119.9986\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 2067.0574 - regression_loss: 501.6262 - val_loss: 270.0292 - val_regression_loss: 121.1523\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2061.0769 - regression_loss: 500.2914 - val_loss: 272.4508 - val_regression_loss: 121.2497\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2121.9924 - regression_loss: 502.4041 - val_loss: 254.6953 - val_regression_loss: 111.7197\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2006.8750 - regression_loss: 475.4197 - val_loss: 229.2211 - val_regression_loss: 98.7702\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1911.2368 - regression_loss: 439.1232 - val_loss: 215.3393 - val_regression_loss: 92.0630\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1800.1721 - regression_loss: 423.7755 - val_loss: 213.0217 - val_regression_loss: 91.5505\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1824.2736 - regression_loss: 423.0385 - val_loss: 212.0539 - val_regression_loss: 92.0538\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1768.8936 - regression_loss: 410.1251 - val_loss: 212.3365 - val_regression_loss: 93.0990\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1659.4286 - regression_loss: 393.8170 - val_loss: 218.1856 - val_regression_loss: 96.5300\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1666.9019 - regression_loss: 389.9789 - val_loss: 225.8163 - val_regression_loss: 100.5550\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1693.0232 - regression_loss: 394.8319 - val_loss: 227.1087 - val_regression_loss: 101.2586\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1684.2837 - regression_loss: 389.7230 - val_loss: 222.5399 - val_regression_loss: 98.9159\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1602.5759 - regression_loss: 376.2549 - val_loss: 218.7311 - val_regression_loss: 96.7806\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1566.1240 - regression_loss: 368.3260 - val_loss: 216.7284 - val_regression_loss: 95.4457\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1582.0197 - regression_loss: 365.3452 - val_loss: 213.7498 - val_regression_loss: 93.5555\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1599.9735 - regression_loss: 358.4086 - val_loss: 210.6855 - val_regression_loss: 91.7711\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1531.5703 - regression_loss: 351.3051 - val_loss: 209.6649 - val_regression_loss: 91.2271\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1541.0608 - regression_loss: 347.7356 - val_loss: 208.7986 - val_regression_loss: 91.0030\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1491.2887 - regression_loss: 343.4870 - val_loss: 208.2822 - val_regression_loss: 91.1198\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1471.7460 - regression_loss: 339.9677 - val_loss: 208.3053 - val_regression_loss: 91.4973\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1491.3314 - regression_loss: 338.4883 - val_loss: 208.6183 - val_regression_loss: 91.8302\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1442.7909 - regression_loss: 336.5595 - val_loss: 206.7683 - val_regression_loss: 90.8152\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1446.6960 - regression_loss: 331.7890 - val_loss: 204.2221 - val_regression_loss: 89.3109\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1423.6886 - regression_loss: 326.9219 - val_loss: 201.7288 - val_regression_loss: 87.8181\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1412.9391 - regression_loss: 323.0195 - val_loss: 200.6276 - val_regression_loss: 87.1751\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1394.9832 - regression_loss: 318.8823 - val_loss: 198.9397 - val_regression_loss: 86.3561\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1382.9286 - regression_loss: 316.0711 - val_loss: 197.5059 - val_regression_loss: 85.7957\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1381.7147 - regression_loss: 313.6551 - val_loss: 197.1239 - val_regression_loss: 85.6257\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1387.9897 - regression_loss: 310.6104 - val_loss: 196.6548 - val_regression_loss: 85.3401\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1367.8993 - regression_loss: 307.8553 - val_loss: 196.6034 - val_regression_loss: 85.1916\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1358.6627 - regression_loss: 305.3112 - val_loss: 196.7072 - val_regression_loss: 85.3079\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1338.6350 - regression_loss: 303.1481 - val_loss: 194.9309 - val_regression_loss: 84.5150\n",
            "***************************** elapsed_time is:  6.335737943649292\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 235ms/step - loss: 76071.0938 - regression_loss: 20479.9512 - val_loss: 6754.1987 - val_regression_loss: 3337.6814\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 60973.0000 - regression_loss: 16437.8008 - val_loss: 5119.4717 - val_regression_loss: 2524.9434\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 46460.4219 - regression_loss: 12492.3223 - val_loss: 3224.7510 - val_regression_loss: 1583.8071\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 28958.8281 - regression_loss: 7791.1777 - val_loss: 1572.3644 - val_regression_loss: 765.7227\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 13393.7197 - regression_loss: 3515.9121 - val_loss: 1257.2078 - val_regression_loss: 616.8832\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7659.0278 - regression_loss: 2101.0820 - val_loss: 2045.6582 - val_regression_loss: 1014.4146\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 13063.0273 - regression_loss: 3499.7307 - val_loss: 1730.5851 - val_regression_loss: 853.6930\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11716.5938 - regression_loss: 3140.5103 - val_loss: 905.4049 - val_regression_loss: 436.8234\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6741.6689 - regression_loss: 1761.3212 - val_loss: 494.8300 - val_regression_loss: 228.2177\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4539.0332 - regression_loss: 1161.9767 - val_loss: 540.1702 - val_regression_loss: 249.2827\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5333.1748 - regression_loss: 1392.8557 - val_loss: 666.3007 - val_regression_loss: 312.4586\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6414.7871 - regression_loss: 1660.2233 - val_loss: 659.0721 - val_regression_loss: 310.1774\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6060.8652 - regression_loss: 1559.9910 - val_loss: 548.7522 - val_regression_loss: 256.9717\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4527.0039 - regression_loss: 1184.3342 - val_loss: 461.3976 - val_regression_loss: 215.2845\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 3473.5149 - regression_loss: 868.1866 - val_loss: 483.3488 - val_regression_loss: 227.8165\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3079.0112 - regression_loss: 799.2609 - val_loss: 573.5013 - val_regression_loss: 273.6678\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3572.4004 - regression_loss: 924.6500 - val_loss: 608.9940 - val_regression_loss: 291.3977\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3817.0266 - regression_loss: 971.7253 - val_loss: 543.3901 - val_regression_loss: 258.0355\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3313.0149 - regression_loss: 847.8584 - val_loss: 451.2522 - val_regression_loss: 211.1638\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2772.7351 - regression_loss: 692.0129 - val_loss: 402.5899 - val_regression_loss: 186.0143\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 2547.2898 - regression_loss: 642.6691 - val_loss: 391.5600 - val_regression_loss: 179.8295\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2673.2334 - regression_loss: 662.8042 - val_loss: 381.5891 - val_regression_loss: 174.4654\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2680.5276 - regression_loss: 666.5819 - val_loss: 360.1891 - val_regression_loss: 163.6550\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2518.5647 - regression_loss: 618.1358 - val_loss: 344.5206 - val_regression_loss: 155.9084\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2253.5190 - regression_loss: 558.0686 - val_loss: 347.7561 - val_regression_loss: 157.7259\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2219.5076 - regression_loss: 532.2719 - val_loss: 360.1611 - val_regression_loss: 164.2261\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2163.0327 - regression_loss: 529.6548 - val_loss: 360.5915 - val_regression_loss: 164.7229\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2108.5850 - regression_loss: 518.9869 - val_loss: 344.0247 - val_regression_loss: 156.5993\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2018.2015 - regression_loss: 489.7019 - val_loss: 323.2777 - val_regression_loss: 146.1903\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1927.4526 - regression_loss: 463.2541 - val_loss: 309.4461 - val_regression_loss: 139.0864\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1883.7982 - regression_loss: 456.2421 - val_loss: 297.9578 - val_regression_loss: 133.0780\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1846.6952 - regression_loss: 448.9099 - val_loss: 286.2618 - val_regression_loss: 127.0192\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1827.1100 - regression_loss: 431.4999 - val_loss: 278.4332 - val_regression_loss: 122.9512\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1786.6238 - regression_loss: 412.8661 - val_loss: 276.6773 - val_regression_loss: 121.9785\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1740.5856 - regression_loss: 404.8854 - val_loss: 275.3231 - val_regression_loss: 121.2683\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1718.2231 - regression_loss: 400.4305 - val_loss: 267.1494 - val_regression_loss: 117.1908\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1671.0300 - regression_loss: 390.2357 - val_loss: 255.4561 - val_regression_loss: 111.3826\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1652.4696 - regression_loss: 378.7316 - val_loss: 245.9131 - val_regression_loss: 106.6409\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1601.2688 - regression_loss: 372.5479 - val_loss: 240.0431 - val_regression_loss: 103.7498\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1600.4387 - regression_loss: 368.7903 - val_loss: 236.2570 - val_regression_loss: 101.9272\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1564.3208 - regression_loss: 362.2854 - val_loss: 234.5755 - val_regression_loss: 101.1548\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1570.2559 - regression_loss: 355.5758 - val_loss: 232.7370 - val_regression_loss: 100.2428\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1505.2572 - regression_loss: 351.1147 - val_loss: 228.4625 - val_regression_loss: 98.0511\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1481.0569 - regression_loss: 346.7700 - val_loss: 220.6880 - val_regression_loss: 94.0551\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1507.1405 - regression_loss: 340.4964 - val_loss: 213.1252 - val_regression_loss: 90.2121\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1463.8701 - regression_loss: 335.6430 - val_loss: 207.1918 - val_regression_loss: 87.2270\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1451.5302 - regression_loss: 331.7950 - val_loss: 203.2368 - val_regression_loss: 85.3193\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1456.9884 - regression_loss: 327.8768 - val_loss: 201.2516 - val_regression_loss: 84.4241\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1441.1764 - regression_loss: 324.1149 - val_loss: 199.3098 - val_regression_loss: 83.5235\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1396.8711 - regression_loss: 320.9961 - val_loss: 196.8935 - val_regression_loss: 82.3393\n",
            "***************************** elapsed_time is:  6.010076522827148\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 82703.7656 - regression_loss: 22504.1797 - val_loss: 7604.6660 - val_regression_loss: 3777.7153\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 65916.7188 - regression_loss: 17747.3457 - val_loss: 5622.3237 - val_regression_loss: 2788.1255\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 48614.0664 - regression_loss: 12995.2490 - val_loss: 3508.0081 - val_regression_loss: 1732.2659\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 29364.4902 - regression_loss: 7908.1382 - val_loss: 1843.5745 - val_regression_loss: 901.0854\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 14835.2168 - regression_loss: 3904.6541 - val_loss: 1517.8458 - val_regression_loss: 739.0892\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10724.5283 - regression_loss: 2867.1982 - val_loss: 1870.1952 - val_regression_loss: 916.0093\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 12974.6572 - regression_loss: 3495.3315 - val_loss: 1591.1376 - val_regression_loss: 777.0027\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 11142.8965 - regression_loss: 2961.1487 - val_loss: 1017.2777 - val_regression_loss: 490.2670\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7241.1748 - regression_loss: 1883.5839 - val_loss: 687.4481 - val_regression_loss: 325.3368\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5209.7021 - regression_loss: 1338.9692 - val_loss: 733.8695 - val_regression_loss: 348.5374\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6054.4653 - regression_loss: 1589.0656 - val_loss: 841.1823 - val_regression_loss: 402.3129\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 7137.0815 - regression_loss: 1890.6859 - val_loss: 793.6003 - val_regression_loss: 378.7512\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 6790.4272 - regression_loss: 1773.1752 - val_loss: 635.4394 - val_regression_loss: 299.9267\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5296.9897 - regression_loss: 1364.8706 - val_loss: 509.2665 - val_regression_loss: 237.0315\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3906.2625 - regression_loss: 1004.5012 - val_loss: 497.8847 - val_regression_loss: 231.4242\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3591.9854 - regression_loss: 910.3782 - val_loss: 566.4643 - val_regression_loss: 265.7087\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3757.0449 - regression_loss: 1010.0632 - val_loss: 610.0125 - val_regression_loss: 287.4413\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 4203.4419 - regression_loss: 1092.4312 - val_loss: 573.0163 - val_regression_loss: 268.8980\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4044.7566 - regression_loss: 1028.8444 - val_loss: 489.3004 - val_regression_loss: 226.9935\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3472.7803 - regression_loss: 881.9046 - val_loss: 425.9361 - val_regression_loss: 195.2448\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3138.7930 - regression_loss: 791.7126 - val_loss: 403.7910 - val_regression_loss: 184.0785\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3071.9832 - regression_loss: 780.1318 - val_loss: 400.3304 - val_regression_loss: 182.2494\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 3111.5654 - regression_loss: 791.9337 - val_loss: 389.7058 - val_regression_loss: 176.8635\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3070.5466 - regression_loss: 770.1447 - val_loss: 371.5286 - val_regression_loss: 167.7415\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2938.2065 - regression_loss: 722.9551 - val_loss: 359.7351 - val_regression_loss: 161.8463\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2768.9858 - regression_loss: 683.3661 - val_loss: 356.7139 - val_regression_loss: 160.3575\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2740.9597 - regression_loss: 666.8553 - val_loss: 352.5954 - val_regression_loss: 158.3206\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2605.6843 - regression_loss: 647.0902 - val_loss: 343.3369 - val_regression_loss: 153.6963\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2488.1187 - regression_loss: 621.8671 - val_loss: 335.3904 - val_regression_loss: 149.7081\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2471.8457 - regression_loss: 599.3450 - val_loss: 333.8199 - val_regression_loss: 148.9038\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2430.1216 - regression_loss: 588.8909 - val_loss: 333.2610 - val_regression_loss: 148.6139\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 2352.1812 - regression_loss: 578.3739 - val_loss: 329.8610 - val_regression_loss: 146.9241\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2282.2878 - regression_loss: 560.7589 - val_loss: 325.2120 - val_regression_loss: 144.6290\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2241.2146 - regression_loss: 540.6867 - val_loss: 322.9718 - val_regression_loss: 143.5407\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2064.8210 - regression_loss: 528.4996 - val_loss: 321.6716 - val_regression_loss: 142.9147\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2191.8525 - regression_loss: 521.5967 - val_loss: 318.8383 - val_regression_loss: 141.5212\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2082.1372 - regression_loss: 510.4860 - val_loss: 315.4115 - val_regression_loss: 139.8294\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2074.4795 - regression_loss: 496.7947 - val_loss: 313.3546 - val_regression_loss: 138.8305\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1993.3601 - regression_loss: 485.1974 - val_loss: 312.1019 - val_regression_loss: 138.2468\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1889.0236 - regression_loss: 476.1294 - val_loss: 310.1891 - val_regression_loss: 137.3466\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1943.1915 - regression_loss: 467.0637 - val_loss: 307.8134 - val_regression_loss: 136.2144\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1953.6129 - regression_loss: 457.8969 - val_loss: 305.4664 - val_regression_loss: 135.0857\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1892.8215 - regression_loss: 449.0284 - val_loss: 303.3044 - val_regression_loss: 134.0425\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1806.2698 - regression_loss: 440.7699 - val_loss: 301.6849 - val_regression_loss: 133.2617\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1830.1290 - regression_loss: 433.0664 - val_loss: 300.4721 - val_regression_loss: 132.6880\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1807.6755 - regression_loss: 426.5192 - val_loss: 298.9098 - val_regression_loss: 131.9451\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1803.3337 - regression_loss: 419.3830 - val_loss: 297.4350 - val_regression_loss: 131.2494\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1767.2969 - regression_loss: 412.4832 - val_loss: 296.1690 - val_regression_loss: 130.6590\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1688.7362 - regression_loss: 406.0934 - val_loss: 295.0805 - val_regression_loss: 130.1549\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1740.0121 - regression_loss: 400.3926 - val_loss: 294.3940 - val_regression_loss: 129.8480\n",
            "***************************** elapsed_time is:  5.875404357910156\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 1s 409ms/step - loss: 64727.5312 - regression_loss: 18032.3340 - val_loss: 6398.6587 - val_regression_loss: 3197.8652\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 53689.9609 - regression_loss: 14617.9971 - val_loss: 4898.2856 - val_regression_loss: 2442.5857\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 41452.4961 - regression_loss: 11138.2354 - val_loss: 3133.3247 - val_regression_loss: 1552.3088\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 26371.0566 - regression_loss: 7027.5801 - val_loss: 1547.8773 - val_regression_loss: 747.7836\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 12797.6406 - regression_loss: 3338.3489 - val_loss: 1052.1226 - val_regression_loss: 484.3668\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8072.9067 - regression_loss: 2062.3904 - val_loss: 1392.3870 - val_regression_loss: 649.8025\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 10904.4111 - regression_loss: 2794.1396 - val_loss: 1122.0548 - val_regression_loss: 524.5117\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 9292.7998 - regression_loss: 2408.4819 - val_loss: 627.7865 - val_regression_loss: 289.9861\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5836.0698 - regression_loss: 1518.6223 - val_loss: 532.1960 - val_regression_loss: 251.3830\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5219.1973 - regression_loss: 1391.7251 - val_loss: 680.7460 - val_regression_loss: 329.2801\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6216.7217 - regression_loss: 1678.0795 - val_loss: 699.5744 - val_regression_loss: 337.6469\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6211.5259 - regression_loss: 1648.9869 - val_loss: 556.9177 - val_regression_loss: 262.6660\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4864.0732 - regression_loss: 1269.6909 - val_loss: 416.9781 - val_regression_loss: 188.3224\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3645.4138 - regression_loss: 930.5806 - val_loss: 397.0871 - val_regression_loss: 174.6877\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3488.2783 - regression_loss: 856.2961 - val_loss: 458.0915 - val_regression_loss: 203.3577\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3925.2458 - regression_loss: 980.3724 - val_loss: 474.1572 - val_regression_loss: 211.7985\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4073.7021 - regression_loss: 1011.2557 - val_loss: 409.2075 - val_regression_loss: 181.5220\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3578.4543 - regression_loss: 880.2753 - val_loss: 341.0512 - val_regression_loss: 150.4876\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3050.7361 - regression_loss: 736.2245 - val_loss: 329.8962 - val_regression_loss: 147.8045\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2871.1919 - regression_loss: 717.9842 - val_loss: 353.7093 - val_regression_loss: 161.8187\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3030.4854 - regression_loss: 762.5039 - val_loss: 362.8161 - val_regression_loss: 167.3019\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2945.9097 - regression_loss: 768.4573 - val_loss: 344.1227 - val_regression_loss: 157.7702\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2824.9336 - regression_loss: 707.0960 - val_loss: 322.7334 - val_regression_loss: 146.0030\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2579.4663 - regression_loss: 637.7081 - val_loss: 320.7077 - val_regression_loss: 143.4934\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2499.8389 - regression_loss: 610.4927 - val_loss: 327.8537 - val_regression_loss: 145.6870\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2539.4661 - regression_loss: 606.8145 - val_loss: 321.4368 - val_regression_loss: 141.6912\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2422.3545 - regression_loss: 583.7727 - val_loss: 302.9297 - val_regression_loss: 132.3987\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2291.9448 - regression_loss: 547.8169 - val_loss: 287.9965 - val_regression_loss: 125.5011\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2227.4746 - regression_loss: 529.1497 - val_loss: 280.9485 - val_regression_loss: 122.7801\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2177.0374 - regression_loss: 528.4697 - val_loss: 273.8748 - val_regression_loss: 119.8889\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2118.6287 - regression_loss: 520.5820 - val_loss: 265.9197 - val_regression_loss: 116.2509\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 2109.5298 - regression_loss: 502.6703 - val_loss: 264.3131 - val_regression_loss: 115.4012\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2003.8368 - regression_loss: 487.2202 - val_loss: 268.2754 - val_regression_loss: 117.1851\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1936.3540 - regression_loss: 481.6387 - val_loss: 267.9720 - val_regression_loss: 116.8048\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1927.7476 - regression_loss: 470.5797 - val_loss: 262.0183 - val_regression_loss: 113.7135\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1919.3210 - regression_loss: 456.2579 - val_loss: 255.9810 - val_regression_loss: 110.6403\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1895.4852 - regression_loss: 446.7969 - val_loss: 253.0615 - val_regression_loss: 109.0656\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1745.1893 - regression_loss: 441.7111 - val_loss: 250.4611 - val_regression_loss: 107.5596\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1860.6436 - regression_loss: 433.8528 - val_loss: 250.2376 - val_regression_loss: 107.1738\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1786.3595 - regression_loss: 424.1257 - val_loss: 252.4939 - val_regression_loss: 108.1458\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1732.8469 - regression_loss: 418.7433 - val_loss: 250.8281 - val_regression_loss: 107.4452\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1786.4132 - regression_loss: 414.4597 - val_loss: 246.8488 - val_regression_loss: 105.7461\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1762.7505 - regression_loss: 409.6868 - val_loss: 243.7089 - val_regression_loss: 104.4459\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1728.8379 - regression_loss: 406.7979 - val_loss: 243.3244 - val_regression_loss: 104.2518\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1710.6204 - regression_loss: 403.1627 - val_loss: 245.3631 - val_regression_loss: 105.0045\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1703.3251 - regression_loss: 396.9987 - val_loss: 248.2915 - val_regression_loss: 106.1332\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1688.4750 - regression_loss: 391.7991 - val_loss: 249.0983 - val_regression_loss: 106.3239\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1691.2500 - regression_loss: 387.6133 - val_loss: 245.7150 - val_regression_loss: 104.6598\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1666.9623 - regression_loss: 383.9191 - val_loss: 241.1841 - val_regression_loss: 102.5428\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1630.8949 - regression_loss: 380.9748 - val_loss: 239.4050 - val_regression_loss: 101.6747\n",
            "***************************** elapsed_time is:  6.445261001586914\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 223ms/step - loss: 570415.7500 - regression_loss: 156110.5938 - val_loss: 63831.1367 - val_regression_loss: 31860.7695\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 539819.5000 - regression_loss: 146058.8125 - val_loss: 58599.6641 - val_regression_loss: 29251.8418\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 492768.7500 - regression_loss: 134301.0938 - val_loss: 51281.7734 - val_regression_loss: 25601.0703\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 429745.4688 - regression_loss: 117944.2891 - val_loss: 41759.8320 - val_regression_loss: 20849.4922\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 360929.3750 - regression_loss: 96902.7891 - val_loss: 30935.4902 - val_regression_loss: 15447.1719\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 265563.0000 - regression_loss: 72700.7578 - val_loss: 20609.9590 - val_regression_loss: 10293.8789\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 177375.8906 - regression_loss: 49809.2422 - val_loss: 13563.9395 - val_regression_loss: 6779.0693\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 129466.1562 - regression_loss: 35082.8125 - val_loss: 12655.0400 - val_regression_loss: 6329.3281\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 120868.3516 - regression_loss: 32769.6211 - val_loss: 14025.6162 - val_regression_loss: 7010.1802\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 128322.3984 - regression_loss: 35426.6133 - val_loss: 11887.5898 - val_regression_loss: 5927.5107\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 112461.0391 - regression_loss: 30652.8691 - val_loss: 8547.6484 - val_regression_loss: 4245.3384\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 88457.6172 - regression_loss: 24394.6133 - val_loss: 6726.8584 - val_regression_loss: 3331.5251\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 81968.8047 - regression_loss: 21881.6543 - val_loss: 6745.6221 - val_regression_loss: 3342.4229\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 85589.8672 - regression_loss: 23086.9668 - val_loss: 7361.3730 - val_regression_loss: 3652.9062\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 88202.9922 - regression_loss: 24585.8398 - val_loss: 7339.2490 - val_regression_loss: 3644.7964\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 90930.0234 - regression_loss: 24172.2266 - val_loss: 6573.8828 - val_regression_loss: 3265.2124\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 80457.3672 - regression_loss: 21804.2324 - val_loss: 5703.3823 - val_regression_loss: 2832.9336\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 72402.9219 - regression_loss: 19205.3184 - val_loss: 5297.2402 - val_regression_loss: 2632.5271\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 65291.2109 - regression_loss: 17570.9277 - val_loss: 5492.7153 - val_regression_loss: 2732.3523\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 64606.5938 - regression_loss: 17344.8887 - val_loss: 5822.7056 - val_regression_loss: 2898.5623\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 64518.7695 - regression_loss: 17532.1484 - val_loss: 5702.4448 - val_regression_loss: 2838.5483\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 60767.2266 - regression_loss: 16966.4199 - val_loss: 5151.6025 - val_regression_loss: 2562.2639\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 57424.7812 - regression_loss: 15660.7959 - val_loss: 4603.4834 - val_regression_loss: 2286.7473\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 53105.6992 - regression_loss: 14448.1074 - val_loss: 4305.9214 - val_regression_loss: 2136.4229\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 49662.4766 - regression_loss: 13688.2959 - val_loss: 4153.4800 - val_regression_loss: 2059.1011\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 48260.4844 - regression_loss: 13089.5137 - val_loss: 3964.1902 - val_regression_loss: 1964.1237\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 45413.9102 - regression_loss: 12218.2520 - val_loss: 3791.6567 - val_regression_loss: 1878.1860\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 39566.2773 - regression_loss: 11161.1084 - val_loss: 3780.6860 - val_regression_loss: 1873.3743\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 37708.7383 - regression_loss: 10360.6230 - val_loss: 3871.1226 - val_regression_loss: 1919.1880\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 34848.4023 - regression_loss: 9801.0127 - val_loss: 3767.7107 - val_regression_loss: 1867.5714\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 33682.7266 - regression_loss: 9120.8799 - val_loss: 3433.8750 - val_regression_loss: 1700.2756\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 29003.4941 - regression_loss: 8251.7617 - val_loss: 3077.4934 - val_regression_loss: 1521.5439\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 26125.0527 - regression_loss: 7562.4355 - val_loss: 2825.2900 - val_regression_loss: 1395.2242\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 26219.1484 - regression_loss: 7032.3022 - val_loss: 2644.4812 - val_regression_loss: 1305.2981\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 23958.8164 - regression_loss: 6383.7891 - val_loss: 2576.6313 - val_regression_loss: 1272.2189\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 21943.9375 - regression_loss: 5783.9502 - val_loss: 2489.3630 - val_regression_loss: 1229.0618\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 20122.9316 - regression_loss: 5345.7222 - val_loss: 2202.9871 - val_regression_loss: 1085.6528\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 18179.8555 - regression_loss: 4831.9595 - val_loss: 1918.0072 - val_regression_loss: 942.6901\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 16931.9414 - regression_loss: 4437.3960 - val_loss: 1746.3158 - val_regression_loss: 856.6282\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 15712.3262 - regression_loss: 4111.9341 - val_loss: 1690.0129 - val_regression_loss: 828.8022\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 14354.1572 - regression_loss: 3755.1692 - val_loss: 1691.2415 - val_regression_loss: 829.7991\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 13180.7871 - regression_loss: 3503.4800 - val_loss: 1563.7914 - val_regression_loss: 765.9684\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12338.0254 - regression_loss: 3248.3499 - val_loss: 1405.0994 - val_regression_loss: 686.3193\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11471.4502 - regression_loss: 3051.7922 - val_loss: 1324.4426 - val_regression_loss: 646.0021\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 9746.2373 - regression_loss: 2873.0940 - val_loss: 1322.4557 - val_regression_loss: 645.4662\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10273.2500 - regression_loss: 2726.4954 - val_loss: 1331.9301 - val_regression_loss: 650.5646\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 9777.1055 - regression_loss: 2630.5798 - val_loss: 1220.5278 - val_regression_loss: 594.6386\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 9537.7363 - regression_loss: 2493.2490 - val_loss: 1160.9120 - val_regression_loss: 564.7030\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8473.2100 - regression_loss: 2413.5581 - val_loss: 1153.6196 - val_regression_loss: 561.3260\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 8691.7744 - regression_loss: 2321.1172 - val_loss: 1185.7407 - val_regression_loss: 577.7570\n",
            "***************************** elapsed_time is:  5.97028660774231\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 235ms/step - loss: 137145.5156 - regression_loss: 36644.7617 - val_loss: 15111.9385 - val_regression_loss: 7468.3198\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 118062.0781 - regression_loss: 31575.2676 - val_loss: 12762.1064 - val_regression_loss: 6299.7305\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 95664.3125 - regression_loss: 26257.8047 - val_loss: 9652.0820 - val_regression_loss: 4757.4609\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 72624.7188 - regression_loss: 19491.1211 - val_loss: 6058.3262 - val_regression_loss: 2979.8735\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 44197.5898 - regression_loss: 11917.6689 - val_loss: 2941.3987 - val_regression_loss: 1445.4177\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 20870.7207 - regression_loss: 5758.6025 - val_loss: 1792.4225 - val_regression_loss: 893.8689\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 15642.8682 - regression_loss: 4358.5713 - val_loss: 2442.6628 - val_regression_loss: 1226.2408\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 23698.5000 - regression_loss: 6540.9072 - val_loss: 2306.3889 - val_regression_loss: 1148.8287\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 22820.1523 - regression_loss: 6166.7412 - val_loss: 1574.3646 - val_regression_loss: 770.2775\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 15370.5068 - regression_loss: 4094.7742 - val_loss: 1241.2114 - val_regression_loss: 592.7650\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 10326.9580 - regression_loss: 2904.5051 - val_loss: 1470.8223 - val_regression_loss: 699.9481\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 11557.0879 - regression_loss: 3010.1350 - val_loss: 1796.9965 - val_regression_loss: 860.1506\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 13706.2275 - regression_loss: 3531.4973 - val_loss: 1853.0706 - val_regression_loss: 889.7482\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 13523.2246 - regression_loss: 3615.5896 - val_loss: 1617.9403 - val_regression_loss: 776.8788\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 12177.3896 - regression_loss: 3191.2197 - val_loss: 1265.2849 - val_regression_loss: 606.7687\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 10024.6406 - regression_loss: 2599.8767 - val_loss: 993.8201 - val_regression_loss: 477.2221\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8376.5625 - regression_loss: 2242.3887 - val_loss: 892.3907 - val_regression_loss: 431.3934\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8520.7891 - regression_loss: 2244.2476 - val_loss: 896.3600 - val_regression_loss: 436.1771\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 8789.9102 - regression_loss: 2409.1411 - val_loss: 881.4401 - val_regression_loss: 429.3331\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 9120.7266 - regression_loss: 2414.6299 - val_loss: 812.8246 - val_regression_loss: 393.7500\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8033.5933 - regression_loss: 2170.5559 - val_loss: 760.7202 - val_regression_loss: 365.2322\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 7226.8633 - regression_loss: 1902.8934 - val_loss: 772.7850 - val_regression_loss: 368.3682\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 6718.6797 - regression_loss: 1766.5402 - val_loss: 823.7959 - val_regression_loss: 391.3306\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 6768.6357 - regression_loss: 1741.2867 - val_loss: 844.9927 - val_regression_loss: 400.4678\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6376.5698 - regression_loss: 1711.7266 - val_loss: 803.9293 - val_regression_loss: 379.7924\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5971.8149 - regression_loss: 1612.5197 - val_loss: 715.9915 - val_regression_loss: 336.9342\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 5726.6660 - regression_loss: 1477.3320 - val_loss: 630.7657 - val_regression_loss: 296.2609\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 5296.7432 - regression_loss: 1369.9331 - val_loss: 584.5235 - val_regression_loss: 275.2089\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 5077.8345 - regression_loss: 1326.9170 - val_loss: 570.5338 - val_regression_loss: 269.7286\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4838.3354 - regression_loss: 1285.7793 - val_loss: 572.8857 - val_regression_loss: 271.3965\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4496.6416 - regression_loss: 1200.4185 - val_loss: 592.2015 - val_regression_loss: 280.5409\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4265.2085 - regression_loss: 1108.6041 - val_loss: 628.1178 - val_regression_loss: 297.4641\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4124.4116 - regression_loss: 1050.9454 - val_loss: 656.5878 - val_regression_loss: 310.8415\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3906.5071 - regression_loss: 1012.0626 - val_loss: 650.8331 - val_regression_loss: 307.8234\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3775.7710 - regression_loss: 962.6673 - val_loss: 616.3203 - val_regression_loss: 291.1702\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3602.3342 - regression_loss: 902.5410 - val_loss: 583.5895 - val_regression_loss: 275.6723\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3427.3364 - regression_loss: 860.2021 - val_loss: 566.5588 - val_regression_loss: 267.6794\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3288.5730 - regression_loss: 833.0951 - val_loss: 559.8842 - val_regression_loss: 264.1647\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3162.2031 - regression_loss: 794.0797 - val_loss: 559.9902 - val_regression_loss: 263.6183\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3037.9204 - regression_loss: 747.1976 - val_loss: 561.1729 - val_regression_loss: 263.8375\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2937.8940 - regression_loss: 715.8668 - val_loss: 548.9425 - val_regression_loss: 258.0211\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2787.5955 - regression_loss: 686.8882 - val_loss: 523.6954 - val_regression_loss: 246.2252\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2617.8127 - regression_loss: 656.5219 - val_loss: 499.9410 - val_regression_loss: 235.1176\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2561.4199 - regression_loss: 633.0490 - val_loss: 480.2988 - val_regression_loss: 225.5066\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2478.2764 - regression_loss: 609.7927 - val_loss: 464.1212 - val_regression_loss: 217.0823\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2411.0828 - regression_loss: 582.4056 - val_loss: 451.6626 - val_regression_loss: 210.3528\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2311.9551 - regression_loss: 558.8389 - val_loss: 439.3204 - val_regression_loss: 203.8958\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2244.2424 - regression_loss: 538.8112 - val_loss: 423.1383 - val_regression_loss: 196.1431\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2003.8213 - regression_loss: 516.8089 - val_loss: 409.2860 - val_regression_loss: 189.5529\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2083.7327 - regression_loss: 500.9093 - val_loss: 397.8794 - val_regression_loss: 184.2004\n",
            "***************************** elapsed_time is:  6.088967323303223\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 225ms/step - loss: 111970.5938 - regression_loss: 30482.9297 - val_loss: 11913.6689 - val_regression_loss: 5938.0752\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 92654.3047 - regression_loss: 25116.7188 - val_loss: 9416.3164 - val_regression_loss: 4688.5459\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 71588.9297 - regression_loss: 19559.8145 - val_loss: 6411.3965 - val_regression_loss: 3183.9707\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 48824.7969 - regression_loss: 13003.6250 - val_loss: 3442.3621 - val_regression_loss: 1695.3845\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 24705.1387 - regression_loss: 6630.9331 - val_loss: 1832.8752 - val_regression_loss: 884.3749\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 12790.3896 - regression_loss: 3400.9868 - val_loss: 2349.0430 - val_regression_loss: 1137.0344\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 17751.7188 - regression_loss: 4710.9487 - val_loss: 2327.6379 - val_regression_loss: 1127.4966\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 17891.6406 - regression_loss: 4764.6450 - val_loss: 1500.6384 - val_regression_loss: 718.8434\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 11791.3535 - regression_loss: 3015.0073 - val_loss: 936.0622 - val_regression_loss: 441.5202\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6958.9287 - regression_loss: 1779.3583 - val_loss: 951.0616 - val_regression_loss: 452.6820\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6248.2524 - regression_loss: 1769.2051 - val_loss: 1221.9872 - val_regression_loss: 590.4028\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 8542.2305 - regression_loss: 2278.7202 - val_loss: 1355.4279 - val_regression_loss: 658.2833\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 9381.7939 - regression_loss: 2527.5085 - val_loss: 1239.3490 - val_regression_loss: 600.7035\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8703.1660 - regression_loss: 2285.0239 - val_loss: 981.6884 - val_regression_loss: 471.9287\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6624.2080 - regression_loss: 1761.9126 - val_loss: 754.9977 - val_regression_loss: 358.4487\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4980.2451 - regression_loss: 1328.6890 - val_loss: 666.8829 - val_regression_loss: 314.2018\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4709.5532 - regression_loss: 1208.7778 - val_loss: 701.5032 - val_regression_loss: 331.3622\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 5122.8940 - regression_loss: 1323.1243 - val_loss: 745.5042 - val_regression_loss: 353.3297\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 5191.5591 - regression_loss: 1426.4567 - val_loss: 720.0087 - val_regression_loss: 340.6967\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 5251.0522 - regression_loss: 1358.3125 - val_loss: 653.6274 - val_regression_loss: 307.7719\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4704.5732 - regression_loss: 1186.3635 - val_loss: 611.2818 - val_regression_loss: 286.9776\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4251.8760 - regression_loss: 1073.1210 - val_loss: 607.3756 - val_regression_loss: 285.4498\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 4146.1143 - regression_loss: 1049.5399 - val_loss: 610.6985 - val_regression_loss: 287.5198\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4005.2910 - regression_loss: 1052.4962 - val_loss: 594.5273 - val_regression_loss: 279.7859\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3948.2937 - regression_loss: 1025.4384 - val_loss: 557.8745 - val_regression_loss: 261.7286\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3850.6194 - regression_loss: 970.6951 - val_loss: 521.8201 - val_regression_loss: 243.8570\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3430.7578 - regression_loss: 915.1055 - val_loss: 499.8063 - val_regression_loss: 232.8606\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3531.0906 - regression_loss: 891.2950 - val_loss: 484.0596 - val_regression_loss: 224.8334\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3312.4590 - regression_loss: 862.1395 - val_loss: 467.7286 - val_regression_loss: 216.4075\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 3239.3899 - regression_loss: 817.3715 - val_loss: 454.4141 - val_regression_loss: 209.4791\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2937.4116 - regression_loss: 775.5493 - val_loss: 449.1916 - val_regression_loss: 206.6743\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3001.2510 - regression_loss: 746.7221 - val_loss: 447.7471 - val_regression_loss: 205.8778\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2908.5183 - regression_loss: 729.1750 - val_loss: 440.5477 - val_regression_loss: 202.3129\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2825.7610 - regression_loss: 704.0803 - val_loss: 427.5093 - val_regression_loss: 195.9082\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2760.3940 - regression_loss: 671.5815 - val_loss: 415.1020 - val_regression_loss: 189.8680\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2588.8040 - regression_loss: 641.5714 - val_loss: 406.7607 - val_regression_loss: 185.8646\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2559.2183 - regression_loss: 622.6526 - val_loss: 398.3915 - val_regression_loss: 181.8014\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 2481.5955 - regression_loss: 604.6149 - val_loss: 387.4136 - val_regression_loss: 176.3617\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2393.6179 - regression_loss: 582.1552 - val_loss: 376.9288 - val_regression_loss: 171.1075\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2284.9441 - regression_loss: 561.5271 - val_loss: 368.3978 - val_regression_loss: 166.7857\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2226.1475 - regression_loss: 544.9644 - val_loss: 360.4900 - val_regression_loss: 162.7394\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2134.4912 - regression_loss: 526.9337 - val_loss: 353.2757 - val_regression_loss: 159.0263\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2123.3362 - regression_loss: 510.4667 - val_loss: 346.5192 - val_regression_loss: 155.5513\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2036.3329 - regression_loss: 495.2500 - val_loss: 338.5000 - val_regression_loss: 151.4935\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2028.6334 - regression_loss: 481.4931 - val_loss: 327.6878 - val_regression_loss: 146.1019\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1929.6948 - regression_loss: 467.1841 - val_loss: 316.8891 - val_regression_loss: 140.7523\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1929.1066 - regression_loss: 454.0532 - val_loss: 306.3693 - val_regression_loss: 135.5279\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1819.7394 - regression_loss: 442.8183 - val_loss: 295.4732 - val_regression_loss: 130.0945\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1834.2157 - regression_loss: 432.0411 - val_loss: 286.8410 - val_regression_loss: 125.7144\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1783.5941 - regression_loss: 419.9617 - val_loss: 278.9013 - val_regression_loss: 121.6744\n",
            "***************************** elapsed_time is:  6.154756307601929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "32zLJ4yfLOj8",
        "outputId": "e1c41238-9b5c-4b5c-d1a8-e1ccd080a469"
      },
      "source": [
        "df_dragonnet=pd.DataFrame([train_result, test_result])\n",
        "df_dragonnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.867694</td>\n",
              "      <td>1.112769</td>\n",
              "      <td>4.865397</td>\n",
              "      <td>0.887813</td>\n",
              "      <td>0.388687</td>\n",
              "      <td>1.824552</td>\n",
              "      <td>Dragonnet</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.977260</td>\n",
              "      <td>1.089498</td>\n",
              "      <td>4.789291</td>\n",
              "      <td>0.948663</td>\n",
              "      <td>0.366697</td>\n",
              "      <td>2.089349</td>\n",
              "      <td>Dragonnet</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pehe_score-mean  pehe_score-median  ...     method  train\n",
              "0         2.867694           1.112769  ...  Dragonnet   True\n",
              "1         2.977260           1.089498  ...  Dragonnet  False\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imeaA1mMLOkD"
      },
      "source": [
        "### 1.7.1 Neural Network Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9IhdaTnLOkF",
        "outputId": "dbb2e24d-a2f0-436d-a40f-0b9e254533de"
      },
      "source": [
        "random_state = 1\n",
        "\n",
        "results_df = list()\n",
        "test_scores = list()\n",
        "train_scores = list()\n",
        "\n",
        "\n",
        "\n",
        "train, test = train_test_split(\n",
        "        replications[n], train_size=train_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "# REPLACE this with the function you implemented and want to evaluate\n",
        "train_ite, test_ite = causal_forest(train, test, model)\n",
        "\n",
        "# Calculate the scores and append them to a dataframe\n",
        "train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n",
        "test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n",
        "\n",
        "# Summarize the scores and save them in a dataframe\n",
        "train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n",
        "train_result.update({'method': 'Dragonnet', 'train': True})\n",
        "test_result.update({'method': 'Dragonnet', 'train': False})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2/2 [==============================] - 0s 219ms/step - loss: 14489.2100 - regression_loss: 3822.8059 - val_loss: 1511.5845 - val_regression_loss: 731.9551\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 10279.2158 - regression_loss: 2657.6401 - val_loss: 1012.0696 - val_regression_loss: 483.3006\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 6556.9863 - regression_loss: 1702.4139 - val_loss: 614.4638 - val_regression_loss: 285.9970\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 4212.3027 - regression_loss: 1056.7352 - val_loss: 415.5472 - val_regression_loss: 187.4562\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3114.6497 - regression_loss: 773.2344 - val_loss: 294.4085 - val_regression_loss: 126.5547\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2240.2683 - regression_loss: 525.2856 - val_loss: 352.0093 - val_regression_loss: 154.4985\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2441.1006 - regression_loss: 583.0164 - val_loss: 441.4073 - val_regression_loss: 199.1939\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2838.8892 - regression_loss: 685.7755 - val_loss: 380.1395 - val_regression_loss: 169.5991\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2368.8943 - regression_loss: 557.3992 - val_loss: 282.5587 - val_regression_loss: 122.1893\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1781.6119 - regression_loss: 409.7526 - val_loss: 245.3478 - val_regression_loss: 104.7990\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1691.1265 - regression_loss: 406.2710 - val_loss: 253.9899 - val_regression_loss: 109.8786\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1902.5256 - regression_loss: 460.8337 - val_loss: 258.4543 - val_regression_loss: 112.3943\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1853.2192 - regression_loss: 452.8369 - val_loss: 261.7142 - val_regression_loss: 114.0047\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1671.4828 - regression_loss: 412.8320 - val_loss: 268.8247 - val_regression_loss: 117.3967\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1657.6006 - regression_loss: 388.1223 - val_loss: 267.7581 - val_regression_loss: 116.6371\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1583.9889 - regression_loss: 369.3977 - val_loss: 259.8343 - val_regression_loss: 112.3913\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1577.7413 - regression_loss: 359.9023 - val_loss: 252.8808 - val_regression_loss: 108.6571\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1591.0590 - regression_loss: 363.4840 - val_loss: 244.8430 - val_regression_loss: 104.5862\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1558.9528 - regression_loss: 359.6653 - val_loss: 235.0256 - val_regression_loss: 99.8833\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1495.4583 - regression_loss: 341.0717 - val_loss: 231.3001 - val_regression_loss: 98.3285\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1450.8372 - regression_loss: 329.8843 - val_loss: 233.0686 - val_regression_loss: 99.4672\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1465.8912 - regression_loss: 331.4657 - val_loss: 231.6008 - val_regression_loss: 98.9091\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1446.2877 - regression_loss: 331.0474 - val_loss: 227.9686 - val_regression_loss: 97.1700\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1399.6967 - regression_loss: 325.5187 - val_loss: 226.0096 - val_regression_loss: 96.1546\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1406.9932 - regression_loss: 320.1585 - val_loss: 226.8421 - val_regression_loss: 96.4264\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1397.6418 - regression_loss: 312.9734 - val_loss: 230.8304 - val_regression_loss: 98.2606\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1350.6317 - regression_loss: 309.7099 - val_loss: 233.2337 - val_regression_loss: 99.3728\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1361.4341 - regression_loss: 309.0619 - val_loss: 228.4771 - val_regression_loss: 97.0206\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1373.2126 - regression_loss: 305.4669 - val_loss: 221.4500 - val_regression_loss: 93.6175\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1296.2048 - regression_loss: 300.9272 - val_loss: 217.9362 - val_regression_loss: 91.9853\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1324.9930 - regression_loss: 298.3178 - val_loss: 216.4722 - val_regression_loss: 91.3506\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1284.0845 - regression_loss: 296.3699 - val_loss: 216.0292 - val_regression_loss: 91.1591\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1324.8525 - regression_loss: 294.4551 - val_loss: 214.7684 - val_regression_loss: 90.5062\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1289.2483 - regression_loss: 291.3338 - val_loss: 212.6691 - val_regression_loss: 89.3938\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1288.0698 - regression_loss: 286.9825 - val_loss: 211.5793 - val_regression_loss: 88.7814\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1285.4668 - regression_loss: 283.9676 - val_loss: 210.1851 - val_regression_loss: 88.0473\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1260.3462 - regression_loss: 281.5642 - val_loss: 208.1064 - val_regression_loss: 87.0121\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1244.5294 - regression_loss: 278.5510 - val_loss: 207.4413 - val_regression_loss: 86.7048\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1240.3983 - regression_loss: 275.4487 - val_loss: 204.2106 - val_regression_loss: 85.1380\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1224.7224 - regression_loss: 272.3471 - val_loss: 202.6928 - val_regression_loss: 84.3956\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1219.4624 - regression_loss: 269.7318 - val_loss: 201.5051 - val_regression_loss: 83.7941\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1206.7306 - regression_loss: 266.6734 - val_loss: 199.8919 - val_regression_loss: 82.9641\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1179.9105 - regression_loss: 263.5375 - val_loss: 197.6671 - val_regression_loss: 81.8232\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1170.6857 - regression_loss: 260.4488 - val_loss: 197.1826 - val_regression_loss: 81.5676\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1133.9711 - regression_loss: 257.6524 - val_loss: 197.3330 - val_regression_loss: 81.6449\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1148.6444 - regression_loss: 255.5994 - val_loss: 193.6387 - val_regression_loss: 79.8265\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1164.2795 - regression_loss: 253.0775 - val_loss: 191.0431 - val_regression_loss: 78.5511\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1125.1763 - regression_loss: 251.5287 - val_loss: 191.9344 - val_regression_loss: 78.9827\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1141.5591 - regression_loss: 249.6476 - val_loss: 192.0247 - val_regression_loss: 79.0046\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1119.7576 - regression_loss: 248.4330 - val_loss: 188.7923 - val_regression_loss: 77.3838\n",
            "***************************** elapsed_time is:  6.348628759384155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "VvttPAc8LOkJ",
        "outputId": "a0335a45-cb8a-4944-d0a4-004e18647155"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = (0,0,0)\n",
        "# Plot\n",
        "plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n",
        "plt.title('Scatter of treatment effects')\n",
        "plt.xlabel('real treatment effect')\n",
        "plt.ylabel('estimated treatment effect')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ZW3Us6LOkN"
      },
      "source": [
        "## QUESTION 6\n",
        "\n",
        "IS THE DRAGONNET NEURAL NETWORK ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWTUJW8fRBwe"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\r\n",
        "\r\n",
        "- **'YES'** it is estimating it well as in general sense neural networks can estimate local distribution mappings quite effectively.\r\n",
        "\r\n",
        "- However the effect is not uniform and the distributions aren't quite uniform on the lower side\r\n",
        "\r\n",
        "- We see great improvement in **pehe_score** metric with less variance though\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D09tYZt9LOkO"
      },
      "source": [
        "## 1.8 Comparison of the methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "pti3CRzkLOkO",
        "outputId": "367b6cb2-52de-4f95-fa6d-5ec3252603a9"
      },
      "source": [
        "pd.concat([df_S_learner_LR, df_PSW_LR, df_S_learner_RF, df_T_learner_LR, df_T_learner_RF, df_causal_forest, df_dragonnet ], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pehe_score-mean</th>\n",
              "      <th>pehe_score-median</th>\n",
              "      <th>pehe_score-std</th>\n",
              "      <th>mean_absolute-mean</th>\n",
              "      <th>mean_absolute-median</th>\n",
              "      <th>mean_absolute-std</th>\n",
              "      <th>method</th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.633660</td>\n",
              "      <td>2.623297</td>\n",
              "      <td>8.362125</td>\n",
              "      <td>0.732443</td>\n",
              "      <td>0.238185</td>\n",
              "      <td>1.493276</td>\n",
              "      <td>S-Learner LR</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.625971</td>\n",
              "      <td>2.635993</td>\n",
              "      <td>8.213626</td>\n",
              "      <td>1.292668</td>\n",
              "      <td>0.396246</td>\n",
              "      <td>2.474603</td>\n",
              "      <td>S-Learner LR</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.595322</td>\n",
              "      <td>2.537818</td>\n",
              "      <td>8.244302</td>\n",
              "      <td>0.412006</td>\n",
              "      <td>0.284332</td>\n",
              "      <td>0.457697</td>\n",
              "      <td>PSW</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.837997</td>\n",
              "      <td>3.484394</td>\n",
              "      <td>8.323623</td>\n",
              "      <td>3.783440</td>\n",
              "      <td>2.649187</td>\n",
              "      <td>3.225824</td>\n",
              "      <td>PSW</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.133693</td>\n",
              "      <td>1.047904</td>\n",
              "      <td>4.861394</td>\n",
              "      <td>0.511598</td>\n",
              "      <td>0.125918</td>\n",
              "      <td>0.971366</td>\n",
              "      <td>S-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.320106</td>\n",
              "      <td>1.234456</td>\n",
              "      <td>5.197415</td>\n",
              "      <td>0.451143</td>\n",
              "      <td>0.137152</td>\n",
              "      <td>1.047357</td>\n",
              "      <td>S-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3.106716</td>\n",
              "      <td>1.051650</td>\n",
              "      <td>4.788258</td>\n",
              "      <td>0.500543</td>\n",
              "      <td>0.124951</td>\n",
              "      <td>0.933657</td>\n",
              "      <td>T-Learner LR</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.311096</td>\n",
              "      <td>1.267110</td>\n",
              "      <td>5.155404</td>\n",
              "      <td>0.445728</td>\n",
              "      <td>0.134426</td>\n",
              "      <td>1.022285</td>\n",
              "      <td>T-Learner LR</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.783826</td>\n",
              "      <td>0.947208</td>\n",
              "      <td>2.263913</td>\n",
              "      <td>0.130565</td>\n",
              "      <td>0.104154</td>\n",
              "      <td>0.130342</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.512660</td>\n",
              "      <td>1.109184</td>\n",
              "      <td>3.501475</td>\n",
              "      <td>0.210897</td>\n",
              "      <td>0.123812</td>\n",
              "      <td>0.277706</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4.172661</td>\n",
              "      <td>1.921242</td>\n",
              "      <td>6.330031</td>\n",
              "      <td>0.441738</td>\n",
              "      <td>0.198733</td>\n",
              "      <td>0.823171</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4.378083</td>\n",
              "      <td>1.797608</td>\n",
              "      <td>6.606150</td>\n",
              "      <td>0.683703</td>\n",
              "      <td>0.233695</td>\n",
              "      <td>1.354973</td>\n",
              "      <td>T-Learner RF</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2.919889</td>\n",
              "      <td>1.107028</td>\n",
              "      <td>4.972546</td>\n",
              "      <td>0.826205</td>\n",
              "      <td>0.353679</td>\n",
              "      <td>1.817544</td>\n",
              "      <td>Dragonnet</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3.036828</td>\n",
              "      <td>1.033174</td>\n",
              "      <td>4.930624</td>\n",
              "      <td>0.925162</td>\n",
              "      <td>0.348241</td>\n",
              "      <td>2.146797</td>\n",
              "      <td>Dragonnet</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pehe_score-mean  pehe_score-median  ...        method  train\n",
              "0          5.633660           2.623297  ...  S-Learner LR   True\n",
              "1          5.625971           2.635993  ...  S-Learner LR  False\n",
              "2          5.595322           2.537818  ...           PSW   True\n",
              "3          6.837997           3.484394  ...           PSW  False\n",
              "4          3.133693           1.047904  ...  S-Learner RF   True\n",
              "5          3.320106           1.234456  ...  S-Learner RF  False\n",
              "6          3.106716           1.051650  ...  T-Learner LR   True\n",
              "7          3.311096           1.267110  ...  T-Learner LR  False\n",
              "8          1.783826           0.947208  ...  T-Learner RF   True\n",
              "9          2.512660           1.109184  ...  T-Learner RF  False\n",
              "10         4.172661           1.921242  ...  T-Learner RF   True\n",
              "11         4.378083           1.797608  ...  T-Learner RF  False\n",
              "12         2.919889           1.107028  ...     Dragonnet   True\n",
              "13         3.036828           1.033174  ...     Dragonnet  False\n",
              "\n",
              "[14 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kl0lnoxLOkU"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## QUESTION 7\n",
        "\n",
        "HOW DO THE DIFFERENT MODELS COMPARE IN TERMS OF MEAN PEHE ACCURACY? WHAT ASPECTS DO YOU THINK DETERMINE WHETHER ONE MODEL PERFOMS BETTER THAN ANOTER?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGZyDdG5S9Xa"
      },
      "source": [
        "\r\n",
        "<div class=\"alert alert-block alert-warning\">\r\n",
        "\r\n",
        "The following observations can be made on mean pehe accuracy and other aspects:\r\n",
        "\r\n",
        "- Generally, T-Learner performs well in this case in comparision to S-learner owing to the fact that it uses a tree based method and get's two means to estimate upon\r\n",
        "\r\n",
        "- We see more accuracy in RF based methods in comparision to LR based methods due to poor approximation by heterogeneity and variance function by LR\r\n",
        "\r\n",
        "- Neural networks outperfoms in mean and another criterion named pehe_score-median depicting better generalization\r\n",
        "\r\n",
        "- pehe_score-std however shows there exists some irregular distribution in data which is better expained in RF based methods both in train as well in non-train data\r\n",
        "\r\n",
        "- Average treatment effect on individual treatment shows same findings what we found in pehe\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EoN6GiAWl6s"
      },
      "source": [
        " Let's see if there is any relation between **mean & pehe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fzmpIfvaEbu"
      },
      "source": [
        "df = pd.concat([df_S_learner_LR, df_PSW_LR, df_S_learner_RF, df_T_learner_LR, df_T_learner_RF, df_causal_forest, df_dragonnet ], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "GeVV9T69V0FF",
        "outputId": "42f0f4e6-fdba-4047-9f30-afbca2f6683c"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "colors = (0,0,0)\r\n",
        "# Plot\r\n",
        "plt.scatter(df['mean_absolute-std'],df['pehe_score-std'],  c=colors, alpha=0.5)\r\n",
        "plt.title('Scatter of std pehe score')\r\n",
        "plt.xlabel('mean')\r\n",
        "plt.ylabel('pehe')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWcHboUlWznT"
      },
      "source": [
        "It has somewhat increasing relationship in mean and pehe but in terms of evaluation pehe score justifies the story"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__C6cq_PXIBf"
      },
      "source": [
        "***** References*****\r\n",
        "\r\n",
        "Strong inspiration and understanding has been taken from this [source](https://justcause.readthedocs.io/en/latest/_downloads/e054f7a0fc9cf9e680173600cb4b4350/thesis-mfranz.pdf)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4QcWXIXWKo1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}