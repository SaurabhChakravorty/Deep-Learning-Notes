{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Assignment_handout.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"tDEnfPD1LOed"},"source":["# Importing libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"id":"Snr_CjcABfhV","executionInfo":{"status":"ok","timestamp":1606318231380,"user_tz":-60,"elapsed":15611,"user":{"displayName":"Florian Ellsaesser","photoUrl":"","userId":"02356441736154567181"}},"outputId":"ae6b691c-0bda-410f-8c8f-e7efa908b995"},"source":["!pip install rpy2==2.9.6b"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting rpy2==2.9.6b\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/eb/2f7115990ee54eaa075f6c10d06a6225531b2a643b5779d30dda601cdbed/rpy2-2.9.6b0.tar.gz (194kB)\n","\r\u001b[K     |█▊                              | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 18.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 11.2MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rpy2==2.9.6b) (1.15.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from rpy2==2.9.6b) (2.11.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->rpy2==2.9.6b) (1.1.1)\n","Building wheels for collected packages: rpy2\n","  Building wheel for rpy2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rpy2: filename=rpy2-2.9.6b0-cp36-cp36m-linux_x86_64.whl size=316075 sha256=62d799aaaafe9b65f6cd9be3f1228e6b0fbd36673651e842dc8fd835c6771820\n","  Stored in directory: /root/.cache/pip/wheels/02/d6/dc/8c3faafb8cb7165a30d67f899ff7e88766e20260dda41ace88\n","Successfully built rpy2\n","Installing collected packages: rpy2\n","  Found existing installation: rpy2 3.2.7\n","    Uninstalling rpy2-3.2.7:\n","      Successfully uninstalled rpy2-3.2.7\n","Successfully installed rpy2-2.9.6b0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["rpy2"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"8mVRUQreMkgc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606318231382,"user_tz":-60,"elapsed":15324,"user":{"displayName":"Florian Ellsaesser","photoUrl":"","userId":"02356441736154567181"}},"outputId":"9ccdcd9c-b03e-4866-fe7b-0d71a521dcf3"},"source":["%load_ext rpy2.ipython"],"execution_count":3,"outputs":[{"output_type":"stream","text":["The rpy2.ipython extension is already loaded. To reload it, use:\n","  %reload_ext rpy2.ipython\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3gGjI1cMNsCX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606318424873,"user_tz":-60,"elapsed":208626,"user":{"displayName":"Florian Ellsaesser","photoUrl":"","userId":"02356441736154567181"}},"outputId":"8ca0dea8-330c-4cc6-f30e-17109d082672"},"source":["%%R\n","install.packages(\"grf\")\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n","(as ‘lib’ is unspecified)\n","\n","R[write to console]: also installing the dependencies ‘zoo’, ‘DiceKriging’, ‘lmtest’, ‘sandwich’, ‘RcppEigen’\n","\n","\n","R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/zoo_1.8-8.tar.gz'\n","\n","R[write to console]: Content type 'application/x-gzip'\n","R[write to console]:  length 849487 bytes (829 KB)\n","\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: \n","\n","R[write to console]: downloaded 829 KB\n","\n","\n","R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/DiceKriging_1.5.8.tar.gz'\n","\n","R[write to console]: Content type 'application/x-gzip'\n","R[write to console]:  length 95896 bytes (93 KB)\n","\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: \n","\n","R[write to console]: downloaded 93 KB\n","\n","\n","R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/lmtest_0.9-38.tar.gz'\n","\n","R[write to console]: Content type 'application/x-gzip'\n","R[write to console]:  length 227052 bytes (221 KB)\n","\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: \n","\n","R[write to console]: downloaded 221 KB\n","\n","\n","R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/sandwich_3.0-0.tar.gz'\n","\n","R[write to console]: Content type 'application/x-gzip'\n","R[write to console]:  length 1445320 bytes (1.4 MB)\n","\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: \n","\n","R[write to console]: downloaded 1.4 MB\n","\n","\n","R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/RcppEigen_0.3.3.7.0.tar.gz'\n","\n","R[write to console]: Content type 'application/x-gzip'\n","R[write to console]:  length 1643103 bytes (1.6 MB)\n","\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: \n","\n","R[write to console]: downloaded 1.6 MB\n","\n","\n","R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/grf_1.2.0.tar.gz'\n","\n","R[write to console]: Content type 'application/x-gzip'\n","R[write to console]:  length 136352 bytes (133 KB)\n","\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: =\n","R[write to console]: \n","\n","R[write to console]: downloaded 133 KB\n","\n","\n","R[write to console]: \n","\n","R[write to console]: \n","R[write to console]: The downloaded source packages are in\n","\t‘/tmp/Rtmp09PP7N/downloaded_packages’\n","R[write to console]: \n","R[write to console]: \n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"M2WB2i7qbusi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606318441574,"user_tz":-60,"elapsed":225159,"user":{"displayName":"Florian Ellsaesser","photoUrl":"","userId":"02356441736154567181"}},"outputId":"d7807635-b057-4c66-a1d5-a27b05356201"},"source":["!pip install justcause==0.3.2"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting justcause==0.3.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/ec/767d453b101a39a719167408e5687863fde44cee57b1a844b4ee157f707d/JustCause-0.3.2-py2.py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n","\u001b[?25hCollecting causalml\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/5a/bd02301c37208c30b8b8556281e7bf6c72b5171369f9d3405f4d273ddc29/causalml-0.9.0.tar.gz (236kB)\n","\u001b[K     |████████████████████████████████| 245kB 6.9MB/s \n","\u001b[?25hCollecting pygam\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/be/775033ef08a8945bec6ad7973b161ca909f852442e0d7cfb8d1a214de1ac/pygam-0.8.0-py2.py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 28.3MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (0.14.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.18.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (0.22.2.post1)\n","Requirement already satisfied: rpy2 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (2.9.6b0)\n","Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from justcause==0.3.2) (1.1.4)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (50.3.2)\n","Requirement already satisfied: pip>=10.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (19.3.1)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (1.4.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (3.2.2)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.10.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.11.0)\n","Requirement already satisfied: Cython>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.29.21)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.90)\n","Requirement already satisfied: pydotplus in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (4.41.1)\n","Collecting shap\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/a3/c0eab9dd6a894165e2cb87504ff5b2710ac5ede3447d9138620b7341b6a2/shap-0.37.0.tar.gz (326kB)\n","\u001b[K     |████████████████████████████████| 327kB 24.2MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (0.3.3)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.2.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (20.4)\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.4.3)\n","Requirement already satisfied: tensorflow>=1.15.2 in /usr/local/lib/python3.6/dist-packages (from causalml->justcause==0.3.2) (2.3.0)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from pygam->justcause==0.3.2) (3.38.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pygam->justcause==0.3.2) (0.16.0)\n","Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow->justcause==0.3.2) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->justcause==0.3.2) (0.17.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from rpy2->justcause==0.3.2) (2.11.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->justcause==0.3.2) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->justcause==0.3.2) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->causalml->justcause==0.3.2) (2.4.7)\n","Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.9.0->causalml->justcause==0.3.2) (0.5.1)\n","Collecting slicer==0.0.3\n","  Downloading https://files.pythonhosted.org/packages/02/a6/c708c5a0f338e99cfbcb6288b88794525548e4fc1b8457feec2c552a81a4/slicer-0.0.3-py3-none-any.whl\n","Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from shap->causalml->justcause==0.3.2) (0.48.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause==0.3.2) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->causalml->justcause==0.3.2) (3.13)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.3.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.10.0)\n","Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.1.2)\n","Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.3.0)\n","Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.3.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.3.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.1.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.6.3)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.2.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.12.4)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.33.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.35.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.12.1)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->pygam->justcause==0.3.2) (2.4.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->rpy2->justcause==0.3.2) (1.1.1)\n","Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->shap->causalml->justcause==0.3.2) (0.31.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.4.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.3.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.7.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.17.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (2.10)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (4.1.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.2.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (3.4.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=1.15.2->causalml->justcause==0.3.2) (0.4.8)\n","Building wheels for collected packages: causalml, shap\n","  Building wheel for causalml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for causalml: filename=causalml-0.9.0-cp36-cp36m-linux_x86_64.whl size=482223 sha256=bc4708d1ed666ae6a8e32f8058ff5a557e42749d9d23b90b03f3e8b833d52865\n","  Stored in directory: /root/.cache/pip/wheels/6d/c4/e2/451f0ebc2f9a7540256b0a705a2fd09864893131aee58d5af9\n","  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shap: filename=shap-0.37.0-cp36-cp36m-linux_x86_64.whl size=463928 sha256=38e660c5ce5e8793f7915a3958b31fb1433aaadb55204069a087718b225b6965\n","  Stored in directory: /root/.cache/pip/wheels/df/ad/b0/aa7815ec68850d66551ef618095eccb962c8f6022f1d3dd989\n","Successfully built causalml shap\n","Installing collected packages: slicer, shap, pygam, causalml, justcause\n","Successfully installed causalml-0.9.0 justcause-0.3.2 pygam-0.8.0 shap-0.37.0 slicer-0.0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lPzAPlfebvbX"},"source":["import os\n","os.kill(os.getpid(), 9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4UpBS6mNOCW"},"source":["from justcause.data import Col\n","from justcause.data.sets import load_ihdp\n","from justcause.metrics import pehe_score, mean_absolute\n","from justcause.evaluation import calc_scores, summarize_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CfvwSmoLOe-"},"source":["%load_ext autoreload\n","\n","%autoreload 2\n","\n","# Loading all required packages \n","import itertools\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jufTu9k5LOfL"},"source":["#### Infant Health Development Program Data-Set used in this exercise\n","\n","- Original study constructed to study the effect of special child care for low birthweight, premature infants.\n","- In total, six continuous and 19 binary pretreatment variables\n","- Using the covariates of all instances in both treatment groups, the potential outcomes are generated synthetically\n","- Finally, manipulation of observational study by omitting a non-random set of samples from the treatment group.\n","- The way the subset is generated from the experimental data does not ensure complete overlap - latent confounder\n","- Specifically, the observational subset is created by throwing away the set of all children with nonwhite mothers from the treatment group\n","- Following data generation process used for potentia outcomes\n","- After the adaptions from Hill, we are left with 139 instances in the treated group and 608 instances in the control group."]},{"cell_type":"markdown","metadata":{"id":"yLQd0vO_LOfP"},"source":["# 1. Running the causal models"]},{"cell_type":"code","metadata":{"id":"yg5b4PoxLOfU"},"source":["# We Import the IHDP data-set \n","# There are 1000 replications in this data-set, each with a different individual treament effect\n","# produced from an underlying generative function. \n","# Check out https://justcause.readthedocs.io/en/latest/\n","\n","\n","# We load 100 of the 1000 data-sets\n","replications = load_ihdp(select_rep=np.arange(100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyTH9JV_LOfh"},"source":["# Defining global parameters\n","train_size = 0.8        # Size of the training data-set \n","random_state = 42        # Setting the random state\n","\n","n= 0       # number of the data-sets we look at \n","\n","metrics = [pehe_score, mean_absolute]    ## Defining the metrics that will be calculated below"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vQWUbE6HLOfw"},"source":["## 1.1 S-Learner Linear Regression"]},{"cell_type":"code","metadata":{"id":"rT9UjEovLOf0"},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import SLearner\n","\n","\n","# Defining the S-Learner function that returns the ITE\n","# We define a function that takes the data, splits it up and returns individual treatment effect accuracies for the train and the test data-set\n","# The function takes each, the train and test data separately and selects the relevant variales and coverts them into np arrays\n","# The relevant variables have the followings names: x (the covariates), t (the treatment), y (the outcome)\n","# Note that the treatment needs to be explicityl defined\n","\n","\n","\n","def basic_slearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    slearner = model     # Select linear regression as a method to find the ITE for the S-Learner\n","    slearner.fit(train_X, train_t, train_y)      # Fitting the s-learner with linear regression\n","    return (\n","        slearner.predict_ite(train_X, train_t, train_y),   # Returning the predicting values for ITE for train\n","        slearner.predict_ite(test_X, test_t, test_y)       # Returning the prediction values for ITE for test\n","    )\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxwuoQxwLOgD"},"source":["results_df = list()     # We define the list that contains the results\n","test_scores = list()    # Storing the test scores in a list\n","train_scores = list()   # Storing the train scores in a list\n","\n","\n","# Here we define the model that is going to be used for the S-learner\n","# Please instantiate linear regression for the simple learner\n","\n","\n","##-----------------Question------------------###\n","# Pass a LinearRegression Model into the S-Learner\n","# No particular parameter-settings necessary\n","\n","\n","model=SLearner()\n","\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = basic_slearner(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'S-Learner LR', 'train': True})\n","test_result.update({'method': 'S-Learner LR', 'train': False})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Gb37xoDuLOgQ","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"7588f3c1-e31b-4edc-f390-3aec187e588e"},"source":["df_S_learner_LR=pd.DataFrame([train_result, test_result])\n","df_S_learner_LR"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.633660</td>\n","      <td>2.623297</td>\n","      <td>8.362125</td>\n","      <td>0.732443</td>\n","      <td>0.238185</td>\n","      <td>1.493276</td>\n","      <td>S-Learner LR</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5.625971</td>\n","      <td>2.635993</td>\n","      <td>8.213626</td>\n","      <td>1.292668</td>\n","      <td>0.396246</td>\n","      <td>2.474603</td>\n","      <td>S-Learner LR</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...        method  train\n","0         5.633660           2.623297  ...  S-Learner LR   True\n","1         5.625971           2.635993  ...  S-Learner LR  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"yfhFVRd4LOgk"},"source":["### 1.1.1 S-Learner Visualization"]},{"cell_type":"code","metadata":{"id":"xbbouLxaLOgo"},"source":["# We run the same analysis again but only on an indvidual run of the data\n","# The reason is that the data generating process is varied every time,... \n","# ...so we can only look at individual runs of the ITE effect\n","results_df = list()    # We define the list that contains the results\n","test_scores = list()   # Storing the test scores in a list\n","train_scores = list()  # Storing the train scores in a list\n","\n","\n","#for rep in replications:\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state     # Use train_test_split  to split the data-set (replications[n]) \n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = basic_slearner(train, test, model)         # using the pre-defined basic learner function to retunr train, test\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))    # Using the just cause API to calcualte the scores from the estimate ITE for the training data\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))     # Using the just cause API to calcualte the scores from the estimate ITE for the test data\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)   #summary of the scores \n","train_result.update({'method': 'S-Learner LR', 'train': True})\n","test_result.update({'method': 'S-Learner LR', 'train': False})\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AsxWY_9mLOg2","colab":{"base_uri":"https://localhost:8080/","height":333},"outputId":"650b3d82-b748-434c-faf7-077865128739"},"source":["# Importing Matplotlib \n","import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()\n","\n","# If the treatment effect is perfectly represented there should be a 45 degree line!"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"HSiTTv7KLOhI"},"source":["## QUESTION 1\n","\n","IS THE S-LEARNER WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"dJe9Cnj6LOhL"},"source":["## 1.2 Propensity Score Weighing with Linear Regression"]},{"cell_type":"code","metadata":{"id":"w7TpRNPdLOhP"},"source":["# Importing the relevant PSWEstimator\n","\n","from justcause.learners import PSWEstimator\n","\n","\n","#Defining the Propoensity Score weighing function that returns the ITE\n","\n","def propensity_score_weighing(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    pswestimator = model\n","\n","    return (\n","        pswestimator.estimate_ate(train_X, train_t, train_y),\n","        pswestimator.estimate_ate(test_X, test_t, test_y)\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNoIPNfgLOhb"},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","\n","\n","model = PSWEstimator(propensity_learner=None, delta=0.001)\n","\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = propensity_score_weighing(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'PSW', 'train': True})\n","test_result.update({'method': 'PSW', 'train': False})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"caopoaQrLOhm","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"a4c6ae4f-43e0-4893-a535-3dce75246679"},"source":["df_PSW_LR=pd.DataFrame([train_result, test_result])\n","df_PSW_LR"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.595322</td>\n","      <td>2.537818</td>\n","      <td>8.244302</td>\n","      <td>0.412006</td>\n","      <td>0.284332</td>\n","      <td>0.457697</td>\n","      <td>PSW</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.837997</td>\n","      <td>3.484394</td>\n","      <td>8.323623</td>\n","      <td>3.783440</td>\n","      <td>2.649187</td>\n","      <td>3.225824</td>\n","      <td>PSW</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...  method  train\n","0         5.595322           2.537818  ...     PSW   True\n","1         6.837997           3.484394  ...     PSW  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"jAcuhZhKLOhy"},"source":["## 1.3 S-Learner Random Forest"]},{"cell_type":"code","metadata":{"id":"wtiDtGbyLOh3"},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","\n","\n","# Importing the relevant S-Learner estimator\n","from justcause.learners import SLearner\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def basic_slearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    slearner = model  \n","    slearner.fit(train_X, train_t, train_y)\n","    return (\n","        slearner.predict_ite(train_X, train_t, train_y),\n","        slearner.predict_ite(test_X, test_t, test_y)\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XW0XLJ0ELOiG"},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","#---------------------------Question--------------------------------#\n","# Pass a RandomForestRegressor into the S-learner\n","\n","\n","\n","model = SLearner()\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = basic_slearner(train, test, model )\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'S-Learner RF', 'train': True})\n","test_result.update({'method': 'S-Learner RF', 'train': False})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"QgIkGA9MLOiR","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"b33241b5-4214-4a0d-d800-1d7322c986f5"},"source":["df_S_learner_RF=pd.DataFrame([train_result, test_result])\n","df_S_learner_RF"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.133693</td>\n","      <td>1.047904</td>\n","      <td>4.861394</td>\n","      <td>0.511598</td>\n","      <td>0.125918</td>\n","      <td>0.971366</td>\n","      <td>S-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.320106</td>\n","      <td>1.234456</td>\n","      <td>5.197415</td>\n","      <td>0.451143</td>\n","      <td>0.137152</td>\n","      <td>1.047357</td>\n","      <td>S-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...        method  train\n","0         3.133693           1.047904  ...  S-Learner RF   True\n","1         3.320106           1.234456  ...  S-Learner RF  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"LyU9_8rcLOid"},"source":["### 1.3.1 Random Forest Visualization"]},{"cell_type":"code","metadata":{"id":"Meb4UZjELOig"},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","\n","\n","# Importing the relevant SLearner module\n","from justcause.learners import SLearner\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def basic_slearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    slearner = model\n","    slearner.fit(train_X, train_t, train_y)\n","    return (\n","        slearner.predict_ite(train_X, train_t, train_y),\n","        slearner.predict_ite(test_X, test_t, test_y)\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"eF2mk_AxLOin","colab":{"base_uri":"https://localhost:8080/","height":333},"outputId":"685c923b-2bdf-4119-a5d2-331dd8bc9e07"},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Q0t4K4CfLOit"},"source":["## QUESTION 2\n","\n","IS THE S-LEARNER WITH RANDOM FOREST ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"vowzj2teLOiw"},"source":["## 1.4 T-Learner Linear Regression"]},{"cell_type":"code","metadata":{"id":"811XNGgLLOix"},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import TLearner\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def basic_tlearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    tlearner = model\n","    tlearner.fit(train_X, train_t, train_y)\n","    return (\n","        tlearner.predict_ite(train_X, train_t, train_y),\n","        tlearner.predict_ite(test_X, test_t, test_y)\n","    )\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_GCgPWtLOi2"},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","#------------------Question------------------------------#\n","# Pass linear regression into the T-Learner\n","\n","\n","\n","tlearner = TLearner()\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = basic_tlearner(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner LR', 'train': True})\n","test_result.update({'method': 'T-Learner LR', 'train': False})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"tMeJsu4zLOi6","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"0d21fce0-603d-48ce-816e-6e61a04d1515"},"source":["df_T_learner_LR=pd.DataFrame([train_result, test_result])\n","df_T_learner_LR"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.106716</td>\n","      <td>1.05165</td>\n","      <td>4.788258</td>\n","      <td>0.500543</td>\n","      <td>0.124951</td>\n","      <td>0.933657</td>\n","      <td>T-Learner LR</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.311096</td>\n","      <td>1.26711</td>\n","      <td>5.155404</td>\n","      <td>0.445728</td>\n","      <td>0.134426</td>\n","      <td>1.022285</td>\n","      <td>T-Learner LR</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...        method  train\n","0         3.106716            1.05165  ...  T-Learner LR   True\n","1         3.311096            1.26711  ...  T-Learner LR  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"-k49nGyQLOi9"},"source":["### 1.4.1 T-Learner Linear Regression Visualization"]},{"cell_type":"code","metadata":{"id":"9u2DMEWTLOi-"},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state\n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = basic_tlearner(train, test, model)\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner LR', 'train': True})\n","test_result.update({'method': 'T-Learner LR', 'train': False})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VtFgO5PZLOjC","colab":{"base_uri":"https://localhost:8080/","height":333},"outputId":"d3d347ea-f35c-406a-99d5-0cf68b3f60f8"},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nygoAY4xJUP8Prr2RJMobhfMAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"61rEzJoLLOjF"},"source":["## QUESTION 3\n","\n","IS THE T-LEARNER WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"RO36FBY1LOjG"},"source":["## 1.5 T-Learner Random Forest"]},{"cell_type":"code","metadata":{"id":"pjA-HNpCLOjH"},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import TLearner\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def basic_tlearner(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    tlearner = model\n","    tlearner.fit(train_X, train_t, train_y)\n","    return (\n","        tlearner.predict_ite(train_X, train_t, train_y),\n","        tlearner.predict_ite(test_X, test_t, test_y)\n","    )\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMXoNfexLOjL","colab":{"base_uri":"https://localhost:8080/","height":239},"outputId":"3d970424-5f66-4356-9914-533b70177ba4"},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","#--------------------Question----------------------------------#\n","# Pass a Random Forest into the T-Learner\n","\n","model = TLearner()\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = basic_tlearner(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner RF', 'train': True})\n","test_result.update({'method': 'T-Learner RF', 'train': False})"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dbb5c9e28b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreplications\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'TLearner' is not defined"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"psT80zSaLOjP","colab":{"base_uri":"https://localhost:8080/","height":186},"outputId":"9ef2ec3d-8ca3-46a0-8dcb-a2d38028c5e0"},"source":["df_T_learner_RF=pd.DataFrame([train_result, test_result])\n","df_T_learner_RF"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-0007b694cf31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_T_learner_RF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_T_learner_RF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"XSU2Kyj9LOjT"},"source":["### 1.5.1 T-Learner with Random Forrest Visualization"]},{"cell_type":"code","metadata":{"id":"BKNP1x2KLOjU"},"source":["results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state\n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = basic_tlearner(train, test, model)\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner RF', 'train': True})\n","test_result.update({'method': 'T-Learner RF', 'train': False})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ha8GizLtLOjY","colab":{"base_uri":"https://localhost:8080/","height":333},"outputId":"8d81acd6-73b1-420c-d4c0-010b9f989deb"},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"6x_BryEaLOjd"},"source":["## QUESTION 4\n","\n","IS THE T-LEARNER WITH RANDOM FOREST ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"jh0sdhdKLOjf"},"source":["## 1.6 Causal Forest"]},{"cell_type":"code","metadata":{"id":"8GWzTUTlLOjg"},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import CausalForest\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def causal_forest(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    causalforest = model\n","    causalforest.fit(train_X, train_t, train_y)\n","    return (\n","        causalforest.predict_ite(train_X, train_t, train_y),\n","        causalforest.predict_ite(test_X, test_t, test_y)\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZY-qvwfHLOjk"},"source":["random_state = 1\n","\n","results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","model = CausalForest(random_state=random_state)\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = causal_forest(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner RF', 'train': True})\n","test_result.update({'method': 'T-Learner RF', 'train': False})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B78davrGLOjn","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"d6d325e0-2af7-4b8c-816e-15dd989cada2"},"source":["df_causal_forest=pd.DataFrame([train_result, test_result])\n","df_causal_forest"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.172661</td>\n","      <td>1.921242</td>\n","      <td>6.330031</td>\n","      <td>0.441738</td>\n","      <td>0.198733</td>\n","      <td>0.823171</td>\n","      <td>T-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.378083</td>\n","      <td>1.797608</td>\n","      <td>6.606150</td>\n","      <td>0.683703</td>\n","      <td>0.233695</td>\n","      <td>1.354973</td>\n","      <td>T-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...        method  train\n","0         4.172661           1.921242  ...  T-Learner RF   True\n","1         4.378083           1.797608  ...  T-Learner RF  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"GfQBHXUTLOjt"},"source":["### 1.6.1 Causal Forest Visualization "]},{"cell_type":"code","metadata":{"id":"Z4c3Y-XDLOju"},"source":["random_state = 1\n","\n","results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state\n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = causal_forest(train, test, model)\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'T-Learner RF', 'train': True})\n","test_result.update({'method': 'T-Learner RF', 'train': False})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3mIANgDLOjw","colab":{"base_uri":"https://localhost:8080/","height":333},"outputId":"a815666e-0dcb-4cd8-a402-c0c59fc1a45e"},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAmOMyXL/F6/vfNvN7Y1mAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"3ingDVHQLOj0"},"source":["## QUESTION 5\n","\n","IS THE CAUSAL FOREST WITH LINEAR REGRESSION ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"JnFLlHMKLOj1"},"source":["## 1.7 Neural Network"]},{"cell_type":"code","metadata":{"id":"QXE4Usi1LOj2"},"source":["# Importing the relevant SLearner module\n","\n","from justcause.learners import DragonNet\n","\n","\n","#Defining the S-Learner function that returns the ITE\n","\n","def causal_forest(train, test, model):\n","    \"\"\" \"\"\"\n","    train_X, train_t, train_y = train.np.X, train.np.t, train.np.y\n","    test_X, test_t, test_y = test.np.X, test.np.t, test.np.y\n","\n","    dragonnet = model\n","    dragonnet.fit(train_X, train_t, train_y)\n","    return (\n","        dragonnet.predict_ite(train_X, train_t, train_y),\n","        dragonnet.predict_ite(test_X, test_t, test_y)\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeJ5_9NBLOj5","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e675c6ef-a0c7-486f-e8c5-267f448b872b"},"source":["random_state = 1\n","\n","results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","#---------------------------Question----------------------------#\n","# Set the model to the DragonNet neural network from JustCause\n","\n","\n","model = \n","\n","\n","for rep in replications:\n","\n","    train, test = train_test_split(\n","        rep, train_size=train_size, random_state=random_state\n","    )\n","\n","    # REPLACE this with the function you implemented and want to evaluate\n","    train_ite, test_ite = causal_forest(train, test, model)\n","\n","    # Calculate the scores and append them to a dataframe\n","    train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","    test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'Dragonnet', 'train': True})\n","test_result.update({'method': 'Dragonnet', 'train': False})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch 48/50\n","537/537 [==============================] - 0s 61us/step - loss: 1628.4845 - regression_loss: 691.1127 - val_loss: 276.8921 - val_regression_loss: 120.7643\n","Epoch 49/50\n","537/537 [==============================] - 0s 77us/step - loss: 1622.6759 - regression_loss: 689.6605 - val_loss: 275.0143 - val_regression_loss: 119.7951\n","Epoch 50/50\n","537/537 [==============================] - 0s 70us/step - loss: 1558.4554 - regression_loss: 657.8106 - val_loss: 273.3792 - val_regression_loss: 118.9410\n","***************************** elapsed_time is:  3.818852663040161\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 171939.8187 - regression_loss: 86364.8814 - val_loss: 17097.1992 - val_regression_loss: 8591.0752\n","Epoch 2/50\n","537/537 [==============================] - 0s 62us/step - loss: 149083.0884 - regression_loss: 74918.7058 - val_loss: 14047.2402 - val_regression_loss: 7062.7783\n","Epoch 3/50\n","537/537 [==============================] - 0s 64us/step - loss: 121654.0672 - regression_loss: 61200.8664 - val_loss: 10230.0479 - val_regression_loss: 5145.4824\n","Epoch 4/50\n","537/537 [==============================] - 0s 60us/step - loss: 88490.8674 - regression_loss: 44547.5732 - val_loss: 6076.2539 - val_regression_loss: 3048.5037\n","Epoch 5/50\n","537/537 [==============================] - 0s 56us/step - loss: 52329.6475 - regression_loss: 26307.3379 - val_loss: 3016.2871 - val_regression_loss: 1471.2446\n","Epoch 6/50\n","537/537 [==============================] - 0s 64us/step - loss: 24406.6873 - regression_loss: 11986.3216 - val_loss: 3144.1858 - val_regression_loss: 1436.9562\n","Epoch 7/50\n","537/537 [==============================] - 0s 59us/step - loss: 21414.1682 - regression_loss: 9701.3422 - val_loss: 4342.4893 - val_regression_loss: 1989.2384\n","Epoch 8/50\n","537/537 [==============================] - 0s 61us/step - loss: 30205.1238 - regression_loss: 13719.8104 - val_loss: 3544.7864 - val_regression_loss: 1640.5367\n","Epoch 9/50\n","537/537 [==============================] - 0s 64us/step - loss: 24540.3828 - regression_loss: 11289.5803 - val_loss: 2133.5483 - val_regression_loss: 996.9889\n","Epoch 10/50\n","537/537 [==============================] - 0s 60us/step - loss: 14537.4714 - regression_loss: 6778.8414 - val_loss: 1411.7555 - val_regression_loss: 675.1291\n","Epoch 11/50\n","537/537 [==============================] - 0s 67us/step - loss: 9861.7714 - regression_loss: 4754.6459 - val_loss: 1436.7783 - val_regression_loss: 706.6097\n","Epoch 12/50\n","537/537 [==============================] - 0s 68us/step - loss: 11149.8901 - regression_loss: 5541.7116 - val_loss: 1678.6500 - val_regression_loss: 834.1602\n","Epoch 13/50\n","537/537 [==============================] - 0s 70us/step - loss: 13414.7619 - regression_loss: 6724.0673 - val_loss: 1747.5012 - val_regression_loss: 868.2779\n","Epoch 14/50\n","537/537 [==============================] - 0s 74us/step - loss: 13819.9478 - regression_loss: 6917.5501 - val_loss: 1575.2235 - val_regression_loss: 778.1464\n","Epoch 15/50\n","537/537 [==============================] - 0s 65us/step - loss: 11995.8713 - regression_loss: 5969.7059 - val_loss: 1295.1403 - val_regression_loss: 632.3006\n","Epoch 16/50\n","537/537 [==============================] - 0s 61us/step - loss: 9353.0726 - regression_loss: 4603.0462 - val_loss: 1093.0372 - val_regression_loss: 524.9468\n","Epoch 17/50\n","537/537 [==============================] - 0s 66us/step - loss: 7395.1472 - regression_loss: 3569.3057 - val_loss: 1079.9034 - val_regression_loss: 512.7794\n","Epoch 18/50\n","537/537 [==============================] - 0s 67us/step - loss: 6941.4331 - regression_loss: 3297.1351 - val_loss: 1195.4330 - val_regression_loss: 566.7671\n","Epoch 19/50\n","537/537 [==============================] - 0s 71us/step - loss: 7594.4192 - regression_loss: 3591.7372 - val_loss: 1259.7913 - val_regression_loss: 597.6638\n","Epoch 20/50\n","537/537 [==============================] - 0s 64us/step - loss: 8061.2491 - regression_loss: 3811.7497 - val_loss: 1176.3158 - val_regression_loss: 557.0117\n","Epoch 21/50\n","537/537 [==============================] - 0s 64us/step - loss: 7487.5613 - regression_loss: 3533.6735 - val_loss: 1019.7910 - val_regression_loss: 481.3404\n","Epoch 22/50\n","537/537 [==============================] - 0s 63us/step - loss: 6529.9058 - regression_loss: 3076.0289 - val_loss: 904.5415 - val_regression_loss: 426.9024\n","Epoch 23/50\n","537/537 [==============================] - 0s 65us/step - loss: 5686.1024 - regression_loss: 2679.1347 - val_loss: 864.6414 - val_regression_loss: 409.9772\n","Epoch 24/50\n","537/537 [==============================] - 0s 65us/step - loss: 5675.1475 - regression_loss: 2698.7575 - val_loss: 857.2730 - val_regression_loss: 408.6140\n","Epoch 25/50\n","537/537 [==============================] - 0s 74us/step - loss: 5739.4079 - regression_loss: 2750.8709 - val_loss: 836.8947 - val_regression_loss: 399.7914\n","Epoch 26/50\n","537/537 [==============================] - 0s 74us/step - loss: 5651.8364 - regression_loss: 2715.7153 - val_loss: 792.6565 - val_regression_loss: 378.0793\n","Epoch 27/50\n","537/537 [==============================] - 0s 62us/step - loss: 5323.6532 - regression_loss: 2554.5221 - val_loss: 751.2565 - val_regression_loss: 356.9568\n","Epoch 28/50\n","537/537 [==============================] - 0s 115us/step - loss: 4870.4656 - regression_loss: 2332.0723 - val_loss: 732.2193 - val_regression_loss: 346.3621\n","Epoch 29/50\n","537/537 [==============================] - 0s 60us/step - loss: 4712.7622 - regression_loss: 2239.0788 - val_loss: 729.8823 - val_regression_loss: 343.8919\n","Epoch 30/50\n","537/537 [==============================] - 0s 64us/step - loss: 4591.5829 - regression_loss: 2169.4958 - val_loss: 723.1901 - val_regression_loss: 339.3592\n","Epoch 31/50\n","537/537 [==============================] - 0s 59us/step - loss: 4414.3896 - regression_loss: 2066.4398 - val_loss: 701.3755 - val_regression_loss: 327.7353\n","Epoch 32/50\n","537/537 [==============================] - 0s 58us/step - loss: 4216.0875 - regression_loss: 1960.0824 - val_loss: 677.8915 - val_regression_loss: 315.8684\n","Epoch 33/50\n","537/537 [==============================] - 0s 63us/step - loss: 3979.2128 - regression_loss: 1840.3416 - val_loss: 660.6308 - val_regression_loss: 307.7026\n","Epoch 34/50\n","537/537 [==============================] - 0s 63us/step - loss: 3927.0302 - regression_loss: 1813.5738 - val_loss: 646.7920 - val_regression_loss: 301.6411\n","Epoch 35/50\n","537/537 [==============================] - 0s 64us/step - loss: 3633.6525 - regression_loss: 1677.6755 - val_loss: 631.4438 - val_regression_loss: 294.8736\n","Epoch 36/50\n","537/537 [==============================] - 0s 62us/step - loss: 3477.0886 - regression_loss: 1608.0031 - val_loss: 614.1389 - val_regression_loss: 287.0065\n","Epoch 37/50\n","537/537 [==============================] - 0s 65us/step - loss: 3481.8266 - regression_loss: 1610.0207 - val_loss: 601.2313 - val_regression_loss: 281.1506\n","Epoch 38/50\n","537/537 [==============================] - 0s 66us/step - loss: 3298.1230 - regression_loss: 1526.5472 - val_loss: 593.3922 - val_regression_loss: 277.5748\n","Epoch 39/50\n","537/537 [==============================] - 0s 59us/step - loss: 3269.8344 - regression_loss: 1515.7583 - val_loss: 585.1987 - val_regression_loss: 273.5641\n","Epoch 40/50\n","537/537 [==============================] - 0s 65us/step - loss: 3211.5169 - regression_loss: 1490.0309 - val_loss: 571.1851 - val_regression_loss: 266.4572\n","Epoch 41/50\n","537/537 [==============================] - 0s 73us/step - loss: 3051.9169 - regression_loss: 1405.7637 - val_loss: 557.1275 - val_regression_loss: 259.3293\n","Epoch 42/50\n","537/537 [==============================] - 0s 64us/step - loss: 2962.7269 - regression_loss: 1360.0537 - val_loss: 546.8922 - val_regression_loss: 254.0330\n","Epoch 43/50\n","537/537 [==============================] - 0s 66us/step - loss: 2769.3651 - regression_loss: 1257.9829 - val_loss: 539.1740 - val_regression_loss: 250.0691\n","Epoch 44/50\n","537/537 [==============================] - 0s 69us/step - loss: 2801.6201 - regression_loss: 1273.6214 - val_loss: 530.7721 - val_regression_loss: 245.8264\n","Epoch 45/50\n","537/537 [==============================] - 0s 66us/step - loss: 2667.1138 - regression_loss: 1198.8741 - val_loss: 523.5059 - val_regression_loss: 242.3417\n","Epoch 46/50\n","537/537 [==============================] - 0s 64us/step - loss: 2649.0346 - regression_loss: 1192.7800 - val_loss: 516.1238 - val_regression_loss: 238.9947\n","Epoch 47/50\n","537/537 [==============================] - 0s 65us/step - loss: 2537.8644 - regression_loss: 1138.6900 - val_loss: 508.4839 - val_regression_loss: 235.6610\n","Epoch 48/50\n","537/537 [==============================] - 0s 69us/step - loss: 2478.8231 - regression_loss: 1114.8654 - val_loss: 499.2387 - val_regression_loss: 231.5538\n","Epoch 49/50\n","537/537 [==============================] - 0s 69us/step - loss: 2441.1699 - regression_loss: 1099.2074 - val_loss: 489.9414 - val_regression_loss: 227.3439\n","Epoch 50/50\n","537/537 [==============================] - 0s 61us/step - loss: 2334.0566 - regression_loss: 1048.7398 - val_loss: 481.6129 - val_regression_loss: 223.3508\n","***************************** elapsed_time is:  4.2032716274261475\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 847629.1543 - regression_loss: 423740.2278 - val_loss: 95035.1953 - val_regression_loss: 47508.5195\n","Epoch 2/50\n","537/537 [==============================] - 0s 61us/step - loss: 797921.0467 - regression_loss: 398899.4050 - val_loss: 88394.6562 - val_regression_loss: 44189.5078\n","Epoch 3/50\n","537/537 [==============================] - 0s 59us/step - loss: 760169.3564 - regression_loss: 380035.6890 - val_loss: 79021.8750 - val_regression_loss: 39503.6094\n","Epoch 4/50\n","537/537 [==============================] - 0s 61us/step - loss: 678608.7788 - regression_loss: 339262.9969 - val_loss: 66478.3984 - val_regression_loss: 33231.3047\n","Epoch 5/50\n","537/537 [==============================] - 0s 53us/step - loss: 577924.2442 - regression_loss: 288913.6748 - val_loss: 51398.7695 - val_regression_loss: 25689.6113\n","Epoch 6/50\n","537/537 [==============================] - 0s 61us/step - loss: 454075.4946 - regression_loss: 226975.5046 - val_loss: 35429.8438 - val_regression_loss: 17701.5410\n","Epoch 7/50\n","537/537 [==============================] - 0s 59us/step - loss: 310948.3061 - regression_loss: 155377.4832 - val_loss: 21323.6621 - val_regression_loss: 10641.7314\n","Epoch 8/50\n","537/537 [==============================] - 0s 57us/step - loss: 188700.3245 - regression_loss: 94201.3473 - val_loss: 13540.9932 - val_regression_loss: 6738.0923\n","Epoch 9/50\n","537/537 [==============================] - 0s 56us/step - loss: 137999.9165 - regression_loss: 68761.5256 - val_loss: 16408.7949 - val_regression_loss: 8158.2754\n","Epoch 10/50\n","537/537 [==============================] - 0s 61us/step - loss: 149957.0993 - regression_loss: 74632.7998 - val_loss: 20510.9219 - val_regression_loss: 10208.1240\n","Epoch 11/50\n","537/537 [==============================] - 0s 69us/step - loss: 187051.5813 - regression_loss: 93171.9155 - val_loss: 17252.4238 - val_regression_loss: 8588.3213\n","Epoch 12/50\n","537/537 [==============================] - 0s 67us/step - loss: 163470.5299 - regression_loss: 81459.4719 - val_loss: 11756.5693 - val_regression_loss: 5850.7300\n","Epoch 13/50\n","537/537 [==============================] - 0s 59us/step - loss: 128309.7883 - regression_loss: 63964.3603 - val_loss: 8992.4219 - val_regression_loss: 4475.9336\n","Epoch 14/50\n","537/537 [==============================] - 0s 59us/step - loss: 107459.7462 - regression_loss: 53596.0740 - val_loss: 9054.6816 - val_regression_loss: 4511.0713\n","Epoch 15/50\n","537/537 [==============================] - 0s 61us/step - loss: 114265.7530 - regression_loss: 57029.3174 - val_loss: 10158.1826 - val_regression_loss: 5064.3999\n","Epoch 16/50\n","537/537 [==============================] - 0s 64us/step - loss: 119264.5917 - regression_loss: 59541.1413 - val_loss: 10692.5498 - val_regression_loss: 5331.3896\n","Epoch 17/50\n","537/537 [==============================] - 0s 63us/step - loss: 124885.3303 - regression_loss: 62345.6275 - val_loss: 10165.2979 - val_regression_loss: 5066.3262\n","Epoch 18/50\n","537/537 [==============================] - 0s 67us/step - loss: 118154.0669 - regression_loss: 58966.4511 - val_loss: 9008.1953 - val_regression_loss: 4485.6748\n","Epoch 19/50\n","537/537 [==============================] - 0s 66us/step - loss: 107276.5945 - regression_loss: 53508.0574 - val_loss: 7926.9038 - val_regression_loss: 3942.8411\n","Epoch 20/50\n","537/537 [==============================] - 0s 64us/step - loss: 95644.5314 - regression_loss: 47680.2564 - val_loss: 7493.1235 - val_regression_loss: 3724.1394\n","Epoch 21/50\n","537/537 [==============================] - 0s 60us/step - loss: 88969.0437 - regression_loss: 44329.9412 - val_loss: 7755.2778 - val_regression_loss: 3854.1099\n","Epoch 22/50\n","537/537 [==============================] - 0s 62us/step - loss: 89759.4936 - regression_loss: 44717.2752 - val_loss: 8131.3521 - val_regression_loss: 4041.9194\n","Epoch 23/50\n","537/537 [==============================] - 0s 65us/step - loss: 91475.0261 - regression_loss: 45576.9548 - val_loss: 7944.8667 - val_regression_loss: 3949.2278\n","Epoch 24/50\n","537/537 [==============================] - 0s 71us/step - loss: 89037.2380 - regression_loss: 44364.2802 - val_loss: 7211.0723 - val_regression_loss: 3583.3525\n","Epoch 25/50\n","537/537 [==============================] - 0s 58us/step - loss: 81421.4957 - regression_loss: 40562.8436 - val_loss: 6442.8169 - val_regression_loss: 3200.4832\n","Epoch 26/50\n","537/537 [==============================] - 0s 66us/step - loss: 76036.7523 - regression_loss: 37877.9300 - val_loss: 5990.7959 - val_regression_loss: 2975.7300\n","Epoch 27/50\n","537/537 [==============================] - 0s 64us/step - loss: 71368.9588 - regression_loss: 35549.4671 - val_loss: 5794.4092 - val_regression_loss: 2878.6052\n","Epoch 28/50\n","537/537 [==============================] - 0s 62us/step - loss: 69427.5685 - regression_loss: 34587.2215 - val_loss: 5631.6079 - val_regression_loss: 2797.9141\n","Epoch 29/50\n","537/537 [==============================] - 0s 60us/step - loss: 66873.5481 - regression_loss: 33313.9443 - val_loss: 5397.8525 - val_regression_loss: 2681.3164\n","Epoch 30/50\n","537/537 [==============================] - 0s 62us/step - loss: 61373.2356 - regression_loss: 30569.6776 - val_loss: 5186.6050 - val_regression_loss: 2575.5879\n","Epoch 31/50\n","537/537 [==============================] - 0s 60us/step - loss: 57793.3768 - regression_loss: 28778.6579 - val_loss: 5129.3950 - val_regression_loss: 2546.6147\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 52507.6227 - regression_loss: 26130.7102 - val_loss: 5152.8442 - val_regression_loss: 2557.8960\n","Epoch 33/50\n","537/537 [==============================] - 0s 62us/step - loss: 51038.6819 - regression_loss: 25394.7993 - val_loss: 5040.9736 - val_regression_loss: 2501.5876\n","Epoch 34/50\n","537/537 [==============================] - 0s 83us/step - loss: 47955.7712 - regression_loss: 23848.6537 - val_loss: 4681.0688 - val_regression_loss: 2321.4993\n","Epoch 35/50\n","537/537 [==============================] - 0s 69us/step - loss: 43261.9706 - regression_loss: 21498.6429 - val_loss: 4205.7100 - val_regression_loss: 2083.9875\n","Epoch 36/50\n","537/537 [==============================] - 0s 66us/step - loss: 39841.4500 - regression_loss: 19786.5343 - val_loss: 3788.1133 - val_regression_loss: 1875.5559\n","Epoch 37/50\n","537/537 [==============================] - 0s 63us/step - loss: 36619.1480 - regression_loss: 18175.4139 - val_loss: 3480.6887 - val_regression_loss: 1722.1274\n","Epoch 38/50\n","537/537 [==============================] - 0s 61us/step - loss: 29119.9073 - regression_loss: 14429.8379 - val_loss: 3237.5002 - val_regression_loss: 1600.4806\n","Epoch 39/50\n","537/537 [==============================] - 0s 61us/step - loss: 33005.0572 - regression_loss: 16372.9205 - val_loss: 3098.3655 - val_regression_loss: 1530.4009\n","Epoch 40/50\n","537/537 [==============================] - 0s 63us/step - loss: 29873.3138 - regression_loss: 14797.8639 - val_loss: 3026.0713 - val_regression_loss: 1493.7638\n","Epoch 41/50\n","537/537 [==============================] - 0s 69us/step - loss: 28256.3709 - regression_loss: 13993.5517 - val_loss: 2902.1343 - val_regression_loss: 1431.5925\n","Epoch 42/50\n","537/537 [==============================] - 0s 78us/step - loss: 26802.6786 - regression_loss: 13263.9040 - val_loss: 2621.1492 - val_regression_loss: 1291.4170\n","Epoch 43/50\n","537/537 [==============================] - 0s 70us/step - loss: 24465.7577 - regression_loss: 12093.7537 - val_loss: 2305.2271 - val_regression_loss: 1134.0564\n","Epoch 44/50\n","537/537 [==============================] - 0s 63us/step - loss: 19931.4770 - regression_loss: 9831.5516 - val_loss: 2063.4316 - val_regression_loss: 1013.5786\n","Epoch 45/50\n","537/537 [==============================] - 0s 67us/step - loss: 19665.2203 - regression_loss: 9701.8472 - val_loss: 1917.8973 - val_regression_loss: 940.7203\n","Epoch 46/50\n","537/537 [==============================] - 0s 63us/step - loss: 20511.6444 - regression_loss: 10125.3740 - val_loss: 1876.5404 - val_regression_loss: 919.5108\n","Epoch 47/50\n","537/537 [==============================] - 0s 65us/step - loss: 19022.2950 - regression_loss: 9379.5009 - val_loss: 1871.0338 - val_regression_loss: 916.3478\n","Epoch 48/50\n","537/537 [==============================] - 0s 64us/step - loss: 17771.4449 - regression_loss: 8755.0454 - val_loss: 1775.1521 - val_regression_loss: 868.4654\n","Epoch 49/50\n","537/537 [==============================] - 0s 66us/step - loss: 16696.5365 - regression_loss: 8217.9862 - val_loss: 1600.5925 - val_regression_loss: 781.6403\n","Epoch 50/50\n","537/537 [==============================] - 0s 66us/step - loss: 15415.0888 - regression_loss: 7584.1553 - val_loss: 1486.1267 - val_regression_loss: 724.7217\n","***************************** elapsed_time is:  3.8921117782592773\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 185852.9552 - regression_loss: 92700.0176 - val_loss: 24225.2695 - val_regression_loss: 12083.2109\n","Epoch 2/50\n","537/537 [==============================] - 0s 54us/step - loss: 169437.5596 - regression_loss: 84503.6031 - val_loss: 21571.8711 - val_regression_loss: 10758.5449\n","Epoch 3/50\n","537/537 [==============================] - 0s 51us/step - loss: 150645.8704 - regression_loss: 75123.4873 - val_loss: 18184.5840 - val_regression_loss: 9067.3926\n","Epoch 4/50\n","537/537 [==============================] - 0s 65us/step - loss: 126011.2388 - regression_loss: 62823.0528 - val_loss: 14110.4766 - val_regression_loss: 7033.0791\n","Epoch 5/50\n","537/537 [==============================] - 0s 51us/step - loss: 96292.2360 - regression_loss: 47983.5800 - val_loss: 9873.4629 - val_regression_loss: 4917.2168\n","Epoch 6/50\n","537/537 [==============================] - 0s 51us/step - loss: 66025.3038 - regression_loss: 32868.4550 - val_loss: 6530.7529 - val_regression_loss: 3247.9197\n","Epoch 7/50\n","537/537 [==============================] - 0s 55us/step - loss: 46298.3185 - regression_loss: 23018.3256 - val_loss: 5394.9614 - val_regression_loss: 2680.8782\n","Epoch 8/50\n","537/537 [==============================] - 0s 61us/step - loss: 43210.5681 - regression_loss: 21477.7707 - val_loss: 5847.7974 - val_regression_loss: 2906.9224\n","Epoch 9/50\n","537/537 [==============================] - 0s 59us/step - loss: 51510.7628 - regression_loss: 25622.8226 - val_loss: 5486.1855 - val_regression_loss: 2725.6384\n","Epoch 10/50\n","537/537 [==============================] - 0s 78us/step - loss: 50487.7562 - regression_loss: 25105.8270 - val_loss: 4594.2065 - val_regression_loss: 2279.5728\n","Epoch 11/50\n","537/537 [==============================] - 0s 68us/step - loss: 39596.8382 - regression_loss: 19658.0003 - val_loss: 4207.3672 - val_regression_loss: 2086.2114\n","Epoch 12/50\n","537/537 [==============================] - 0s 65us/step - loss: 34826.2454 - regression_loss: 17275.7765 - val_loss: 4427.5796 - val_regression_loss: 2196.3398\n","Epoch 13/50\n","537/537 [==============================] - 0s 62us/step - loss: 32441.0603 - regression_loss: 16083.2664 - val_loss: 4784.6299 - val_regression_loss: 2374.8508\n","Epoch 14/50\n","537/537 [==============================] - 0s 62us/step - loss: 33871.9221 - regression_loss: 16801.7657 - val_loss: 4832.6626 - val_regression_loss: 2398.8240\n","Epoch 15/50\n","537/537 [==============================] - 0s 63us/step - loss: 35266.9462 - regression_loss: 17497.4258 - val_loss: 4481.3120 - val_regression_loss: 2223.0723\n","Epoch 16/50\n","537/537 [==============================] - 0s 70us/step - loss: 30420.7452 - regression_loss: 15073.6581 - val_loss: 3929.5774 - val_regression_loss: 1947.0917\n","Epoch 17/50\n","537/537 [==============================] - 0s 68us/step - loss: 28903.5576 - regression_loss: 14317.6182 - val_loss: 3424.2754 - val_regression_loss: 1694.3007\n","Epoch 18/50\n","537/537 [==============================] - 0s 58us/step - loss: 26589.6318 - regression_loss: 13158.3933 - val_loss: 3118.3210 - val_regression_loss: 1541.2043\n","Epoch 19/50\n","537/537 [==============================] - 0s 65us/step - loss: 25537.8717 - regression_loss: 12631.7931 - val_loss: 2939.8694 - val_regression_loss: 1451.9315\n","Epoch 20/50\n","537/537 [==============================] - 0s 69us/step - loss: 25339.2914 - regression_loss: 12535.5963 - val_loss: 2725.4124 - val_regression_loss: 1344.7427\n","Epoch 21/50\n","537/537 [==============================] - 0s 65us/step - loss: 23377.9715 - regression_loss: 11553.0439 - val_loss: 2495.1333 - val_regression_loss: 1229.7029\n","Epoch 22/50\n","537/537 [==============================] - 0s 63us/step - loss: 21354.8434 - regression_loss: 10544.6097 - val_loss: 2363.6597 - val_regression_loss: 1164.0836\n","Epoch 23/50\n","537/537 [==============================] - 0s 68us/step - loss: 19946.1234 - regression_loss: 9839.9451 - val_loss: 2263.6531 - val_regression_loss: 1114.1910\n","Epoch 24/50\n","537/537 [==============================] - 0s 60us/step - loss: 17499.5036 - regression_loss: 8617.3844 - val_loss: 2054.2192 - val_regression_loss: 1009.5717\n","Epoch 25/50\n","537/537 [==============================] - 0s 61us/step - loss: 17554.5227 - regression_loss: 8647.5220 - val_loss: 1738.3871 - val_regression_loss: 851.7096\n","Epoch 26/50\n","537/537 [==============================] - 0s 65us/step - loss: 14900.3071 - regression_loss: 7317.4552 - val_loss: 1497.8234 - val_regression_loss: 731.4343\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 14177.7999 - regression_loss: 6958.7033 - val_loss: 1345.8831 - val_regression_loss: 655.4677\n","Epoch 28/50\n","537/537 [==============================] - 0s 71us/step - loss: 13028.6591 - regression_loss: 6383.0665 - val_loss: 1212.1160 - val_regression_loss: 588.6553\n","Epoch 29/50\n","537/537 [==============================] - 0s 70us/step - loss: 11817.8450 - regression_loss: 5779.7199 - val_loss: 1127.4971 - val_regression_loss: 546.4618\n","Epoch 30/50\n","537/537 [==============================] - 0s 60us/step - loss: 10762.9248 - regression_loss: 5253.2825 - val_loss: 1058.8137 - val_regression_loss: 512.2033\n","Epoch 31/50\n","537/537 [==============================] - 0s 64us/step - loss: 10412.3793 - regression_loss: 5078.6903 - val_loss: 907.2662 - val_regression_loss: 436.4144\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 8084.8242 - regression_loss: 3917.6637 - val_loss: 769.1675 - val_regression_loss: 367.2867\n","Epoch 33/50\n","537/537 [==============================] - 0s 62us/step - loss: 7370.9049 - regression_loss: 3557.2674 - val_loss: 697.8506 - val_regression_loss: 331.6104\n","Epoch 34/50\n","537/537 [==============================] - 0s 59us/step - loss: 7947.1737 - regression_loss: 3847.6239 - val_loss: 616.0206 - val_regression_loss: 290.7917\n","Epoch 35/50\n","537/537 [==============================] - 0s 61us/step - loss: 6534.0709 - regression_loss: 3141.2740 - val_loss: 562.0153 - val_regression_loss: 263.9345\n","Epoch 36/50\n","537/537 [==============================] - 0s 64us/step - loss: 6796.8712 - regression_loss: 3274.4156 - val_loss: 512.4770 - val_regression_loss: 239.1830\n","Epoch 37/50\n","537/537 [==============================] - 0s 67us/step - loss: 6331.0290 - regression_loss: 3039.7846 - val_loss: 489.6071 - val_regression_loss: 227.6668\n","Epoch 38/50\n","537/537 [==============================] - 0s 63us/step - loss: 5776.6163 - regression_loss: 2762.6545 - val_loss: 492.4775 - val_regression_loss: 229.0398\n","Epoch 39/50\n","537/537 [==============================] - 0s 61us/step - loss: 5537.1842 - regression_loss: 2645.5926 - val_loss: 441.4368 - val_regression_loss: 203.5564\n","Epoch 40/50\n","537/537 [==============================] - 0s 62us/step - loss: 4637.2908 - regression_loss: 2193.4919 - val_loss: 419.6666 - val_regression_loss: 192.6905\n","Epoch 41/50\n","537/537 [==============================] - 0s 61us/step - loss: 4929.0554 - regression_loss: 2341.7480 - val_loss: 431.0984 - val_regression_loss: 198.2614\n","Epoch 42/50\n","537/537 [==============================] - 0s 61us/step - loss: 4674.7668 - regression_loss: 2213.6445 - val_loss: 462.6224 - val_regression_loss: 213.9125\n","Epoch 43/50\n","537/537 [==============================] - 0s 65us/step - loss: 4133.3872 - regression_loss: 1944.7623 - val_loss: 420.8439 - val_regression_loss: 193.0653\n","Epoch 44/50\n","537/537 [==============================] - 0s 62us/step - loss: 4312.8712 - regression_loss: 2031.2510 - val_loss: 407.8465 - val_regression_loss: 186.5901\n","Epoch 45/50\n","537/537 [==============================] - 0s 59us/step - loss: 4174.3594 - regression_loss: 1967.3463 - val_loss: 417.7770 - val_regression_loss: 191.5331\n","Epoch 46/50\n","537/537 [==============================] - 0s 69us/step - loss: 4000.5495 - regression_loss: 1878.0921 - val_loss: 431.4848 - val_regression_loss: 198.3842\n","Epoch 47/50\n","537/537 [==============================] - 0s 67us/step - loss: 3900.8915 - regression_loss: 1826.0491 - val_loss: 403.2648 - val_regression_loss: 184.3592\n","Epoch 48/50\n","537/537 [==============================] - 0s 62us/step - loss: 3642.9502 - regression_loss: 1695.9480 - val_loss: 378.8438 - val_regression_loss: 172.2348\n","Epoch 49/50\n","537/537 [==============================] - 0s 67us/step - loss: 3685.5437 - regression_loss: 1719.1666 - val_loss: 367.8266 - val_regression_loss: 166.7701\n","Epoch 50/50\n","537/537 [==============================] - 0s 66us/step - loss: 3569.2071 - regression_loss: 1661.9736 - val_loss: 373.8099 - val_regression_loss: 169.7608\n","***************************** elapsed_time is:  3.8025968074798584\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 19345.2743 - regression_loss: 9584.0569 - val_loss: 1544.3619 - val_regression_loss: 753.0599\n","Epoch 2/50\n","537/537 [==============================] - 0s 63us/step - loss: 14499.3782 - regression_loss: 7136.1124 - val_loss: 1021.6791 - val_regression_loss: 486.7644\n","Epoch 3/50\n","537/537 [==============================] - 0s 67us/step - loss: 9391.4036 - regression_loss: 4536.6578 - val_loss: 625.8197 - val_regression_loss: 281.8621\n","Epoch 4/50\n","537/537 [==============================] - 0s 59us/step - loss: 5316.7192 - regression_loss: 2444.0805 - val_loss: 605.9287 - val_regression_loss: 266.0431\n","Epoch 5/50\n","537/537 [==============================] - 0s 60us/step - loss: 4608.1559 - regression_loss: 2038.6570 - val_loss: 467.1943 - val_regression_loss: 202.1985\n","Epoch 6/50\n","537/537 [==============================] - 0s 61us/step - loss: 3583.5105 - regression_loss: 1567.2343 - val_loss: 307.6396 - val_regression_loss: 132.6786\n","Epoch 7/50\n","537/537 [==============================] - 0s 60us/step - loss: 2620.0277 - regression_loss: 1169.6835 - val_loss: 358.1355 - val_regression_loss: 165.9498\n","Epoch 8/50\n","537/537 [==============================] - 0s 60us/step - loss: 3250.4950 - regression_loss: 1550.1090 - val_loss: 385.2358 - val_regression_loss: 181.3028\n","Epoch 9/50\n","537/537 [==============================] - 0s 68us/step - loss: 3449.9301 - regression_loss: 1662.9737 - val_loss: 305.0320 - val_regression_loss: 138.8324\n","Epoch 10/50\n","537/537 [==============================] - 0s 61us/step - loss: 2650.0953 - regression_loss: 1241.5371 - val_loss: 245.1920 - val_regression_loss: 104.9256\n","Epoch 11/50\n","537/537 [==============================] - 0s 59us/step - loss: 1960.5282 - regression_loss: 862.4132 - val_loss: 264.1732 - val_regression_loss: 110.7026\n","Epoch 12/50\n","537/537 [==============================] - 0s 59us/step - loss: 1992.3340 - regression_loss: 847.3996 - val_loss: 294.9750 - val_regression_loss: 123.9784\n","Epoch 13/50\n","537/537 [==============================] - 0s 57us/step - loss: 2214.3785 - regression_loss: 941.2911 - val_loss: 282.9442 - val_regression_loss: 117.7982\n","Epoch 14/50\n","537/537 [==============================] - 0s 58us/step - loss: 2152.9614 - regression_loss: 910.0343 - val_loss: 256.9252 - val_regression_loss: 106.0669\n","Epoch 15/50\n","537/537 [==============================] - 0s 70us/step - loss: 1916.8924 - regression_loss: 800.9882 - val_loss: 242.3595 - val_regression_loss: 100.6651\n","Epoch 16/50\n","537/537 [==============================] - 0s 61us/step - loss: 1859.9442 - regression_loss: 792.9194 - val_loss: 230.4263 - val_regression_loss: 96.5232\n","Epoch 17/50\n","537/537 [==============================] - 0s 62us/step - loss: 1802.1782 - regression_loss: 776.0954 - val_loss: 219.3784 - val_regression_loss: 92.5546\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 1714.7732 - regression_loss: 744.0234 - val_loss: 217.5355 - val_regression_loss: 92.6699\n","Epoch 19/50\n","537/537 [==============================] - 0s 62us/step - loss: 1641.5310 - regression_loss: 717.2574 - val_loss: 224.8110 - val_regression_loss: 96.6805\n","Epoch 20/50\n","537/537 [==============================] - 0s 76us/step - loss: 1696.0410 - regression_loss: 747.8939 - val_loss: 228.1303 - val_regression_loss: 98.0194\n","Epoch 21/50\n","537/537 [==============================] - 0s 67us/step - loss: 1670.3350 - regression_loss: 728.9708 - val_loss: 222.3374 - val_regression_loss: 94.4143\n","Epoch 22/50\n","537/537 [==============================] - 0s 59us/step - loss: 1607.6014 - regression_loss: 694.2554 - val_loss: 217.7405 - val_regression_loss: 91.3142\n","Epoch 23/50\n","537/537 [==============================] - 0s 74us/step - loss: 1572.9377 - regression_loss: 670.4379 - val_loss: 216.0134 - val_regression_loss: 89.7534\n","Epoch 24/50\n","537/537 [==============================] - 0s 71us/step - loss: 1546.0576 - regression_loss: 653.4063 - val_loss: 214.0750 - val_regression_loss: 88.2645\n","Epoch 25/50\n","537/537 [==============================] - 0s 61us/step - loss: 1497.2037 - regression_loss: 626.0624 - val_loss: 212.2458 - val_regression_loss: 87.0663\n","Epoch 26/50\n","537/537 [==============================] - 0s 64us/step - loss: 1527.3987 - regression_loss: 638.6355 - val_loss: 212.0168 - val_regression_loss: 87.0369\n","Epoch 27/50\n","537/537 [==============================] - 0s 62us/step - loss: 1529.7241 - regression_loss: 639.7114 - val_loss: 211.1590 - val_regression_loss: 87.0468\n","Epoch 28/50\n","537/537 [==============================] - 0s 64us/step - loss: 1457.2628 - regression_loss: 606.8875 - val_loss: 208.9121 - val_regression_loss: 86.5510\n","Epoch 29/50\n","537/537 [==============================] - 0s 64us/step - loss: 1446.9804 - regression_loss: 607.8707 - val_loss: 208.3240 - val_regression_loss: 86.8361\n","Epoch 30/50\n","537/537 [==============================] - 0s 63us/step - loss: 1416.9702 - regression_loss: 594.6401 - val_loss: 209.2789 - val_regression_loss: 87.6796\n","Epoch 31/50\n","537/537 [==============================] - 0s 64us/step - loss: 1391.6944 - regression_loss: 586.5193 - val_loss: 208.5390 - val_regression_loss: 87.2906\n","Epoch 32/50\n","537/537 [==============================] - 0s 71us/step - loss: 1429.7767 - regression_loss: 607.0901 - val_loss: 206.9902 - val_regression_loss: 86.2349\n","Epoch 33/50\n","537/537 [==============================] - 0s 65us/step - loss: 1360.9322 - regression_loss: 568.9110 - val_loss: 206.4327 - val_regression_loss: 85.5887\n","Epoch 34/50\n","537/537 [==============================] - 0s 66us/step - loss: 1356.2945 - regression_loss: 562.9895 - val_loss: 205.1555 - val_regression_loss: 84.6997\n","Epoch 35/50\n","537/537 [==============================] - 0s 63us/step - loss: 1316.6878 - regression_loss: 541.3107 - val_loss: 203.9276 - val_regression_loss: 84.0042\n","Epoch 36/50\n","537/537 [==============================] - 0s 62us/step - loss: 1358.3352 - regression_loss: 560.5246 - val_loss: 204.2235 - val_regression_loss: 84.0948\n","Epoch 37/50\n","537/537 [==============================] - 0s 61us/step - loss: 1331.4870 - regression_loss: 548.7502 - val_loss: 205.0433 - val_regression_loss: 84.4663\n","Epoch 38/50\n","537/537 [==============================] - 0s 63us/step - loss: 1317.5823 - regression_loss: 541.4630 - val_loss: 205.6421 - val_regression_loss: 84.8503\n","Epoch 39/50\n","537/537 [==============================] - 0s 58us/step - loss: 1295.3120 - regression_loss: 529.8625 - val_loss: 205.1226 - val_regression_loss: 84.7684\n","Epoch 40/50\n","537/537 [==============================] - 0s 60us/step - loss: 1310.5084 - regression_loss: 541.6583 - val_loss: 203.3509 - val_regression_loss: 84.0401\n","Epoch 41/50\n","537/537 [==============================] - 0s 64us/step - loss: 1297.2807 - regression_loss: 538.6895 - val_loss: 202.6385 - val_regression_loss: 83.7057\n","Epoch 42/50\n","537/537 [==============================] - 0s 59us/step - loss: 1266.5760 - regression_loss: 524.0675 - val_loss: 202.5905 - val_regression_loss: 83.5446\n","Epoch 43/50\n","537/537 [==============================] - 0s 68us/step - loss: 1272.7190 - regression_loss: 524.4663 - val_loss: 203.2923 - val_regression_loss: 83.6786\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 1268.4608 - regression_loss: 521.8160 - val_loss: 203.9768 - val_regression_loss: 83.9117\n","Epoch 45/50\n","537/537 [==============================] - 0s 59us/step - loss: 1244.6171 - regression_loss: 508.8525 - val_loss: 203.5913 - val_regression_loss: 83.7248\n","Epoch 46/50\n","537/537 [==============================] - 0s 63us/step - loss: 1234.3325 - regression_loss: 501.8566 - val_loss: 202.4740 - val_regression_loss: 83.2988\n","Epoch 47/50\n","537/537 [==============================] - 0s 61us/step - loss: 1259.1538 - regression_loss: 515.8404 - val_loss: 201.9480 - val_regression_loss: 83.0387\n","Epoch 48/50\n","537/537 [==============================] - 0s 62us/step - loss: 1256.5783 - regression_loss: 516.1298 - val_loss: 202.1814 - val_regression_loss: 83.1337\n","Epoch 49/50\n","537/537 [==============================] - 0s 63us/step - loss: 1228.2774 - regression_loss: 501.5754 - val_loss: 201.9506 - val_regression_loss: 83.0482\n","Epoch 50/50\n","537/537 [==============================] - 0s 68us/step - loss: 1239.5388 - regression_loss: 505.2088 - val_loss: 201.3144 - val_regression_loss: 82.7904\n","***************************** elapsed_time is:  4.23347544670105\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 7766.1729 - regression_loss: 3703.9628 - val_loss: 682.7428 - val_regression_loss: 319.0082\n","Epoch 2/50\n","537/537 [==============================] - 0s 57us/step - loss: 4689.0599 - regression_loss: 2185.4304 - val_loss: 420.1759 - val_regression_loss: 190.2034\n","Epoch 3/50\n","537/537 [==============================] - 0s 67us/step - loss: 2975.9252 - regression_loss: 1349.0968 - val_loss: 266.0418 - val_regression_loss: 113.4284\n","Epoch 4/50\n","537/537 [==============================] - 0s 55us/step - loss: 2095.8858 - regression_loss: 911.5762 - val_loss: 191.1980 - val_regression_loss: 73.2343\n","Epoch 5/50\n","537/537 [==============================] - 0s 52us/step - loss: 1582.9514 - regression_loss: 631.0303 - val_loss: 255.3244 - val_regression_loss: 102.2020\n","Epoch 6/50\n","537/537 [==============================] - 0s 56us/step - loss: 2016.5281 - regression_loss: 820.7240 - val_loss: 269.1325 - val_regression_loss: 109.9667\n","Epoch 7/50\n","537/537 [==============================] - 0s 52us/step - loss: 1975.1302 - regression_loss: 804.7538 - val_loss: 210.3653 - val_regression_loss: 83.7785\n","Epoch 8/50\n","537/537 [==============================] - 0s 55us/step - loss: 1457.0249 - regression_loss: 571.8835 - val_loss: 175.5938 - val_regression_loss: 69.8149\n","Epoch 9/50\n","537/537 [==============================] - 0s 58us/step - loss: 1262.5049 - regression_loss: 502.0856 - val_loss: 177.0580 - val_regression_loss: 73.1200\n","Epoch 10/50\n","537/537 [==============================] - 0s 61us/step - loss: 1377.2683 - regression_loss: 578.9474 - val_loss: 182.7970 - val_regression_loss: 77.1891\n","Epoch 11/50\n","537/537 [==============================] - 0s 60us/step - loss: 1452.6922 - regression_loss: 626.7355 - val_loss: 176.7281 - val_regression_loss: 74.1025\n","Epoch 12/50\n","537/537 [==============================] - 0s 60us/step - loss: 1361.5822 - regression_loss: 579.0544 - val_loss: 168.5875 - val_regression_loss: 69.1682\n","Epoch 13/50\n","537/537 [==============================] - 0s 55us/step - loss: 1240.9223 - regression_loss: 510.6519 - val_loss: 164.9652 - val_regression_loss: 65.9657\n","Epoch 14/50\n","537/537 [==============================] - 0s 53us/step - loss: 1206.6767 - regression_loss: 484.6848 - val_loss: 166.1414 - val_regression_loss: 65.1110\n","Epoch 15/50\n","537/537 [==============================] - 0s 58us/step - loss: 1234.7607 - regression_loss: 484.5826 - val_loss: 165.8732 - val_regression_loss: 64.2258\n","Epoch 16/50\n","537/537 [==============================] - 0s 63us/step - loss: 1261.3550 - regression_loss: 490.7913 - val_loss: 160.4642 - val_regression_loss: 61.8942\n","Epoch 17/50\n","537/537 [==============================] - 0s 58us/step - loss: 1212.1746 - regression_loss: 467.8889 - val_loss: 155.5588 - val_regression_loss: 60.4950\n","Epoch 18/50\n","537/537 [==============================] - 0s 58us/step - loss: 1164.8805 - regression_loss: 455.9996 - val_loss: 155.9172 - val_regression_loss: 61.7008\n","Epoch 19/50\n","537/537 [==============================] - 0s 54us/step - loss: 1167.5136 - regression_loss: 464.9770 - val_loss: 158.2962 - val_regression_loss: 63.5345\n","Epoch 20/50\n","537/537 [==============================] - 0s 55us/step - loss: 1173.2934 - regression_loss: 477.6008 - val_loss: 158.4124 - val_regression_loss: 63.7801\n","Epoch 21/50\n","537/537 [==============================] - 0s 66us/step - loss: 1178.6106 - regression_loss: 482.4360 - val_loss: 156.6054 - val_regression_loss: 62.6631\n","Epoch 22/50\n","537/537 [==============================] - 0s 60us/step - loss: 1168.0535 - regression_loss: 474.7081 - val_loss: 154.1429 - val_regression_loss: 60.9519\n","Epoch 23/50\n","537/537 [==============================] - 0s 54us/step - loss: 1147.0952 - regression_loss: 460.9381 - val_loss: 154.5198 - val_regression_loss: 60.5588\n","Epoch 24/50\n","537/537 [==============================] - 0s 58us/step - loss: 1140.1531 - regression_loss: 453.0715 - val_loss: 157.4690 - val_regression_loss: 61.5568\n","Epoch 25/50\n","537/537 [==============================] - 0s 55us/step - loss: 1144.7466 - regression_loss: 452.7051 - val_loss: 157.5371 - val_regression_loss: 61.4007\n","Epoch 26/50\n","537/537 [==============================] - 0s 62us/step - loss: 1162.3328 - regression_loss: 460.5003 - val_loss: 156.2059 - val_regression_loss: 60.8210\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 1132.8963 - regression_loss: 446.2540 - val_loss: 154.0615 - val_regression_loss: 60.0331\n","Epoch 28/50\n","537/537 [==============================] - 0s 53us/step - loss: 1132.8347 - regression_loss: 450.0703 - val_loss: 153.8368 - val_regression_loss: 60.2162\n","Epoch 29/50\n","537/537 [==============================] - 0s 61us/step - loss: 1135.1406 - regression_loss: 452.9593 - val_loss: 156.3473 - val_regression_loss: 61.6231\n","Epoch 30/50\n","537/537 [==============================] - 0s 51us/step - loss: 1115.8185 - regression_loss: 445.6119 - val_loss: 157.8194 - val_regression_loss: 62.3831\n","Epoch 31/50\n","537/537 [==============================] - 0s 59us/step - loss: 1124.6271 - regression_loss: 450.1208 - val_loss: 157.4019 - val_regression_loss: 62.0889\n","Epoch 32/50\n","537/537 [==============================] - 0s 58us/step - loss: 1127.9204 - regression_loss: 452.3810 - val_loss: 155.5827 - val_regression_loss: 61.0004\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 1117.9790 - regression_loss: 444.6084 - val_loss: 154.4045 - val_regression_loss: 60.2452\n","Epoch 34/50\n","537/537 [==============================] - 0s 61us/step - loss: 1131.3824 - regression_loss: 452.9256 - val_loss: 155.4646 - val_regression_loss: 60.6893\n","Epoch 35/50\n","537/537 [==============================] - 0s 61us/step - loss: 1137.5450 - regression_loss: 453.6548 - val_loss: 155.9075 - val_regression_loss: 60.9482\n","\n","Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 36/50\n","537/537 [==============================] - 0s 66us/step - loss: 1129.3197 - regression_loss: 451.0857 - val_loss: 155.3288 - val_regression_loss: 60.7415\n","Epoch 37/50\n","537/537 [==============================] - 0s 70us/step - loss: 1127.7844 - regression_loss: 451.3818 - val_loss: 154.4859 - val_regression_loss: 60.4171\n","Epoch 38/50\n","537/537 [==============================] - 0s 83us/step - loss: 1113.7716 - regression_loss: 446.5098 - val_loss: 154.2221 - val_regression_loss: 60.3821\n","Epoch 39/50\n","537/537 [==============================] - 0s 51us/step - loss: 1119.8116 - regression_loss: 448.4449 - val_loss: 154.8481 - val_regression_loss: 60.7569\n","Epoch 40/50\n","537/537 [==============================] - 0s 54us/step - loss: 1121.4449 - regression_loss: 451.9482 - val_loss: 155.3922 - val_regression_loss: 61.0540\n","Epoch 41/50\n","537/537 [==============================] - 0s 60us/step - loss: 1115.9052 - regression_loss: 445.5943 - val_loss: 155.2652 - val_regression_loss: 60.9921\n","Epoch 42/50\n","537/537 [==============================] - 0s 60us/step - loss: 1115.1147 - regression_loss: 447.1241 - val_loss: 155.3299 - val_regression_loss: 60.9861\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 1098.3355 - regression_loss: 437.7811 - val_loss: 155.3345 - val_regression_loss: 60.9373\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 1119.9921 - regression_loss: 450.0527 - val_loss: 154.8861 - val_regression_loss: 60.6676\n","Epoch 45/50\n","537/537 [==============================] - 0s 64us/step - loss: 1111.0572 - regression_loss: 443.1441 - val_loss: 154.9549 - val_regression_loss: 60.6700\n","Epoch 46/50\n","537/537 [==============================] - 0s 55us/step - loss: 1117.5111 - regression_loss: 446.2878 - val_loss: 154.9606 - val_regression_loss: 60.6687\n","Epoch 47/50\n","537/537 [==============================] - 0s 64us/step - loss: 1118.4044 - regression_loss: 449.3433 - val_loss: 155.0365 - val_regression_loss: 60.7213\n","Epoch 48/50\n","537/537 [==============================] - 0s 58us/step - loss: 1099.0881 - regression_loss: 436.1402 - val_loss: 155.3254 - val_regression_loss: 60.8923\n","\n","Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 49/50\n","537/537 [==============================] - 0s 55us/step - loss: 1102.3922 - regression_loss: 441.6705 - val_loss: 155.4225 - val_regression_loss: 60.9533\n","Epoch 50/50\n","537/537 [==============================] - 0s 53us/step - loss: 1099.8896 - regression_loss: 439.6317 - val_loss: 155.5840 - val_regression_loss: 61.0500\n","***************************** elapsed_time is:  3.8928165435791016\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 18868.8032 - regression_loss: 9280.6975 - val_loss: 1780.9919 - val_regression_loss: 868.9956\n","Epoch 2/50\n","537/537 [==============================] - 0s 60us/step - loss: 13452.6695 - regression_loss: 6561.5239 - val_loss: 1156.4893 - val_regression_loss: 555.4963\n","Epoch 3/50\n","537/537 [==============================] - 0s 71us/step - loss: 8603.8625 - regression_loss: 4127.2754 - val_loss: 735.0146 - val_regression_loss: 343.7079\n","Epoch 4/50\n","537/537 [==============================] - 0s 57us/step - loss: 5279.2164 - regression_loss: 2459.1645 - val_loss: 542.2788 - val_regression_loss: 248.2486\n","Epoch 5/50\n","537/537 [==============================] - 0s 58us/step - loss: 4066.9194 - regression_loss: 1860.8388 - val_loss: 338.7648 - val_regression_loss: 149.2502\n","Epoch 6/50\n","537/537 [==============================] - 0s 61us/step - loss: 2263.5358 - regression_loss: 980.9060 - val_loss: 347.9047 - val_regression_loss: 156.7273\n","Epoch 7/50\n","537/537 [==============================] - 0s 53us/step - loss: 2089.2018 - regression_loss: 917.9926 - val_loss: 512.7818 - val_regression_loss: 240.8005\n","Epoch 8/50\n","537/537 [==============================] - 0s 56us/step - loss: 3204.6800 - regression_loss: 1487.8912 - val_loss: 491.8279 - val_regression_loss: 230.3571\n","Epoch 9/50\n","537/537 [==============================] - 0s 66us/step - loss: 2944.1189 - regression_loss: 1359.1583 - val_loss: 351.8586 - val_regression_loss: 159.6711\n","Epoch 10/50\n","537/537 [==============================] - 0s 59us/step - loss: 2003.3572 - regression_loss: 883.1470 - val_loss: 256.5613 - val_regression_loss: 111.0879\n","Epoch 11/50\n","537/537 [==============================] - 0s 57us/step - loss: 1523.4317 - regression_loss: 636.0390 - val_loss: 245.1643 - val_regression_loss: 104.5436\n","Epoch 12/50\n","537/537 [==============================] - 0s 58us/step - loss: 1651.6721 - regression_loss: 692.2534 - val_loss: 263.2631 - val_regression_loss: 113.1184\n","Epoch 13/50\n","537/537 [==============================] - 0s 56us/step - loss: 1864.7653 - regression_loss: 794.9315 - val_loss: 268.0040 - val_regression_loss: 115.4821\n","Epoch 14/50\n","537/537 [==============================] - 0s 55us/step - loss: 1844.8808 - regression_loss: 783.9421 - val_loss: 264.5158 - val_regression_loss: 114.1036\n","Epoch 15/50\n","537/537 [==============================] - 0s 61us/step - loss: 1694.1013 - regression_loss: 710.4308 - val_loss: 261.8399 - val_regression_loss: 113.3314\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 1553.3576 - regression_loss: 646.4061 - val_loss: 257.4968 - val_regression_loss: 111.7704\n","\n","Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 17/50\n","537/537 [==============================] - 0s 57us/step - loss: 1465.6477 - regression_loss: 607.2016 - val_loss: 253.0817 - val_regression_loss: 109.8495\n","Epoch 18/50\n","537/537 [==============================] - 0s 65us/step - loss: 1430.5190 - regression_loss: 591.8943 - val_loss: 247.3610 - val_regression_loss: 107.2279\n","Epoch 19/50\n","537/537 [==============================] - 0s 57us/step - loss: 1413.0050 - regression_loss: 585.2099 - val_loss: 242.5338 - val_regression_loss: 104.9799\n","Epoch 20/50\n","537/537 [==============================] - 0s 55us/step - loss: 1413.5529 - regression_loss: 589.1090 - val_loss: 238.0130 - val_regression_loss: 102.7933\n","Epoch 21/50\n","537/537 [==============================] - 0s 55us/step - loss: 1435.7344 - regression_loss: 598.7908 - val_loss: 233.5470 - val_regression_loss: 100.5461\n","Epoch 22/50\n","537/537 [==============================] - 0s 57us/step - loss: 1407.6174 - regression_loss: 584.7265 - val_loss: 229.1622 - val_regression_loss: 98.2609\n","Epoch 23/50\n","537/537 [==============================] - 0s 59us/step - loss: 1378.5176 - regression_loss: 569.7061 - val_loss: 226.0460 - val_regression_loss: 96.5694\n","Epoch 24/50\n","537/537 [==============================] - 0s 56us/step - loss: 1359.1869 - regression_loss: 559.1710 - val_loss: 224.5852 - val_regression_loss: 95.6966\n","Epoch 25/50\n","537/537 [==============================] - 0s 64us/step - loss: 1337.1522 - regression_loss: 545.5805 - val_loss: 223.6307 - val_regression_loss: 95.0967\n","Epoch 26/50\n","537/537 [==============================] - 0s 60us/step - loss: 1329.0807 - regression_loss: 544.2455 - val_loss: 222.2494 - val_regression_loss: 94.3128\n","Epoch 27/50\n","537/537 [==============================] - 0s 55us/step - loss: 1340.2761 - regression_loss: 546.1531 - val_loss: 219.9396 - val_regression_loss: 93.0999\n","Epoch 28/50\n","537/537 [==============================] - 0s 60us/step - loss: 1321.8755 - regression_loss: 537.4482 - val_loss: 216.7905 - val_regression_loss: 91.5055\n","Epoch 29/50\n","537/537 [==============================] - 0s 56us/step - loss: 1334.7690 - regression_loss: 543.4827 - val_loss: 213.7809 - val_regression_loss: 90.0162\n","Epoch 30/50\n","537/537 [==============================] - 0s 52us/step - loss: 1325.5467 - regression_loss: 541.1342 - val_loss: 211.7198 - val_regression_loss: 89.0290\n","Epoch 31/50\n","537/537 [==============================] - 0s 73us/step - loss: 1307.9502 - regression_loss: 532.8790 - val_loss: 210.8166 - val_regression_loss: 88.6402\n","Epoch 32/50\n","537/537 [==============================] - 0s 64us/step - loss: 1288.9248 - regression_loss: 522.5323 - val_loss: 210.8323 - val_regression_loss: 88.7181\n","Epoch 33/50\n","537/537 [==============================] - 0s 61us/step - loss: 1298.2621 - regression_loss: 528.8760 - val_loss: 211.6873 - val_regression_loss: 89.2063\n","Epoch 34/50\n","537/537 [==============================] - 0s 62us/step - loss: 1300.9827 - regression_loss: 530.3985 - val_loss: 212.2275 - val_regression_loss: 89.5182\n","Epoch 35/50\n","537/537 [==============================] - 0s 60us/step - loss: 1289.2251 - regression_loss: 527.3267 - val_loss: 212.2031 - val_regression_loss: 89.5320\n","Epoch 36/50\n","537/537 [==============================] - 0s 59us/step - loss: 1289.2903 - regression_loss: 524.4756 - val_loss: 211.5108 - val_regression_loss: 89.1895\n","Epoch 37/50\n","537/537 [==============================] - 0s 63us/step - loss: 1280.8804 - regression_loss: 521.2361 - val_loss: 209.5454 - val_regression_loss: 88.1857\n","Epoch 38/50\n","537/537 [==============================] - 0s 66us/step - loss: 1275.9409 - regression_loss: 517.0573 - val_loss: 207.8734 - val_regression_loss: 87.3208\n","Epoch 39/50\n","537/537 [==============================] - 0s 66us/step - loss: 1237.4629 - regression_loss: 499.9603 - val_loss: 206.2875 - val_regression_loss: 86.4967\n","Epoch 40/50\n","537/537 [==============================] - 0s 71us/step - loss: 1283.6237 - regression_loss: 522.5176 - val_loss: 205.6320 - val_regression_loss: 86.1390\n","Epoch 41/50\n","537/537 [==============================] - 0s 63us/step - loss: 1278.5885 - regression_loss: 518.4320 - val_loss: 205.4078 - val_regression_loss: 86.0016\n","Epoch 42/50\n","537/537 [==============================] - 0s 68us/step - loss: 1272.0338 - regression_loss: 514.8488 - val_loss: 205.6767 - val_regression_loss: 86.1234\n","Epoch 43/50\n","537/537 [==============================] - 0s 62us/step - loss: 1249.0994 - regression_loss: 503.0504 - val_loss: 205.8978 - val_regression_loss: 86.2335\n","Epoch 44/50\n","537/537 [==============================] - 0s 63us/step - loss: 1255.0455 - regression_loss: 509.4264 - val_loss: 205.2495 - val_regression_loss: 85.9150\n","\n","Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 45/50\n","537/537 [==============================] - 0s 63us/step - loss: 1271.6909 - regression_loss: 516.7778 - val_loss: 204.7944 - val_regression_loss: 85.6910\n","Epoch 46/50\n","537/537 [==============================] - 0s 58us/step - loss: 1247.2693 - regression_loss: 506.5571 - val_loss: 204.2709 - val_regression_loss: 85.4361\n","Epoch 47/50\n","537/537 [==============================] - 0s 69us/step - loss: 1263.0523 - regression_loss: 513.1385 - val_loss: 203.6965 - val_regression_loss: 85.1564\n","Epoch 48/50\n","537/537 [==============================] - 0s 68us/step - loss: 1272.3624 - regression_loss: 516.9626 - val_loss: 203.0389 - val_regression_loss: 84.8334\n","Epoch 49/50\n","537/537 [==============================] - 0s 61us/step - loss: 1262.9829 - regression_loss: 514.2290 - val_loss: 202.7742 - val_regression_loss: 84.7077\n","\n","Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","***************************** elapsed_time is:  3.832061529159546\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 204919.6294 - regression_loss: 101953.6018 - val_loss: 22895.8906 - val_regression_loss: 11387.4473\n","Epoch 2/50\n","537/537 [==============================] - 0s 62us/step - loss: 183214.4117 - regression_loss: 91145.5747 - val_loss: 19545.7520 - val_regression_loss: 9719.1055\n","Epoch 3/50\n","537/537 [==============================] - 0s 57us/step - loss: 155670.3161 - regression_loss: 77425.3897 - val_loss: 15275.0635 - val_regression_loss: 7592.5225\n","Epoch 4/50\n","537/537 [==============================] - 0s 58us/step - loss: 120415.1828 - regression_loss: 59858.2257 - val_loss: 10131.4648 - val_regression_loss: 5031.6963\n","Epoch 5/50\n","537/537 [==============================] - 0s 58us/step - loss: 79531.5113 - regression_loss: 39514.8900 - val_loss: 5195.4443 - val_regression_loss: 2575.7183\n","Epoch 6/50\n","537/537 [==============================] - 0s 55us/step - loss: 38964.8568 - regression_loss: 19322.6797 - val_loss: 2069.6284 - val_regression_loss: 1024.1089\n","Epoch 7/50\n","537/537 [==============================] - 0s 59us/step - loss: 14641.7150 - regression_loss: 7250.7228 - val_loss: 2376.3347 - val_regression_loss: 1183.8654\n","Epoch 8/50\n","537/537 [==============================] - 0s 60us/step - loss: 17586.9688 - regression_loss: 8767.4273 - val_loss: 3737.5989 - val_regression_loss: 1860.0776\n","Epoch 9/50\n","537/537 [==============================] - 0s 68us/step - loss: 29792.3906 - regression_loss: 14823.9738 - val_loss: 3328.4612 - val_regression_loss: 1647.5465\n","Epoch 10/50\n","537/537 [==============================] - 0s 72us/step - loss: 27030.9881 - regression_loss: 13375.5182 - val_loss: 2055.0129 - val_regression_loss: 1006.7860\n","Epoch 11/50\n","537/537 [==============================] - 0s 69us/step - loss: 16321.3806 - regression_loss: 7992.0045 - val_loss: 1276.3296 - val_regression_loss: 615.9689\n","Epoch 12/50\n","537/537 [==============================] - 0s 69us/step - loss: 9603.5360 - regression_loss: 4618.1075 - val_loss: 1255.7968 - val_regression_loss: 604.8524\n","Epoch 13/50\n","537/537 [==============================] - 0s 66us/step - loss: 8717.7141 - regression_loss: 4179.5213 - val_loss: 1566.3953 - val_regression_loss: 759.6725\n","Epoch 14/50\n","537/537 [==============================] - 0s 64us/step - loss: 10933.1134 - regression_loss: 5288.3167 - val_loss: 1781.8171 - val_regression_loss: 867.6674\n","Epoch 15/50\n","537/537 [==============================] - 0s 66us/step - loss: 12618.4774 - regression_loss: 6134.0003 - val_loss: 1747.6389 - val_regression_loss: 851.7241\n","Epoch 16/50\n","537/537 [==============================] - 0s 65us/step - loss: 12307.6606 - regression_loss: 5988.7080 - val_loss: 1520.1744 - val_regression_loss: 739.7537\n","Epoch 17/50\n","537/537 [==============================] - 0s 65us/step - loss: 10541.3678 - regression_loss: 5120.1111 - val_loss: 1239.7284 - val_regression_loss: 601.5367\n","Epoch 18/50\n","537/537 [==============================] - 0s 70us/step - loss: 8411.5783 - regression_loss: 4070.8881 - val_loss: 1044.5728 - val_regression_loss: 505.7753\n","Epoch 19/50\n","537/537 [==============================] - 0s 61us/step - loss: 6750.1111 - regression_loss: 3253.2567 - val_loss: 1000.7230 - val_regression_loss: 485.1043\n","Epoch 20/50\n","537/537 [==============================] - 0s 66us/step - loss: 6246.7489 - regression_loss: 3011.1941 - val_loss: 1066.2991 - val_regression_loss: 518.4120\n","Epoch 21/50\n","537/537 [==============================] - 0s 65us/step - loss: 6937.0941 - regression_loss: 3360.9027 - val_loss: 1133.1726 - val_regression_loss: 551.7296\n","Epoch 22/50\n","537/537 [==============================] - 0s 68us/step - loss: 7408.1054 - regression_loss: 3591.3913 - val_loss: 1121.6720 - val_regression_loss: 545.4626\n","Epoch 23/50\n","537/537 [==============================] - 0s 81us/step - loss: 7086.5405 - regression_loss: 3431.2827 - val_loss: 1044.7177 - val_regression_loss: 506.2950\n","Epoch 24/50\n","537/537 [==============================] - 0s 62us/step - loss: 6493.8723 - regression_loss: 3127.8444 - val_loss: 963.7856 - val_regression_loss: 465.1007\n","Epoch 25/50\n","537/537 [==============================] - 0s 67us/step - loss: 5906.6355 - regression_loss: 2826.2738 - val_loss: 926.1233 - val_regression_loss: 445.5773\n","Epoch 26/50\n","537/537 [==============================] - 0s 63us/step - loss: 5563.4257 - regression_loss: 2651.7503 - val_loss: 928.5740 - val_regression_loss: 446.2139\n","Epoch 27/50\n","537/537 [==============================] - 0s 63us/step - loss: 5681.0656 - regression_loss: 2705.7786 - val_loss: 939.8800 - val_regression_loss: 451.4796\n","Epoch 28/50\n","537/537 [==============================] - 0s 66us/step - loss: 5803.5792 - regression_loss: 2764.4590 - val_loss: 933.2593 - val_regression_loss: 448.0370\n","Epoch 29/50\n","537/537 [==============================] - 0s 62us/step - loss: 5720.7118 - regression_loss: 2723.7878 - val_loss: 906.5079 - val_regression_loss: 434.7621\n","Epoch 30/50\n","537/537 [==============================] - 0s 72us/step - loss: 5493.8123 - regression_loss: 2610.6013 - val_loss: 874.6651 - val_regression_loss: 419.1333\n","Epoch 31/50\n","537/537 [==============================] - 0s 71us/step - loss: 5083.6045 - regression_loss: 2407.7199 - val_loss: 854.1459 - val_regression_loss: 409.2466\n","Epoch 32/50\n","537/537 [==============================] - 0s 66us/step - loss: 4855.9574 - regression_loss: 2296.3566 - val_loss: 847.7553 - val_regression_loss: 406.4256\n","Epoch 33/50\n","537/537 [==============================] - 0s 63us/step - loss: 4856.7982 - regression_loss: 2298.0862 - val_loss: 844.9188 - val_regression_loss: 405.3172\n","Epoch 34/50\n","537/537 [==============================] - 0s 70us/step - loss: 4925.8638 - regression_loss: 2337.8458 - val_loss: 833.3569 - val_regression_loss: 399.7224\n","Epoch 35/50\n","537/537 [==============================] - 0s 66us/step - loss: 4813.1180 - regression_loss: 2283.1688 - val_loss: 811.7606 - val_regression_loss: 388.9311\n","Epoch 36/50\n","537/537 [==============================] - 0s 67us/step - loss: 4676.4641 - regression_loss: 2213.2277 - val_loss: 788.3904 - val_regression_loss: 377.1128\n","Epoch 37/50\n","537/537 [==============================] - 0s 60us/step - loss: 4419.3799 - regression_loss: 2084.4009 - val_loss: 770.4830 - val_regression_loss: 367.9136\n","Epoch 38/50\n","537/537 [==============================] - 0s 70us/step - loss: 4339.9760 - regression_loss: 2044.0241 - val_loss: 757.7826 - val_regression_loss: 361.3206\n","Epoch 39/50\n","537/537 [==============================] - 0s 77us/step - loss: 4244.0823 - regression_loss: 1995.8224 - val_loss: 745.3446 - val_regression_loss: 354.8865\n","Epoch 40/50\n","537/537 [==============================] - 0s 70us/step - loss: 4276.0020 - regression_loss: 2008.7617 - val_loss: 730.6521 - val_regression_loss: 347.4387\n","Epoch 41/50\n","537/537 [==============================] - 0s 70us/step - loss: 3952.3646 - regression_loss: 1847.7537 - val_loss: 715.1173 - val_regression_loss: 339.6382\n","Epoch 42/50\n","537/537 [==============================] - 0s 67us/step - loss: 4061.3449 - regression_loss: 1903.9342 - val_loss: 700.3727 - val_regression_loss: 332.3092\n","Epoch 43/50\n","537/537 [==============================] - 0s 60us/step - loss: 3862.3397 - regression_loss: 1805.0644 - val_loss: 686.7797 - val_regression_loss: 325.5735\n","Epoch 44/50\n","537/537 [==============================] - 0s 65us/step - loss: 3699.1930 - regression_loss: 1719.4248 - val_loss: 671.6998 - val_regression_loss: 318.1113\n","Epoch 45/50\n","537/537 [==============================] - 0s 68us/step - loss: 3672.4576 - regression_loss: 1710.2933 - val_loss: 654.6765 - val_regression_loss: 309.6237\n","Epoch 46/50\n","537/537 [==============================] - 0s 77us/step - loss: 3716.2106 - regression_loss: 1731.6660 - val_loss: 636.9619 - val_regression_loss: 300.7555\n","Epoch 47/50\n","537/537 [==============================] - 0s 66us/step - loss: 3567.8679 - regression_loss: 1656.9388 - val_loss: 620.2141 - val_regression_loss: 292.3352\n","Epoch 48/50\n","537/537 [==============================] - 0s 70us/step - loss: 3470.9813 - regression_loss: 1609.5464 - val_loss: 604.1490 - val_regression_loss: 284.2630\n","Epoch 49/50\n","537/537 [==============================] - 0s 70us/step - loss: 3459.9267 - regression_loss: 1603.1272 - val_loss: 588.6656 - val_regression_loss: 276.5121\n","Epoch 50/50\n","537/537 [==============================] - 0s 63us/step - loss: 3395.7906 - regression_loss: 1571.2792 - val_loss: 573.6282 - val_regression_loss: 268.9834\n","***************************** elapsed_time is:  3.9248616695404053\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 22422.3045 - regression_loss: 11077.2984 - val_loss: 1894.8777 - val_regression_loss: 928.3183\n","Epoch 2/50\n","537/537 [==============================] - 0s 70us/step - loss: 16816.3156 - regression_loss: 8262.6893 - val_loss: 1234.4471 - val_regression_loss: 595.9217\n","Epoch 3/50\n","537/537 [==============================] - 0s 69us/step - loss: 11269.7363 - regression_loss: 5471.9728 - val_loss: 706.7886 - val_regression_loss: 329.4051\n","Epoch 4/50\n","537/537 [==============================] - 0s 59us/step - loss: 6792.2082 - regression_loss: 3210.8373 - val_loss: 596.6628 - val_regression_loss: 272.7666\n","Epoch 5/50\n","537/537 [==============================] - 0s 57us/step - loss: 5454.0296 - regression_loss: 2528.8361 - val_loss: 486.6182 - val_regression_loss: 220.2455\n","Epoch 6/50\n","537/537 [==============================] - 0s 60us/step - loss: 4043.8365 - regression_loss: 1845.8440 - val_loss: 360.1140 - val_regression_loss: 161.3148\n","Epoch 7/50\n","537/537 [==============================] - 0s 62us/step - loss: 2721.0375 - regression_loss: 1219.9162 - val_loss: 426.4619 - val_regression_loss: 198.2747\n","Epoch 8/50\n","537/537 [==============================] - 0s 68us/step - loss: 3131.4186 - regression_loss: 1456.1577 - val_loss: 506.4922 - val_regression_loss: 240.0148\n","Epoch 9/50\n","537/537 [==============================] - 0s 73us/step - loss: 3729.2944 - regression_loss: 1767.9119 - val_loss: 435.7908 - val_regression_loss: 204.5377\n","Epoch 10/50\n","537/537 [==============================] - 0s 63us/step - loss: 3279.5998 - regression_loss: 1541.3001 - val_loss: 307.6363 - val_regression_loss: 139.3101\n","Epoch 11/50\n","537/537 [==============================] - 0s 67us/step - loss: 2368.1537 - regression_loss: 1075.0888 - val_loss: 232.4090 - val_regression_loss: 100.1006\n","Epoch 12/50\n","537/537 [==============================] - 0s 60us/step - loss: 1932.3738 - regression_loss: 840.7461 - val_loss: 230.8628 - val_regression_loss: 97.7354\n","Epoch 13/50\n","537/537 [==============================] - 0s 67us/step - loss: 2046.2321 - regression_loss: 886.6781 - val_loss: 248.4359 - val_regression_loss: 105.4433\n","Epoch 14/50\n","537/537 [==============================] - 0s 66us/step - loss: 2275.4185 - regression_loss: 990.9841 - val_loss: 239.5437 - val_regression_loss: 100.7769\n","Epoch 15/50\n","537/537 [==============================] - 0s 61us/step - loss: 2219.0491 - regression_loss: 958.3146 - val_loss: 220.3618 - val_regression_loss: 91.7169\n","Epoch 16/50\n","537/537 [==============================] - 0s 67us/step - loss: 2011.3372 - regression_loss: 857.1539 - val_loss: 214.5539 - val_regression_loss: 89.7120\n","Epoch 17/50\n","537/537 [==============================] - 0s 66us/step - loss: 1857.7440 - regression_loss: 793.0769 - val_loss: 217.2502 - val_regression_loss: 91.9980\n","Epoch 18/50\n","537/537 [==============================] - 0s 65us/step - loss: 1746.0767 - regression_loss: 744.7690 - val_loss: 221.2484 - val_regression_loss: 94.8108\n","Epoch 19/50\n","537/537 [==============================] - 0s 67us/step - loss: 1688.8838 - regression_loss: 724.4540 - val_loss: 230.3700 - val_regression_loss: 99.9417\n","Epoch 20/50\n","537/537 [==============================] - 0s 75us/step - loss: 1676.8019 - regression_loss: 721.4870 - val_loss: 241.1901 - val_regression_loss: 105.5846\n","Epoch 21/50\n","537/537 [==============================] - 0s 63us/step - loss: 1674.9091 - regression_loss: 722.7824 - val_loss: 241.2766 - val_regression_loss: 105.4861\n","Epoch 22/50\n","537/537 [==============================] - 0s 76us/step - loss: 1692.3643 - regression_loss: 731.1147 - val_loss: 229.1113 - val_regression_loss: 99.0137\n","Epoch 23/50\n","537/537 [==============================] - 0s 63us/step - loss: 1588.6487 - regression_loss: 678.5585 - val_loss: 218.5866 - val_regression_loss: 93.2890\n","Epoch 24/50\n","537/537 [==============================] - 0s 66us/step - loss: 1550.0821 - regression_loss: 653.7602 - val_loss: 214.4112 - val_regression_loss: 90.7689\n","Epoch 25/50\n","537/537 [==============================] - 0s 64us/step - loss: 1516.7015 - regression_loss: 634.6122 - val_loss: 213.0008 - val_regression_loss: 89.6823\n","Epoch 26/50\n","537/537 [==============================] - 0s 69us/step - loss: 1513.1183 - regression_loss: 631.6181 - val_loss: 215.4132 - val_regression_loss: 90.6292\n","Epoch 27/50\n","537/537 [==============================] - 0s 69us/step - loss: 1507.7408 - regression_loss: 623.2452 - val_loss: 219.7420 - val_regression_loss: 92.7399\n","Epoch 28/50\n","537/537 [==============================] - 0s 64us/step - loss: 1518.3034 - regression_loss: 629.6067 - val_loss: 221.1007 - val_regression_loss: 93.5690\n","Epoch 29/50\n","537/537 [==============================] - 0s 65us/step - loss: 1512.0982 - regression_loss: 628.3792 - val_loss: 220.9558 - val_regression_loss: 93.7775\n","Epoch 30/50\n","537/537 [==============================] - 0s 62us/step - loss: 1453.4216 - regression_loss: 604.4547 - val_loss: 223.5354 - val_regression_loss: 95.3098\n","Epoch 31/50\n","537/537 [==============================] - 0s 68us/step - loss: 1431.2948 - regression_loss: 593.0311 - val_loss: 227.6714 - val_regression_loss: 97.5258\n","Epoch 32/50\n","537/537 [==============================] - 0s 67us/step - loss: 1448.0452 - regression_loss: 604.1715 - val_loss: 232.2095 - val_regression_loss: 99.8227\n","Epoch 33/50\n","537/537 [==============================] - 0s 66us/step - loss: 1418.3177 - regression_loss: 589.1088 - val_loss: 234.4926 - val_regression_loss: 100.9306\n","Epoch 34/50\n","537/537 [==============================] - 0s 64us/step - loss: 1369.9380 - regression_loss: 564.5611 - val_loss: 233.0944 - val_regression_loss: 100.1706\n","Epoch 35/50\n","537/537 [==============================] - 0s 63us/step - loss: 1411.9077 - regression_loss: 585.5543 - val_loss: 231.8020 - val_regression_loss: 99.4079\n","Epoch 36/50\n","537/537 [==============================] - 0s 61us/step - loss: 1391.9437 - regression_loss: 575.2125 - val_loss: 231.3315 - val_regression_loss: 99.0621\n","Epoch 37/50\n","537/537 [==============================] - 0s 75us/step - loss: 1383.8310 - regression_loss: 572.4226 - val_loss: 232.9673 - val_regression_loss: 99.7473\n","Epoch 38/50\n","537/537 [==============================] - 0s 67us/step - loss: 1367.1006 - regression_loss: 562.7114 - val_loss: 233.2495 - val_regression_loss: 99.8386\n","Epoch 39/50\n","537/537 [==============================] - 0s 54us/step - loss: 1367.5960 - regression_loss: 560.1775 - val_loss: 232.5098 - val_regression_loss: 99.5112\n","Epoch 40/50\n","537/537 [==============================] - 0s 67us/step - loss: 1331.6506 - regression_loss: 545.5569 - val_loss: 231.8349 - val_regression_loss: 99.2450\n","Epoch 41/50\n","537/537 [==============================] - 0s 61us/step - loss: 1346.3255 - regression_loss: 554.0453 - val_loss: 231.4549 - val_regression_loss: 99.1380\n","Epoch 42/50\n","537/537 [==============================] - 0s 70us/step - loss: 1341.9221 - regression_loss: 552.5852 - val_loss: 233.0995 - val_regression_loss: 99.9742\n","Epoch 43/50\n","537/537 [==============================] - 0s 86us/step - loss: 1329.4461 - regression_loss: 545.7946 - val_loss: 234.0801 - val_regression_loss: 100.4532\n","Epoch 44/50\n","537/537 [==============================] - 0s 77us/step - loss: 1345.5930 - regression_loss: 554.2294 - val_loss: 234.2451 - val_regression_loss: 100.5166\n","Epoch 45/50\n","537/537 [==============================] - 0s 66us/step - loss: 1330.0130 - regression_loss: 548.1588 - val_loss: 233.1706 - val_regression_loss: 99.9668\n","Epoch 46/50\n","537/537 [==============================] - 0s 64us/step - loss: 1323.8523 - regression_loss: 544.5614 - val_loss: 233.0488 - val_regression_loss: 99.8752\n","Epoch 47/50\n","537/537 [==============================] - 0s 64us/step - loss: 1290.8159 - regression_loss: 527.3310 - val_loss: 233.7990 - val_regression_loss: 100.1773\n","Epoch 48/50\n","537/537 [==============================] - 0s 70us/step - loss: 1311.6315 - regression_loss: 538.4199 - val_loss: 233.2115 - val_regression_loss: 99.8408\n","Epoch 49/50\n","537/537 [==============================] - 0s 71us/step - loss: 1290.0949 - regression_loss: 527.0325 - val_loss: 231.8954 - val_regression_loss: 99.1917\n","Epoch 50/50\n","537/537 [==============================] - 0s 66us/step - loss: 1273.2868 - regression_loss: 519.7267 - val_loss: 229.8991 - val_regression_loss: 98.2433\n","***************************** elapsed_time is:  4.3613128662109375\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 765748.5673 - regression_loss: 382606.2970 - val_loss: 98031.8594 - val_regression_loss: 48984.0039\n","Epoch 2/50\n","537/537 [==============================] - 0s 63us/step - loss: 723296.6390 - regression_loss: 361409.6918 - val_loss: 90561.4375 - val_regression_loss: 45252.3594\n","Epoch 3/50\n","537/537 [==============================] - 0s 63us/step - loss: 673420.7499 - regression_loss: 336498.8376 - val_loss: 80379.2266 - val_regression_loss: 40164.8164\n","Epoch 4/50\n","537/537 [==============================] - 0s 73us/step - loss: 595119.5789 - regression_loss: 297376.8707 - val_loss: 67385.2500 - val_regression_loss: 33671.0469\n","Epoch 5/50\n","537/537 [==============================] - 0s 63us/step - loss: 494030.5883 - regression_loss: 246857.4679 - val_loss: 52277.0938 - val_regression_loss: 26119.4336\n","Epoch 6/50\n","537/537 [==============================] - 0s 60us/step - loss: 370510.4739 - regression_loss: 185116.4952 - val_loss: 36564.5234 - val_regression_loss: 18264.3496\n","Epoch 7/50\n","537/537 [==============================] - 0s 71us/step - loss: 253499.4121 - regression_loss: 126621.2330 - val_loss: 22506.9492 - val_regression_loss: 11235.1426\n","Epoch 8/50\n","537/537 [==============================] - 0s 56us/step - loss: 139692.9529 - regression_loss: 69711.8661 - val_loss: 13541.6709 - val_regression_loss: 6751.1763\n","Epoch 9/50\n","537/537 [==============================] - 0s 69us/step - loss: 86950.3657 - regression_loss: 43328.3668 - val_loss: 13763.0449 - val_regression_loss: 6862.0854\n","Epoch 10/50\n","537/537 [==============================] - 0s 69us/step - loss: 106614.2305 - regression_loss: 53165.5884 - val_loss: 17939.7246 - val_regression_loss: 8953.0361\n","Epoch 11/50\n","537/537 [==============================] - 0s 88us/step - loss: 145791.7373 - regression_loss: 72776.6770 - val_loss: 16585.1582 - val_regression_loss: 8277.5332\n","Epoch 12/50\n","537/537 [==============================] - 0s 61us/step - loss: 137440.2187 - regression_loss: 68616.8878 - val_loss: 12634.1152 - val_regression_loss: 6302.3091\n","Epoch 13/50\n","537/537 [==============================] - 0s 59us/step - loss: 99213.2648 - regression_loss: 49503.6157 - val_loss: 10616.3809 - val_regression_loss: 5292.7773\n","Epoch 14/50\n","537/537 [==============================] - 0s 58us/step - loss: 74830.0835 - regression_loss: 37310.1724 - val_loss: 11002.9121 - val_regression_loss: 5484.8662\n","Epoch 15/50\n","537/537 [==============================] - 0s 64us/step - loss: 71858.0561 - regression_loss: 35816.1764 - val_loss: 12296.7227 - val_regression_loss: 6130.4673\n","Epoch 16/50\n","537/537 [==============================] - 0s 60us/step - loss: 75897.8152 - regression_loss: 37822.5160 - val_loss: 13229.6973 - val_regression_loss: 6595.9219\n","Epoch 17/50\n","537/537 [==============================] - 0s 58us/step - loss: 83929.5512 - regression_loss: 41832.7382 - val_loss: 13197.3623 - val_regression_loss: 6579.2505\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 81085.8267 - regression_loss: 40404.5920 - val_loss: 12294.1299 - val_regression_loss: 6127.6802\n","Epoch 19/50\n","537/537 [==============================] - 0s 59us/step - loss: 76917.8176 - regression_loss: 38325.5602 - val_loss: 10876.0068 - val_regression_loss: 5419.0425\n","Epoch 20/50\n","537/537 [==============================] - 0s 65us/step - loss: 69518.8219 - regression_loss: 34623.8615 - val_loss: 9474.4199 - val_regression_loss: 4718.8643\n","Epoch 21/50\n","537/537 [==============================] - 0s 66us/step - loss: 59006.5582 - regression_loss: 29372.9712 - val_loss: 8553.0117 - val_regression_loss: 4258.7241\n","Epoch 22/50\n","537/537 [==============================] - 0s 64us/step - loss: 59022.4193 - regression_loss: 29386.8524 - val_loss: 8183.5962 - val_regression_loss: 4074.4084\n","Epoch 23/50\n","537/537 [==============================] - 0s 64us/step - loss: 59062.8124 - regression_loss: 29408.9953 - val_loss: 8073.8164 - val_regression_loss: 4019.7529\n","Epoch 24/50\n","537/537 [==============================] - 0s 67us/step - loss: 62033.0916 - regression_loss: 30896.4047 - val_loss: 7891.3760 - val_regression_loss: 3928.6104\n","Epoch 25/50\n","537/537 [==============================] - 0s 60us/step - loss: 59982.9844 - regression_loss: 29874.9785 - val_loss: 7628.6904 - val_regression_loss: 3797.1211\n","Epoch 26/50\n","537/537 [==============================] - 0s 63us/step - loss: 56792.5028 - regression_loss: 28278.4737 - val_loss: 7480.7979 - val_regression_loss: 3722.7346\n","Epoch 27/50\n","537/537 [==============================] - 0s 67us/step - loss: 52739.8916 - regression_loss: 26251.3276 - val_loss: 7529.8379 - val_regression_loss: 3746.6021\n","Epoch 28/50\n","537/537 [==============================] - 0s 58us/step - loss: 51479.8051 - regression_loss: 25617.1303 - val_loss: 7645.3765 - val_regression_loss: 3803.7241\n","Epoch 29/50\n","537/537 [==============================] - 0s 64us/step - loss: 49254.7094 - regression_loss: 24497.8070 - val_loss: 7653.6826 - val_regression_loss: 3807.4509\n","Epoch 30/50\n","537/537 [==============================] - 0s 62us/step - loss: 48984.4549 - regression_loss: 24360.0854 - val_loss: 7439.3262 - val_regression_loss: 3700.1841\n","Epoch 31/50\n","537/537 [==============================] - 0s 67us/step - loss: 45161.1550 - regression_loss: 22450.4681 - val_loss: 7046.4980 - val_regression_loss: 3504.0137\n","Epoch 32/50\n","537/537 [==============================] - 0s 70us/step - loss: 44780.6634 - regression_loss: 22264.6104 - val_loss: 6575.9683 - val_regression_loss: 3269.2102\n","Epoch 33/50\n","537/537 [==============================] - 0s 69us/step - loss: 43504.2289 - regression_loss: 21629.8120 - val_loss: 6143.1841 - val_regression_loss: 3053.2844\n","Epoch 34/50\n","537/537 [==============================] - 0s 67us/step - loss: 38692.0928 - regression_loss: 19223.8343 - val_loss: 5820.6753 - val_regression_loss: 2892.3079\n","Epoch 35/50\n","537/537 [==============================] - 0s 73us/step - loss: 38158.0356 - regression_loss: 18962.2995 - val_loss: 5551.5562 - val_regression_loss: 2757.6038\n","Epoch 36/50\n","537/537 [==============================] - 0s 63us/step - loss: 36803.3444 - regression_loss: 18285.6284 - val_loss: 5319.4414 - val_regression_loss: 2640.9258\n","Epoch 37/50\n","537/537 [==============================] - 0s 58us/step - loss: 37216.3460 - regression_loss: 18488.8784 - val_loss: 5136.4150 - val_regression_loss: 2548.5745\n","Epoch 38/50\n","537/537 [==============================] - 0s 60us/step - loss: 34472.7336 - regression_loss: 17113.2636 - val_loss: 5013.3726 - val_regression_loss: 2486.1025\n","Epoch 39/50\n","537/537 [==============================] - 0s 64us/step - loss: 32905.4309 - regression_loss: 16323.2412 - val_loss: 4916.3979 - val_regression_loss: 2436.9329\n","Epoch 40/50\n","537/537 [==============================] - 0s 62us/step - loss: 29767.6501 - regression_loss: 14747.4372 - val_loss: 4783.4375 - val_regression_loss: 2370.1509\n","Epoch 41/50\n","537/537 [==============================] - 0s 63us/step - loss: 30310.4440 - regression_loss: 15021.9008 - val_loss: 4576.8955 - val_regression_loss: 2266.8262\n","Epoch 42/50\n","537/537 [==============================] - 0s 66us/step - loss: 28891.4685 - regression_loss: 14308.6215 - val_loss: 4346.3135 - val_regression_loss: 2151.6650\n","Epoch 43/50\n","537/537 [==============================] - 0s 70us/step - loss: 25688.0416 - regression_loss: 12705.9027 - val_loss: 4143.2183 - val_regression_loss: 2050.2651\n","Epoch 44/50\n","537/537 [==============================] - 0s 66us/step - loss: 23941.1331 - regression_loss: 11830.8426 - val_loss: 3972.2083 - val_regression_loss: 1964.8723\n","Epoch 45/50\n","537/537 [==============================] - 0s 67us/step - loss: 23943.1768 - regression_loss: 11834.2892 - val_loss: 3832.6443 - val_regression_loss: 1895.0830\n","Epoch 46/50\n","537/537 [==============================] - 0s 58us/step - loss: 20775.1619 - regression_loss: 10250.3401 - val_loss: 3727.0635 - val_regression_loss: 1842.3135\n","Epoch 47/50\n","537/537 [==============================] - 0s 59us/step - loss: 21624.6395 - regression_loss: 10673.6845 - val_loss: 3629.8459 - val_regression_loss: 1793.6833\n","Epoch 48/50\n","537/537 [==============================] - 0s 62us/step - loss: 19627.8416 - regression_loss: 9674.4552 - val_loss: 3543.3394 - val_regression_loss: 1750.3530\n","Epoch 49/50\n","537/537 [==============================] - 0s 62us/step - loss: 18555.0516 - regression_loss: 9129.7907 - val_loss: 3442.4231 - val_regression_loss: 1699.7943\n","Epoch 50/50\n","537/537 [==============================] - 0s 65us/step - loss: 17834.5523 - regression_loss: 8770.2988 - val_loss: 3335.7102 - val_regression_loss: 1646.2692\n","***************************** elapsed_time is:  3.9507405757904053\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 19930.9910 - regression_loss: 9741.1861 - val_loss: 1793.8329 - val_regression_loss: 871.6832\n","Epoch 2/50\n","537/537 [==============================] - 0s 60us/step - loss: 14965.5538 - regression_loss: 7294.8867 - val_loss: 1193.6730 - val_regression_loss: 576.6992\n","Epoch 3/50\n","537/537 [==============================] - 0s 61us/step - loss: 9550.1813 - regression_loss: 4626.5723 - val_loss: 755.8599 - val_regression_loss: 363.6655\n","Epoch 4/50\n","537/537 [==============================] - 0s 63us/step - loss: 5903.8985 - regression_loss: 2850.9415 - val_loss: 650.0107 - val_regression_loss: 314.5688\n","Epoch 5/50\n","537/537 [==============================] - 0s 55us/step - loss: 5118.5002 - regression_loss: 2487.1690 - val_loss: 435.6283 - val_regression_loss: 205.1603\n","Epoch 6/50\n","537/537 [==============================] - 0s 58us/step - loss: 3453.8945 - regression_loss: 1634.1131 - val_loss: 276.8805 - val_regression_loss: 120.3075\n","Epoch 7/50\n","537/537 [==============================] - 0s 57us/step - loss: 2290.7447 - regression_loss: 1000.4905 - val_loss: 345.1901 - val_regression_loss: 149.9026\n","Epoch 8/50\n","537/537 [==============================] - 0s 84us/step - loss: 2883.5971 - regression_loss: 1260.0368 - val_loss: 406.1927 - val_regression_loss: 179.6617\n","Epoch 9/50\n","537/537 [==============================] - 0s 62us/step - loss: 3299.9951 - regression_loss: 1463.8965 - val_loss: 340.3362 - val_regression_loss: 148.5607\n","Epoch 10/50\n","537/537 [==============================] - 0s 72us/step - loss: 2718.4339 - regression_loss: 1190.1764 - val_loss: 248.8660 - val_regression_loss: 105.4669\n","Epoch 11/50\n","537/537 [==============================] - 0s 62us/step - loss: 2009.2553 - regression_loss: 857.8655 - val_loss: 213.2085 - val_regression_loss: 90.1873\n","Epoch 12/50\n","537/537 [==============================] - 0s 61us/step - loss: 1740.2556 - regression_loss: 745.2393 - val_loss: 234.8394 - val_regression_loss: 102.9025\n","Epoch 13/50\n","537/537 [==============================] - 0s 63us/step - loss: 1916.8746 - regression_loss: 849.6802 - val_loss: 256.9367 - val_regression_loss: 114.8224\n","Epoch 14/50\n","537/537 [==============================] - 0s 68us/step - loss: 2043.8689 - regression_loss: 920.0037 - val_loss: 250.4469 - val_regression_loss: 111.4309\n","Epoch 15/50\n","537/537 [==============================] - 0s 63us/step - loss: 1954.3755 - regression_loss: 876.9478 - val_loss: 236.4890 - val_regression_loss: 103.6365\n","Epoch 16/50\n","537/537 [==============================] - 0s 66us/step - loss: 1769.7527 - regression_loss: 775.4834 - val_loss: 231.9494 - val_regression_loss: 100.3410\n","Epoch 17/50\n","537/537 [==============================] - 0s 66us/step - loss: 1649.5423 - regression_loss: 705.8855 - val_loss: 230.7055 - val_regression_loss: 98.8081\n","Epoch 18/50\n","537/537 [==============================] - 0s 66us/step - loss: 1630.4429 - regression_loss: 688.6465 - val_loss: 229.9704 - val_regression_loss: 97.7424\n","Epoch 19/50\n","537/537 [==============================] - 0s 65us/step - loss: 1569.5939 - regression_loss: 652.3968 - val_loss: 231.1306 - val_regression_loss: 97.9290\n","Epoch 20/50\n","537/537 [==============================] - 0s 61us/step - loss: 1602.1825 - regression_loss: 659.5591 - val_loss: 231.6608 - val_regression_loss: 98.2822\n","Epoch 21/50\n","537/537 [==============================] - 0s 59us/step - loss: 1589.8883 - regression_loss: 652.7884 - val_loss: 229.2386 - val_regression_loss: 97.5897\n","Epoch 22/50\n","537/537 [==============================] - 0s 57us/step - loss: 1530.1599 - regression_loss: 630.9542 - val_loss: 228.8837 - val_regression_loss: 98.0878\n","Epoch 23/50\n","537/537 [==============================] - 0s 66us/step - loss: 1482.4804 - regression_loss: 611.9844 - val_loss: 234.3048 - val_regression_loss: 101.3901\n","Epoch 24/50\n","537/537 [==============================] - 0s 64us/step - loss: 1434.2918 - regression_loss: 594.1386 - val_loss: 241.7933 - val_regression_loss: 105.5726\n","Epoch 25/50\n","537/537 [==============================] - 0s 73us/step - loss: 1457.4549 - regression_loss: 610.0923 - val_loss: 245.1956 - val_regression_loss: 107.5527\n","Epoch 26/50\n","537/537 [==============================] - 0s 61us/step - loss: 1409.1070 - regression_loss: 587.0358 - val_loss: 245.9984 - val_regression_loss: 108.0364\n","Epoch 27/50\n","537/537 [==============================] - 0s 63us/step - loss: 1418.1754 - regression_loss: 590.5142 - val_loss: 245.0258 - val_regression_loss: 107.4150\n","Epoch 28/50\n","537/537 [==============================] - 0s 60us/step - loss: 1396.4647 - regression_loss: 581.2220 - val_loss: 244.1371 - val_regression_loss: 106.6552\n","Epoch 29/50\n","537/537 [==============================] - 0s 59us/step - loss: 1355.9496 - regression_loss: 554.9543 - val_loss: 244.8984 - val_regression_loss: 106.6780\n","Epoch 30/50\n","537/537 [==============================] - 0s 65us/step - loss: 1360.2887 - regression_loss: 553.7627 - val_loss: 247.4596 - val_regression_loss: 107.7631\n","Epoch 31/50\n","537/537 [==============================] - 0s 59us/step - loss: 1350.7760 - regression_loss: 547.4626 - val_loss: 249.0461 - val_regression_loss: 108.6246\n","Epoch 32/50\n","537/537 [==============================] - 0s 63us/step - loss: 1368.8765 - regression_loss: 558.1055 - val_loss: 249.3575 - val_regression_loss: 109.0350\n","Epoch 33/50\n","537/537 [==============================] - 0s 59us/step - loss: 1330.8451 - regression_loss: 541.4499 - val_loss: 248.5193 - val_regression_loss: 108.9174\n","Epoch 34/50\n","537/537 [==============================] - 0s 61us/step - loss: 1331.0950 - regression_loss: 541.5865 - val_loss: 246.0762 - val_regression_loss: 107.9547\n","Epoch 35/50\n","537/537 [==============================] - 0s 59us/step - loss: 1320.6385 - regression_loss: 542.5757 - val_loss: 241.3117 - val_regression_loss: 105.6342\n","Epoch 36/50\n","537/537 [==============================] - 0s 63us/step - loss: 1294.9501 - regression_loss: 526.5237 - val_loss: 237.5941 - val_regression_loss: 103.8001\n","Epoch 37/50\n","537/537 [==============================] - 0s 60us/step - loss: 1297.9968 - regression_loss: 529.9669 - val_loss: 234.5967 - val_regression_loss: 102.2671\n","Epoch 38/50\n","537/537 [==============================] - 0s 60us/step - loss: 1305.7161 - regression_loss: 532.9599 - val_loss: 232.1818 - val_regression_loss: 100.9975\n","Epoch 39/50\n","537/537 [==============================] - 0s 59us/step - loss: 1288.6872 - regression_loss: 523.0188 - val_loss: 230.4663 - val_regression_loss: 100.0639\n","Epoch 40/50\n","537/537 [==============================] - 0s 57us/step - loss: 1291.6373 - regression_loss: 524.9697 - val_loss: 227.4275 - val_regression_loss: 98.4238\n","Epoch 41/50\n","537/537 [==============================] - 0s 57us/step - loss: 1284.7379 - regression_loss: 521.7356 - val_loss: 223.6719 - val_regression_loss: 96.5201\n","Epoch 42/50\n","537/537 [==============================] - 0s 57us/step - loss: 1277.8806 - regression_loss: 518.2288 - val_loss: 219.1065 - val_regression_loss: 94.3154\n","Epoch 43/50\n","537/537 [==============================] - 0s 58us/step - loss: 1268.9525 - regression_loss: 515.1430 - val_loss: 214.8602 - val_regression_loss: 92.3271\n","Epoch 44/50\n","537/537 [==============================] - 0s 57us/step - loss: 1286.8012 - regression_loss: 524.1577 - val_loss: 211.7214 - val_regression_loss: 90.8280\n","Epoch 45/50\n","537/537 [==============================] - 0s 64us/step - loss: 1263.6542 - regression_loss: 514.5061 - val_loss: 208.1830 - val_regression_loss: 89.0536\n","Epoch 46/50\n","537/537 [==============================] - 0s 60us/step - loss: 1200.7187 - regression_loss: 485.0683 - val_loss: 204.2164 - val_regression_loss: 87.0443\n","Epoch 47/50\n","537/537 [==============================] - 0s 60us/step - loss: 1272.1015 - regression_loss: 520.5772 - val_loss: 199.8464 - val_regression_loss: 84.7962\n","Epoch 48/50\n","537/537 [==============================] - 0s 58us/step - loss: 1224.8327 - regression_loss: 494.4253 - val_loss: 196.6689 - val_regression_loss: 83.1323\n","Epoch 49/50\n","537/537 [==============================] - 0s 60us/step - loss: 1250.4563 - regression_loss: 508.8484 - val_loss: 193.8395 - val_regression_loss: 81.6850\n","Epoch 50/50\n","537/537 [==============================] - 0s 57us/step - loss: 1250.9830 - regression_loss: 506.6277 - val_loss: 191.6886 - val_regression_loss: 80.6355\n","***************************** elapsed_time is:  3.838151216506958\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 32091.9970 - regression_loss: 16031.1376 - val_loss: 2863.4775 - val_regression_loss: 1420.5283\n","Epoch 2/50\n","537/537 [==============================] - 0s 56us/step - loss: 25421.8700 - regression_loss: 12651.0906 - val_loss: 2097.2456 - val_regression_loss: 1031.6586\n","Epoch 3/50\n","537/537 [==============================] - 0s 69us/step - loss: 18648.5102 - regression_loss: 9215.3646 - val_loss: 1308.9434 - val_regression_loss: 628.9603\n","Epoch 4/50\n","537/537 [==============================] - 0s 56us/step - loss: 11253.9835 - regression_loss: 5454.2210 - val_loss: 861.4033 - val_regression_loss: 393.3342\n","Epoch 5/50\n","537/537 [==============================] - 0s 57us/step - loss: 7217.5784 - regression_loss: 3339.3917 - val_loss: 905.2192 - val_regression_loss: 408.1256\n","Epoch 6/50\n","537/537 [==============================] - 0s 58us/step - loss: 7140.1827 - regression_loss: 3245.5244 - val_loss: 643.5468 - val_regression_loss: 284.5411\n","Epoch 7/50\n","537/537 [==============================] - 0s 55us/step - loss: 5242.5111 - regression_loss: 2353.8467 - val_loss: 367.8441 - val_regression_loss: 160.3937\n","Epoch 8/50\n","537/537 [==============================] - 0s 54us/step - loss: 3230.7251 - regression_loss: 1451.4326 - val_loss: 448.2364 - val_regression_loss: 212.6207\n","Epoch 9/50\n","537/537 [==============================] - 0s 59us/step - loss: 4108.8093 - regression_loss: 1986.6489 - val_loss: 545.6920 - val_regression_loss: 265.5042\n","Epoch 10/50\n","537/537 [==============================] - 0s 72us/step - loss: 5070.2772 - regression_loss: 2500.0360 - val_loss: 441.1924 - val_regression_loss: 211.0473\n","Epoch 11/50\n","537/537 [==============================] - 0s 56us/step - loss: 4059.2809 - regression_loss: 1973.9897 - val_loss: 320.9125 - val_regression_loss: 146.0082\n","Epoch 12/50\n","537/537 [==============================] - 0s 61us/step - loss: 2968.5406 - regression_loss: 1388.8915 - val_loss: 313.4928 - val_regression_loss: 137.2490\n","Epoch 13/50\n","537/537 [==============================] - 0s 57us/step - loss: 2729.5706 - regression_loss: 1228.2457 - val_loss: 355.5391 - val_regression_loss: 154.7353\n","Epoch 14/50\n","537/537 [==============================] - 0s 58us/step - loss: 2993.8914 - regression_loss: 1332.7363 - val_loss: 348.1572 - val_regression_loss: 149.7365\n","Epoch 15/50\n","537/537 [==============================] - 0s 74us/step - loss: 2903.5092 - regression_loss: 1279.0452 - val_loss: 310.4670 - val_regression_loss: 131.4637\n","Epoch 16/50\n","537/537 [==============================] - 0s 60us/step - loss: 2709.3150 - regression_loss: 1186.2078 - val_loss: 290.3908 - val_regression_loss: 123.0331\n","Epoch 17/50\n","537/537 [==============================] - 0s 60us/step - loss: 2530.2063 - regression_loss: 1106.5673 - val_loss: 280.4510 - val_regression_loss: 119.8066\n","Epoch 18/50\n","537/537 [==============================] - 0s 65us/step - loss: 2502.7277 - regression_loss: 1114.6122 - val_loss: 260.4796 - val_regression_loss: 111.1583\n","Epoch 19/50\n","537/537 [==============================] - 0s 67us/step - loss: 2377.6189 - regression_loss: 1064.5079 - val_loss: 239.6953 - val_regression_loss: 101.7326\n","Epoch 20/50\n","537/537 [==============================] - 0s 63us/step - loss: 2169.5282 - regression_loss: 965.4540 - val_loss: 235.6454 - val_regression_loss: 100.3949\n","Epoch 21/50\n","537/537 [==============================] - 0s 61us/step - loss: 2083.0226 - regression_loss: 928.9054 - val_loss: 239.0521 - val_regression_loss: 102.5197\n","Epoch 22/50\n","537/537 [==============================] - 0s 60us/step - loss: 2090.3585 - regression_loss: 936.1707 - val_loss: 231.4431 - val_regression_loss: 98.9032\n","Epoch 23/50\n","537/537 [==============================] - 0s 62us/step - loss: 2045.6078 - regression_loss: 915.7326 - val_loss: 218.7424 - val_regression_loss: 92.5076\n","Epoch 24/50\n","537/537 [==============================] - 0s 62us/step - loss: 1898.9664 - regression_loss: 840.1835 - val_loss: 214.2956 - val_regression_loss: 89.9469\n","Epoch 25/50\n","537/537 [==============================] - 0s 64us/step - loss: 1914.8121 - regression_loss: 847.7693 - val_loss: 213.5245 - val_regression_loss: 88.8016\n","Epoch 26/50\n","537/537 [==============================] - 0s 59us/step - loss: 1830.8382 - regression_loss: 801.4226 - val_loss: 213.7592 - val_regression_loss: 87.8528\n","Epoch 27/50\n","537/537 [==============================] - 0s 62us/step - loss: 1793.9151 - regression_loss: 771.4343 - val_loss: 219.6231 - val_regression_loss: 89.7787\n","Epoch 28/50\n","537/537 [==============================] - 0s 64us/step - loss: 1741.4540 - regression_loss: 739.6804 - val_loss: 227.8020 - val_regression_loss: 93.2857\n","Epoch 29/50\n","537/537 [==============================] - 0s 63us/step - loss: 1749.2825 - regression_loss: 740.6943 - val_loss: 224.8683 - val_regression_loss: 91.9750\n","Epoch 30/50\n","537/537 [==============================] - 0s 59us/step - loss: 1718.9332 - regression_loss: 726.4257 - val_loss: 214.8943 - val_regression_loss: 87.6310\n","Epoch 31/50\n","537/537 [==============================] - 0s 61us/step - loss: 1667.2065 - regression_loss: 707.6779 - val_loss: 207.2755 - val_regression_loss: 84.4809\n","Epoch 32/50\n","537/537 [==============================] - 0s 75us/step - loss: 1643.2512 - regression_loss: 697.2342 - val_loss: 203.8986 - val_regression_loss: 83.1877\n","Epoch 33/50\n","537/537 [==============================] - 0s 70us/step - loss: 1600.9784 - regression_loss: 681.4697 - val_loss: 204.3550 - val_regression_loss: 83.4618\n","Epoch 34/50\n","537/537 [==============================] - 0s 62us/step - loss: 1556.3267 - regression_loss: 659.0779 - val_loss: 206.6086 - val_regression_loss: 84.5218\n","Epoch 35/50\n","537/537 [==============================] - 0s 59us/step - loss: 1553.2172 - regression_loss: 658.3996 - val_loss: 206.8373 - val_regression_loss: 84.5893\n","Epoch 36/50\n","537/537 [==============================] - 0s 61us/step - loss: 1539.8276 - regression_loss: 648.8483 - val_loss: 205.3988 - val_regression_loss: 83.8880\n","Epoch 37/50\n","537/537 [==============================] - 0s 58us/step - loss: 1489.6233 - regression_loss: 623.9877 - val_loss: 204.6739 - val_regression_loss: 83.4841\n","Epoch 38/50\n","537/537 [==============================] - 0s 59us/step - loss: 1485.7975 - regression_loss: 622.2834 - val_loss: 206.1986 - val_regression_loss: 83.9799\n","Epoch 39/50\n","537/537 [==============================] - 0s 60us/step - loss: 1469.5130 - regression_loss: 610.0354 - val_loss: 208.4025 - val_regression_loss: 84.7975\n","Epoch 40/50\n","537/537 [==============================] - 0s 67us/step - loss: 1446.6863 - regression_loss: 599.6709 - val_loss: 210.5321 - val_regression_loss: 85.7092\n","Epoch 41/50\n","537/537 [==============================] - 0s 63us/step - loss: 1410.4086 - regression_loss: 577.4479 - val_loss: 209.7622 - val_regression_loss: 85.5077\n","Epoch 42/50\n","537/537 [==============================] - 0s 62us/step - loss: 1386.7986 - regression_loss: 567.2542 - val_loss: 207.5226 - val_regression_loss: 84.7641\n","Epoch 43/50\n","537/537 [==============================] - 0s 62us/step - loss: 1382.4918 - regression_loss: 567.1675 - val_loss: 206.8833 - val_regression_loss: 84.6639\n","Epoch 44/50\n","537/537 [==============================] - 0s 59us/step - loss: 1362.7468 - regression_loss: 564.4989 - val_loss: 208.5820 - val_regression_loss: 85.4517\n","Epoch 45/50\n","537/537 [==============================] - 0s 60us/step - loss: 1384.6951 - regression_loss: 573.5398 - val_loss: 212.0993 - val_regression_loss: 87.0168\n","Epoch 46/50\n","537/537 [==============================] - 0s 61us/step - loss: 1349.0819 - regression_loss: 554.0773 - val_loss: 212.6023 - val_regression_loss: 87.2413\n","Epoch 47/50\n","537/537 [==============================] - 0s 60us/step - loss: 1331.0642 - regression_loss: 546.3480 - val_loss: 211.6413 - val_regression_loss: 86.8582\n","Epoch 48/50\n","537/537 [==============================] - 0s 60us/step - loss: 1361.6416 - regression_loss: 559.5951 - val_loss: 212.2138 - val_regression_loss: 87.1816\n","Epoch 49/50\n","537/537 [==============================] - 0s 71us/step - loss: 1318.0521 - regression_loss: 539.5065 - val_loss: 214.7852 - val_regression_loss: 88.3279\n","Epoch 50/50\n","537/537 [==============================] - 0s 64us/step - loss: 1315.7880 - regression_loss: 539.1483 - val_loss: 217.0533 - val_regression_loss: 89.3289\n","***************************** elapsed_time is:  4.075013875961304\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 47705.9223 - regression_loss: 23661.2470 - val_loss: 4859.4482 - val_regression_loss: 2404.9268\n","Epoch 2/50\n","537/537 [==============================] - 0s 56us/step - loss: 38127.6083 - regression_loss: 18880.4780 - val_loss: 3533.7463 - val_regression_loss: 1742.9038\n","Epoch 3/50\n","537/537 [==============================] - 0s 72us/step - loss: 28722.7530 - regression_loss: 14183.7423 - val_loss: 2181.4639 - val_regression_loss: 1067.4624\n","Epoch 4/50\n","537/537 [==============================] - 0s 64us/step - loss: 18544.2189 - regression_loss: 9099.3229 - val_loss: 1444.2103 - val_regression_loss: 699.5115\n","Epoch 5/50\n","537/537 [==============================] - 0s 56us/step - loss: 14228.9978 - regression_loss: 6946.8775 - val_loss: 1268.7448 - val_regression_loss: 612.8046\n","Epoch 6/50\n","537/537 [==============================] - 0s 64us/step - loss: 12946.3768 - regression_loss: 6313.5348 - val_loss: 801.7401 - val_regression_loss: 380.4387\n","Epoch 7/50\n","537/537 [==============================] - 0s 63us/step - loss: 8763.1061 - regression_loss: 4230.6830 - val_loss: 520.0679 - val_regression_loss: 240.3997\n","Epoch 8/50\n","537/537 [==============================] - 0s 59us/step - loss: 5617.5829 - regression_loss: 2663.5949 - val_loss: 708.9593 - val_regression_loss: 335.2600\n","Epoch 9/50\n","537/537 [==============================] - 0s 65us/step - loss: 6616.2046 - regression_loss: 3166.7211 - val_loss: 915.2917 - val_regression_loss: 438.7522\n","Epoch 10/50\n","537/537 [==============================] - 0s 65us/step - loss: 8082.3935 - regression_loss: 3901.3758 - val_loss: 814.3060 - val_regression_loss: 388.6867\n","Epoch 11/50\n","537/537 [==============================] - 0s 65us/step - loss: 7386.6344 - regression_loss: 3555.4269 - val_loss: 590.9929 - val_regression_loss: 277.4974\n","Epoch 12/50\n","537/537 [==============================] - 0s 59us/step - loss: 5774.9163 - regression_loss: 2755.2839 - val_loss: 456.6469 - val_regression_loss: 210.7013\n","Epoch 13/50\n","537/537 [==============================] - 0s 62us/step - loss: 5065.0890 - regression_loss: 2402.7512 - val_loss: 414.8921 - val_regression_loss: 190.0329\n","Epoch 14/50\n","537/537 [==============================] - 0s 63us/step - loss: 4950.1758 - regression_loss: 2347.7435 - val_loss: 394.3727 - val_regression_loss: 179.8427\n","Epoch 15/50\n","537/537 [==============================] - 0s 60us/step - loss: 4675.8703 - regression_loss: 2209.8488 - val_loss: 404.6648 - val_regression_loss: 184.9974\n","Epoch 16/50\n","537/537 [==============================] - 0s 61us/step - loss: 4399.7533 - regression_loss: 2071.6642 - val_loss: 446.2517 - val_regression_loss: 205.8120\n","Epoch 17/50\n","537/537 [==============================] - 0s 67us/step - loss: 4476.8121 - regression_loss: 2111.3287 - val_loss: 455.6743 - val_regression_loss: 210.5885\n","Epoch 18/50\n","537/537 [==============================] - 0s 62us/step - loss: 4385.4110 - regression_loss: 2066.1949 - val_loss: 410.1376 - val_regression_loss: 187.9217\n","Epoch 19/50\n","537/537 [==============================] - 0s 59us/step - loss: 3992.4903 - regression_loss: 1869.0346 - val_loss: 362.3827 - val_regression_loss: 164.1370\n","Epoch 20/50\n","537/537 [==============================] - 0s 63us/step - loss: 3586.1513 - regression_loss: 1666.5304 - val_loss: 342.8532 - val_regression_loss: 154.4143\n","Epoch 21/50\n","537/537 [==============================] - 0s 67us/step - loss: 3456.5004 - regression_loss: 1600.4267 - val_loss: 333.6551 - val_regression_loss: 149.7983\n","Epoch 22/50\n","537/537 [==============================] - 0s 67us/step - loss: 3298.7670 - regression_loss: 1523.6641 - val_loss: 332.2402 - val_regression_loss: 149.0419\n","Epoch 23/50\n","537/537 [==============================] - 0s 67us/step - loss: 3118.3331 - regression_loss: 1431.6131 - val_loss: 339.1663 - val_regression_loss: 152.4607\n","Epoch 24/50\n","537/537 [==============================] - 0s 61us/step - loss: 3035.8398 - regression_loss: 1390.6053 - val_loss: 333.2304 - val_regression_loss: 149.4789\n","Epoch 25/50\n","537/537 [==============================] - 0s 59us/step - loss: 2938.1549 - regression_loss: 1342.4736 - val_loss: 315.6051 - val_regression_loss: 140.6870\n","Epoch 26/50\n","537/537 [==============================] - 0s 64us/step - loss: 2482.4503 - regression_loss: 1115.0399 - val_loss: 310.6640 - val_regression_loss: 138.2435\n","Epoch 27/50\n","537/537 [==============================] - 0s 64us/step - loss: 2577.8361 - regression_loss: 1162.3034 - val_loss: 315.5226 - val_regression_loss: 140.6848\n","Epoch 28/50\n","537/537 [==============================] - 0s 61us/step - loss: 2578.9627 - regression_loss: 1164.5798 - val_loss: 319.1379 - val_regression_loss: 142.4831\n","Epoch 29/50\n","537/537 [==============================] - 0s 65us/step - loss: 2417.0582 - regression_loss: 1081.0644 - val_loss: 322.6870 - val_regression_loss: 144.2435\n","Epoch 30/50\n","537/537 [==============================] - 0s 65us/step - loss: 2368.1584 - regression_loss: 1059.3518 - val_loss: 317.2541 - val_regression_loss: 141.5277\n","Epoch 31/50\n","537/537 [==============================] - 0s 60us/step - loss: 2277.6497 - regression_loss: 1014.4631 - val_loss: 307.3715 - val_regression_loss: 136.5895\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 2205.2974 - regression_loss: 979.3432 - val_loss: 297.3090 - val_regression_loss: 131.5561\n","Epoch 33/50\n","537/537 [==============================] - 0s 59us/step - loss: 2168.2678 - regression_loss: 958.6042 - val_loss: 284.4207 - val_regression_loss: 125.1024\n","Epoch 34/50\n","537/537 [==============================] - 0s 61us/step - loss: 2075.0703 - regression_loss: 915.7197 - val_loss: 277.4387 - val_regression_loss: 121.6044\n","Epoch 35/50\n","537/537 [==============================] - 0s 59us/step - loss: 2013.7553 - regression_loss: 884.4661 - val_loss: 276.4339 - val_regression_loss: 121.1057\n","Epoch 36/50\n","537/537 [==============================] - 0s 62us/step - loss: 1902.6739 - regression_loss: 828.9717 - val_loss: 279.7945 - val_regression_loss: 122.7943\n","Epoch 37/50\n","537/537 [==============================] - 0s 64us/step - loss: 1845.7195 - regression_loss: 801.5240 - val_loss: 281.0867 - val_regression_loss: 123.4480\n","Epoch 38/50\n","537/537 [==============================] - 0s 68us/step - loss: 1831.4472 - regression_loss: 792.4824 - val_loss: 275.5320 - val_regression_loss: 120.6760\n","Epoch 39/50\n","537/537 [==============================] - 0s 65us/step - loss: 1778.5607 - regression_loss: 765.1753 - val_loss: 271.3258 - val_regression_loss: 118.5800\n","Epoch 40/50\n","537/537 [==============================] - 0s 60us/step - loss: 1699.9682 - regression_loss: 729.4600 - val_loss: 269.7465 - val_regression_loss: 117.7989\n","Epoch 41/50\n","537/537 [==============================] - 0s 61us/step - loss: 1665.9409 - regression_loss: 710.7711 - val_loss: 269.5523 - val_regression_loss: 117.7093\n","Epoch 42/50\n","537/537 [==============================] - 0s 61us/step - loss: 1657.6141 - regression_loss: 707.2959 - val_loss: 273.6219 - val_regression_loss: 119.7513\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 1637.2171 - regression_loss: 698.5079 - val_loss: 269.4652 - val_regression_loss: 117.6803\n","Epoch 44/50\n","537/537 [==============================] - 0s 62us/step - loss: 1563.2787 - regression_loss: 662.2740 - val_loss: 264.0465 - val_regression_loss: 114.9781\n","Epoch 45/50\n","537/537 [==============================] - 0s 60us/step - loss: 1574.9735 - regression_loss: 668.7161 - val_loss: 262.9672 - val_regression_loss: 114.4433\n","Epoch 46/50\n","537/537 [==============================] - 0s 64us/step - loss: 1544.2179 - regression_loss: 652.7288 - val_loss: 255.5918 - val_regression_loss: 110.7610\n","Epoch 47/50\n","537/537 [==============================] - 0s 65us/step - loss: 1490.1812 - regression_loss: 625.1003 - val_loss: 247.3281 - val_regression_loss: 106.6317\n","Epoch 48/50\n","537/537 [==============================] - 0s 62us/step - loss: 1484.5905 - regression_loss: 625.5622 - val_loss: 241.8118 - val_regression_loss: 103.8742\n","Epoch 49/50\n","537/537 [==============================] - 0s 62us/step - loss: 1495.5389 - regression_loss: 628.2232 - val_loss: 241.8558 - val_regression_loss: 103.8925\n","Epoch 50/50\n","537/537 [==============================] - 0s 62us/step - loss: 1468.0978 - regression_loss: 615.2648 - val_loss: 239.4108 - val_regression_loss: 102.6648\n","***************************** elapsed_time is:  3.8693792819976807\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 59745.8094 - regression_loss: 29901.1029 - val_loss: 6269.0132 - val_regression_loss: 3129.6870\n","Epoch 2/50\n","537/537 [==============================] - 0s 53us/step - loss: 48221.7975 - regression_loss: 24104.9394 - val_loss: 4907.5576 - val_regression_loss: 2443.6763\n","Epoch 3/50\n","537/537 [==============================] - 0s 63us/step - loss: 36670.5652 - regression_loss: 18287.9251 - val_loss: 3414.6848 - val_regression_loss: 1689.3024\n","Epoch 4/50\n","537/537 [==============================] - 0s 62us/step - loss: 23315.4792 - regression_loss: 11548.6645 - val_loss: 2317.6677 - val_regression_loss: 1129.0283\n","Epoch 5/50\n","537/537 [==============================] - 0s 54us/step - loss: 13024.4045 - regression_loss: 6308.6092 - val_loss: 2407.6895 - val_regression_loss: 1160.5726\n","Epoch 6/50\n","537/537 [==============================] - 0s 52us/step - loss: 12237.7867 - regression_loss: 5809.9784 - val_loss: 2530.1362 - val_regression_loss: 1221.0087\n","Epoch 7/50\n","537/537 [==============================] - 0s 53us/step - loss: 12880.4192 - regression_loss: 6128.4531 - val_loss: 2008.8771 - val_regression_loss: 969.7117\n","Epoch 8/50\n","537/537 [==============================] - 0s 56us/step - loss: 9957.2862 - regression_loss: 4738.9750 - val_loss: 1519.4994 - val_regression_loss: 736.2017\n","Epoch 9/50\n","537/537 [==============================] - 0s 57us/step - loss: 7135.8872 - regression_loss: 3414.3834 - val_loss: 1407.9806 - val_regression_loss: 688.6289\n","Epoch 10/50\n","537/537 [==============================] - 0s 58us/step - loss: 6970.4815 - regression_loss: 3396.6248 - val_loss: 1470.6307 - val_regression_loss: 723.4691\n","Epoch 11/50\n","537/537 [==============================] - 0s 53us/step - loss: 7876.3772 - regression_loss: 3874.7903 - val_loss: 1434.8081 - val_regression_loss: 705.1910\n","Epoch 12/50\n","537/537 [==============================] - 0s 70us/step - loss: 7401.8792 - regression_loss: 3632.6828 - val_loss: 1283.4003 - val_regression_loss: 626.8435\n","Epoch 13/50\n","537/537 [==============================] - 0s 62us/step - loss: 6088.4195 - regression_loss: 2953.0878 - val_loss: 1149.2853 - val_regression_loss: 556.2632\n","Epoch 14/50\n","537/537 [==============================] - 0s 62us/step - loss: 4693.4776 - regression_loss: 2225.0164 - val_loss: 1130.7516 - val_regression_loss: 543.7175\n","Epoch 15/50\n","537/537 [==============================] - 0s 60us/step - loss: 4372.5216 - regression_loss: 2040.6034 - val_loss: 1181.9111 - val_regression_loss: 567.1631\n","Epoch 16/50\n","537/537 [==============================] - 0s 61us/step - loss: 4727.7109 - regression_loss: 2198.6017 - val_loss: 1166.0284 - val_regression_loss: 558.8892\n","Epoch 17/50\n","537/537 [==============================] - 0s 63us/step - loss: 4771.9787 - regression_loss: 2217.2996 - val_loss: 1060.0996 - val_regression_loss: 507.3212\n","Epoch 18/50\n","537/537 [==============================] - 0s 64us/step - loss: 4036.7854 - regression_loss: 1858.4532 - val_loss: 973.5156 - val_regression_loss: 466.5093\n","Epoch 19/50\n","537/537 [==============================] - 0s 63us/step - loss: 3572.3059 - regression_loss: 1645.0634 - val_loss: 964.0730 - val_regression_loss: 464.4705\n","Epoch 20/50\n","537/537 [==============================] - 0s 64us/step - loss: 3515.0513 - regression_loss: 1637.3441 - val_loss: 965.3517 - val_regression_loss: 467.0672\n","Epoch 21/50\n","537/537 [==============================] - 0s 85us/step - loss: 3520.2879 - regression_loss: 1652.8100 - val_loss: 927.5984 - val_regression_loss: 449.0836\n","Epoch 22/50\n","537/537 [==============================] - 0s 58us/step - loss: 3154.1144 - regression_loss: 1476.1974 - val_loss: 895.9952 - val_regression_loss: 433.4432\n","Epoch 23/50\n","537/537 [==============================] - 0s 60us/step - loss: 2949.1035 - regression_loss: 1375.8112 - val_loss: 888.6561 - val_regression_loss: 429.5290\n","Epoch 24/50\n","537/537 [==============================] - 0s 73us/step - loss: 2965.4217 - regression_loss: 1380.0898 - val_loss: 870.1858 - val_regression_loss: 419.8406\n","Epoch 25/50\n","537/537 [==============================] - 0s 75us/step - loss: 2890.1344 - regression_loss: 1341.0597 - val_loss: 832.0281 - val_regression_loss: 400.1955\n","Epoch 26/50\n","537/537 [==============================] - 0s 66us/step - loss: 2622.2479 - regression_loss: 1203.9114 - val_loss: 802.5670 - val_regression_loss: 384.9026\n","Epoch 27/50\n","537/537 [==============================] - 0s 64us/step - loss: 2540.7285 - regression_loss: 1161.6715 - val_loss: 786.0965 - val_regression_loss: 376.0361\n","Epoch 28/50\n","537/537 [==============================] - 0s 64us/step - loss: 2520.9896 - regression_loss: 1146.5888 - val_loss: 762.0728 - val_regression_loss: 363.1671\n","Epoch 29/50\n","537/537 [==============================] - 0s 63us/step - loss: 2373.0529 - regression_loss: 1071.7495 - val_loss: 733.6916 - val_regression_loss: 348.0354\n","Epoch 30/50\n","537/537 [==============================] - 0s 57us/step - loss: 2199.0380 - regression_loss: 977.3316 - val_loss: 715.8527 - val_regression_loss: 338.5218\n","Epoch 31/50\n","537/537 [==============================] - 0s 61us/step - loss: 2013.9816 - regression_loss: 879.0368 - val_loss: 703.2877 - val_regression_loss: 332.3364\n","Epoch 32/50\n","537/537 [==============================] - 0s 70us/step - loss: 2168.0491 - regression_loss: 960.1502 - val_loss: 686.5507 - val_regression_loss: 324.6280\n","Epoch 33/50\n","537/537 [==============================] - 0s 66us/step - loss: 2044.4864 - regression_loss: 904.1375 - val_loss: 671.5142 - val_regression_loss: 317.9390\n","Epoch 34/50\n","537/537 [==============================] - 0s 61us/step - loss: 1956.9187 - regression_loss: 866.2750 - val_loss: 659.9304 - val_regression_loss: 312.6853\n","Epoch 35/50\n","537/537 [==============================] - 0s 68us/step - loss: 1923.1157 - regression_loss: 860.0581 - val_loss: 645.9447 - val_regression_loss: 305.5524\n","Epoch 36/50\n","537/537 [==============================] - 0s 62us/step - loss: 1940.3355 - regression_loss: 863.7757 - val_loss: 630.6144 - val_regression_loss: 297.2707\n","Epoch 37/50\n","537/537 [==============================] - 0s 90us/step - loss: 1861.9538 - regression_loss: 818.7808 - val_loss: 617.6649 - val_regression_loss: 290.1292\n","Epoch 38/50\n","537/537 [==============================] - 0s 57us/step - loss: 1834.7565 - regression_loss: 800.5392 - val_loss: 604.0145 - val_regression_loss: 283.0151\n","Epoch 39/50\n","537/537 [==============================] - 0s 59us/step - loss: 1764.9339 - regression_loss: 764.3714 - val_loss: 591.6616 - val_regression_loss: 276.9242\n","Epoch 40/50\n","537/537 [==============================] - 0s 62us/step - loss: 1739.9119 - regression_loss: 752.3440 - val_loss: 581.5099 - val_regression_loss: 272.0496\n","Epoch 41/50\n","537/537 [==============================] - 0s 70us/step - loss: 1713.6039 - regression_loss: 741.1359 - val_loss: 572.9387 - val_regression_loss: 267.7296\n","Epoch 42/50\n","537/537 [==============================] - 0s 63us/step - loss: 1666.6614 - regression_loss: 717.9898 - val_loss: 564.1993 - val_regression_loss: 263.2104\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 1641.0982 - regression_loss: 705.6630 - val_loss: 556.7413 - val_regression_loss: 259.4459\n","Epoch 44/50\n","537/537 [==============================] - 0s 63us/step - loss: 1589.9941 - regression_loss: 678.4977 - val_loss: 546.4985 - val_regression_loss: 254.6364\n","Epoch 45/50\n","537/537 [==============================] - 0s 67us/step - loss: 1578.8849 - regression_loss: 675.3256 - val_loss: 536.9056 - val_regression_loss: 250.1155\n","Epoch 46/50\n","537/537 [==============================] - 0s 62us/step - loss: 1596.5774 - regression_loss: 684.4744 - val_loss: 527.2695 - val_regression_loss: 245.3033\n","Epoch 47/50\n","537/537 [==============================] - 0s 61us/step - loss: 1559.9880 - regression_loss: 667.8389 - val_loss: 517.7526 - val_regression_loss: 240.3212\n","Epoch 48/50\n","537/537 [==============================] - 0s 61us/step - loss: 1548.4786 - regression_loss: 659.0256 - val_loss: 508.6569 - val_regression_loss: 235.5507\n","Epoch 49/50\n","537/537 [==============================] - 0s 66us/step - loss: 1499.3131 - regression_loss: 631.8106 - val_loss: 500.0887 - val_regression_loss: 231.2799\n","Epoch 50/50\n","537/537 [==============================] - 0s 66us/step - loss: 1508.8557 - regression_loss: 637.0099 - val_loss: 492.3253 - val_regression_loss: 227.5277\n","***************************** elapsed_time is:  3.8502087593078613\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 101991.3241 - regression_loss: 50624.8963 - val_loss: 10596.6494 - val_regression_loss: 5251.6558\n","Epoch 2/50\n","537/537 [==============================] - 0s 57us/step - loss: 88718.1416 - regression_loss: 43996.8612 - val_loss: 8656.3652 - val_regression_loss: 4283.8516\n","Epoch 3/50\n","537/537 [==============================] - 0s 60us/step - loss: 73413.6868 - regression_loss: 36370.9901 - val_loss: 6171.9663 - val_regression_loss: 3046.3420\n","Epoch 4/50\n","537/537 [==============================] - 0s 55us/step - loss: 52879.3662 - regression_loss: 26137.8463 - val_loss: 3597.5466 - val_regression_loss: 1767.5134\n","Epoch 5/50\n","537/537 [==============================] - 0s 54us/step - loss: 30445.3043 - regression_loss: 14987.0101 - val_loss: 2249.1707 - val_regression_loss: 1105.1902\n","Epoch 6/50\n","537/537 [==============================] - 0s 57us/step - loss: 20401.3497 - regression_loss: 10058.7306 - val_loss: 2772.0776 - val_regression_loss: 1375.7407\n","Epoch 7/50\n","537/537 [==============================] - 0s 55us/step - loss: 25825.0703 - regression_loss: 12842.9995 - val_loss: 2499.9182 - val_regression_loss: 1238.0183\n","Epoch 8/50\n","537/537 [==============================] - 0s 55us/step - loss: 25049.3070 - regression_loss: 12440.1495 - val_loss: 1625.3732 - val_regression_loss: 794.7389\n","Epoch 9/50\n","537/537 [==============================] - 0s 70us/step - loss: 18416.2378 - regression_loss: 9073.7295 - val_loss: 1229.9056 - val_regression_loss: 591.0503\n","Epoch 10/50\n","537/537 [==============================] - 0s 59us/step - loss: 14981.7049 - regression_loss: 7312.0019 - val_loss: 1441.2042 - val_regression_loss: 692.9685\n","Epoch 11/50\n","537/537 [==============================] - 0s 72us/step - loss: 16228.1719 - regression_loss: 7904.0626 - val_loss: 1683.1337 - val_regression_loss: 813.2004\n","Epoch 12/50\n","537/537 [==============================] - 0s 61us/step - loss: 17254.3227 - regression_loss: 8414.8826 - val_loss: 1615.2117 - val_regression_loss: 780.8889\n","Epoch 13/50\n","537/537 [==============================] - 0s 61us/step - loss: 16328.4148 - regression_loss: 7966.2475 - val_loss: 1295.5935 - val_regression_loss: 624.2640\n","Epoch 14/50\n","537/537 [==============================] - 0s 62us/step - loss: 13842.1766 - regression_loss: 6749.2386 - val_loss: 983.8557 - val_regression_loss: 472.1233\n","Epoch 15/50\n","537/537 [==============================] - 0s 59us/step - loss: 11614.6304 - regression_loss: 5665.7983 - val_loss: 872.6314 - val_regression_loss: 419.7580\n","Epoch 16/50\n","537/537 [==============================] - 0s 63us/step - loss: 10963.7637 - regression_loss: 5367.9778 - val_loss: 898.1378 - val_regression_loss: 434.4489\n","Epoch 17/50\n","537/537 [==============================] - 0s 64us/step - loss: 11670.1873 - regression_loss: 5736.7940 - val_loss: 871.9989 - val_regression_loss: 421.7555\n","Epoch 18/50\n","537/537 [==============================] - 0s 66us/step - loss: 11180.8253 - regression_loss: 5494.6275 - val_loss: 773.7089 - val_regression_loss: 371.5955\n","Epoch 19/50\n","537/537 [==============================] - 0s 74us/step - loss: 9980.8555 - regression_loss: 4887.7706 - val_loss: 740.6384 - val_regression_loss: 353.2692\n","Epoch 20/50\n","537/537 [==============================] - 0s 66us/step - loss: 8859.5925 - regression_loss: 4313.1185 - val_loss: 800.8129 - val_regression_loss: 381.5437\n","Epoch 21/50\n","537/537 [==============================] - 0s 58us/step - loss: 8371.5914 - regression_loss: 4055.8726 - val_loss: 840.6632 - val_regression_loss: 400.3202\n","Epoch 22/50\n","537/537 [==============================] - 0s 60us/step - loss: 8464.2584 - regression_loss: 4092.3651 - val_loss: 773.4819 - val_regression_loss: 366.5533\n","Epoch 23/50\n","537/537 [==============================] - 0s 61us/step - loss: 7877.5029 - regression_loss: 3795.7851 - val_loss: 660.6021 - val_regression_loss: 310.6864\n","Epoch 24/50\n","537/537 [==============================] - 0s 63us/step - loss: 6808.2372 - regression_loss: 3263.7882 - val_loss: 592.8323 - val_regression_loss: 277.6258\n","Epoch 25/50\n","537/537 [==============================] - 0s 60us/step - loss: 6468.3276 - regression_loss: 3096.8389 - val_loss: 560.6660 - val_regression_loss: 262.2414\n","Epoch 26/50\n","537/537 [==============================] - 0s 65us/step - loss: 6515.1075 - regression_loss: 3125.8225 - val_loss: 520.7408 - val_regression_loss: 242.5553\n","Epoch 27/50\n","537/537 [==============================] - 0s 67us/step - loss: 5989.2685 - regression_loss: 2866.6301 - val_loss: 493.2114 - val_regression_loss: 228.6381\n","Epoch 28/50\n","537/537 [==============================] - 0s 64us/step - loss: 5374.6224 - regression_loss: 2560.7805 - val_loss: 495.9979 - val_regression_loss: 229.7964\n","Epoch 29/50\n","537/537 [==============================] - 0s 66us/step - loss: 5370.9839 - regression_loss: 2556.8494 - val_loss: 487.6847 - val_regression_loss: 225.7650\n","Epoch 30/50\n","537/537 [==============================] - 0s 58us/step - loss: 5190.4338 - regression_loss: 2466.5198 - val_loss: 456.0070 - val_regression_loss: 210.4481\n","Epoch 31/50\n","537/537 [==============================] - 0s 63us/step - loss: 4698.7950 - regression_loss: 2228.9116 - val_loss: 436.1503 - val_regression_loss: 201.0453\n","Epoch 32/50\n","537/537 [==============================] - 0s 59us/step - loss: 4569.4890 - regression_loss: 2168.0377 - val_loss: 424.4370 - val_regression_loss: 195.3350\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 4365.4510 - regression_loss: 2060.0122 - val_loss: 399.3543 - val_regression_loss: 182.5347\n","Epoch 34/50\n","537/537 [==============================] - 0s 62us/step - loss: 4170.6823 - regression_loss: 1961.1941 - val_loss: 370.9628 - val_regression_loss: 167.8727\n","Epoch 35/50\n","537/537 [==============================] - 0s 63us/step - loss: 3730.7004 - regression_loss: 1735.6006 - val_loss: 350.0586 - val_regression_loss: 157.2746\n","Epoch 36/50\n","537/537 [==============================] - 0s 64us/step - loss: 3769.4262 - regression_loss: 1752.7397 - val_loss: 329.7117 - val_regression_loss: 147.4881\n","Epoch 37/50\n","537/537 [==============================] - 0s 66us/step - loss: 3508.0541 - regression_loss: 1625.7023 - val_loss: 319.6565 - val_regression_loss: 143.1606\n","Epoch 38/50\n","537/537 [==============================] - 0s 70us/step - loss: 3277.7749 - regression_loss: 1511.6897 - val_loss: 315.3813 - val_regression_loss: 141.5633\n","Epoch 39/50\n","537/537 [==============================] - 0s 59us/step - loss: 3073.1642 - regression_loss: 1414.2352 - val_loss: 298.3599 - val_regression_loss: 133.1859\n","Epoch 40/50\n","537/537 [==============================] - 0s 61us/step - loss: 2976.4981 - regression_loss: 1365.3664 - val_loss: 283.1770 - val_regression_loss: 125.5786\n","Epoch 41/50\n","537/537 [==============================] - 0s 60us/step - loss: 2751.5512 - regression_loss: 1252.6197 - val_loss: 273.1538 - val_regression_loss: 120.6147\n","Epoch 42/50\n","537/537 [==============================] - 0s 66us/step - loss: 2751.1262 - regression_loss: 1253.4613 - val_loss: 273.0866 - val_regression_loss: 120.8461\n","Epoch 43/50\n","537/537 [==============================] - 0s 60us/step - loss: 2618.8635 - regression_loss: 1188.6627 - val_loss: 276.8966 - val_regression_loss: 122.9413\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 2481.0788 - regression_loss: 1119.1197 - val_loss: 273.1128 - val_regression_loss: 121.0277\n","Epoch 45/50\n","537/537 [==============================] - 0s 66us/step - loss: 2417.8337 - regression_loss: 1088.1444 - val_loss: 265.2846 - val_regression_loss: 116.9834\n","Epoch 46/50\n","537/537 [==============================] - 0s 73us/step - loss: 2302.0728 - regression_loss: 1030.7347 - val_loss: 261.2021 - val_regression_loss: 114.8661\n","Epoch 47/50\n","537/537 [==============================] - 0s 72us/step - loss: 2270.3084 - regression_loss: 1010.7147 - val_loss: 268.0970 - val_regression_loss: 118.5249\n","Epoch 48/50\n","537/537 [==============================] - 0s 65us/step - loss: 2243.5888 - regression_loss: 1002.6906 - val_loss: 279.0284 - val_regression_loss: 124.2247\n","Epoch 49/50\n","537/537 [==============================] - 0s 63us/step - loss: 2215.2199 - regression_loss: 987.5731 - val_loss: 279.0387 - val_regression_loss: 124.2374\n","Epoch 50/50\n","537/537 [==============================] - 0s 66us/step - loss: 2167.2000 - regression_loss: 964.0368 - val_loss: 272.9418 - val_regression_loss: 121.0205\n","***************************** elapsed_time is:  4.122764825820923\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 26779.0079 - regression_loss: 13386.9130 - val_loss: 2580.9092 - val_regression_loss: 1283.6508\n","Epoch 2/50\n","537/537 [==============================] - 0s 56us/step - loss: 20157.7808 - regression_loss: 9989.9092 - val_loss: 1709.6912 - val_regression_loss: 835.0800\n","Epoch 3/50\n","537/537 [==============================] - 0s 53us/step - loss: 13395.7153 - regression_loss: 6504.8130 - val_loss: 1085.2529 - val_regression_loss: 503.3892\n","Epoch 4/50\n","537/537 [==============================] - 0s 53us/step - loss: 8648.4916 - regression_loss: 3977.3685 - val_loss: 924.5574 - val_regression_loss: 407.3411\n","Epoch 5/50\n","537/537 [==============================] - 0s 59us/step - loss: 7798.1848 - regression_loss: 3430.2091 - val_loss: 665.4304 - val_regression_loss: 285.3300\n","Epoch 6/50\n","537/537 [==============================] - 0s 66us/step - loss: 5327.7910 - regression_loss: 2257.7303 - val_loss: 474.3841 - val_regression_loss: 208.9471\n","Epoch 7/50\n","537/537 [==============================] - 0s 56us/step - loss: 3046.5912 - regression_loss: 1275.5595 - val_loss: 612.3869 - val_regression_loss: 296.1306\n","Epoch 8/50\n","537/537 [==============================] - 0s 55us/step - loss: 3433.3082 - regression_loss: 1620.0248 - val_loss: 785.5309 - val_regression_loss: 391.0257\n","Epoch 9/50\n","537/537 [==============================] - 0s 61us/step - loss: 4535.1701 - regression_loss: 2241.0665 - val_loss: 721.0175 - val_regression_loss: 357.9859\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 4175.1299 - regression_loss: 2058.2089 - val_loss: 547.6229 - val_regression_loss: 266.2536\n","Epoch 11/50\n","537/537 [==============================] - 0s 61us/step - loss: 3140.0061 - regression_loss: 1501.0023 - val_loss: 422.2269 - val_regression_loss: 197.5648\n","Epoch 12/50\n","537/537 [==============================] - 0s 57us/step - loss: 2484.3599 - regression_loss: 1120.8766 - val_loss: 378.3181 - val_regression_loss: 170.7078\n","Epoch 13/50\n","537/537 [==============================] - 0s 64us/step - loss: 2331.6085 - regression_loss: 1002.9166 - val_loss: 364.0717 - val_regression_loss: 160.7971\n","Epoch 14/50\n","537/537 [==============================] - 0s 56us/step - loss: 2511.9094 - regression_loss: 1065.8443 - val_loss: 351.5906 - val_regression_loss: 153.8791\n","Epoch 15/50\n","537/537 [==============================] - 0s 60us/step - loss: 2516.6142 - regression_loss: 1062.7386 - val_loss: 348.8072 - val_regression_loss: 153.3238\n","Epoch 16/50\n","537/537 [==============================] - 0s 54us/step - loss: 2496.4065 - regression_loss: 1055.2400 - val_loss: 346.4354 - val_regression_loss: 153.7101\n","Epoch 17/50\n","537/537 [==============================] - 0s 51us/step - loss: 2358.1540 - regression_loss: 1004.1005 - val_loss: 331.9768 - val_regression_loss: 148.2120\n","Epoch 18/50\n","537/537 [==============================] - 0s 58us/step - loss: 2184.9346 - regression_loss: 929.4784 - val_loss: 316.0434 - val_regression_loss: 142.0452\n","Epoch 19/50\n","537/537 [==============================] - 0s 57us/step - loss: 1986.8122 - regression_loss: 849.5279 - val_loss: 311.6868 - val_regression_loss: 141.7872\n","Epoch 20/50\n","537/537 [==============================] - 0s 55us/step - loss: 1948.6392 - regression_loss: 846.8754 - val_loss: 312.5199 - val_regression_loss: 144.1501\n","Epoch 21/50\n","537/537 [==============================] - 0s 57us/step - loss: 1942.6964 - regression_loss: 860.2740 - val_loss: 312.8080 - val_regression_loss: 145.8916\n","Epoch 22/50\n","537/537 [==============================] - 0s 58us/step - loss: 1905.1275 - regression_loss: 853.0417 - val_loss: 312.9618 - val_regression_loss: 146.6951\n","Epoch 23/50\n","537/537 [==============================] - 0s 53us/step - loss: 1813.8384 - regression_loss: 810.5655 - val_loss: 305.4634 - val_regression_loss: 142.4660\n","Epoch 24/50\n","537/537 [==============================] - 0s 55us/step - loss: 1826.7696 - regression_loss: 816.5186 - val_loss: 286.9392 - val_regression_loss: 131.6700\n","Epoch 25/50\n","537/537 [==============================] - 0s 61us/step - loss: 1758.1641 - regression_loss: 773.1772 - val_loss: 271.4276 - val_regression_loss: 122.0327\n","Epoch 26/50\n","537/537 [==============================] - 0s 55us/step - loss: 1699.2382 - regression_loss: 723.7786 - val_loss: 264.6594 - val_regression_loss: 117.3715\n","Epoch 27/50\n","537/537 [==============================] - 0s 57us/step - loss: 1713.7146 - regression_loss: 723.0843 - val_loss: 263.8752 - val_regression_loss: 116.6321\n","Epoch 28/50\n","537/537 [==============================] - 0s 59us/step - loss: 1639.3239 - regression_loss: 678.2605 - val_loss: 264.9909 - val_regression_loss: 117.4041\n","Epoch 29/50\n","537/537 [==============================] - 0s 63us/step - loss: 1655.6616 - regression_loss: 690.9398 - val_loss: 258.8930 - val_regression_loss: 114.4070\n","Epoch 30/50\n","537/537 [==============================] - 0s 68us/step - loss: 1628.0970 - regression_loss: 677.9033 - val_loss: 251.4344 - val_regression_loss: 110.7779\n","Epoch 31/50\n","537/537 [==============================] - 0s 67us/step - loss: 1591.9035 - regression_loss: 661.8727 - val_loss: 250.1976 - val_regression_loss: 110.5855\n","Epoch 32/50\n","537/537 [==============================] - 0s 67us/step - loss: 1596.2030 - regression_loss: 669.4416 - val_loss: 252.2710 - val_regression_loss: 112.3578\n","Epoch 33/50\n","537/537 [==============================] - 0s 67us/step - loss: 1497.0503 - regression_loss: 631.6295 - val_loss: 253.8944 - val_regression_loss: 113.7201\n","Epoch 34/50\n","537/537 [==============================] - 0s 76us/step - loss: 1540.4918 - regression_loss: 657.0208 - val_loss: 248.4890 - val_regression_loss: 111.1725\n","Epoch 35/50\n","537/537 [==============================] - 0s 71us/step - loss: 1550.5309 - regression_loss: 665.6657 - val_loss: 236.8199 - val_regression_loss: 104.8966\n","Epoch 36/50\n","537/537 [==============================] - 0s 62us/step - loss: 1468.8027 - regression_loss: 618.9470 - val_loss: 233.0456 - val_regression_loss: 102.8023\n","Epoch 37/50\n","537/537 [==============================] - 0s 67us/step - loss: 1498.7608 - regression_loss: 629.6538 - val_loss: 233.0224 - val_regression_loss: 102.7207\n","Epoch 38/50\n","537/537 [==============================] - 0s 62us/step - loss: 1375.1177 - regression_loss: 569.6119 - val_loss: 232.4144 - val_regression_loss: 102.2597\n","Epoch 39/50\n","537/537 [==============================] - 0s 58us/step - loss: 1475.3574 - regression_loss: 618.9807 - val_loss: 222.0404 - val_regression_loss: 96.5901\n","Epoch 40/50\n","537/537 [==============================] - 0s 63us/step - loss: 1443.4413 - regression_loss: 599.6986 - val_loss: 218.6657 - val_regression_loss: 94.9333\n","Epoch 41/50\n","537/537 [==============================] - 0s 67us/step - loss: 1402.1183 - regression_loss: 578.1679 - val_loss: 219.2549 - val_regression_loss: 95.4238\n","Epoch 42/50\n","537/537 [==============================] - 0s 72us/step - loss: 1432.9918 - regression_loss: 597.2633 - val_loss: 216.1934 - val_regression_loss: 94.0488\n","Epoch 43/50\n","537/537 [==============================] - 0s 65us/step - loss: 1429.6786 - regression_loss: 598.4834 - val_loss: 213.3990 - val_regression_loss: 92.7387\n","\n","Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 44/50\n","537/537 [==============================] - 0s 65us/step - loss: 1409.2967 - regression_loss: 589.0286 - val_loss: 211.2152 - val_regression_loss: 91.6667\n","Epoch 45/50\n","537/537 [==============================] - 0s 54us/step - loss: 1384.3018 - regression_loss: 575.4496 - val_loss: 207.9219 - val_regression_loss: 89.9758\n","Epoch 46/50\n","537/537 [==============================] - 0s 61us/step - loss: 1412.8191 - regression_loss: 588.6040 - val_loss: 206.3299 - val_regression_loss: 89.2565\n","Epoch 47/50\n","537/537 [==============================] - 0s 59us/step - loss: 1385.5167 - regression_loss: 576.3292 - val_loss: 205.6426 - val_regression_loss: 89.0036\n","Epoch 48/50\n","537/537 [==============================] - 0s 65us/step - loss: 1412.5367 - regression_loss: 591.5533 - val_loss: 203.4417 - val_regression_loss: 87.8529\n","\n","Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","***************************** elapsed_time is:  3.764256477355957\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 19344.2266 - regression_loss: 9448.6936 - val_loss: 1729.6521 - val_regression_loss: 840.9783\n","Epoch 2/50\n","537/537 [==============================] - 0s 54us/step - loss: 14699.0974 - regression_loss: 7163.8556 - val_loss: 1201.8228 - val_regression_loss: 582.5239\n","Epoch 3/50\n","537/537 [==============================] - 0s 60us/step - loss: 10550.1702 - regression_loss: 5138.7484 - val_loss: 832.0335 - val_regression_loss: 404.2477\n","Epoch 4/50\n","537/537 [==============================] - 0s 56us/step - loss: 6996.9893 - regression_loss: 3415.1192 - val_loss: 783.7515 - val_regression_loss: 384.9797\n","Epoch 5/50\n","537/537 [==============================] - 0s 60us/step - loss: 6304.9924 - regression_loss: 3108.6311 - val_loss: 553.6494 - val_regression_loss: 268.2158\n","Epoch 6/50\n","537/537 [==============================] - 0s 63us/step - loss: 4410.8862 - regression_loss: 2146.6993 - val_loss: 276.5902 - val_regression_loss: 124.4664\n","Epoch 7/50\n","537/537 [==============================] - 0s 54us/step - loss: 2261.6615 - regression_loss: 1028.6337 - val_loss: 226.9575 - val_regression_loss: 93.2967\n","Epoch 8/50\n","537/537 [==============================] - 0s 55us/step - loss: 2112.5405 - regression_loss: 898.2499 - val_loss: 331.0442 - val_regression_loss: 141.0384\n","Epoch 9/50\n","537/537 [==============================] - 0s 63us/step - loss: 3130.2274 - regression_loss: 1371.7878 - val_loss: 332.2284 - val_regression_loss: 141.2836\n","Epoch 10/50\n","537/537 [==============================] - 0s 55us/step - loss: 3172.7944 - regression_loss: 1387.8111 - val_loss: 257.0435 - val_regression_loss: 106.0553\n","Epoch 11/50\n","537/537 [==============================] - 0s 64us/step - loss: 2351.8855 - regression_loss: 1002.1522 - val_loss: 208.6751 - val_regression_loss: 85.1299\n","Epoch 12/50\n","537/537 [==============================] - 0s 55us/step - loss: 1812.4214 - regression_loss: 758.9314 - val_loss: 205.9770 - val_regression_loss: 86.7004\n","Epoch 13/50\n","537/537 [==============================] - 0s 63us/step - loss: 1730.5563 - regression_loss: 746.9098 - val_loss: 219.8442 - val_regression_loss: 95.5323\n","Epoch 14/50\n","537/537 [==============================] - 0s 57us/step - loss: 1797.6586 - regression_loss: 795.5676 - val_loss: 227.3723 - val_regression_loss: 100.1219\n","Epoch 15/50\n","537/537 [==============================] - 0s 62us/step - loss: 1884.4696 - regression_loss: 845.2663 - val_loss: 224.8374 - val_regression_loss: 98.9174\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 1845.6680 - regression_loss: 824.6057 - val_loss: 213.3895 - val_regression_loss: 92.8727\n","Epoch 17/50\n","537/537 [==============================] - 0s 56us/step - loss: 1758.9066 - regression_loss: 778.0993 - val_loss: 196.3713 - val_regression_loss: 83.8658\n","Epoch 18/50\n","537/537 [==============================] - 0s 66us/step - loss: 1659.0254 - regression_loss: 720.3295 - val_loss: 181.5339 - val_regression_loss: 75.7442\n","Epoch 19/50\n","537/537 [==============================] - 0s 61us/step - loss: 1514.1380 - regression_loss: 641.6004 - val_loss: 174.4148 - val_regression_loss: 71.2629\n","Epoch 20/50\n","537/537 [==============================] - 0s 55us/step - loss: 1493.4142 - regression_loss: 620.3320 - val_loss: 172.5013 - val_regression_loss: 69.4349\n","Epoch 21/50\n","537/537 [==============================] - 0s 57us/step - loss: 1503.2863 - regression_loss: 615.9853 - val_loss: 171.4705 - val_regression_loss: 68.4036\n","Epoch 22/50\n","537/537 [==============================] - 0s 53us/step - loss: 1484.6483 - regression_loss: 602.8430 - val_loss: 171.8239 - val_regression_loss: 68.5110\n","Epoch 23/50\n","537/537 [==============================] - 0s 57us/step - loss: 1472.0616 - regression_loss: 595.3023 - val_loss: 173.6240 - val_regression_loss: 69.7052\n","Epoch 24/50\n","537/537 [==============================] - 0s 64us/step - loss: 1421.2134 - regression_loss: 577.4859 - val_loss: 174.6670 - val_regression_loss: 70.7490\n","Epoch 25/50\n","537/537 [==============================] - 0s 75us/step - loss: 1376.5198 - regression_loss: 559.0116 - val_loss: 175.5724 - val_regression_loss: 71.8930\n","Epoch 26/50\n","537/537 [==============================] - 0s 62us/step - loss: 1382.3742 - regression_loss: 566.5734 - val_loss: 178.9149 - val_regression_loss: 74.1233\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 1367.2915 - regression_loss: 564.4516 - val_loss: 181.5368 - val_regression_loss: 75.6601\n","Epoch 28/50\n","537/537 [==============================] - 0s 64us/step - loss: 1388.4655 - regression_loss: 578.3769 - val_loss: 181.4663 - val_regression_loss: 75.5380\n","Epoch 29/50\n","537/537 [==============================] - 0s 63us/step - loss: 1389.2444 - regression_loss: 578.4531 - val_loss: 179.5894 - val_regression_loss: 74.3478\n","Epoch 30/50\n","537/537 [==============================] - 0s 63us/step - loss: 1338.4219 - regression_loss: 549.5160 - val_loss: 176.9248 - val_regression_loss: 72.7507\n","Epoch 31/50\n","537/537 [==============================] - 0s 63us/step - loss: 1323.5495 - regression_loss: 541.3400 - val_loss: 174.1385 - val_regression_loss: 71.1373\n","Epoch 32/50\n","537/537 [==============================] - 0s 70us/step - loss: 1330.7153 - regression_loss: 544.0340 - val_loss: 171.8607 - val_regression_loss: 69.7656\n","Epoch 33/50\n","537/537 [==============================] - 0s 75us/step - loss: 1306.0766 - regression_loss: 528.7924 - val_loss: 169.5686 - val_regression_loss: 68.3984\n","Epoch 34/50\n","537/537 [==============================] - 0s 65us/step - loss: 1318.7610 - regression_loss: 536.9010 - val_loss: 167.5376 - val_regression_loss: 67.2539\n","Epoch 35/50\n","537/537 [==============================] - 0s 59us/step - loss: 1333.7830 - regression_loss: 542.2213 - val_loss: 166.4126 - val_regression_loss: 66.7038\n","Epoch 36/50\n","537/537 [==============================] - 0s 61us/step - loss: 1320.8959 - regression_loss: 536.1404 - val_loss: 166.3043 - val_regression_loss: 66.8132\n","Epoch 37/50\n","537/537 [==============================] - 0s 62us/step - loss: 1289.6545 - regression_loss: 521.8909 - val_loss: 167.1326 - val_regression_loss: 67.3852\n","Epoch 38/50\n","537/537 [==============================] - 0s 66us/step - loss: 1271.7543 - regression_loss: 517.4270 - val_loss: 167.7399 - val_regression_loss: 67.7115\n","Epoch 39/50\n","537/537 [==============================] - 0s 64us/step - loss: 1292.5020 - regression_loss: 528.8096 - val_loss: 167.9788 - val_regression_loss: 67.8645\n","Epoch 40/50\n","537/537 [==============================] - 0s 65us/step - loss: 1274.2178 - regression_loss: 519.6299 - val_loss: 167.1664 - val_regression_loss: 67.4765\n","Epoch 41/50\n","537/537 [==============================] - 0s 63us/step - loss: 1262.7566 - regression_loss: 513.4900 - val_loss: 165.7074 - val_regression_loss: 66.7324\n","Epoch 42/50\n","537/537 [==============================] - 0s 66us/step - loss: 1288.3377 - regression_loss: 527.9638 - val_loss: 164.1699 - val_regression_loss: 65.8613\n","Epoch 43/50\n","537/537 [==============================] - 0s 62us/step - loss: 1256.1110 - regression_loss: 510.0006 - val_loss: 163.2600 - val_regression_loss: 65.2994\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 1244.6629 - regression_loss: 504.2749 - val_loss: 163.1267 - val_regression_loss: 65.1777\n","Epoch 45/50\n","537/537 [==============================] - 0s 60us/step - loss: 1257.5175 - regression_loss: 511.4610 - val_loss: 162.7287 - val_regression_loss: 64.9293\n","Epoch 46/50\n","537/537 [==============================] - 0s 62us/step - loss: 1245.7887 - regression_loss: 505.6433 - val_loss: 162.2976 - val_regression_loss: 64.6440\n","Epoch 47/50\n","537/537 [==============================] - 0s 58us/step - loss: 1225.5557 - regression_loss: 494.6297 - val_loss: 161.5646 - val_regression_loss: 64.2392\n","Epoch 48/50\n","537/537 [==============================] - 0s 66us/step - loss: 1240.5460 - regression_loss: 504.5270 - val_loss: 161.1125 - val_regression_loss: 64.0356\n","Epoch 49/50\n","537/537 [==============================] - 0s 66us/step - loss: 1208.2036 - regression_loss: 488.0155 - val_loss: 160.6024 - val_regression_loss: 63.7628\n","Epoch 50/50\n","537/537 [==============================] - 0s 68us/step - loss: 1203.2551 - regression_loss: 488.2688 - val_loss: 160.3858 - val_regression_loss: 63.5558\n","***************************** elapsed_time is:  3.8280978202819824\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 3235612.6566 - regression_loss: 1614774.4415 - val_loss: 355173.6875 - val_regression_loss: 177236.6094\n","Epoch 2/50\n","537/537 [==============================] - 0s 55us/step - loss: 3135138.8572 - regression_loss: 1564750.6523 - val_loss: 343904.8125 - val_regression_loss: 171620.2344\n","Epoch 3/50\n","537/537 [==============================] - 0s 54us/step - loss: 3024255.7628 - regression_loss: 1509433.6514 - val_loss: 328147.3438 - val_regression_loss: 163769.4844\n","Epoch 4/50\n","537/537 [==============================] - 0s 55us/step - loss: 2875525.3765 - regression_loss: 1435326.3826 - val_loss: 306918.5625 - val_regression_loss: 153198.0625\n","Epoch 5/50\n","537/537 [==============================] - 0s 54us/step - loss: 2718975.8495 - regression_loss: 1357393.0624 - val_loss: 280294.1250 - val_regression_loss: 139944.6562\n","Epoch 6/50\n","537/537 [==============================] - 0s 54us/step - loss: 2449851.1021 - regression_loss: 1223331.8765 - val_loss: 248940.1094 - val_regression_loss: 124343.9922\n","Epoch 7/50\n","537/537 [==============================] - 0s 52us/step - loss: 2188339.3641 - regression_loss: 1093224.4560 - val_loss: 213576.5312 - val_regression_loss: 106761.0156\n","Epoch 8/50\n","537/537 [==============================] - 0s 59us/step - loss: 1949086.8367 - regression_loss: 974513.6114 - val_loss: 174579.7188 - val_regression_loss: 87402.4688\n","Epoch 9/50\n","537/537 [==============================] - 0s 56us/step - loss: 1643927.9198 - regression_loss: 823080.7860 - val_loss: 132350.9531 - val_regression_loss: 66521.6875\n","Epoch 10/50\n","537/537 [==============================] - 0s 52us/step - loss: 1268508.0280 - regression_loss: 637543.8720 - val_loss: 88612.5000 - val_regression_loss: 45030.3906\n","Epoch 11/50\n","537/537 [==============================] - 0s 69us/step - loss: 916577.6211 - regression_loss: 464982.5271 - val_loss: 49144.6875 - val_regression_loss: 25536.1797\n","Epoch 12/50\n","537/537 [==============================] - 0s 56us/step - loss: 558930.2252 - regression_loss: 287456.1984 - val_loss: 25974.7344 - val_regression_loss: 13329.9590\n","Epoch 13/50\n","537/537 [==============================] - 0s 58us/step - loss: 374287.3134 - regression_loss: 188440.6549 - val_loss: 32492.9121 - val_regression_loss: 15311.4561\n","Epoch 14/50\n","537/537 [==============================] - 0s 61us/step - loss: 412560.9714 - regression_loss: 196269.4611 - val_loss: 55473.4492 - val_regression_loss: 25993.3652\n","Epoch 15/50\n","537/537 [==============================] - 0s 57us/step - loss: 573347.9634 - regression_loss: 269576.0259 - val_loss: 56481.6445 - val_regression_loss: 26598.3203\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 575932.7793 - regression_loss: 271682.8724 - val_loss: 40179.4805 - val_regression_loss: 19050.9238\n","Epoch 17/50\n","537/537 [==============================] - 0s 59us/step - loss: 439937.1631 - regression_loss: 209268.5959 - val_loss: 26024.9004 - val_regression_loss: 12686.1074\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 329999.9009 - regression_loss: 160624.8456 - val_loss: 20605.7695 - val_regression_loss: 10583.5469\n","Epoch 19/50\n","537/537 [==============================] - 0s 64us/step - loss: 299494.1373 - regression_loss: 150680.2799 - val_loss: 21167.2930 - val_regression_loss: 11285.4053\n","Epoch 20/50\n","537/537 [==============================] - 0s 62us/step - loss: 312528.3414 - regression_loss: 160872.5338 - val_loss: 23167.2500 - val_regression_loss: 12523.4414\n","Epoch 21/50\n","537/537 [==============================] - 0s 57us/step - loss: 323596.6495 - regression_loss: 168384.4931 - val_loss: 24072.0215 - val_regression_loss: 13062.6025\n","Epoch 22/50\n","537/537 [==============================] - 0s 56us/step - loss: 314535.7039 - regression_loss: 164624.8375 - val_loss: 23186.3027 - val_regression_loss: 12584.1406\n","Epoch 23/50\n","537/537 [==============================] - 0s 61us/step - loss: 323713.8700 - regression_loss: 168663.0169 - val_loss: 20922.7559 - val_regression_loss: 11319.8271\n","Epoch 24/50\n","537/537 [==============================] - 0s 54us/step - loss: 287642.9127 - regression_loss: 149750.2421 - val_loss: 18808.5996 - val_regression_loss: 10055.7314\n","Epoch 25/50\n","537/537 [==============================] - 0s 54us/step - loss: 255303.9474 - regression_loss: 131777.2714 - val_loss: 18220.3965 - val_regression_loss: 9512.0859\n","Epoch 26/50\n","537/537 [==============================] - 0s 53us/step - loss: 235540.5809 - regression_loss: 120206.7149 - val_loss: 19587.5918 - val_regression_loss: 9941.5176\n","Epoch 27/50\n","537/537 [==============================] - 0s 70us/step - loss: 229538.8899 - regression_loss: 115240.0227 - val_loss: 21985.7461 - val_regression_loss: 10932.4746\n","Epoch 28/50\n","537/537 [==============================] - 0s 59us/step - loss: 230018.4262 - regression_loss: 113580.7069 - val_loss: 23596.4121 - val_regression_loss: 11621.7930\n","Epoch 29/50\n","537/537 [==============================] - 0s 64us/step - loss: 223694.2849 - regression_loss: 109430.1701 - val_loss: 23351.4551 - val_regression_loss: 11483.3145\n","Epoch 30/50\n","537/537 [==============================] - 0s 59us/step - loss: 219742.5760 - regression_loss: 107613.9288 - val_loss: 21740.3027 - val_regression_loss: 10739.0391\n","Epoch 31/50\n","537/537 [==============================] - 0s 55us/step - loss: 211021.9610 - regression_loss: 103687.2603 - val_loss: 19896.6172 - val_regression_loss: 9922.3252\n","Epoch 32/50\n","537/537 [==============================] - 0s 62us/step - loss: 199745.1543 - regression_loss: 99131.4627 - val_loss: 18451.8750 - val_regression_loss: 9307.2783\n","Epoch 33/50\n","537/537 [==============================] - 0s 61us/step - loss: 200518.3164 - regression_loss: 100507.6247 - val_loss: 17480.3926 - val_regression_loss: 8902.2129\n","Epoch 34/50\n","537/537 [==============================] - 0s 57us/step - loss: 193549.5425 - regression_loss: 97703.0075 - val_loss: 16978.0625 - val_regression_loss: 8693.7314\n","Epoch 35/50\n","537/537 [==============================] - 0s 59us/step - loss: 190489.9007 - regression_loss: 96527.7659 - val_loss: 16738.0195 - val_regression_loss: 8581.6650\n","Epoch 36/50\n","537/537 [==============================] - 0s 59us/step - loss: 184035.2925 - regression_loss: 93185.2493 - val_loss: 16869.8926 - val_regression_loss: 8629.9512\n","Epoch 37/50\n","537/537 [==============================] - 0s 57us/step - loss: 174509.8894 - regression_loss: 88572.8060 - val_loss: 17299.0215 - val_regression_loss: 8810.9355\n","Epoch 38/50\n","537/537 [==============================] - 0s 59us/step - loss: 159469.3645 - regression_loss: 80700.6754 - val_loss: 17675.8965 - val_regression_loss: 8967.3682\n","Epoch 39/50\n","537/537 [==============================] - 0s 54us/step - loss: 162995.4434 - regression_loss: 82185.0558 - val_loss: 17778.4824 - val_regression_loss: 8995.5430\n","Epoch 40/50\n","537/537 [==============================] - 0s 60us/step - loss: 161399.6041 - regression_loss: 81107.8415 - val_loss: 17335.4238 - val_regression_loss: 8767.4521\n","Epoch 41/50\n","537/537 [==============================] - 0s 60us/step - loss: 155490.4069 - regression_loss: 78147.7807 - val_loss: 16369.1289 - val_regression_loss: 8291.9111\n","Epoch 42/50\n","537/537 [==============================] - 0s 58us/step - loss: 148049.2266 - regression_loss: 74293.6884 - val_loss: 15236.1904 - val_regression_loss: 7743.7300\n","Epoch 43/50\n","537/537 [==============================] - 0s 52us/step - loss: 131440.4036 - regression_loss: 66290.0547 - val_loss: 14293.5537 - val_regression_loss: 7288.8438\n","Epoch 44/50\n","537/537 [==============================] - 0s 54us/step - loss: 140658.5214 - regression_loss: 70767.0382 - val_loss: 13708.1250 - val_regression_loss: 7008.7969\n","Epoch 45/50\n","537/537 [==============================] - 0s 65us/step - loss: 133140.7080 - regression_loss: 67255.5644 - val_loss: 13431.5205 - val_regression_loss: 6873.3228\n","Epoch 46/50\n","537/537 [==============================] - 0s 86us/step - loss: 130372.9991 - regression_loss: 65914.3727 - val_loss: 13525.9160 - val_regression_loss: 6916.4951\n","Epoch 47/50\n","537/537 [==============================] - 0s 64us/step - loss: 122845.6530 - regression_loss: 62118.9685 - val_loss: 13736.9717 - val_regression_loss: 7015.8169\n","Epoch 48/50\n","537/537 [==============================] - 0s 63us/step - loss: 122645.9185 - regression_loss: 62057.4548 - val_loss: 13949.2725 - val_regression_loss: 7116.4009\n","Epoch 49/50\n","537/537 [==============================] - 0s 72us/step - loss: 117329.1132 - regression_loss: 59289.3858 - val_loss: 13945.6797 - val_regression_loss: 7113.8184\n","Epoch 50/50\n","537/537 [==============================] - 0s 68us/step - loss: 112635.0833 - regression_loss: 56994.8581 - val_loss: 13588.8232 - val_regression_loss: 6939.2217\n","***************************** elapsed_time is:  3.6868669986724854\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 24160.1251 - regression_loss: 11907.8712 - val_loss: 1799.5953 - val_regression_loss: 877.0353\n","Epoch 2/50\n","537/537 [==============================] - 0s 59us/step - loss: 18231.5101 - regression_loss: 8946.5457 - val_loss: 1197.4702 - val_regression_loss: 576.3732\n","Epoch 3/50\n","537/537 [==============================] - 0s 59us/step - loss: 12040.5812 - regression_loss: 5854.6277 - val_loss: 724.6611 - val_regression_loss: 340.4927\n","Epoch 4/50\n","537/537 [==============================] - 0s 60us/step - loss: 7340.8956 - regression_loss: 3509.1302 - val_loss: 694.8688 - val_regression_loss: 326.4372\n","Epoch 5/50\n","537/537 [==============================] - 0s 55us/step - loss: 5845.4106 - regression_loss: 2768.7060 - val_loss: 609.6253 - val_regression_loss: 284.9089\n","Epoch 6/50\n","537/537 [==============================] - 0s 56us/step - loss: 4783.2860 - regression_loss: 2246.3304 - val_loss: 366.8189 - val_regression_loss: 164.3485\n","Epoch 7/50\n","537/537 [==============================] - 0s 56us/step - loss: 3074.3404 - regression_loss: 1398.4260 - val_loss: 289.4261 - val_regression_loss: 126.0667\n","Epoch 8/50\n","537/537 [==============================] - 0s 56us/step - loss: 2868.6397 - regression_loss: 1299.3100 - val_loss: 354.1549 - val_regression_loss: 158.6591\n","Epoch 9/50\n","537/537 [==============================] - 0s 67us/step - loss: 3774.2221 - regression_loss: 1754.9680 - val_loss: 315.1831 - val_regression_loss: 139.4640\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 3401.2961 - regression_loss: 1569.7646 - val_loss: 211.3929 - val_regression_loss: 87.9129\n","Epoch 11/50\n","537/537 [==============================] - 0s 67us/step - loss: 2338.7847 - regression_loss: 1041.1500 - val_loss: 177.3779 - val_regression_loss: 71.2055\n","Epoch 12/50\n","537/537 [==============================] - 0s 63us/step - loss: 1834.9224 - regression_loss: 793.3253 - val_loss: 224.4209 - val_regression_loss: 94.9045\n","Epoch 13/50\n","537/537 [==============================] - 0s 64us/step - loss: 2064.7479 - regression_loss: 909.2059 - val_loss: 257.4249 - val_regression_loss: 111.4729\n","Epoch 14/50\n","537/537 [==============================] - 0s 61us/step - loss: 2326.5106 - regression_loss: 1040.0926 - val_loss: 231.4472 - val_regression_loss: 98.4997\n","Epoch 15/50\n","537/537 [==============================] - 0s 61us/step - loss: 2128.3108 - regression_loss: 941.2182 - val_loss: 198.8657 - val_regression_loss: 82.2224\n","Epoch 16/50\n","537/537 [==============================] - 0s 62us/step - loss: 1961.0810 - regression_loss: 858.0191 - val_loss: 186.7548 - val_regression_loss: 76.2142\n","Epoch 17/50\n","537/537 [==============================] - 0s 57us/step - loss: 1918.7372 - regression_loss: 837.0206 - val_loss: 179.0673 - val_regression_loss: 72.4528\n","\n","Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 18/50\n","537/537 [==============================] - 0s 70us/step - loss: 1747.0486 - regression_loss: 752.2530 - val_loss: 176.9598 - val_regression_loss: 71.4410\n","Epoch 19/50\n","537/537 [==============================] - 0s 54us/step - loss: 1722.1686 - regression_loss: 739.0059 - val_loss: 179.5459 - val_regression_loss: 72.7688\n","Epoch 20/50\n","537/537 [==============================] - 0s 60us/step - loss: 1684.4603 - regression_loss: 720.2313 - val_loss: 187.2426 - val_regression_loss: 76.6370\n","Epoch 21/50\n","537/537 [==============================] - 0s 65us/step - loss: 1641.5463 - regression_loss: 699.5727 - val_loss: 194.8160 - val_regression_loss: 80.4303\n","Epoch 22/50\n","537/537 [==============================] - 0s 88us/step - loss: 1643.1856 - regression_loss: 698.9051 - val_loss: 196.1416 - val_regression_loss: 81.0925\n","Epoch 23/50\n","537/537 [==============================] - 0s 69us/step - loss: 1617.2853 - regression_loss: 687.9749 - val_loss: 190.5380 - val_regression_loss: 78.2872\n","Epoch 24/50\n","537/537 [==============================] - 0s 59us/step - loss: 1604.0411 - regression_loss: 682.5792 - val_loss: 181.6264 - val_regression_loss: 73.8253\n","Epoch 25/50\n","537/537 [==============================] - 0s 66us/step - loss: 1521.2332 - regression_loss: 641.7307 - val_loss: 174.0061 - val_regression_loss: 70.0078\n","Epoch 26/50\n","537/537 [==============================] - 0s 57us/step - loss: 1515.1166 - regression_loss: 637.0261 - val_loss: 169.9644 - val_regression_loss: 67.9811\n","Epoch 27/50\n","537/537 [==============================] - 0s 58us/step - loss: 1515.0324 - regression_loss: 637.8367 - val_loss: 168.8822 - val_regression_loss: 67.4363\n","Epoch 28/50\n","537/537 [==============================] - 0s 56us/step - loss: 1487.8669 - regression_loss: 623.8738 - val_loss: 169.9993 - val_regression_loss: 67.9971\n","Epoch 29/50\n","537/537 [==============================] - 0s 61us/step - loss: 1501.4359 - regression_loss: 628.9385 - val_loss: 172.9174 - val_regression_loss: 69.4610\n","Epoch 30/50\n","537/537 [==============================] - 0s 61us/step - loss: 1473.5069 - regression_loss: 615.7022 - val_loss: 176.5297 - val_regression_loss: 71.2713\n","Epoch 31/50\n","537/537 [==============================] - 0s 60us/step - loss: 1441.7240 - regression_loss: 602.1339 - val_loss: 179.5389 - val_regression_loss: 72.7769\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 1421.9609 - regression_loss: 591.9903 - val_loss: 180.1141 - val_regression_loss: 73.0625\n","Epoch 33/50\n","537/537 [==============================] - 0s 67us/step - loss: 1428.4851 - regression_loss: 593.5715 - val_loss: 178.9234 - val_regression_loss: 72.4622\n","Epoch 34/50\n","537/537 [==============================] - 0s 59us/step - loss: 1394.2883 - regression_loss: 576.5786 - val_loss: 177.2089 - val_regression_loss: 71.6006\n","Epoch 35/50\n","537/537 [==============================] - 0s 60us/step - loss: 1399.7617 - regression_loss: 579.3815 - val_loss: 176.6651 - val_regression_loss: 71.3245\n","Epoch 36/50\n","537/537 [==============================] - 0s 60us/step - loss: 1365.9431 - regression_loss: 562.8549 - val_loss: 176.6322 - val_regression_loss: 71.3049\n","Epoch 37/50\n","537/537 [==============================] - 0s 61us/step - loss: 1351.8922 - regression_loss: 554.4125 - val_loss: 177.2000 - val_regression_loss: 71.5877\n","Epoch 38/50\n","537/537 [==============================] - 0s 60us/step - loss: 1345.0102 - regression_loss: 552.5599 - val_loss: 179.1210 - val_regression_loss: 72.5503\n","Epoch 39/50\n","537/537 [==============================] - 0s 64us/step - loss: 1372.7106 - regression_loss: 567.9203 - val_loss: 180.9531 - val_regression_loss: 73.4676\n","Epoch 40/50\n","537/537 [==============================] - 0s 64us/step - loss: 1322.0042 - regression_loss: 543.1706 - val_loss: 181.8979 - val_regression_loss: 73.9380\n","Epoch 41/50\n","537/537 [==============================] - 0s 65us/step - loss: 1350.4834 - regression_loss: 556.0979 - val_loss: 181.1225 - val_regression_loss: 73.5465\n","Epoch 42/50\n","537/537 [==============================] - 0s 67us/step - loss: 1317.5392 - regression_loss: 538.2336 - val_loss: 179.6311 - val_regression_loss: 72.7921\n","Epoch 43/50\n","537/537 [==============================] - 0s 66us/step - loss: 1302.5705 - regression_loss: 530.9739 - val_loss: 178.8541 - val_regression_loss: 72.3978\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 1309.9729 - regression_loss: 534.0772 - val_loss: 178.7868 - val_regression_loss: 72.3595\n","Epoch 45/50\n","537/537 [==============================] - 0s 62us/step - loss: 1324.7498 - regression_loss: 544.9526 - val_loss: 179.2729 - val_regression_loss: 72.5997\n","Epoch 46/50\n","537/537 [==============================] - 0s 60us/step - loss: 1308.0276 - regression_loss: 532.9872 - val_loss: 180.4088 - val_regression_loss: 73.1660\n","Epoch 47/50\n","537/537 [==============================] - 0s 62us/step - loss: 1298.8137 - regression_loss: 528.5336 - val_loss: 181.6119 - val_regression_loss: 73.7663\n","Epoch 48/50\n","537/537 [==============================] - 0s 60us/step - loss: 1285.8181 - regression_loss: 523.5083 - val_loss: 182.5245 - val_regression_loss: 74.2189\n","Epoch 49/50\n","537/537 [==============================] - 0s 60us/step - loss: 1276.3668 - regression_loss: 518.0227 - val_loss: 182.1451 - val_regression_loss: 74.0266\n","Epoch 50/50\n","537/537 [==============================] - 0s 63us/step - loss: 1276.0030 - regression_loss: 519.8634 - val_loss: 181.1938 - val_regression_loss: 73.5470\n","***************************** elapsed_time is:  4.224869966506958\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 9383.2190 - regression_loss: 4532.7070 - val_loss: 784.2839 - val_regression_loss: 369.1886\n","Epoch 2/50\n","537/537 [==============================] - 0s 58us/step - loss: 6608.5848 - regression_loss: 3129.8658 - val_loss: 508.0658 - val_regression_loss: 229.0839\n","Epoch 3/50\n","537/537 [==============================] - 0s 59us/step - loss: 4036.5032 - regression_loss: 1829.5703 - val_loss: 340.6007 - val_regression_loss: 143.6253\n","Epoch 4/50\n","537/537 [==============================] - 0s 56us/step - loss: 2418.5440 - regression_loss: 1007.4839 - val_loss: 284.8131 - val_regression_loss: 117.5311\n","Epoch 5/50\n","537/537 [==============================] - 0s 53us/step - loss: 1883.7930 - regression_loss: 755.9606 - val_loss: 289.1080 - val_regression_loss: 124.2178\n","Epoch 6/50\n","537/537 [==============================] - 0s 64us/step - loss: 2008.1969 - regression_loss: 853.9586 - val_loss: 312.4219 - val_regression_loss: 139.4623\n","Epoch 7/50\n","537/537 [==============================] - 0s 61us/step - loss: 2179.9647 - regression_loss: 968.1102 - val_loss: 261.7173 - val_regression_loss: 114.5976\n","Epoch 8/50\n","537/537 [==============================] - 0s 52us/step - loss: 1888.9220 - regression_loss: 826.8573 - val_loss: 191.9737 - val_regression_loss: 78.6433\n","Epoch 9/50\n","537/537 [==============================] - 0s 64us/step - loss: 1375.0151 - regression_loss: 559.7605 - val_loss: 174.1735 - val_regression_loss: 68.4337\n","Epoch 10/50\n","537/537 [==============================] - 0s 53us/step - loss: 1340.3312 - regression_loss: 530.4373 - val_loss: 188.1280 - val_regression_loss: 74.6464\n","Epoch 11/50\n","537/537 [==============================] - 0s 59us/step - loss: 1534.8338 - regression_loss: 621.5729 - val_loss: 191.9740 - val_regression_loss: 76.6319\n","Epoch 12/50\n","537/537 [==============================] - 0s 55us/step - loss: 1533.8216 - regression_loss: 622.3783 - val_loss: 179.8480 - val_regression_loss: 71.3556\n","Epoch 13/50\n","537/537 [==============================] - 0s 75us/step - loss: 1395.1625 - regression_loss: 557.7803 - val_loss: 169.6751 - val_regression_loss: 67.4891\n","Epoch 14/50\n","537/537 [==============================] - 0s 57us/step - loss: 1269.2785 - regression_loss: 505.7781 - val_loss: 171.8822 - val_regression_loss: 69.9104\n","Epoch 15/50\n","537/537 [==============================] - 0s 57us/step - loss: 1229.5673 - regression_loss: 494.5590 - val_loss: 181.5405 - val_regression_loss: 75.7832\n","Epoch 16/50\n","537/537 [==============================] - 0s 55us/step - loss: 1298.8599 - regression_loss: 539.8783 - val_loss: 185.8690 - val_regression_loss: 78.3842\n","Epoch 17/50\n","537/537 [==============================] - 0s 55us/step - loss: 1303.1490 - regression_loss: 544.3111 - val_loss: 179.8981 - val_regression_loss: 75.1393\n","Epoch 18/50\n","537/537 [==============================] - 0s 57us/step - loss: 1257.3461 - regression_loss: 519.8805 - val_loss: 168.5266 - val_regression_loss: 68.7110\n","Epoch 19/50\n","537/537 [==============================] - 0s 52us/step - loss: 1208.9451 - regression_loss: 491.0656 - val_loss: 159.0255 - val_regression_loss: 63.1226\n","Epoch 20/50\n","537/537 [==============================] - 0s 66us/step - loss: 1174.0088 - regression_loss: 467.6553 - val_loss: 154.3113 - val_regression_loss: 60.1519\n","Epoch 21/50\n","537/537 [==============================] - 0s 59us/step - loss: 1221.1984 - regression_loss: 486.7599 - val_loss: 153.0783 - val_regression_loss: 59.3070\n","Epoch 22/50\n","537/537 [==============================] - 0s 66us/step - loss: 1223.7050 - regression_loss: 486.6301 - val_loss: 153.7663 - val_regression_loss: 59.7929\n","Epoch 23/50\n","537/537 [==============================] - 0s 58us/step - loss: 1219.7333 - regression_loss: 484.7502 - val_loss: 156.9033 - val_regression_loss: 61.7503\n","Epoch 24/50\n","537/537 [==============================] - 0s 60us/step - loss: 1179.7296 - regression_loss: 467.7323 - val_loss: 161.9920 - val_regression_loss: 64.7536\n","Epoch 25/50\n","537/537 [==============================] - 0s 56us/step - loss: 1160.4148 - regression_loss: 465.1562 - val_loss: 165.9233 - val_regression_loss: 67.0451\n","Epoch 26/50\n","537/537 [==============================] - 0s 60us/step - loss: 1188.1204 - regression_loss: 478.3014 - val_loss: 165.6410 - val_regression_loss: 66.9802\n","Epoch 27/50\n","537/537 [==============================] - 0s 63us/step - loss: 1196.5952 - regression_loss: 487.2434 - val_loss: 162.3129 - val_regression_loss: 65.1656\n","Epoch 28/50\n","537/537 [==============================] - 0s 58us/step - loss: 1164.0039 - regression_loss: 469.7998 - val_loss: 158.3213 - val_regression_loss: 62.8678\n","Epoch 29/50\n","537/537 [==============================] - 0s 54us/step - loss: 1167.8458 - regression_loss: 470.3350 - val_loss: 156.0988 - val_regression_loss: 61.4711\n","Epoch 30/50\n","537/537 [==============================] - 0s 64us/step - loss: 1169.1063 - regression_loss: 469.6848 - val_loss: 156.2006 - val_regression_loss: 61.3129\n","\n","Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 31/50\n","537/537 [==============================] - 0s 63us/step - loss: 1180.8972 - regression_loss: 472.8708 - val_loss: 156.5342 - val_regression_loss: 61.4468\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 1173.8359 - regression_loss: 470.4964 - val_loss: 156.5745 - val_regression_loss: 61.5014\n","Epoch 33/50\n","537/537 [==============================] - 0s 63us/step - loss: 1179.7340 - regression_loss: 472.1315 - val_loss: 157.0097 - val_regression_loss: 61.7935\n","Epoch 34/50\n","537/537 [==============================] - 0s 57us/step - loss: 1171.5247 - regression_loss: 468.8353 - val_loss: 157.6857 - val_regression_loss: 62.2299\n","Epoch 35/50\n","537/537 [==============================] - 0s 51us/step - loss: 1164.7434 - regression_loss: 466.7828 - val_loss: 158.7585 - val_regression_loss: 62.8589\n","\n","Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","***************************** elapsed_time is:  3.2879724502563477\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 269688.9355 - regression_loss: 135375.8260 - val_loss: 30512.3281 - val_regression_loss: 15330.7070\n","Epoch 2/50\n","537/537 [==============================] - 0s 70us/step - loss: 246840.4372 - regression_loss: 123931.9223 - val_loss: 27307.0098 - val_regression_loss: 13728.2988\n","Epoch 3/50\n","537/537 [==============================] - 0s 75us/step - loss: 222443.2383 - regression_loss: 111756.0780 - val_loss: 23028.1816 - val_regression_loss: 11590.5801\n","Epoch 4/50\n","537/537 [==============================] - 0s 59us/step - loss: 187076.0354 - regression_loss: 94106.9235 - val_loss: 17741.0781 - val_regression_loss: 8949.3184\n","Epoch 5/50\n","537/537 [==============================] - 0s 51us/step - loss: 144317.8627 - regression_loss: 72732.7684 - val_loss: 12067.4336 - val_regression_loss: 6112.2051\n","Epoch 6/50\n","537/537 [==============================] - 0s 65us/step - loss: 102326.7671 - regression_loss: 51737.1001 - val_loss: 7250.9565 - val_regression_loss: 3683.7278\n","Epoch 7/50\n","537/537 [==============================] - 0s 60us/step - loss: 65704.9344 - regression_loss: 33270.0428 - val_loss: 5041.8677 - val_regression_loss: 2447.0691\n","Epoch 8/50\n","537/537 [==============================] - 0s 59us/step - loss: 50205.2470 - regression_loss: 24506.9101 - val_loss: 6017.0010 - val_regression_loss: 2615.4390\n","Epoch 9/50\n","537/537 [==============================] - 0s 58us/step - loss: 60666.0134 - regression_loss: 27436.4221 - val_loss: 6242.9497 - val_regression_loss: 2807.8674\n","Epoch 10/50\n","537/537 [==============================] - 0s 71us/step - loss: 63293.8904 - regression_loss: 29442.2497 - val_loss: 5037.2896 - val_regression_loss: 2407.7307\n","Epoch 11/50\n","537/537 [==============================] - 0s 58us/step - loss: 53403.5494 - regression_loss: 26079.0151 - val_loss: 4104.3755 - val_regression_loss: 2029.8140\n","Epoch 12/50\n","537/537 [==============================] - 0s 59us/step - loss: 42937.4082 - regression_loss: 21394.5739 - val_loss: 3984.7961 - val_regression_loss: 1997.7594\n","Epoch 13/50\n","537/537 [==============================] - 0s 57us/step - loss: 42181.7527 - regression_loss: 21218.1895 - val_loss: 4252.4106 - val_regression_loss: 2137.0396\n","Epoch 14/50\n","537/537 [==============================] - 0s 59us/step - loss: 42503.5075 - regression_loss: 21373.3756 - val_loss: 4368.4473 - val_regression_loss: 2192.3850\n","Epoch 15/50\n","537/537 [==============================] - 0s 57us/step - loss: 43603.3694 - regression_loss: 21896.6954 - val_loss: 4149.1777 - val_regression_loss: 2076.5261\n","Epoch 16/50\n","537/537 [==============================] - 0s 55us/step - loss: 38961.1638 - regression_loss: 19508.0298 - val_loss: 3734.3386 - val_regression_loss: 1861.1886\n","Epoch 17/50\n","537/537 [==============================] - 0s 64us/step - loss: 37454.2089 - regression_loss: 18708.1568 - val_loss: 3328.9595 - val_regression_loss: 1649.5598\n","Epoch 18/50\n","537/537 [==============================] - 0s 74us/step - loss: 34635.1821 - regression_loss: 17238.4719 - val_loss: 3085.1250 - val_regression_loss: 1518.6700\n","Epoch 19/50\n","537/537 [==============================] - 0s 70us/step - loss: 33008.0633 - regression_loss: 16375.1998 - val_loss: 3002.2144 - val_regression_loss: 1469.9091\n","Epoch 20/50\n","537/537 [==============================] - 0s 64us/step - loss: 31644.2812 - regression_loss: 15643.4468 - val_loss: 2947.5068 - val_regression_loss: 1439.2292\n","Epoch 21/50\n","537/537 [==============================] - 0s 61us/step - loss: 29942.5640 - regression_loss: 14795.2374 - val_loss: 2804.2952 - val_regression_loss: 1369.3635\n","Epoch 22/50\n","537/537 [==============================] - 0s 62us/step - loss: 28468.3084 - regression_loss: 14078.1834 - val_loss: 2581.5505 - val_regression_loss: 1264.2925\n","Epoch 23/50\n","537/537 [==============================] - 0s 65us/step - loss: 25706.7607 - regression_loss: 12772.1405 - val_loss: 2392.7896 - val_regression_loss: 1177.2028\n","Epoch 24/50\n","537/537 [==============================] - 0s 64us/step - loss: 21104.9542 - regression_loss: 10507.9474 - val_loss: 2265.8506 - val_regression_loss: 1119.8275\n","Epoch 25/50\n","537/537 [==============================] - 0s 61us/step - loss: 23126.6896 - regression_loss: 11607.5245 - val_loss: 2133.7505 - val_regression_loss: 1054.9158\n","Epoch 26/50\n","537/537 [==============================] - 0s 65us/step - loss: 17129.3759 - regression_loss: 8582.7792 - val_loss: 1972.1429 - val_regression_loss: 970.4650\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 19779.6995 - regression_loss: 9989.6542 - val_loss: 1864.5920 - val_regression_loss: 905.6532\n","Epoch 28/50\n","537/537 [==============================] - 0s 66us/step - loss: 17702.5043 - regression_loss: 8939.8833 - val_loss: 1843.7405 - val_regression_loss: 880.4338\n","Epoch 29/50\n","537/537 [==============================] - 0s 79us/step - loss: 16382.2128 - regression_loss: 8288.2337 - val_loss: 1763.3037 - val_regression_loss: 830.5334\n","Epoch 30/50\n","537/537 [==============================] - 0s 68us/step - loss: 15131.4646 - regression_loss: 7698.7059 - val_loss: 1530.3461 - val_regression_loss: 720.9703\n","Epoch 31/50\n","537/537 [==============================] - 0s 66us/step - loss: 13642.7263 - regression_loss: 7058.2338 - val_loss: 1285.6678 - val_regression_loss: 620.1300\n","Epoch 32/50\n","537/537 [==============================] - 0s 68us/step - loss: 12208.4295 - regression_loss: 6553.9897 - val_loss: 1144.3820 - val_regression_loss: 566.7811\n","Epoch 33/50\n","537/537 [==============================] - 0s 60us/step - loss: 10835.2875 - regression_loss: 5969.4824 - val_loss: 1057.0554 - val_regression_loss: 519.5055\n","Epoch 34/50\n","537/537 [==============================] - 0s 61us/step - loss: 10271.6150 - regression_loss: 5769.0401 - val_loss: 1034.8452 - val_regression_loss: 474.7368\n","Epoch 35/50\n","537/537 [==============================] - 0s 70us/step - loss: 9254.2087 - regression_loss: 5025.9436 - val_loss: 1023.3278 - val_regression_loss: 444.1899\n","Epoch 36/50\n","537/537 [==============================] - 0s 62us/step - loss: 8827.1017 - regression_loss: 4663.6165 - val_loss: 902.8804 - val_regression_loss: 408.0957\n","Epoch 37/50\n","537/537 [==============================] - 0s 62us/step - loss: 8320.9394 - regression_loss: 4640.7718 - val_loss: 793.6972 - val_regression_loss: 383.3102\n","Epoch 38/50\n","537/537 [==============================] - 0s 62us/step - loss: 7649.5758 - regression_loss: 4480.6714 - val_loss: 739.6323 - val_regression_loss: 360.6869\n","Epoch 39/50\n","537/537 [==============================] - 0s 61us/step - loss: 7253.0733 - regression_loss: 4243.9439 - val_loss: 721.9995 - val_regression_loss: 335.9495\n","Epoch 40/50\n","537/537 [==============================] - 0s 63us/step - loss: 6708.2430 - regression_loss: 3807.6033 - val_loss: 724.1599 - val_regression_loss: 322.2601\n","Epoch 41/50\n","537/537 [==============================] - 0s 62us/step - loss: 6295.6619 - regression_loss: 3453.2445 - val_loss: 690.9459 - val_regression_loss: 307.7574\n","Epoch 42/50\n","537/537 [==============================] - 0s 63us/step - loss: 5825.2548 - regression_loss: 3143.0934 - val_loss: 644.3817 - val_regression_loss: 293.7615\n","Epoch 43/50\n","537/537 [==============================] - 0s 63us/step - loss: 5526.7702 - regression_loss: 3032.6672 - val_loss: 627.9029 - val_regression_loss: 284.4703\n","Epoch 44/50\n","537/537 [==============================] - 0s 70us/step - loss: 5411.5580 - regression_loss: 2944.4510 - val_loss: 633.9351 - val_regression_loss: 280.1461\n","Epoch 45/50\n","537/537 [==============================] - 0s 68us/step - loss: 4964.5364 - regression_loss: 2640.8184 - val_loss: 633.6296 - val_regression_loss: 276.8472\n","Epoch 46/50\n","537/537 [==============================] - 0s 61us/step - loss: 4843.5995 - regression_loss: 2533.7117 - val_loss: 598.4054 - val_regression_loss: 264.3828\n","Epoch 47/50\n","537/537 [==============================] - 0s 62us/step - loss: 4659.7501 - regression_loss: 2466.5964 - val_loss: 569.1840 - val_regression_loss: 253.6554\n","Epoch 48/50\n","537/537 [==============================] - 0s 65us/step - loss: 4506.4287 - regression_loss: 2407.7910 - val_loss: 576.2422 - val_regression_loss: 252.6859\n","Epoch 49/50\n","537/537 [==============================] - 0s 67us/step - loss: 4268.7385 - regression_loss: 2217.8678 - val_loss: 590.3695 - val_regression_loss: 255.1651\n","Epoch 50/50\n","537/537 [==============================] - 0s 63us/step - loss: 4207.3253 - regression_loss: 2150.7783 - val_loss: 561.4934 - val_regression_loss: 243.9373\n","***************************** elapsed_time is:  3.7286643981933594\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 7142.9301 - regression_loss: 3389.1744 - val_loss: 702.3898 - val_regression_loss: 323.7011\n","Epoch 2/50\n","537/537 [==============================] - 0s 59us/step - loss: 4654.2034 - regression_loss: 2126.4206 - val_loss: 518.1959 - val_regression_loss: 229.0395\n","Epoch 3/50\n","537/537 [==============================] - 0s 62us/step - loss: 3457.1003 - regression_loss: 1506.1691 - val_loss: 314.6771 - val_regression_loss: 128.6519\n","Epoch 4/50\n","537/537 [==============================] - 0s 59us/step - loss: 1963.0040 - regression_loss: 772.2834 - val_loss: 204.3817 - val_regression_loss: 77.7082\n","Epoch 5/50\n","537/537 [==============================] - 0s 56us/step - loss: 1458.6541 - regression_loss: 555.5062 - val_loss: 228.8480 - val_regression_loss: 92.9512\n","Epoch 6/50\n","537/537 [==============================] - 0s 58us/step - loss: 2133.8592 - regression_loss: 918.3420 - val_loss: 201.1682 - val_regression_loss: 79.4914\n","Epoch 7/50\n","537/537 [==============================] - 0s 55us/step - loss: 1842.9970 - regression_loss: 775.6415 - val_loss: 177.8880 - val_regression_loss: 68.1424\n","Epoch 8/50\n","537/537 [==============================] - 0s 59us/step - loss: 1465.6643 - regression_loss: 588.4596 - val_loss: 181.7422 - val_regression_loss: 70.6710\n","Epoch 9/50\n","537/537 [==============================] - 0s 55us/step - loss: 1316.4123 - regression_loss: 517.7305 - val_loss: 215.6720 - val_regression_loss: 87.8868\n","Epoch 10/50\n","537/537 [==============================] - 0s 63us/step - loss: 1455.0636 - regression_loss: 588.1799 - val_loss: 225.5880 - val_regression_loss: 92.7291\n","Epoch 11/50\n","537/537 [==============================] - 0s 62us/step - loss: 1516.1497 - regression_loss: 617.7270 - val_loss: 207.6261 - val_regression_loss: 83.7318\n","Epoch 12/50\n","537/537 [==============================] - 0s 56us/step - loss: 1425.0908 - regression_loss: 570.7736 - val_loss: 183.2332 - val_regression_loss: 72.2297\n","Epoch 13/50\n","537/537 [==============================] - 0s 60us/step - loss: 1306.6722 - regression_loss: 520.0984 - val_loss: 160.0648 - val_regression_loss: 62.0319\n","Epoch 14/50\n","537/537 [==============================] - 0s 60us/step - loss: 1228.2109 - regression_loss: 493.2192 - val_loss: 154.5771 - val_regression_loss: 60.6081\n","Epoch 15/50\n","537/537 [==============================] - 0s 69us/step - loss: 1284.5330 - regression_loss: 534.0961 - val_loss: 153.9721 - val_regression_loss: 60.6793\n","Epoch 16/50\n","537/537 [==============================] - 0s 64us/step - loss: 1314.1128 - regression_loss: 552.0408 - val_loss: 152.4110 - val_regression_loss: 59.3849\n","Epoch 17/50\n","537/537 [==============================] - 0s 61us/step - loss: 1267.8572 - regression_loss: 525.2652 - val_loss: 155.3058 - val_regression_loss: 59.9466\n","Epoch 18/50\n","537/537 [==============================] - 0s 62us/step - loss: 1239.6600 - regression_loss: 504.0858 - val_loss: 158.7761 - val_regression_loss: 61.0916\n","Epoch 19/50\n","537/537 [==============================] - 0s 59us/step - loss: 1221.8431 - regression_loss: 491.3231 - val_loss: 165.5278 - val_regression_loss: 64.0943\n","Epoch 20/50\n","537/537 [==============================] - 0s 63us/step - loss: 1216.6747 - regression_loss: 483.0201 - val_loss: 167.2572 - val_regression_loss: 64.7976\n","Epoch 21/50\n","537/537 [==============================] - 0s 57us/step - loss: 1236.5117 - regression_loss: 493.3421 - val_loss: 164.3797 - val_regression_loss: 63.3910\n","Epoch 22/50\n","537/537 [==============================] - 0s 63us/step - loss: 1227.3192 - regression_loss: 491.4893 - val_loss: 159.4279 - val_regression_loss: 61.2614\n","Epoch 23/50\n","537/537 [==============================] - 0s 60us/step - loss: 1211.5354 - regression_loss: 483.5096 - val_loss: 152.8403 - val_regression_loss: 58.5839\n","Epoch 24/50\n","537/537 [==============================] - 0s 68us/step - loss: 1192.4635 - regression_loss: 480.9553 - val_loss: 150.2160 - val_regression_loss: 57.7564\n","Epoch 25/50\n","537/537 [==============================] - 0s 64us/step - loss: 1192.8813 - regression_loss: 483.7073 - val_loss: 149.2911 - val_regression_loss: 57.3272\n","Epoch 26/50\n","537/537 [==============================] - 0s 57us/step - loss: 1221.7272 - regression_loss: 498.8080 - val_loss: 150.6484 - val_regression_loss: 57.7685\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 1179.2357 - regression_loss: 476.0242 - val_loss: 151.7581 - val_regression_loss: 58.1303\n","Epoch 28/50\n","537/537 [==============================] - 0s 59us/step - loss: 1182.2475 - regression_loss: 477.8813 - val_loss: 153.9658 - val_regression_loss: 59.0414\n","Epoch 29/50\n","537/537 [==============================] - 0s 60us/step - loss: 1171.5325 - regression_loss: 470.8005 - val_loss: 155.7033 - val_regression_loss: 59.7397\n","Epoch 30/50\n","537/537 [==============================] - 0s 60us/step - loss: 1193.6600 - regression_loss: 480.9789 - val_loss: 155.7615 - val_regression_loss: 59.7018\n","Epoch 31/50\n","537/537 [==============================] - 0s 56us/step - loss: 1166.9683 - regression_loss: 465.9980 - val_loss: 153.8483 - val_regression_loss: 58.8427\n","Epoch 32/50\n","537/537 [==============================] - 0s 60us/step - loss: 1176.5847 - regression_loss: 475.3277 - val_loss: 151.4518 - val_regression_loss: 57.8401\n","Epoch 33/50\n","537/537 [==============================] - 0s 64us/step - loss: 1164.5470 - regression_loss: 469.9363 - val_loss: 150.0919 - val_regression_loss: 57.3140\n","Epoch 34/50\n","537/537 [==============================] - 0s 60us/step - loss: 1172.4440 - regression_loss: 473.8942 - val_loss: 149.6318 - val_regression_loss: 57.1094\n","Epoch 35/50\n","537/537 [==============================] - 0s 62us/step - loss: 1148.6501 - regression_loss: 464.4033 - val_loss: 150.6929 - val_regression_loss: 57.5136\n","Epoch 36/50\n","537/537 [==============================] - 0s 60us/step - loss: 1165.4437 - regression_loss: 468.8034 - val_loss: 151.1431 - val_regression_loss: 57.7161\n","Epoch 37/50\n","537/537 [==============================] - 0s 58us/step - loss: 1175.8765 - regression_loss: 477.0052 - val_loss: 152.1770 - val_regression_loss: 58.1070\n","Epoch 38/50\n","537/537 [==============================] - 0s 59us/step - loss: 1178.1172 - regression_loss: 476.0995 - val_loss: 152.4064 - val_regression_loss: 58.1114\n","Epoch 39/50\n","537/537 [==============================] - 0s 60us/step - loss: 1149.5818 - regression_loss: 463.0487 - val_loss: 151.6683 - val_regression_loss: 57.7073\n","Epoch 40/50\n","537/537 [==============================] - 0s 58us/step - loss: 1152.5187 - regression_loss: 464.2316 - val_loss: 150.6598 - val_regression_loss: 57.2721\n","\n","Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 41/50\n","537/537 [==============================] - 0s 82us/step - loss: 1145.1685 - regression_loss: 461.6699 - val_loss: 150.2308 - val_regression_loss: 57.1108\n","Epoch 42/50\n","537/537 [==============================] - 0s 65us/step - loss: 1159.3637 - regression_loss: 466.9931 - val_loss: 150.1613 - val_regression_loss: 57.0883\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 1172.6860 - regression_loss: 478.5916 - val_loss: 150.4844 - val_regression_loss: 57.2198\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 1140.1303 - regression_loss: 458.8430 - val_loss: 150.7763 - val_regression_loss: 57.3611\n","Epoch 45/50\n","537/537 [==============================] - 0s 56us/step - loss: 1134.6879 - regression_loss: 457.5121 - val_loss: 150.9329 - val_regression_loss: 57.4312\n","Epoch 46/50\n","537/537 [==============================] - 0s 60us/step - loss: 1156.7738 - regression_loss: 469.6680 - val_loss: 151.0145 - val_regression_loss: 57.4615\n","Epoch 47/50\n","537/537 [==============================] - 0s 62us/step - loss: 1156.5212 - regression_loss: 468.1569 - val_loss: 150.9672 - val_regression_loss: 57.4278\n","Epoch 48/50\n","537/537 [==============================] - 0s 59us/step - loss: 1159.3355 - regression_loss: 468.8349 - val_loss: 150.9385 - val_regression_loss: 57.3648\n","Epoch 49/50\n","537/537 [==============================] - 0s 71us/step - loss: 1155.3555 - regression_loss: 466.5394 - val_loss: 150.8574 - val_regression_loss: 57.3185\n","Epoch 50/50\n","537/537 [==============================] - 0s 59us/step - loss: 1162.6055 - regression_loss: 470.6966 - val_loss: 150.7207 - val_regression_loss: 57.2369\n","\n","Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","***************************** elapsed_time is:  4.150875091552734\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 42712.5575 - regression_loss: 21117.7365 - val_loss: 4065.1636 - val_regression_loss: 2004.5034\n","Epoch 2/50\n","537/537 [==============================] - 0s 61us/step - loss: 34295.8100 - regression_loss: 16936.3635 - val_loss: 3108.1360 - val_regression_loss: 1528.9774\n","Epoch 3/50\n","537/537 [==============================] - 0s 60us/step - loss: 26622.0272 - regression_loss: 13123.0510 - val_loss: 1978.5647 - val_regression_loss: 967.2795\n","Epoch 4/50\n","537/537 [==============================] - 0s 57us/step - loss: 17206.0682 - regression_loss: 8439.4654 - val_loss: 931.4816 - val_regression_loss: 446.8694\n","Epoch 5/50\n","537/537 [==============================] - 0s 55us/step - loss: 7938.2053 - regression_loss: 3829.9911 - val_loss: 564.3392 - val_regression_loss: 265.9937\n","Epoch 6/50\n","537/537 [==============================] - 0s 56us/step - loss: 4345.4881 - regression_loss: 2054.7016 - val_loss: 895.5052 - val_regression_loss: 432.4393\n","Epoch 7/50\n","537/537 [==============================] - 0s 57us/step - loss: 6928.3240 - regression_loss: 3355.3318 - val_loss: 872.7281 - val_regression_loss: 420.1003\n","Epoch 8/50\n","537/537 [==============================] - 0s 63us/step - loss: 6847.5069 - regression_loss: 3302.1298 - val_loss: 546.1202 - val_regression_loss: 255.8018\n","Epoch 9/50\n","537/537 [==============================] - 0s 64us/step - loss: 4406.6735 - regression_loss: 2072.9912 - val_loss: 354.5437 - val_regression_loss: 159.4878\n","Epoch 10/50\n","537/537 [==============================] - 0s 73us/step - loss: 2894.8519 - regression_loss: 1317.2638 - val_loss: 390.5612 - val_regression_loss: 177.3676\n","Epoch 11/50\n","537/537 [==============================] - 0s 69us/step - loss: 3176.6950 - regression_loss: 1457.2168 - val_loss: 463.7605 - val_regression_loss: 214.1551\n","Epoch 12/50\n","537/537 [==============================] - 0s 69us/step - loss: 3811.2339 - regression_loss: 1776.9542 - val_loss: 447.2534 - val_regression_loss: 206.3010\n","Epoch 13/50\n","537/537 [==============================] - 0s 57us/step - loss: 3754.9549 - regression_loss: 1751.7732 - val_loss: 360.9759 - val_regression_loss: 163.6544\n","Epoch 14/50\n","537/537 [==============================] - 0s 59us/step - loss: 3053.4250 - regression_loss: 1405.4757 - val_loss: 277.5943 - val_regression_loss: 122.4219\n","Epoch 15/50\n","537/537 [==============================] - 0s 65us/step - loss: 2421.3614 - regression_loss: 1092.2002 - val_loss: 247.4730 - val_regression_loss: 107.6892\n","Epoch 16/50\n","537/537 [==============================] - 0s 63us/step - loss: 2182.7325 - regression_loss: 975.5533 - val_loss: 268.6842 - val_regression_loss: 118.4544\n","Epoch 17/50\n","537/537 [==============================] - 0s 64us/step - loss: 2366.8891 - regression_loss: 1068.8078 - val_loss: 295.2316 - val_regression_loss: 131.7455\n","Epoch 18/50\n","537/537 [==============================] - 0s 60us/step - loss: 2546.5213 - regression_loss: 1158.2015 - val_loss: 289.9618 - val_regression_loss: 129.0406\n","Epoch 19/50\n","537/537 [==============================] - 0s 62us/step - loss: 2500.9633 - regression_loss: 1133.0736 - val_loss: 260.1984 - val_regression_loss: 114.0585\n","Epoch 20/50\n","537/537 [==============================] - 0s 62us/step - loss: 2261.2936 - regression_loss: 1014.6520 - val_loss: 235.6830 - val_regression_loss: 101.6756\n","Epoch 21/50\n","537/537 [==============================] - 0s 63us/step - loss: 2042.6514 - regression_loss: 904.7627 - val_loss: 230.9868 - val_regression_loss: 99.1890\n","Epoch 22/50\n","537/537 [==============================] - 0s 58us/step - loss: 2000.2906 - regression_loss: 880.7175 - val_loss: 237.8042 - val_regression_loss: 102.4738\n","Epoch 23/50\n","537/537 [==============================] - 0s 62us/step - loss: 2047.9120 - regression_loss: 902.9146 - val_loss: 241.8846 - val_regression_loss: 104.4309\n","Epoch 24/50\n","537/537 [==============================] - 0s 71us/step - loss: 2037.6793 - regression_loss: 896.8473 - val_loss: 236.1558 - val_regression_loss: 101.5472\n","Epoch 25/50\n","537/537 [==============================] - 0s 64us/step - loss: 1979.8424 - regression_loss: 870.7298 - val_loss: 224.5851 - val_regression_loss: 95.8027\n","Epoch 26/50\n","537/537 [==============================] - 0s 62us/step - loss: 1917.6428 - regression_loss: 838.5802 - val_loss: 215.7509 - val_regression_loss: 91.4604\n","Epoch 27/50\n","537/537 [==============================] - 0s 66us/step - loss: 1825.5020 - regression_loss: 791.8264 - val_loss: 212.5127 - val_regression_loss: 89.9386\n","Epoch 28/50\n","537/537 [==============================] - 0s 59us/step - loss: 1832.1474 - regression_loss: 795.1918 - val_loss: 210.7820 - val_regression_loss: 89.1677\n","Epoch 29/50\n","537/537 [==============================] - 0s 65us/step - loss: 1839.0673 - regression_loss: 801.2364 - val_loss: 206.1944 - val_regression_loss: 86.9406\n","Epoch 30/50\n","537/537 [==============================] - 0s 64us/step - loss: 1785.7371 - regression_loss: 774.2520 - val_loss: 199.1234 - val_regression_loss: 83.4315\n","Epoch 31/50\n","537/537 [==============================] - 0s 59us/step - loss: 1756.6603 - regression_loss: 762.1632 - val_loss: 193.7492 - val_regression_loss: 80.7211\n","Epoch 32/50\n","537/537 [==============================] - 0s 63us/step - loss: 1689.5017 - regression_loss: 728.4285 - val_loss: 191.5525 - val_regression_loss: 79.5724\n","Epoch 33/50\n","537/537 [==============================] - 0s 64us/step - loss: 1663.1745 - regression_loss: 715.3540 - val_loss: 190.4030 - val_regression_loss: 78.9466\n","Epoch 34/50\n","537/537 [==============================] - 0s 70us/step - loss: 1647.6030 - regression_loss: 706.0375 - val_loss: 188.2885 - val_regression_loss: 77.8623\n","Epoch 35/50\n","537/537 [==============================] - 0s 61us/step - loss: 1585.4678 - regression_loss: 675.7780 - val_loss: 186.1040 - val_regression_loss: 76.7646\n","Epoch 36/50\n","537/537 [==============================] - 0s 60us/step - loss: 1586.8240 - regression_loss: 674.1745 - val_loss: 185.3522 - val_regression_loss: 76.4023\n","Epoch 37/50\n","537/537 [==============================] - 0s 59us/step - loss: 1538.4430 - regression_loss: 648.9509 - val_loss: 185.4691 - val_regression_loss: 76.4764\n","Epoch 38/50\n","537/537 [==============================] - 0s 64us/step - loss: 1555.4535 - regression_loss: 658.1652 - val_loss: 183.8289 - val_regression_loss: 75.6723\n","Epoch 39/50\n","537/537 [==============================] - 0s 62us/step - loss: 1525.7683 - regression_loss: 644.6561 - val_loss: 180.9359 - val_regression_loss: 74.2351\n","Epoch 40/50\n","537/537 [==============================] - 0s 54us/step - loss: 1518.0904 - regression_loss: 638.0961 - val_loss: 178.2390 - val_regression_loss: 72.8909\n","Epoch 41/50\n","537/537 [==============================] - 0s 65us/step - loss: 1474.9432 - regression_loss: 617.2481 - val_loss: 175.9550 - val_regression_loss: 71.7609\n","Epoch 42/50\n","537/537 [==============================] - 0s 71us/step - loss: 1478.3098 - regression_loss: 620.8507 - val_loss: 173.9719 - val_regression_loss: 70.7806\n","Epoch 43/50\n","537/537 [==============================] - 0s 60us/step - loss: 1486.7189 - regression_loss: 626.5719 - val_loss: 172.5105 - val_regression_loss: 70.0678\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 1442.2707 - regression_loss: 603.0274 - val_loss: 171.7358 - val_regression_loss: 69.6805\n","Epoch 45/50\n","537/537 [==============================] - 0s 62us/step - loss: 1424.9607 - regression_loss: 594.3634 - val_loss: 171.5120 - val_regression_loss: 69.5539\n","Epoch 46/50\n","537/537 [==============================] - 0s 61us/step - loss: 1425.6289 - regression_loss: 594.3294 - val_loss: 170.8064 - val_regression_loss: 69.1747\n","Epoch 47/50\n","537/537 [==============================] - 0s 63us/step - loss: 1392.7690 - regression_loss: 578.1843 - val_loss: 169.5756 - val_regression_loss: 68.5325\n","Epoch 48/50\n","537/537 [==============================] - 0s 61us/step - loss: 1400.6340 - regression_loss: 581.7745 - val_loss: 167.1346 - val_regression_loss: 67.3092\n","Epoch 49/50\n","537/537 [==============================] - 0s 61us/step - loss: 1393.9724 - regression_loss: 579.7642 - val_loss: 164.8224 - val_regression_loss: 66.1638\n","Epoch 50/50\n","537/537 [==============================] - 0s 67us/step - loss: 1338.5229 - regression_loss: 551.4899 - val_loss: 162.9870 - val_regression_loss: 65.2744\n","***************************** elapsed_time is:  3.791317939758301\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 13959.0421 - regression_loss: 6798.3673 - val_loss: 1020.5176 - val_regression_loss: 488.0767\n","Epoch 2/50\n","537/537 [==============================] - 0s 54us/step - loss: 10614.8180 - regression_loss: 5145.8723 - val_loss: 686.2835 - val_regression_loss: 323.3922\n","Epoch 3/50\n","537/537 [==============================] - 0s 59us/step - loss: 7264.0310 - regression_loss: 3489.8976 - val_loss: 413.6970 - val_regression_loss: 189.8586\n","Epoch 4/50\n","537/537 [==============================] - 0s 54us/step - loss: 4328.8851 - regression_loss: 2044.5161 - val_loss: 387.1203 - val_regression_loss: 178.4876\n","Epoch 5/50\n","537/537 [==============================] - 0s 56us/step - loss: 3459.4595 - regression_loss: 1626.0787 - val_loss: 318.6673 - val_regression_loss: 143.1059\n","Epoch 6/50\n","537/537 [==============================] - 0s 59us/step - loss: 2577.1374 - regression_loss: 1177.0185 - val_loss: 257.5486 - val_regression_loss: 109.8330\n","Epoch 7/50\n","537/537 [==============================] - 0s 50us/step - loss: 1939.5165 - regression_loss: 837.0058 - val_loss: 310.5739 - val_regression_loss: 134.0110\n","Epoch 8/50\n","537/537 [==============================] - 0s 57us/step - loss: 2365.4391 - regression_loss: 1034.0023 - val_loss: 326.6152 - val_regression_loss: 141.5914\n","Epoch 9/50\n","537/537 [==============================] - 0s 59us/step - loss: 2674.7474 - regression_loss: 1184.1782 - val_loss: 249.6397 - val_regression_loss: 104.1932\n","Epoch 10/50\n","537/537 [==============================] - 0s 65us/step - loss: 2071.2345 - regression_loss: 892.7400 - val_loss: 178.7987 - val_regression_loss: 70.3709\n","Epoch 11/50\n","537/537 [==============================] - 0s 53us/step - loss: 1613.2220 - regression_loss: 677.7698 - val_loss: 160.8038 - val_regression_loss: 62.8213\n","Epoch 12/50\n","537/537 [==============================] - 0s 57us/step - loss: 1531.0280 - regression_loss: 648.2835 - val_loss: 175.1678 - val_regression_loss: 70.9630\n","Epoch 13/50\n","537/537 [==============================] - 0s 60us/step - loss: 1678.1480 - regression_loss: 728.2482 - val_loss: 179.7303 - val_regression_loss: 73.6580\n","Epoch 14/50\n","537/537 [==============================] - 0s 61us/step - loss: 1719.9287 - regression_loss: 756.1739 - val_loss: 166.7147 - val_regression_loss: 67.1066\n","Epoch 15/50\n","537/537 [==============================] - 0s 59us/step - loss: 1672.2282 - regression_loss: 727.7110 - val_loss: 155.0459 - val_regression_loss: 60.9074\n","Epoch 16/50\n","537/537 [==============================] - 0s 58us/step - loss: 1548.8759 - regression_loss: 662.8265 - val_loss: 152.6883 - val_regression_loss: 59.1952\n","Epoch 17/50\n","537/537 [==============================] - 0s 69us/step - loss: 1504.5830 - regression_loss: 638.5323 - val_loss: 157.1280 - val_regression_loss: 60.8060\n","Epoch 18/50\n","537/537 [==============================] - 0s 69us/step - loss: 1467.7682 - regression_loss: 614.3251 - val_loss: 165.5788 - val_regression_loss: 64.4510\n","Epoch 19/50\n","537/537 [==============================] - 0s 63us/step - loss: 1483.8739 - regression_loss: 615.4822 - val_loss: 172.9279 - val_regression_loss: 67.7814\n","Epoch 20/50\n","537/537 [==============================] - 0s 60us/step - loss: 1475.5862 - regression_loss: 606.5029 - val_loss: 173.4173 - val_regression_loss: 68.0672\n","Epoch 21/50\n","537/537 [==============================] - 0s 58us/step - loss: 1425.9399 - regression_loss: 585.2041 - val_loss: 168.1564 - val_regression_loss: 65.7742\n","Epoch 22/50\n","537/537 [==============================] - 0s 59us/step - loss: 1428.9138 - regression_loss: 588.6303 - val_loss: 161.5801 - val_regression_loss: 62.9093\n","Epoch 23/50\n","537/537 [==============================] - 0s 62us/step - loss: 1404.1121 - regression_loss: 583.0515 - val_loss: 157.5539 - val_regression_loss: 61.2515\n","Epoch 24/50\n","537/537 [==============================] - 0s 69us/step - loss: 1375.0516 - regression_loss: 570.8766 - val_loss: 156.0267 - val_regression_loss: 60.7504\n","Epoch 25/50\n","537/537 [==============================] - 0s 64us/step - loss: 1386.9962 - regression_loss: 575.9259 - val_loss: 156.6375 - val_regression_loss: 61.2181\n","Epoch 26/50\n","537/537 [==============================] - 0s 64us/step - loss: 1354.4796 - regression_loss: 560.8300 - val_loss: 158.7714 - val_regression_loss: 62.3282\n","Epoch 27/50\n","537/537 [==============================] - 0s 60us/step - loss: 1350.9239 - regression_loss: 558.1083 - val_loss: 160.8989 - val_regression_loss: 63.3159\n","Epoch 28/50\n","537/537 [==============================] - 0s 62us/step - loss: 1329.8845 - regression_loss: 548.2232 - val_loss: 162.2306 - val_regression_loss: 63.8433\n","Epoch 29/50\n","537/537 [==============================] - 0s 61us/step - loss: 1354.5212 - regression_loss: 559.6218 - val_loss: 163.5100 - val_regression_loss: 64.3451\n","Epoch 30/50\n","537/537 [==============================] - 0s 57us/step - loss: 1333.1035 - regression_loss: 547.9743 - val_loss: 164.6575 - val_regression_loss: 64.8268\n","Epoch 31/50\n","537/537 [==============================] - 0s 60us/step - loss: 1330.9184 - regression_loss: 546.8479 - val_loss: 165.6668 - val_regression_loss: 65.3077\n","Epoch 32/50\n","537/537 [==============================] - 0s 76us/step - loss: 1321.0027 - regression_loss: 541.7707 - val_loss: 165.2849 - val_regression_loss: 65.1732\n","Epoch 33/50\n","537/537 [==============================] - 0s 65us/step - loss: 1321.8462 - regression_loss: 543.9856 - val_loss: 163.4149 - val_regression_loss: 64.3412\n","Epoch 34/50\n","537/537 [==============================] - 0s 59us/step - loss: 1317.3746 - regression_loss: 541.6379 - val_loss: 161.5162 - val_regression_loss: 63.4946\n","Epoch 35/50\n","537/537 [==============================] - 0s 65us/step - loss: 1310.6846 - regression_loss: 538.6521 - val_loss: 160.0648 - val_regression_loss: 62.8246\n","Epoch 36/50\n","537/537 [==============================] - 0s 89us/step - loss: 1309.2419 - regression_loss: 538.8155 - val_loss: 158.7183 - val_regression_loss: 62.1772\n","Epoch 37/50\n","537/537 [==============================] - 0s 52us/step - loss: 1290.4097 - regression_loss: 528.6115 - val_loss: 157.8126 - val_regression_loss: 61.7198\n","Epoch 38/50\n","537/537 [==============================] - 0s 61us/step - loss: 1277.6235 - regression_loss: 522.9843 - val_loss: 157.3433 - val_regression_loss: 61.4444\n","Epoch 39/50\n","537/537 [==============================] - 0s 58us/step - loss: 1292.7359 - regression_loss: 529.2657 - val_loss: 157.5708 - val_regression_loss: 61.5012\n","Epoch 40/50\n","537/537 [==============================] - 0s 62us/step - loss: 1252.3994 - regression_loss: 511.4355 - val_loss: 157.9946 - val_regression_loss: 61.6592\n","Epoch 41/50\n","537/537 [==============================] - 0s 62us/step - loss: 1253.8049 - regression_loss: 512.9281 - val_loss: 157.9644 - val_regression_loss: 61.5848\n","Epoch 42/50\n","537/537 [==============================] - 0s 62us/step - loss: 1247.9728 - regression_loss: 511.4984 - val_loss: 157.3760 - val_regression_loss: 61.3102\n","Epoch 43/50\n","537/537 [==============================] - 0s 54us/step - loss: 1264.0525 - regression_loss: 518.0551 - val_loss: 156.4313 - val_regression_loss: 60.8813\n","Epoch 44/50\n","537/537 [==============================] - 0s 59us/step - loss: 1262.0648 - regression_loss: 516.4989 - val_loss: 155.5077 - val_regression_loss: 60.4582\n","Epoch 45/50\n","537/537 [==============================] - 0s 65us/step - loss: 1248.7982 - regression_loss: 511.4929 - val_loss: 155.2274 - val_regression_loss: 60.3165\n","Epoch 46/50\n","537/537 [==============================] - 0s 66us/step - loss: 1243.0515 - regression_loss: 507.5495 - val_loss: 155.0079 - val_regression_loss: 60.1858\n","Epoch 47/50\n","537/537 [==============================] - 0s 67us/step - loss: 1244.0147 - regression_loss: 510.2727 - val_loss: 155.2712 - val_regression_loss: 60.2561\n","Epoch 48/50\n","537/537 [==============================] - 0s 62us/step - loss: 1232.5309 - regression_loss: 502.9679 - val_loss: 155.4288 - val_regression_loss: 60.3206\n","Epoch 49/50\n","537/537 [==============================] - 0s 68us/step - loss: 1249.4131 - regression_loss: 511.7955 - val_loss: 155.8349 - val_regression_loss: 60.5148\n","Epoch 50/50\n","537/537 [==============================] - 0s 62us/step - loss: 1234.9385 - regression_loss: 507.1570 - val_loss: 156.1412 - val_regression_loss: 60.6674\n","***************************** elapsed_time is:  3.7747859954833984\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 9069.9832 - regression_loss: 4367.0874 - val_loss: 701.5286 - val_regression_loss: 328.3222\n","Epoch 2/50\n","537/537 [==============================] - 0s 57us/step - loss: 6168.0838 - regression_loss: 2916.8226 - val_loss: 478.0895 - val_regression_loss: 217.0338\n","Epoch 3/50\n","537/537 [==============================] - 0s 69us/step - loss: 4032.9845 - regression_loss: 1852.5513 - val_loss: 362.8971 - val_regression_loss: 160.6362\n","Epoch 4/50\n","537/537 [==============================] - 0s 65us/step - loss: 2869.6166 - regression_loss: 1279.0370 - val_loss: 237.6635 - val_regression_loss: 100.2003\n","Epoch 5/50\n","537/537 [==============================] - 0s 57us/step - loss: 1721.1637 - regression_loss: 724.6026 - val_loss: 215.5576 - val_regression_loss: 91.3485\n","Epoch 6/50\n","537/537 [==============================] - 0s 59us/step - loss: 1605.9266 - regression_loss: 684.0417 - val_loss: 295.5187 - val_regression_loss: 132.4725\n","Epoch 7/50\n","537/537 [==============================] - 0s 57us/step - loss: 2310.3279 - regression_loss: 1045.6035 - val_loss: 260.3228 - val_regression_loss: 114.6518\n","Epoch 8/50\n","537/537 [==============================] - 0s 63us/step - loss: 2001.8242 - regression_loss: 889.6943 - val_loss: 193.2069 - val_regression_loss: 80.3322\n","Epoch 9/50\n","537/537 [==============================] - 0s 72us/step - loss: 1469.8224 - regression_loss: 618.2208 - val_loss: 172.0478 - val_regression_loss: 68.9430\n","Epoch 10/50\n","537/537 [==============================] - 0s 63us/step - loss: 1322.7038 - regression_loss: 537.8309 - val_loss: 184.0678 - val_regression_loss: 74.3526\n","Epoch 11/50\n","537/537 [==============================] - 0s 56us/step - loss: 1457.8837 - regression_loss: 602.3644 - val_loss: 193.7108 - val_regression_loss: 78.9205\n","Epoch 12/50\n","537/537 [==============================] - 0s 59us/step - loss: 1539.9888 - regression_loss: 640.8634 - val_loss: 189.2184 - val_regression_loss: 76.7410\n","Epoch 13/50\n","537/537 [==============================] - 0s 59us/step - loss: 1471.9008 - regression_loss: 607.6273 - val_loss: 177.8587 - val_regression_loss: 71.3497\n","Epoch 14/50\n","537/537 [==============================] - 0s 59us/step - loss: 1390.5217 - regression_loss: 567.8989 - val_loss: 169.5565 - val_regression_loss: 67.5905\n","Epoch 15/50\n","537/537 [==============================] - 0s 60us/step - loss: 1307.8049 - regression_loss: 531.1946 - val_loss: 169.8931 - val_regression_loss: 68.1391\n","Epoch 16/50\n","537/537 [==============================] - 0s 64us/step - loss: 1266.3698 - regression_loss: 512.1954 - val_loss: 176.2186 - val_regression_loss: 71.5638\n","Epoch 17/50\n","537/537 [==============================] - 0s 61us/step - loss: 1322.0340 - regression_loss: 543.9331 - val_loss: 180.1019 - val_regression_loss: 73.5702\n","Epoch 18/50\n","537/537 [==============================] - 0s 63us/step - loss: 1347.5404 - regression_loss: 557.0004 - val_loss: 177.5499 - val_regression_loss: 72.1631\n","Epoch 19/50\n","537/537 [==============================] - 0s 59us/step - loss: 1289.2860 - regression_loss: 528.9265 - val_loss: 172.3236 - val_regression_loss: 69.2926\n","Epoch 20/50\n","537/537 [==============================] - 0s 64us/step - loss: 1237.2317 - regression_loss: 503.5685 - val_loss: 169.3372 - val_regression_loss: 67.5277\n","Epoch 21/50\n","537/537 [==============================] - 0s 61us/step - loss: 1216.2322 - regression_loss: 492.2699 - val_loss: 169.0081 - val_regression_loss: 67.1644\n","Epoch 22/50\n","537/537 [==============================] - 0s 60us/step - loss: 1239.2606 - regression_loss: 500.7407 - val_loss: 169.4743 - val_regression_loss: 67.3019\n","Epoch 23/50\n","537/537 [==============================] - 0s 76us/step - loss: 1241.3076 - regression_loss: 501.7381 - val_loss: 169.2477 - val_regression_loss: 67.1954\n","Epoch 24/50\n","537/537 [==============================] - 0s 73us/step - loss: 1242.1875 - regression_loss: 502.7828 - val_loss: 168.6199 - val_regression_loss: 66.9662\n","Epoch 25/50\n","537/537 [==============================] - 0s 68us/step - loss: 1218.3808 - regression_loss: 489.4444 - val_loss: 168.4934 - val_regression_loss: 67.0214\n","Epoch 26/50\n","537/537 [==============================] - 0s 68us/step - loss: 1223.8298 - regression_loss: 496.2760 - val_loss: 169.4265 - val_regression_loss: 67.6013\n","\n","Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 27/50\n","537/537 [==============================] - 0s 66us/step - loss: 1214.0746 - regression_loss: 490.8503 - val_loss: 169.9981 - val_regression_loss: 67.9240\n","Epoch 28/50\n","537/537 [==============================] - 0s 63us/step - loss: 1212.9172 - regression_loss: 491.2195 - val_loss: 170.4084 - val_regression_loss: 68.1424\n","Epoch 29/50\n","537/537 [==============================] - 0s 63us/step - loss: 1225.1096 - regression_loss: 495.5937 - val_loss: 170.5442 - val_regression_loss: 68.2011\n","Epoch 30/50\n","537/537 [==============================] - 0s 52us/step - loss: 1231.8820 - regression_loss: 499.3879 - val_loss: 170.5775 - val_regression_loss: 68.1909\n","Epoch 31/50\n","537/537 [==============================] - 0s 59us/step - loss: 1235.3826 - regression_loss: 500.9495 - val_loss: 170.5454 - val_regression_loss: 68.1391\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 1207.8788 - regression_loss: 489.6008 - val_loss: 170.4326 - val_regression_loss: 68.0499\n","Epoch 33/50\n","537/537 [==============================] - 0s 72us/step - loss: 1226.2806 - regression_loss: 499.4412 - val_loss: 170.2356 - val_regression_loss: 67.9231\n","Epoch 34/50\n","537/537 [==============================] - 0s 66us/step - loss: 1209.5589 - regression_loss: 489.5935 - val_loss: 170.0470 - val_regression_loss: 67.8140\n","Epoch 35/50\n","537/537 [==============================] - 0s 54us/step - loss: 1193.9471 - regression_loss: 480.5308 - val_loss: 169.7737 - val_regression_loss: 67.6746\n","Epoch 36/50\n","537/537 [==============================] - 0s 58us/step - loss: 1211.7669 - regression_loss: 488.4283 - val_loss: 169.6177 - val_regression_loss: 67.6068\n","Epoch 37/50\n","537/537 [==============================] - 0s 57us/step - loss: 1200.6060 - regression_loss: 487.8500 - val_loss: 169.4947 - val_regression_loss: 67.5681\n","Epoch 38/50\n","537/537 [==============================] - 0s 61us/step - loss: 1190.8087 - regression_loss: 480.8923 - val_loss: 169.4442 - val_regression_loss: 67.5676\n","Epoch 39/50\n","537/537 [==============================] - 0s 63us/step - loss: 1195.4276 - regression_loss: 485.0331 - val_loss: 169.4409 - val_regression_loss: 67.5882\n","Epoch 40/50\n","537/537 [==============================] - 0s 61us/step - loss: 1176.0971 - regression_loss: 473.2798 - val_loss: 169.4578 - val_regression_loss: 67.6127\n","Epoch 41/50\n","537/537 [==============================] - 0s 66us/step - loss: 1194.2980 - regression_loss: 483.5319 - val_loss: 169.6004 - val_regression_loss: 67.6892\n","Epoch 42/50\n","537/537 [==============================] - 0s 60us/step - loss: 1206.4235 - regression_loss: 489.0015 - val_loss: 169.7359 - val_regression_loss: 67.7563\n","Epoch 43/50\n","537/537 [==============================] - 0s 55us/step - loss: 1200.9473 - regression_loss: 487.9911 - val_loss: 169.9179 - val_regression_loss: 67.8439\n","Epoch 44/50\n","537/537 [==============================] - 0s 57us/step - loss: 1198.0994 - regression_loss: 484.3971 - val_loss: 170.1171 - val_regression_loss: 67.9382\n","Epoch 45/50\n","537/537 [==============================] - 0s 60us/step - loss: 1208.5281 - regression_loss: 489.6733 - val_loss: 170.1684 - val_regression_loss: 67.9552\n","\n","Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 46/50\n","537/537 [==============================] - 0s 58us/step - loss: 1191.3769 - regression_loss: 483.0439 - val_loss: 170.1851 - val_regression_loss: 67.9600\n","Epoch 47/50\n","537/537 [==============================] - 0s 74us/step - loss: 1201.2834 - regression_loss: 490.1171 - val_loss: 170.1445 - val_regression_loss: 67.9382\n","Epoch 48/50\n","537/537 [==============================] - 0s 62us/step - loss: 1186.5986 - regression_loss: 481.3152 - val_loss: 170.1043 - val_regression_loss: 67.9179\n","Epoch 49/50\n","537/537 [==============================] - 0s 61us/step - loss: 1183.3891 - regression_loss: 479.7165 - val_loss: 170.0763 - val_regression_loss: 67.9075\n","Epoch 50/50\n","537/537 [==============================] - 0s 62us/step - loss: 1193.7315 - regression_loss: 485.1949 - val_loss: 170.0189 - val_regression_loss: 67.8809\n","\n","Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","***************************** elapsed_time is:  4.235011339187622\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 460659.1636 - regression_loss: 228886.9873 - val_loss: 45341.9805 - val_regression_loss: 22519.2109\n","Epoch 2/50\n","537/537 [==============================] - 0s 61us/step - loss: 417196.0026 - regression_loss: 207234.9452 - val_loss: 40675.5977 - val_regression_loss: 20200.6230\n","Epoch 3/50\n","537/537 [==============================] - 0s 72us/step - loss: 385254.1826 - regression_loss: 191404.4267 - val_loss: 34132.4609 - val_regression_loss: 16949.9180\n","Epoch 4/50\n","537/537 [==============================] - 0s 61us/step - loss: 326278.9480 - regression_loss: 162088.0652 - val_loss: 25729.1953 - val_regression_loss: 12777.0723\n","Epoch 5/50\n","537/537 [==============================] - 0s 58us/step - loss: 250615.7866 - regression_loss: 124492.9122 - val_loss: 16455.0547 - val_regression_loss: 8176.2354\n","Epoch 6/50\n","537/537 [==============================] - 0s 60us/step - loss: 164797.3304 - regression_loss: 81888.4283 - val_loss: 8540.4775 - val_regression_loss: 4261.1489\n","Epoch 7/50\n","537/537 [==============================] - 0s 57us/step - loss: 90591.9012 - regression_loss: 45150.4998 - val_loss: 5534.0645 - val_regression_loss: 2801.2659\n","Epoch 8/50\n","537/537 [==============================] - 0s 53us/step - loss: 49275.1716 - regression_loss: 24834.2586 - val_loss: 9411.4170 - val_regression_loss: 4770.0742\n","Epoch 9/50\n","537/537 [==============================] - 0s 59us/step - loss: 66909.9046 - regression_loss: 33903.3375 - val_loss: 11373.3535 - val_regression_loss: 5745.5479\n","Epoch 10/50\n","537/537 [==============================] - 0s 55us/step - loss: 77101.7698 - regression_loss: 38937.6867 - val_loss: 8634.5703 - val_regression_loss: 4349.3779\n","Epoch 11/50\n","537/537 [==============================] - 0s 63us/step - loss: 59254.3089 - regression_loss: 29793.1823 - val_loss: 5044.5015 - val_regression_loss: 2525.0813\n","Epoch 12/50\n","537/537 [==============================] - 0s 59us/step - loss: 36620.7216 - regression_loss: 18220.5611 - val_loss: 3049.5637 - val_regression_loss: 1507.0791\n","Epoch 13/50\n","537/537 [==============================] - 0s 57us/step - loss: 27707.7355 - regression_loss: 13592.7771 - val_loss: 2637.6443 - val_regression_loss: 1290.2865\n","Epoch 14/50\n","537/537 [==============================] - 0s 61us/step - loss: 28772.3112 - regression_loss: 14042.7682 - val_loss: 2914.6519 - val_regression_loss: 1424.9526\n","Epoch 15/50\n","537/537 [==============================] - 0s 59us/step - loss: 35510.9481 - regression_loss: 17368.1981 - val_loss: 3138.4189 - val_regression_loss: 1537.8083\n","Epoch 16/50\n","537/537 [==============================] - 0s 57us/step - loss: 37644.6042 - regression_loss: 18449.9012 - val_loss: 3031.2634 - val_regression_loss: 1488.3987\n","Epoch 17/50\n","537/537 [==============================] - 0s 69us/step - loss: 36705.2543 - regression_loss: 18013.2624 - val_loss: 2670.3113 - val_regression_loss: 1314.4421\n","Epoch 18/50\n","537/537 [==============================] - 0s 73us/step - loss: 30377.1791 - regression_loss: 14909.2057 - val_loss: 2319.3062 - val_regression_loss: 1146.9150\n","\n","Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 19/50\n","537/537 [==============================] - 0s 59us/step - loss: 26454.5438 - regression_loss: 13013.8992 - val_loss: 2231.5364 - val_regression_loss: 1107.2874\n","Epoch 20/50\n","537/537 [==============================] - 0s 63us/step - loss: 24297.5804 - regression_loss: 11971.8482 - val_loss: 2240.1511 - val_regression_loss: 1115.7970\n","Epoch 21/50\n","537/537 [==============================] - 0s 60us/step - loss: 21552.3421 - regression_loss: 10635.2843 - val_loss: 2346.0525 - val_regression_loss: 1172.6105\n","Epoch 22/50\n","537/537 [==============================] - 0s 63us/step - loss: 21556.1787 - regression_loss: 10672.5855 - val_loss: 2516.8193 - val_regression_loss: 1261.3029\n","Epoch 23/50\n","537/537 [==============================] - 0s 60us/step - loss: 21381.0801 - regression_loss: 10613.9436 - val_loss: 2692.8333 - val_regression_loss: 1351.8536\n","Epoch 24/50\n","537/537 [==============================] - 0s 54us/step - loss: 21337.9899 - regression_loss: 10611.9586 - val_loss: 2812.6929 - val_regression_loss: 1413.4703\n","Epoch 25/50\n","537/537 [==============================] - 0s 52us/step - loss: 21873.0992 - regression_loss: 10895.8691 - val_loss: 2844.3616 - val_regression_loss: 1430.0774\n","Epoch 26/50\n","537/537 [==============================] - 0s 59us/step - loss: 21548.6487 - regression_loss: 10741.2075 - val_loss: 2784.0642 - val_regression_loss: 1399.8240\n","Epoch 27/50\n","537/537 [==============================] - 0s 56us/step - loss: 20833.4495 - regression_loss: 10377.6704 - val_loss: 2665.9324 - val_regression_loss: 1339.9562\n","Epoch 28/50\n","537/537 [==============================] - 0s 60us/step - loss: 20454.0411 - regression_loss: 10178.3168 - val_loss: 2529.1792 - val_regression_loss: 1270.2798\n","Epoch 29/50\n","537/537 [==============================] - 0s 58us/step - loss: 19271.7984 - regression_loss: 9578.1790 - val_loss: 2398.2737 - val_regression_loss: 1203.2020\n","Epoch 30/50\n","537/537 [==============================] - 0s 56us/step - loss: 19614.6838 - regression_loss: 9730.7927 - val_loss: 2289.5010 - val_regression_loss: 1147.1548\n","Epoch 31/50\n","537/537 [==============================] - 0s 66us/step - loss: 18400.4156 - regression_loss: 9114.3437 - val_loss: 2201.2871 - val_regression_loss: 1101.4673\n","Epoch 32/50\n","537/537 [==============================] - 0s 66us/step - loss: 19104.9896 - regression_loss: 9447.6896 - val_loss: 2132.2131 - val_regression_loss: 1065.6533\n","Epoch 33/50\n","537/537 [==============================] - 0s 59us/step - loss: 18518.6401 - regression_loss: 9146.2067 - val_loss: 2074.5696 - val_regression_loss: 1035.8834\n","Epoch 34/50\n","537/537 [==============================] - 0s 63us/step - loss: 18418.5164 - regression_loss: 9090.7209 - val_loss: 2028.8644 - val_regression_loss: 1012.4617\n","Epoch 35/50\n","537/537 [==============================] - 0s 63us/step - loss: 18031.9139 - regression_loss: 8895.3425 - val_loss: 1993.4103 - val_regression_loss: 994.4869\n","Epoch 36/50\n","537/537 [==============================] - 0s 62us/step - loss: 17884.1456 - regression_loss: 8809.1289 - val_loss: 1974.2694 - val_regression_loss: 985.0518\n","Epoch 37/50\n","537/537 [==============================] - 0s 59us/step - loss: 16760.3794 - regression_loss: 8257.3256 - val_loss: 1959.5184 - val_regression_loss: 977.9523\n","Epoch 38/50\n","537/537 [==============================] - 0s 67us/step - loss: 17150.8940 - regression_loss: 8453.7869 - val_loss: 1953.4484 - val_regression_loss: 975.3778\n","Epoch 39/50\n","537/537 [==============================] - 0s 64us/step - loss: 17029.4860 - regression_loss: 8395.9642 - val_loss: 1945.1234 - val_regression_loss: 971.7207\n","Epoch 40/50\n","537/537 [==============================] - 0s 62us/step - loss: 17047.2363 - regression_loss: 8414.9810 - val_loss: 1929.8701 - val_regression_loss: 964.5479\n","Epoch 41/50\n","537/537 [==============================] - 0s 62us/step - loss: 16570.6848 - regression_loss: 8177.6806 - val_loss: 1902.3951 - val_regression_loss: 951.1486\n","Epoch 42/50\n","537/537 [==============================] - 0s 62us/step - loss: 16360.5543 - regression_loss: 8076.7026 - val_loss: 1867.0002 - val_regression_loss: 933.6528\n","Epoch 43/50\n","537/537 [==============================] - 0s 62us/step - loss: 15757.4506 - regression_loss: 7774.7379 - val_loss: 1826.1132 - val_regression_loss: 913.2531\n","Epoch 44/50\n","537/537 [==============================] - 0s 69us/step - loss: 15541.9150 - regression_loss: 7666.5353 - val_loss: 1781.1378 - val_regression_loss: 890.6724\n","Epoch 45/50\n","537/537 [==============================] - 0s 75us/step - loss: 15215.5150 - regression_loss: 7506.8511 - val_loss: 1737.0453 - val_regression_loss: 868.4205\n","Epoch 46/50\n","537/537 [==============================] - 0s 67us/step - loss: 15032.6778 - regression_loss: 7416.6608 - val_loss: 1693.8384 - val_regression_loss: 846.5447\n","Epoch 47/50\n","537/537 [==============================] - 0s 64us/step - loss: 15045.5892 - regression_loss: 7418.0041 - val_loss: 1651.5125 - val_regression_loss: 825.1169\n","Epoch 48/50\n","537/537 [==============================] - 0s 57us/step - loss: 14507.1396 - regression_loss: 7140.6429 - val_loss: 1613.6385 - val_regression_loss: 805.9731\n","Epoch 49/50\n","537/537 [==============================] - 0s 61us/step - loss: 13644.3466 - regression_loss: 6714.0331 - val_loss: 1571.9655 - val_regression_loss: 784.8682\n","Epoch 50/50\n","537/537 [==============================] - 0s 59us/step - loss: 13674.1431 - regression_loss: 6727.6372 - val_loss: 1533.2380 - val_regression_loss: 765.2822\n","***************************** elapsed_time is:  3.8060264587402344\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 35461.7295 - regression_loss: 17575.4117 - val_loss: 3774.2471 - val_regression_loss: 1866.3253\n","Epoch 2/50\n","537/537 [==============================] - 0s 56us/step - loss: 29027.1155 - regression_loss: 14360.1704 - val_loss: 2863.6770 - val_regression_loss: 1411.0757\n","Epoch 3/50\n","537/537 [==============================] - 0s 66us/step - loss: 21921.9455 - regression_loss: 10806.0595 - val_loss: 1798.1869 - val_regression_loss: 878.0431\n","Epoch 4/50\n","537/537 [==============================] - 0s 60us/step - loss: 12906.5077 - regression_loss: 6296.7324 - val_loss: 905.3652 - val_regression_loss: 431.1090\n","Epoch 5/50\n","537/537 [==============================] - 0s 54us/step - loss: 6549.2878 - regression_loss: 3113.2118 - val_loss: 732.4001 - val_regression_loss: 344.6978\n","Epoch 6/50\n","537/537 [==============================] - 0s 60us/step - loss: 6088.8966 - regression_loss: 2883.5248 - val_loss: 647.5652 - val_regression_loss: 304.1532\n","Epoch 7/50\n","537/537 [==============================] - 0s 57us/step - loss: 5944.4666 - regression_loss: 2826.6284 - val_loss: 414.9774 - val_regression_loss: 190.0387\n","Epoch 8/50\n","537/537 [==============================] - 0s 57us/step - loss: 4236.6320 - regression_loss: 1991.1755 - val_loss: 309.9727 - val_regression_loss: 138.9190\n","Epoch 9/50\n","537/537 [==============================] - 0s 61us/step - loss: 3127.7821 - regression_loss: 1447.6212 - val_loss: 396.3575 - val_regression_loss: 182.6218\n","Epoch 10/50\n","537/537 [==============================] - 0s 78us/step - loss: 3285.4785 - regression_loss: 1530.1782 - val_loss: 489.1696 - val_regression_loss: 228.8910\n","Epoch 11/50\n","537/537 [==============================] - 0s 64us/step - loss: 3750.5624 - regression_loss: 1763.0982 - val_loss: 465.6089 - val_regression_loss: 216.6057\n","Epoch 12/50\n","537/537 [==============================] - 0s 73us/step - loss: 3450.7969 - regression_loss: 1608.5588 - val_loss: 364.8646 - val_regression_loss: 165.5766\n","Epoch 13/50\n","537/537 [==============================] - 0s 64us/step - loss: 2660.6486 - regression_loss: 1207.0959 - val_loss: 263.8898 - val_regression_loss: 114.4279\n","Epoch 14/50\n","537/537 [==============================] - 0s 61us/step - loss: 2040.2242 - regression_loss: 890.1463 - val_loss: 219.5747 - val_regression_loss: 91.7450\n","Epoch 15/50\n","537/537 [==============================] - 0s 61us/step - loss: 1885.3791 - regression_loss: 807.7834 - val_loss: 227.6846 - val_regression_loss: 95.5311\n","Epoch 16/50\n","537/537 [==============================] - 0s 65us/step - loss: 2154.5527 - regression_loss: 945.4882 - val_loss: 237.1034 - val_regression_loss: 100.2578\n","Epoch 17/50\n","537/537 [==============================] - 0s 78us/step - loss: 2283.7118 - regression_loss: 1008.3531 - val_loss: 219.0350 - val_regression_loss: 91.4519\n","Epoch 18/50\n","537/537 [==============================] - 0s 60us/step - loss: 2144.1263 - regression_loss: 939.9715 - val_loss: 193.5413 - val_regression_loss: 79.0185\n","Epoch 19/50\n","537/537 [==============================] - 0s 67us/step - loss: 1817.9764 - regression_loss: 778.6867 - val_loss: 188.4711 - val_regression_loss: 76.7862\n","Epoch 20/50\n","537/537 [==============================] - 0s 66us/step - loss: 1705.1884 - regression_loss: 728.3482 - val_loss: 201.8438 - val_regression_loss: 83.7108\n","Epoch 21/50\n","537/537 [==============================] - 0s 58us/step - loss: 1674.8901 - regression_loss: 714.3393 - val_loss: 214.4060 - val_regression_loss: 90.1526\n","Epoch 22/50\n","537/537 [==============================] - 0s 63us/step - loss: 1801.2578 - regression_loss: 778.9031 - val_loss: 210.8985 - val_regression_loss: 88.4828\n","Epoch 23/50\n","537/537 [==============================] - 0s 65us/step - loss: 1760.9223 - regression_loss: 757.1880 - val_loss: 196.1636 - val_regression_loss: 81.1298\n","Epoch 24/50\n","537/537 [==============================] - 0s 61us/step - loss: 1680.7897 - regression_loss: 720.4864 - val_loss: 180.8716 - val_regression_loss: 73.4269\n","Epoch 25/50\n","537/537 [==============================] - 0s 63us/step - loss: 1651.2153 - regression_loss: 705.0670 - val_loss: 172.3542 - val_regression_loss: 69.0400\n","Epoch 26/50\n","537/537 [==============================] - 0s 63us/step - loss: 1597.7933 - regression_loss: 676.7670 - val_loss: 168.4609 - val_regression_loss: 66.9207\n","Epoch 27/50\n","537/537 [==============================] - 0s 63us/step - loss: 1567.7753 - regression_loss: 661.2664 - val_loss: 166.7678 - val_regression_loss: 65.9016\n","Epoch 28/50\n","537/537 [==============================] - 0s 62us/step - loss: 1583.4326 - regression_loss: 666.4592 - val_loss: 167.6257 - val_regression_loss: 66.2108\n","Epoch 29/50\n","537/537 [==============================] - 0s 59us/step - loss: 1535.3238 - regression_loss: 641.5518 - val_loss: 170.6899 - val_regression_loss: 67.7070\n","Epoch 30/50\n","537/537 [==============================] - 0s 64us/step - loss: 1563.2165 - regression_loss: 657.7508 - val_loss: 173.0170 - val_regression_loss: 68.9142\n","Epoch 31/50\n","537/537 [==============================] - 0s 67us/step - loss: 1538.8917 - regression_loss: 644.0376 - val_loss: 172.3350 - val_regression_loss: 68.6714\n","Epoch 32/50\n","537/537 [==============================] - 0s 63us/step - loss: 1484.4066 - regression_loss: 622.0136 - val_loss: 169.3758 - val_regression_loss: 67.3075\n","Epoch 33/50\n","537/537 [==============================] - 0s 60us/step - loss: 1464.8404 - regression_loss: 613.3932 - val_loss: 166.8621 - val_regression_loss: 66.1684\n","Epoch 34/50\n","537/537 [==============================] - 0s 65us/step - loss: 1495.8074 - regression_loss: 627.4414 - val_loss: 165.4969 - val_regression_loss: 65.5431\n","Epoch 35/50\n","537/537 [==============================] - 0s 66us/step - loss: 1478.3822 - regression_loss: 618.4302 - val_loss: 164.7510 - val_regression_loss: 65.1719\n","Epoch 36/50\n","537/537 [==============================] - 0s 65us/step - loss: 1479.6466 - regression_loss: 618.4779 - val_loss: 164.4116 - val_regression_loss: 64.9552\n","Epoch 37/50\n","537/537 [==============================] - 0s 99us/step - loss: 1459.9175 - regression_loss: 610.7371 - val_loss: 165.1764 - val_regression_loss: 65.2670\n","Epoch 38/50\n","537/537 [==============================] - 0s 58us/step - loss: 1456.9687 - regression_loss: 608.1648 - val_loss: 165.8569 - val_regression_loss: 65.5431\n","Epoch 39/50\n","537/537 [==============================] - 0s 64us/step - loss: 1461.0481 - regression_loss: 610.1700 - val_loss: 165.0856 - val_regression_loss: 65.1102\n","Epoch 40/50\n","537/537 [==============================] - 0s 70us/step - loss: 1415.5138 - regression_loss: 586.9210 - val_loss: 163.4125 - val_regression_loss: 64.2608\n","Epoch 41/50\n","537/537 [==============================] - 0s 62us/step - loss: 1416.1139 - regression_loss: 587.1555 - val_loss: 161.2878 - val_regression_loss: 63.2103\n","Epoch 42/50\n","537/537 [==============================] - 0s 60us/step - loss: 1391.6410 - regression_loss: 575.0195 - val_loss: 160.0231 - val_regression_loss: 62.6101\n","Epoch 43/50\n","537/537 [==============================] - 0s 59us/step - loss: 1375.7896 - regression_loss: 567.2888 - val_loss: 159.5944 - val_regression_loss: 62.4391\n","Epoch 44/50\n","537/537 [==============================] - 0s 68us/step - loss: 1396.6942 - regression_loss: 579.8690 - val_loss: 160.1019 - val_regression_loss: 62.7233\n","Epoch 45/50\n","537/537 [==============================] - 0s 70us/step - loss: 1375.3751 - regression_loss: 570.8225 - val_loss: 161.0721 - val_regression_loss: 63.2173\n","Epoch 46/50\n","537/537 [==============================] - 0s 70us/step - loss: 1352.2096 - regression_loss: 558.2487 - val_loss: 161.1192 - val_regression_loss: 63.2438\n","Epoch 47/50\n","537/537 [==============================] - 0s 62us/step - loss: 1345.8717 - regression_loss: 556.7973 - val_loss: 160.8650 - val_regression_loss: 63.0969\n","Epoch 48/50\n","537/537 [==============================] - 0s 62us/step - loss: 1336.5251 - regression_loss: 549.7925 - val_loss: 160.2210 - val_regression_loss: 62.7500\n","Epoch 49/50\n","537/537 [==============================] - 0s 59us/step - loss: 1329.1412 - regression_loss: 545.1400 - val_loss: 160.1310 - val_regression_loss: 62.6904\n","Epoch 50/50\n","537/537 [==============================] - 0s 58us/step - loss: 1350.0591 - regression_loss: 555.9420 - val_loss: 160.3493 - val_regression_loss: 62.7990\n","***************************** elapsed_time is:  3.8711605072021484\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 16579.5124 - regression_loss: 8111.2420 - val_loss: 1433.6616 - val_regression_loss: 694.1625\n","Epoch 2/50\n","537/537 [==============================] - 0s 59us/step - loss: 11475.2112 - regression_loss: 5572.9533 - val_loss: 876.4031 - val_regression_loss: 417.6949\n","Epoch 3/50\n","537/537 [==============================] - 0s 57us/step - loss: 7173.7248 - regression_loss: 3439.9524 - val_loss: 602.4044 - val_regression_loss: 283.0814\n","Epoch 4/50\n","537/537 [==============================] - 0s 56us/step - loss: 4949.0815 - regression_loss: 2345.7436 - val_loss: 439.9695 - val_regression_loss: 202.4598\n","Epoch 5/50\n","537/537 [==============================] - 0s 56us/step - loss: 3523.4161 - regression_loss: 1636.8835 - val_loss: 253.7509 - val_regression_loss: 108.1949\n","Epoch 6/50\n","537/537 [==============================] - 0s 51us/step - loss: 1938.6894 - regression_loss: 834.7084 - val_loss: 287.8381 - val_regression_loss: 123.4636\n","Epoch 7/50\n","537/537 [==============================] - 0s 52us/step - loss: 2233.3985 - regression_loss: 965.0601 - val_loss: 381.1647 - val_regression_loss: 169.4818\n","Epoch 8/50\n","537/537 [==============================] - 0s 63us/step - loss: 2927.1876 - regression_loss: 1308.6373 - val_loss: 316.8327 - val_regression_loss: 138.0944\n","Epoch 9/50\n","537/537 [==============================] - 0s 74us/step - loss: 2449.9572 - regression_loss: 1076.3036 - val_loss: 207.5210 - val_regression_loss: 84.8216\n","Epoch 10/50\n","537/537 [==============================] - 0s 54us/step - loss: 1672.1650 - regression_loss: 698.9780 - val_loss: 161.1415 - val_regression_loss: 62.9942\n","Epoch 11/50\n","537/537 [==============================] - 0s 51us/step - loss: 1397.1486 - regression_loss: 573.4589 - val_loss: 174.2756 - val_regression_loss: 70.5579\n","Epoch 12/50\n","537/537 [==============================] - 0s 62us/step - loss: 1632.9826 - regression_loss: 697.7047 - val_loss: 187.6924 - val_regression_loss: 77.7629\n","Epoch 13/50\n","537/537 [==============================] - 0s 54us/step - loss: 1782.6112 - regression_loss: 778.8371 - val_loss: 178.6743 - val_regression_loss: 73.3108\n","Epoch 14/50\n","537/537 [==============================] - 0s 63us/step - loss: 1705.0252 - regression_loss: 739.4497 - val_loss: 166.0060 - val_regression_loss: 66.7374\n","Epoch 15/50\n","537/537 [==============================] - 0s 58us/step - loss: 1512.3486 - regression_loss: 642.1541 - val_loss: 160.2184 - val_regression_loss: 63.4165\n","Epoch 16/50\n","537/537 [==============================] - 0s 71us/step - loss: 1457.6234 - regression_loss: 609.1357 - val_loss: 161.0892 - val_regression_loss: 63.3013\n","\n","Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 17/50\n","537/537 [==============================] - 0s 59us/step - loss: 1391.6736 - regression_loss: 574.7921 - val_loss: 163.5434 - val_regression_loss: 64.2598\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 1382.1704 - regression_loss: 566.8697 - val_loss: 165.6143 - val_regression_loss: 65.0938\n","Epoch 19/50\n","537/537 [==============================] - 0s 62us/step - loss: 1392.6691 - regression_loss: 567.5018 - val_loss: 165.7000 - val_regression_loss: 65.0508\n","Epoch 20/50\n","537/537 [==============================] - 0s 59us/step - loss: 1398.0571 - regression_loss: 572.6972 - val_loss: 163.1072 - val_regression_loss: 63.7918\n","Epoch 21/50\n","537/537 [==============================] - 0s 57us/step - loss: 1404.7622 - regression_loss: 573.5872 - val_loss: 158.2015 - val_regression_loss: 61.4824\n","Epoch 22/50\n","537/537 [==============================] - 0s 54us/step - loss: 1381.0153 - regression_loss: 564.4601 - val_loss: 152.9328 - val_regression_loss: 59.0435\n","Epoch 23/50\n","537/537 [==============================] - 0s 50us/step - loss: 1334.3590 - regression_loss: 543.3420 - val_loss: 148.7185 - val_regression_loss: 57.1383\n","Epoch 24/50\n","537/537 [==============================] - 0s 62us/step - loss: 1346.5395 - regression_loss: 553.2562 - val_loss: 145.9532 - val_regression_loss: 55.9168\n","Epoch 25/50\n","537/537 [==============================] - 0s 54us/step - loss: 1342.1311 - regression_loss: 550.6931 - val_loss: 144.1347 - val_regression_loss: 55.1267\n","Epoch 26/50\n","537/537 [==============================] - 0s 61us/step - loss: 1344.4403 - regression_loss: 553.3065 - val_loss: 142.7088 - val_regression_loss: 54.4866\n","Epoch 27/50\n","537/537 [==============================] - 0s 59us/step - loss: 1358.1313 - regression_loss: 560.1409 - val_loss: 141.7783 - val_regression_loss: 54.0438\n","Epoch 28/50\n","537/537 [==============================] - 0s 56us/step - loss: 1323.9925 - regression_loss: 544.8986 - val_loss: 141.3762 - val_regression_loss: 53.8232\n","Epoch 29/50\n","537/537 [==============================] - 0s 55us/step - loss: 1346.5397 - regression_loss: 555.6841 - val_loss: 141.2161 - val_regression_loss: 53.6988\n","Epoch 30/50\n","537/537 [==============================] - 0s 73us/step - loss: 1275.1953 - regression_loss: 517.9412 - val_loss: 141.1445 - val_regression_loss: 53.6088\n","Epoch 31/50\n","537/537 [==============================] - 0s 70us/step - loss: 1330.3699 - regression_loss: 544.0449 - val_loss: 141.0948 - val_regression_loss: 53.5227\n","Epoch 32/50\n","537/537 [==============================] - 0s 62us/step - loss: 1305.5708 - regression_loss: 534.5005 - val_loss: 141.0595 - val_regression_loss: 53.4549\n","Epoch 33/50\n","537/537 [==============================] - 0s 55us/step - loss: 1291.6603 - regression_loss: 527.6950 - val_loss: 140.9779 - val_regression_loss: 53.3797\n","Epoch 34/50\n","537/537 [==============================] - 0s 61us/step - loss: 1313.8450 - regression_loss: 536.8660 - val_loss: 140.5899 - val_regression_loss: 53.1757\n","Epoch 35/50\n","537/537 [==============================] - 0s 62us/step - loss: 1308.5838 - regression_loss: 532.7068 - val_loss: 139.8581 - val_regression_loss: 52.8221\n","\n","Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 36/50\n","537/537 [==============================] - 0s 64us/step - loss: 1310.3041 - regression_loss: 535.3456 - val_loss: 139.3973 - val_regression_loss: 52.6075\n","Epoch 37/50\n","537/537 [==============================] - 0s 73us/step - loss: 1290.8750 - regression_loss: 525.2623 - val_loss: 138.8964 - val_regression_loss: 52.3815\n","Epoch 38/50\n","537/537 [==============================] - 0s 59us/step - loss: 1312.2124 - regression_loss: 536.6871 - val_loss: 138.3699 - val_regression_loss: 52.1465\n","Epoch 39/50\n","537/537 [==============================] - 0s 70us/step - loss: 1284.1746 - regression_loss: 521.9654 - val_loss: 137.8840 - val_regression_loss: 51.9336\n","Epoch 40/50\n","537/537 [==============================] - 0s 66us/step - loss: 1302.8182 - regression_loss: 531.8508 - val_loss: 137.4024 - val_regression_loss: 51.7244\n","\n","Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","***************************** elapsed_time is:  3.4772047996520996\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 20871.2916 - regression_loss: 10230.8806 - val_loss: 2097.4087 - val_regression_loss: 1022.0437\n","Epoch 2/50\n","537/537 [==============================] - 0s 60us/step - loss: 15515.1215 - regression_loss: 7564.0984 - val_loss: 1365.5736 - val_regression_loss: 657.3973\n","Epoch 3/50\n","537/537 [==============================] - 0s 63us/step - loss: 10337.4826 - regression_loss: 4985.5675 - val_loss: 720.4670 - val_regression_loss: 336.4447\n","Epoch 4/50\n","537/537 [==============================] - 0s 57us/step - loss: 5802.8309 - regression_loss: 2730.1697 - val_loss: 507.9182 - val_regression_loss: 232.0167\n","Epoch 5/50\n","537/537 [==============================] - 0s 59us/step - loss: 4963.5374 - regression_loss: 2325.3108 - val_loss: 366.9458 - val_regression_loss: 161.8801\n","Epoch 6/50\n","537/537 [==============================] - 0s 58us/step - loss: 3863.7980 - regression_loss: 1777.3034 - val_loss: 239.1888 - val_regression_loss: 97.3470\n","Epoch 7/50\n","537/537 [==============================] - 0s 63us/step - loss: 2515.8856 - regression_loss: 1096.9372 - val_loss: 294.7568 - val_regression_loss: 124.3721\n","Epoch 8/50\n","537/537 [==============================] - 0s 63us/step - loss: 2484.1105 - regression_loss: 1073.8889 - val_loss: 420.9923 - val_regression_loss: 187.4736\n","Epoch 9/50\n","537/537 [==============================] - 0s 64us/step - loss: 3059.2620 - regression_loss: 1360.3577 - val_loss: 437.8585 - val_regression_loss: 196.8062\n","Epoch 10/50\n","537/537 [==============================] - 0s 59us/step - loss: 2980.3341 - regression_loss: 1327.9393 - val_loss: 350.5565 - val_regression_loss: 154.5528\n","Epoch 11/50\n","537/537 [==============================] - 0s 63us/step - loss: 2348.6060 - regression_loss: 1023.3154 - val_loss: 243.4875 - val_regression_loss: 102.5231\n","Epoch 12/50\n","537/537 [==============================] - 0s 65us/step - loss: 1835.3656 - regression_loss: 776.9349 - val_loss: 182.9495 - val_regression_loss: 73.5929\n","Epoch 13/50\n","537/537 [==============================] - 0s 61us/step - loss: 1635.4745 - regression_loss: 687.9746 - val_loss: 178.3214 - val_regression_loss: 72.2401\n","Epoch 14/50\n","537/537 [==============================] - 0s 68us/step - loss: 1843.3142 - regression_loss: 799.9667 - val_loss: 186.6828 - val_regression_loss: 76.9006\n","Epoch 15/50\n","537/537 [==============================] - 0s 64us/step - loss: 1966.3923 - regression_loss: 863.8031 - val_loss: 180.2767 - val_regression_loss: 73.7570\n","Epoch 16/50\n","537/537 [==============================] - 0s 61us/step - loss: 1824.1470 - regression_loss: 791.8246 - val_loss: 174.5385 - val_regression_loss: 70.6726\n","Epoch 17/50\n","537/537 [==============================] - 0s 60us/step - loss: 1601.1935 - regression_loss: 681.7139 - val_loss: 184.0869 - val_regression_loss: 75.1049\n","Epoch 18/50\n","537/537 [==============================] - 0s 71us/step - loss: 1530.5544 - regression_loss: 641.3080 - val_loss: 199.3019 - val_regression_loss: 82.3532\n","Epoch 19/50\n","537/537 [==============================] - 0s 66us/step - loss: 1562.5833 - regression_loss: 654.5640 - val_loss: 205.3663 - val_regression_loss: 85.0706\n","Epoch 20/50\n","537/537 [==============================] - 0s 63us/step - loss: 1580.2119 - regression_loss: 659.8172 - val_loss: 200.0392 - val_regression_loss: 82.1957\n","Epoch 21/50\n","537/537 [==============================] - 0s 61us/step - loss: 1593.4243 - regression_loss: 663.2806 - val_loss: 187.3337 - val_regression_loss: 75.8168\n","Epoch 22/50\n","537/537 [==============================] - 0s 63us/step - loss: 1527.3300 - regression_loss: 632.1543 - val_loss: 173.9262 - val_regression_loss: 69.2886\n","Epoch 23/50\n","537/537 [==============================] - 0s 58us/step - loss: 1466.7632 - regression_loss: 602.6226 - val_loss: 163.7654 - val_regression_loss: 64.5229\n","Epoch 24/50\n","537/537 [==============================] - 0s 63us/step - loss: 1453.9839 - regression_loss: 599.9957 - val_loss: 159.7552 - val_regression_loss: 62.8252\n","Epoch 25/50\n","537/537 [==============================] - 0s 61us/step - loss: 1426.1587 - regression_loss: 587.7588 - val_loss: 161.7094 - val_regression_loss: 64.0146\n","Epoch 26/50\n","537/537 [==============================] - 0s 56us/step - loss: 1425.0670 - regression_loss: 591.4844 - val_loss: 165.4739 - val_regression_loss: 65.9835\n","Epoch 27/50\n","537/537 [==============================] - 0s 62us/step - loss: 1407.9449 - regression_loss: 582.6859 - val_loss: 166.7565 - val_regression_loss: 66.6087\n","Epoch 28/50\n","537/537 [==============================] - 0s 71us/step - loss: 1416.0640 - regression_loss: 586.1079 - val_loss: 164.6369 - val_regression_loss: 65.4570\n","Epoch 29/50\n","537/537 [==============================] - 0s 62us/step - loss: 1390.4445 - regression_loss: 573.5155 - val_loss: 162.4404 - val_regression_loss: 64.2284\n","Epoch 30/50\n","537/537 [==============================] - 0s 59us/step - loss: 1371.1732 - regression_loss: 564.2610 - val_loss: 162.2655 - val_regression_loss: 64.0138\n","Epoch 31/50\n","537/537 [==============================] - 0s 61us/step - loss: 1358.8062 - regression_loss: 556.4905 - val_loss: 163.6407 - val_regression_loss: 64.6141\n","Epoch 32/50\n","537/537 [==============================] - 0s 70us/step - loss: 1343.9905 - regression_loss: 546.8188 - val_loss: 164.8936 - val_regression_loss: 65.2260\n","Epoch 33/50\n","537/537 [==============================] - 0s 67us/step - loss: 1353.2607 - regression_loss: 553.4676 - val_loss: 164.9117 - val_regression_loss: 65.2821\n","Epoch 34/50\n","537/537 [==============================] - 0s 63us/step - loss: 1329.6005 - regression_loss: 542.0195 - val_loss: 164.1144 - val_regression_loss: 64.9685\n","Epoch 35/50\n","537/537 [==============================] - 0s 62us/step - loss: 1345.2610 - regression_loss: 553.3329 - val_loss: 161.6930 - val_regression_loss: 63.8551\n","Epoch 36/50\n","537/537 [==============================] - 0s 59us/step - loss: 1323.2265 - regression_loss: 539.1582 - val_loss: 159.3595 - val_regression_loss: 62.7689\n","Epoch 37/50\n","537/537 [==============================] - 0s 63us/step - loss: 1304.0405 - regression_loss: 531.9417 - val_loss: 158.6218 - val_regression_loss: 62.4369\n","Epoch 38/50\n","537/537 [==============================] - 0s 62us/step - loss: 1311.3649 - regression_loss: 538.6580 - val_loss: 159.1486 - val_regression_loss: 62.6975\n","Epoch 39/50\n","537/537 [==============================] - 0s 62us/step - loss: 1333.6684 - regression_loss: 547.4373 - val_loss: 159.8046 - val_regression_loss: 62.9944\n","Epoch 40/50\n","537/537 [==============================] - 0s 61us/step - loss: 1301.3255 - regression_loss: 529.8376 - val_loss: 159.8183 - val_regression_loss: 62.9683\n","Epoch 41/50\n","537/537 [==============================] - 0s 66us/step - loss: 1299.5465 - regression_loss: 531.3052 - val_loss: 159.9817 - val_regression_loss: 63.0256\n","Epoch 42/50\n","537/537 [==============================] - 0s 59us/step - loss: 1276.2838 - regression_loss: 519.0566 - val_loss: 159.4471 - val_regression_loss: 62.7654\n","Epoch 43/50\n","537/537 [==============================] - 0s 64us/step - loss: 1290.8235 - regression_loss: 525.7939 - val_loss: 157.6544 - val_regression_loss: 61.9072\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 1294.3076 - regression_loss: 526.4924 - val_loss: 156.6088 - val_regression_loss: 61.4188\n","Epoch 45/50\n","537/537 [==============================] - 0s 63us/step - loss: 1295.6659 - regression_loss: 530.3508 - val_loss: 156.5519 - val_regression_loss: 61.4182\n","Epoch 46/50\n","537/537 [==============================] - 0s 61us/step - loss: 1258.6631 - regression_loss: 513.0325 - val_loss: 156.2727 - val_regression_loss: 61.2975\n","Epoch 47/50\n","537/537 [==============================] - 0s 66us/step - loss: 1278.3049 - regression_loss: 524.3699 - val_loss: 156.6924 - val_regression_loss: 61.5011\n","Epoch 48/50\n","537/537 [==============================] - 0s 61us/step - loss: 1262.6446 - regression_loss: 515.1554 - val_loss: 155.0820 - val_regression_loss: 60.6955\n","Epoch 49/50\n","537/537 [==============================] - 0s 62us/step - loss: 1289.5134 - regression_loss: 528.7333 - val_loss: 153.6511 - val_regression_loss: 59.9606\n","Epoch 50/50\n","537/537 [==============================] - 0s 62us/step - loss: 1257.0421 - regression_loss: 514.1413 - val_loss: 152.1365 - val_regression_loss: 59.1904\n","***************************** elapsed_time is:  4.19515323638916\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 12239.3134 - regression_loss: 5995.1923 - val_loss: 1150.5776 - val_regression_loss: 550.9344\n","Epoch 2/50\n","537/537 [==============================] - 0s 70us/step - loss: 9271.4604 - regression_loss: 4467.4355 - val_loss: 826.0116 - val_regression_loss: 382.7292\n","Epoch 3/50\n","537/537 [==============================] - 0s 56us/step - loss: 6296.9295 - regression_loss: 2929.8781 - val_loss: 651.8871 - val_regression_loss: 289.0783\n","Epoch 4/50\n","537/537 [==============================] - 0s 57us/step - loss: 4281.0949 - regression_loss: 1873.6337 - val_loss: 476.0570 - val_regression_loss: 202.8154\n","Epoch 5/50\n","537/537 [==============================] - 0s 60us/step - loss: 2994.1609 - regression_loss: 1245.6502 - val_loss: 251.2406 - val_regression_loss: 99.3015\n","Epoch 6/50\n","537/537 [==============================] - 0s 63us/step - loss: 1657.2385 - regression_loss: 650.9262 - val_loss: 241.3563 - val_regression_loss: 105.3130\n","Epoch 7/50\n","537/537 [==============================] - 0s 57us/step - loss: 2165.8777 - regression_loss: 992.3636 - val_loss: 278.0438 - val_regression_loss: 127.3601\n","Epoch 8/50\n","537/537 [==============================] - 0s 53us/step - loss: 2676.7391 - regression_loss: 1278.6674 - val_loss: 207.9503 - val_regression_loss: 89.9234\n","Epoch 9/50\n","537/537 [==============================] - 0s 62us/step - loss: 1884.3039 - regression_loss: 861.3195 - val_loss: 176.6874 - val_regression_loss: 70.0844\n","Epoch 10/50\n","537/537 [==============================] - 0s 55us/step - loss: 1450.5575 - regression_loss: 607.9075 - val_loss: 199.6955 - val_regression_loss: 78.0260\n","Epoch 11/50\n","537/537 [==============================] - 0s 56us/step - loss: 1450.3472 - regression_loss: 577.7130 - val_loss: 225.6933 - val_regression_loss: 89.1618\n","Epoch 12/50\n","537/537 [==============================] - 0s 64us/step - loss: 1600.0118 - regression_loss: 638.8931 - val_loss: 235.5892 - val_regression_loss: 93.8173\n","Epoch 13/50\n","537/537 [==============================] - 0s 55us/step - loss: 1657.8497 - regression_loss: 663.2591 - val_loss: 223.2280 - val_regression_loss: 88.3964\n","Epoch 14/50\n","537/537 [==============================] - 0s 58us/step - loss: 1539.4993 - regression_loss: 614.0916 - val_loss: 197.8251 - val_regression_loss: 77.2003\n","Epoch 15/50\n","537/537 [==============================] - 0s 57us/step - loss: 1382.2527 - regression_loss: 548.2073 - val_loss: 178.5729 - val_regression_loss: 69.6370\n","Epoch 16/50\n","537/537 [==============================] - 0s 63us/step - loss: 1304.3210 - regression_loss: 529.2475 - val_loss: 173.2701 - val_regression_loss: 69.1227\n","Epoch 17/50\n","537/537 [==============================] - 0s 57us/step - loss: 1332.6005 - regression_loss: 560.7320 - val_loss: 172.2653 - val_regression_loss: 69.9968\n","Epoch 18/50\n","537/537 [==============================] - 0s 67us/step - loss: 1322.3699 - regression_loss: 565.1678 - val_loss: 169.1721 - val_regression_loss: 68.6639\n","Epoch 19/50\n","537/537 [==============================] - 0s 58us/step - loss: 1355.6305 - regression_loss: 586.4557 - val_loss: 166.8980 - val_regression_loss: 66.6156\n","Epoch 20/50\n","537/537 [==============================] - 0s 59us/step - loss: 1333.7869 - regression_loss: 568.8812 - val_loss: 167.9879 - val_regression_loss: 65.6755\n","Epoch 21/50\n","537/537 [==============================] - 0s 54us/step - loss: 1274.4077 - regression_loss: 525.7387 - val_loss: 172.8693 - val_regression_loss: 66.6759\n","Epoch 22/50\n","537/537 [==============================] - 0s 53us/step - loss: 1268.8610 - regression_loss: 508.4278 - val_loss: 178.4956 - val_regression_loss: 68.5203\n","Epoch 23/50\n","537/537 [==============================] - 0s 61us/step - loss: 1287.6000 - regression_loss: 512.0605 - val_loss: 179.9292 - val_regression_loss: 69.0227\n","Epoch 24/50\n","537/537 [==============================] - 0s 62us/step - loss: 1296.9929 - regression_loss: 510.7368 - val_loss: 175.5847 - val_regression_loss: 67.3983\n","Epoch 25/50\n","537/537 [==============================] - 0s 57us/step - loss: 1268.2381 - regression_loss: 502.5267 - val_loss: 169.4503 - val_regression_loss: 65.2067\n","Epoch 26/50\n","537/537 [==============================] - 0s 54us/step - loss: 1262.6072 - regression_loss: 506.9052 - val_loss: 163.9796 - val_regression_loss: 63.2984\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 1259.5079 - regression_loss: 513.5366 - val_loss: 161.2431 - val_regression_loss: 62.4471\n","Epoch 28/50\n","537/537 [==============================] - 0s 56us/step - loss: 1250.2359 - regression_loss: 513.1562 - val_loss: 160.6980 - val_regression_loss: 62.3941\n","Epoch 29/50\n","537/537 [==============================] - 0s 55us/step - loss: 1249.0328 - regression_loss: 515.2449 - val_loss: 160.8318 - val_regression_loss: 62.3710\n","Epoch 30/50\n","537/537 [==============================] - 0s 60us/step - loss: 1234.5853 - regression_loss: 507.8762 - val_loss: 161.8835 - val_regression_loss: 62.5598\n","Epoch 31/50\n","537/537 [==============================] - 0s 57us/step - loss: 1240.3304 - regression_loss: 505.4221 - val_loss: 163.6781 - val_regression_loss: 63.0197\n","Epoch 32/50\n","537/537 [==============================] - 0s 57us/step - loss: 1230.6860 - regression_loss: 499.4271 - val_loss: 165.7106 - val_regression_loss: 63.6343\n","Epoch 33/50\n","537/537 [==============================] - 0s 61us/step - loss: 1237.4719 - regression_loss: 498.9650 - val_loss: 166.9347 - val_regression_loss: 64.0088\n","Epoch 34/50\n","537/537 [==============================] - 0s 60us/step - loss: 1223.2380 - regression_loss: 490.4742 - val_loss: 166.3695 - val_regression_loss: 63.7810\n","Epoch 35/50\n","537/537 [==============================] - 0s 55us/step - loss: 1229.1881 - regression_loss: 493.1835 - val_loss: 164.5544 - val_regression_loss: 63.1248\n","Epoch 36/50\n","537/537 [==============================] - 0s 61us/step - loss: 1221.4112 - regression_loss: 490.5430 - val_loss: 162.6529 - val_regression_loss: 62.4611\n","Epoch 37/50\n","537/537 [==============================] - 0s 54us/step - loss: 1217.1196 - regression_loss: 491.3187 - val_loss: 161.2227 - val_regression_loss: 62.0388\n","Epoch 38/50\n","537/537 [==============================] - 0s 54us/step - loss: 1212.2084 - regression_loss: 492.2260 - val_loss: 160.7604 - val_regression_loss: 61.9006\n","Epoch 39/50\n","537/537 [==============================] - 0s 57us/step - loss: 1201.3735 - regression_loss: 488.6897 - val_loss: 161.2240 - val_regression_loss: 62.0364\n","Epoch 40/50\n","537/537 [==============================] - 0s 66us/step - loss: 1213.7149 - regression_loss: 491.3949 - val_loss: 161.7278 - val_regression_loss: 62.1472\n","Epoch 41/50\n","537/537 [==============================] - 0s 64us/step - loss: 1194.9479 - regression_loss: 485.0827 - val_loss: 161.9500 - val_regression_loss: 62.1799\n","Epoch 42/50\n","537/537 [==============================] - 0s 58us/step - loss: 1218.3278 - regression_loss: 492.4538 - val_loss: 162.4819 - val_regression_loss: 62.3324\n","Epoch 43/50\n","537/537 [==============================] - 0s 56us/step - loss: 1186.7637 - regression_loss: 481.1955 - val_loss: 162.9746 - val_regression_loss: 62.4453\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 1171.5851 - regression_loss: 472.6995 - val_loss: 161.9436 - val_regression_loss: 62.1050\n","Epoch 45/50\n","537/537 [==============================] - 0s 55us/step - loss: 1180.2936 - regression_loss: 472.9705 - val_loss: 160.9505 - val_regression_loss: 61.7874\n","Epoch 46/50\n","537/537 [==============================] - 0s 56us/step - loss: 1185.3458 - regression_loss: 480.0826 - val_loss: 160.5770 - val_regression_loss: 61.6558\n","Epoch 47/50\n","537/537 [==============================] - 0s 75us/step - loss: 1183.7303 - regression_loss: 477.7617 - val_loss: 160.0841 - val_regression_loss: 61.4981\n","Epoch 48/50\n","537/537 [==============================] - 0s 62us/step - loss: 1194.0250 - regression_loss: 485.2203 - val_loss: 160.4046 - val_regression_loss: 61.5788\n","Epoch 49/50\n","537/537 [==============================] - 0s 55us/step - loss: 1161.0034 - regression_loss: 468.3938 - val_loss: 160.2537 - val_regression_loss: 61.5161\n","Epoch 50/50\n","537/537 [==============================] - 0s 58us/step - loss: 1194.8356 - regression_loss: 485.0680 - val_loss: 160.5278 - val_regression_loss: 61.6317\n","***************************** elapsed_time is:  3.730584144592285\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 2111205.8577 - regression_loss: 1055127.5008 - val_loss: 269884.6250 - val_regression_loss: 134880.3906\n","Epoch 2/50\n","537/537 [==============================] - 0s 62us/step - loss: 2101624.6629 - regression_loss: 1050378.0458 - val_loss: 258831.5312 - val_regression_loss: 129360.9453\n","Epoch 3/50\n","537/537 [==============================] - 0s 63us/step - loss: 2017139.7795 - regression_loss: 1008179.9244 - val_loss: 243195.0938 - val_regression_loss: 121550.3047\n","Epoch 4/50\n","537/537 [==============================] - 0s 55us/step - loss: 1885312.6050 - regression_loss: 942328.7791 - val_loss: 221847.6875 - val_regression_loss: 110884.5469\n","Epoch 5/50\n","537/537 [==============================] - 0s 53us/step - loss: 1739075.1747 - regression_loss: 869267.0867 - val_loss: 194658.4844 - val_regression_loss: 97298.1484\n","Epoch 6/50\n","537/537 [==============================] - 0s 64us/step - loss: 1508937.0508 - regression_loss: 754255.7610 - val_loss: 162132.6250 - val_regression_loss: 81043.1016\n","Epoch 7/50\n","537/537 [==============================] - 0s 59us/step - loss: 1271340.8139 - regression_loss: 635516.4905 - val_loss: 125736.6484 - val_regression_loss: 62852.0039\n","Epoch 8/50\n","537/537 [==============================] - 0s 59us/step - loss: 1028330.4494 - regression_loss: 514061.1665 - val_loss: 88129.6016 - val_regression_loss: 44054.2812\n","Epoch 9/50\n","537/537 [==============================] - 0s 62us/step - loss: 678156.5863 - regression_loss: 339017.4083 - val_loss: 54631.3086 - val_regression_loss: 27311.0352\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 514102.5767 - regression_loss: 257038.6002 - val_loss: 35838.2070 - val_regression_loss: 17918.9883\n","Epoch 11/50\n","537/537 [==============================] - 0s 60us/step - loss: 386327.3716 - regression_loss: 193197.5020 - val_loss: 40979.2383 - val_regression_loss: 20472.4727\n","Epoch 12/50\n","537/537 [==============================] - 0s 66us/step - loss: 455334.1984 - regression_loss: 227545.6800 - val_loss: 48099.2031 - val_regression_loss: 23986.5879\n","Epoch 13/50\n","537/537 [==============================] - 0s 57us/step - loss: 514238.5980 - regression_loss: 256523.4314 - val_loss: 42050.3086 - val_regression_loss: 20935.7676\n","Epoch 14/50\n","537/537 [==============================] - 0s 61us/step - loss: 462023.1394 - regression_loss: 230162.0472 - val_loss: 32584.1348 - val_regression_loss: 16213.3105\n","Epoch 15/50\n","537/537 [==============================] - 0s 64us/step - loss: 375473.6365 - regression_loss: 187018.1420 - val_loss: 29061.9414 - val_regression_loss: 14475.3633\n","Epoch 16/50\n","537/537 [==============================] - 0s 53us/step - loss: 332606.3581 - regression_loss: 165794.2096 - val_loss: 30623.9082 - val_regression_loss: 15274.5410\n","Epoch 17/50\n","537/537 [==============================] - 0s 55us/step - loss: 334391.8571 - regression_loss: 166869.4752 - val_loss: 33251.5938 - val_regression_loss: 16597.6934\n","Epoch 18/50\n","537/537 [==============================] - 0s 67us/step - loss: 351711.8315 - regression_loss: 175608.4268 - val_loss: 34075.4922 - val_regression_loss: 17013.1855\n","Epoch 19/50\n","537/537 [==============================] - 0s 67us/step - loss: 350202.0943 - regression_loss: 174886.3244 - val_loss: 32456.2949 - val_regression_loss: 16204.6309\n","Epoch 20/50\n","537/537 [==============================] - 0s 67us/step - loss: 345472.8708 - regression_loss: 172539.0694 - val_loss: 29076.6875 - val_regression_loss: 14515.2061\n","Epoch 21/50\n","537/537 [==============================] - 0s 62us/step - loss: 315873.8833 - regression_loss: 157742.7410 - val_loss: 25448.1016 - val_regression_loss: 12701.4590\n","Epoch 22/50\n","537/537 [==============================] - 0s 62us/step - loss: 286736.7007 - regression_loss: 143173.8755 - val_loss: 23049.5703 - val_regression_loss: 11503.1895\n","Epoch 23/50\n","537/537 [==============================] - 0s 64us/step - loss: 276415.3216 - regression_loss: 138028.9667 - val_loss: 22568.0078 - val_regression_loss: 11263.7031\n","Epoch 24/50\n","537/537 [==============================] - 0s 62us/step - loss: 271374.4146 - regression_loss: 135515.6179 - val_loss: 23096.6973 - val_regression_loss: 11529.1914\n","Epoch 25/50\n","537/537 [==============================] - 0s 63us/step - loss: 276842.1332 - regression_loss: 138261.1039 - val_loss: 23064.4531 - val_regression_loss: 11513.7002\n","Epoch 26/50\n","537/537 [==============================] - 0s 65us/step - loss: 267634.6110 - regression_loss: 133663.4816 - val_loss: 21914.4961 - val_regression_loss: 10938.8193\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 250752.9262 - regression_loss: 125225.9314 - val_loss: 20598.0762 - val_regression_loss: 10280.5312\n","Epoch 28/50\n","537/537 [==============================] - 0s 65us/step - loss: 241801.9548 - regression_loss: 120748.6049 - val_loss: 19942.3145 - val_regression_loss: 9952.6826\n","Epoch 29/50\n","537/537 [==============================] - 0s 62us/step - loss: 236706.6805 - regression_loss: 118201.4789 - val_loss: 19914.1836 - val_regression_loss: 9938.8203\n","Epoch 30/50\n","537/537 [==============================] - 0s 62us/step - loss: 231068.9327 - regression_loss: 115387.4300 - val_loss: 20056.9785 - val_regression_loss: 10010.4678\n","Epoch 31/50\n","537/537 [==============================] - 0s 64us/step - loss: 226695.6626 - regression_loss: 113204.6051 - val_loss: 20096.6777 - val_regression_loss: 10030.5156\n","Epoch 32/50\n","537/537 [==============================] - 0s 68us/step - loss: 216644.3194 - regression_loss: 108179.1890 - val_loss: 19952.3691 - val_regression_loss: 9958.4160\n","Epoch 33/50\n","537/537 [==============================] - 0s 61us/step - loss: 206161.2658 - regression_loss: 102940.6274 - val_loss: 19926.6152 - val_regression_loss: 9945.4219\n","Epoch 34/50\n","537/537 [==============================] - 0s 61us/step - loss: 198529.1579 - regression_loss: 99124.0665 - val_loss: 19993.9414 - val_regression_loss: 9978.8281\n","Epoch 35/50\n","537/537 [==============================] - 0s 62us/step - loss: 185763.8913 - regression_loss: 92739.4597 - val_loss: 19779.5273 - val_regression_loss: 9871.3145\n","Epoch 36/50\n","537/537 [==============================] - 0s 64us/step - loss: 186130.9629 - regression_loss: 92924.7141 - val_loss: 19081.7500 - val_regression_loss: 9522.1787\n","Epoch 37/50\n","537/537 [==============================] - 0s 63us/step - loss: 174507.0088 - regression_loss: 87113.8699 - val_loss: 17953.0508 - val_regression_loss: 8957.7129\n","Epoch 38/50\n","537/537 [==============================] - 0s 60us/step - loss: 169240.7722 - regression_loss: 84479.7677 - val_loss: 16859.8594 - val_regression_loss: 8411.0859\n","Epoch 39/50\n","537/537 [==============================] - 0s 66us/step - loss: 160939.0789 - regression_loss: 80330.1586 - val_loss: 15962.0635 - val_regression_loss: 7962.1201\n","Epoch 40/50\n","537/537 [==============================] - 0s 67us/step - loss: 134039.6161 - regression_loss: 66878.3144 - val_loss: 15180.0244 - val_regression_loss: 7570.9878\n","Epoch 41/50\n","537/537 [==============================] - 0s 66us/step - loss: 141668.0997 - regression_loss: 70690.9820 - val_loss: 14604.3174 - val_regression_loss: 7282.9810\n","Epoch 42/50\n","537/537 [==============================] - 0s 58us/step - loss: 132959.1559 - regression_loss: 66338.9651 - val_loss: 14126.4375 - val_regression_loss: 7043.9443\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 130213.6553 - regression_loss: 64967.8942 - val_loss: 13476.2432 - val_regression_loss: 6718.8857\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 117116.1642 - regression_loss: 58417.5989 - val_loss: 12425.7686 - val_regression_loss: 6193.8721\n","Epoch 45/50\n","537/537 [==============================] - 0s 57us/step - loss: 97960.5703 - regression_loss: 48840.7713 - val_loss: 11334.8838 - val_regression_loss: 5648.6812\n","Epoch 46/50\n","537/537 [==============================] - 0s 60us/step - loss: 106784.0074 - regression_loss: 53257.8594 - val_loss: 10873.5059 - val_regression_loss: 5417.8809\n","Epoch 47/50\n","537/537 [==============================] - 0s 60us/step - loss: 98225.4124 - regression_loss: 48976.8850 - val_loss: 10456.0234 - val_regression_loss: 5209.0527\n","Epoch 48/50\n","537/537 [==============================] - 0s 60us/step - loss: 91714.1422 - regression_loss: 45720.0153 - val_loss: 9593.0986 - val_regression_loss: 4777.6680\n","Epoch 49/50\n","537/537 [==============================] - 0s 59us/step - loss: 81896.2822 - regression_loss: 40812.3189 - val_loss: 8483.7773 - val_regression_loss: 4223.2178\n","Epoch 50/50\n","537/537 [==============================] - 0s 70us/step - loss: 78347.2241 - regression_loss: 39040.7095 - val_loss: 7951.8589 - val_regression_loss: 3957.2720\n","***************************** elapsed_time is:  3.8019115924835205\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 547704.0769 - regression_loss: 275048.1303 - val_loss: 59068.5586 - val_regression_loss: 29676.2129\n","Epoch 2/50\n","537/537 [==============================] - 0s 58us/step - loss: 516314.2348 - regression_loss: 259325.9962 - val_loss: 54810.3750 - val_regression_loss: 27553.4258\n","Epoch 3/50\n","537/537 [==============================] - 0s 60us/step - loss: 470311.1115 - regression_loss: 236416.6928 - val_loss: 48472.9766 - val_regression_loss: 24393.5371\n","Epoch 4/50\n","537/537 [==============================] - 0s 60us/step - loss: 426096.9234 - regression_loss: 214384.3573 - val_loss: 39665.8086 - val_regression_loss: 20000.7910\n","Epoch 5/50\n","537/537 [==============================] - 0s 56us/step - loss: 354643.1741 - regression_loss: 178809.7835 - val_loss: 28871.4570 - val_regression_loss: 14615.0938\n","Epoch 6/50\n","537/537 [==============================] - 0s 67us/step - loss: 259924.6273 - regression_loss: 131520.4072 - val_loss: 17655.3027 - val_regression_loss: 9009.6641\n","Epoch 7/50\n","537/537 [==============================] - 0s 58us/step - loss: 168160.0328 - regression_loss: 85725.9969 - val_loss: 9132.4258 - val_regression_loss: 4679.4429\n","Epoch 8/50\n","537/537 [==============================] - 0s 55us/step - loss: 89429.3226 - regression_loss: 45815.6580 - val_loss: 8188.5063 - val_regression_loss: 3778.2593\n","Epoch 9/50\n","537/537 [==============================] - 0s 56us/step - loss: 82853.8495 - regression_loss: 39062.6935 - val_loss: 13233.4033 - val_regression_loss: 5712.2246\n","Epoch 10/50\n","537/537 [==============================] - 0s 54us/step - loss: 120994.4890 - regression_loss: 53222.3649 - val_loss: 12229.0576 - val_regression_loss: 5545.2163\n","Epoch 11/50\n","537/537 [==============================] - 0s 57us/step - loss: 113967.0406 - regression_loss: 52344.0895 - val_loss: 8012.8535 - val_regression_loss: 3813.9226\n","Epoch 12/50\n","537/537 [==============================] - 0s 59us/step - loss: 79486.5925 - regression_loss: 38275.5714 - val_loss: 5444.3027 - val_regression_loss: 2683.3125\n","Epoch 13/50\n","537/537 [==============================] - 0s 62us/step - loss: 56826.0871 - regression_loss: 28065.5645 - val_loss: 5160.8140 - val_regression_loss: 2593.6370\n","Epoch 14/50\n","537/537 [==============================] - 0s 58us/step - loss: 60404.4579 - regression_loss: 30318.9676 - val_loss: 5825.8799 - val_regression_loss: 2940.0378\n","Epoch 15/50\n","537/537 [==============================] - 0s 50us/step - loss: 64791.3011 - regression_loss: 32583.1616 - val_loss: 6208.6250 - val_regression_loss: 3129.3811\n","Epoch 16/50\n","537/537 [==============================] - 0s 61us/step - loss: 69164.3209 - regression_loss: 34754.8984 - val_loss: 5809.0898 - val_regression_loss: 2919.2136\n","Epoch 17/50\n","537/537 [==============================] - 0s 56us/step - loss: 66139.9563 - regression_loss: 33140.2616 - val_loss: 4883.0361 - val_regression_loss: 2441.4177\n","Epoch 18/50\n","537/537 [==============================] - 0s 62us/step - loss: 59770.8278 - regression_loss: 29849.0957 - val_loss: 4024.9583 - val_regression_loss: 1996.1704\n","\n","Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 19/50\n","537/537 [==============================] - 0s 68us/step - loss: 48552.5849 - regression_loss: 24095.8960 - val_loss: 3800.0994 - val_regression_loss: 1876.5769\n","Epoch 20/50\n","537/537 [==============================] - 0s 75us/step - loss: 48271.2193 - regression_loss: 23899.6575 - val_loss: 3724.0063 - val_regression_loss: 1832.7267\n","Epoch 21/50\n","537/537 [==============================] - 0s 67us/step - loss: 47560.3844 - regression_loss: 23498.5794 - val_loss: 3741.4338 - val_regression_loss: 1837.5927\n","Epoch 22/50\n","537/537 [==============================] - 0s 55us/step - loss: 46350.6768 - regression_loss: 22864.0674 - val_loss: 3763.1973 - val_regression_loss: 1846.8857\n","Epoch 23/50\n","537/537 [==============================] - 0s 56us/step - loss: 48598.7159 - regression_loss: 23971.2502 - val_loss: 3721.6526 - val_regression_loss: 1826.7128\n","Epoch 24/50\n","537/537 [==============================] - 0s 62us/step - loss: 45723.8312 - regression_loss: 22535.9444 - val_loss: 3604.9387 - val_regression_loss: 1770.7260\n","Epoch 25/50\n","537/537 [==============================] - 0s 60us/step - loss: 46015.6285 - regression_loss: 22705.9645 - val_loss: 3453.8215 - val_regression_loss: 1698.6257\n","Epoch 26/50\n","537/537 [==============================] - 0s 61us/step - loss: 44744.5521 - regression_loss: 22101.2048 - val_loss: 3314.0300 - val_regression_loss: 1632.6553\n","Epoch 27/50\n","537/537 [==============================] - 0s 60us/step - loss: 43106.3339 - regression_loss: 21321.2691 - val_loss: 3210.8152 - val_regression_loss: 1584.8267\n","Epoch 28/50\n","537/537 [==============================] - 0s 56us/step - loss: 41092.3860 - regression_loss: 20341.3070 - val_loss: 3140.5254 - val_regression_loss: 1552.8571\n","Epoch 29/50\n","537/537 [==============================] - 0s 53us/step - loss: 41112.1005 - regression_loss: 20377.5163 - val_loss: 3083.0920 - val_regression_loss: 1526.3412\n","Epoch 30/50\n","537/537 [==============================] - 0s 61us/step - loss: 38180.3395 - regression_loss: 18929.1152 - val_loss: 3018.6880 - val_regression_loss: 1495.2916\n","Epoch 31/50\n","537/537 [==============================] - 0s 63us/step - loss: 36806.6995 - regression_loss: 18250.5796 - val_loss: 2936.0886 - val_regression_loss: 1454.0775\n","Epoch 32/50\n","537/537 [==============================] - 0s 75us/step - loss: 38111.8317 - regression_loss: 18907.6678 - val_loss: 2837.9045 - val_regression_loss: 1404.1530\n","Epoch 33/50\n","537/537 [==============================] - 0s 70us/step - loss: 37568.2239 - regression_loss: 18636.3895 - val_loss: 2737.4924 - val_regression_loss: 1352.4800\n","Epoch 34/50\n","537/537 [==============================] - 0s 59us/step - loss: 36494.6981 - regression_loss: 18083.7191 - val_loss: 2648.0920 - val_regression_loss: 1306.1987\n","Epoch 35/50\n","537/537 [==============================] - 0s 59us/step - loss: 33077.7588 - regression_loss: 16360.0346 - val_loss: 2572.7776 - val_regression_loss: 1267.1617\n","Epoch 36/50\n","537/537 [==============================] - 0s 60us/step - loss: 34257.7434 - regression_loss: 16945.6036 - val_loss: 2507.2932 - val_regression_loss: 1233.4000\n","Epoch 37/50\n","537/537 [==============================] - 0s 61us/step - loss: 32227.1451 - regression_loss: 15919.3243 - val_loss: 2442.8657 - val_regression_loss: 1200.9062\n","Epoch 38/50\n","537/537 [==============================] - 0s 60us/step - loss: 32347.5794 - regression_loss: 15976.7759 - val_loss: 2377.2490 - val_regression_loss: 1168.4576\n","Epoch 39/50\n","537/537 [==============================] - 0s 65us/step - loss: 30721.9905 - regression_loss: 15170.2810 - val_loss: 2309.5405 - val_regression_loss: 1135.5162\n","Epoch 40/50\n","537/537 [==============================] - 0s 63us/step - loss: 30948.1437 - regression_loss: 15289.7646 - val_loss: 2244.8552 - val_regression_loss: 1104.3899\n","Epoch 41/50\n","537/537 [==============================] - 0s 65us/step - loss: 29601.5164 - regression_loss: 14629.8157 - val_loss: 2184.5425 - val_regression_loss: 1075.4425\n","Epoch 42/50\n","537/537 [==============================] - 0s 58us/step - loss: 28573.8952 - regression_loss: 14123.7143 - val_loss: 2128.3940 - val_regression_loss: 1048.3953\n","Epoch 43/50\n","537/537 [==============================] - 0s 64us/step - loss: 27711.8190 - regression_loss: 13704.1146 - val_loss: 2072.3796 - val_regression_loss: 1020.9322\n","Epoch 44/50\n","537/537 [==============================] - 0s 64us/step - loss: 26158.8991 - regression_loss: 12931.6212 - val_loss: 2015.7391 - val_regression_loss: 992.6672\n","Epoch 45/50\n","537/537 [==============================] - 0s 59us/step - loss: 25898.7277 - regression_loss: 12812.2999 - val_loss: 1959.4921 - val_regression_loss: 964.0685\n","Epoch 46/50\n","537/537 [==============================] - 0s 61us/step - loss: 24507.9647 - regression_loss: 12110.8095 - val_loss: 1905.7246 - val_regression_loss: 936.5792\n","Epoch 47/50\n","537/537 [==============================] - 0s 60us/step - loss: 23753.3332 - regression_loss: 11735.3737 - val_loss: 1857.1523 - val_regression_loss: 911.6165\n","Epoch 48/50\n","537/537 [==============================] - 0s 63us/step - loss: 22757.6571 - regression_loss: 11233.9619 - val_loss: 1812.7416 - val_regression_loss: 888.9307\n","Epoch 49/50\n","537/537 [==============================] - 0s 61us/step - loss: 22322.8319 - regression_loss: 11003.9073 - val_loss: 1766.7306 - val_regression_loss: 865.9703\n","Epoch 50/50\n","537/537 [==============================] - 0s 67us/step - loss: 20281.4502 - regression_loss: 9986.0177 - val_loss: 1720.4036 - val_regression_loss: 843.1315\n","***************************** elapsed_time is:  4.224915266036987\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 263603.6376 - regression_loss: 131509.7480 - val_loss: 27431.3496 - val_regression_loss: 13681.2617\n","Epoch 2/50\n","537/537 [==============================] - 0s 57us/step - loss: 236056.7893 - regression_loss: 117759.6467 - val_loss: 23899.1406 - val_regression_loss: 11917.8584\n","Epoch 3/50\n","537/537 [==============================] - 0s 64us/step - loss: 207784.3227 - regression_loss: 103644.2378 - val_loss: 19264.0176 - val_regression_loss: 9603.3574\n","Epoch 4/50\n","537/537 [==============================] - 0s 61us/step - loss: 167688.9214 - regression_loss: 83621.7662 - val_loss: 13585.2061 - val_regression_loss: 6767.3418\n","Epoch 5/50\n","537/537 [==============================] - 0s 63us/step - loss: 119218.5236 - regression_loss: 59415.7217 - val_loss: 7583.8003 - val_regression_loss: 3770.0532\n","Epoch 6/50\n","537/537 [==============================] - 0s 63us/step - loss: 67760.3126 - regression_loss: 33715.2014 - val_loss: 2929.5337 - val_regression_loss: 1445.9480\n","Epoch 7/50\n","537/537 [==============================] - 0s 55us/step - loss: 28040.4780 - regression_loss: 13880.3836 - val_loss: 2200.8730 - val_regression_loss: 1083.8661\n","Epoch 8/50\n","537/537 [==============================] - 0s 58us/step - loss: 19096.9697 - regression_loss: 9426.7077 - val_loss: 5109.1050 - val_regression_loss: 2538.7827\n","Epoch 9/50\n","537/537 [==============================] - 0s 61us/step - loss: 40862.5529 - regression_loss: 20317.5957 - val_loss: 5492.4531 - val_regression_loss: 2729.9734\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 42948.2441 - regression_loss: 21354.9680 - val_loss: 3413.5374 - val_regression_loss: 1689.9490\n","Epoch 11/50\n","537/537 [==============================] - 0s 55us/step - loss: 27988.1131 - regression_loss: 13874.1852 - val_loss: 1636.7609 - val_regression_loss: 801.2994\n","Epoch 12/50\n","537/537 [==============================] - 0s 65us/step - loss: 15214.7191 - regression_loss: 7484.4754 - val_loss: 1149.0493 - val_regression_loss: 557.4168\n","Epoch 13/50\n","537/537 [==============================] - 0s 60us/step - loss: 12643.5466 - regression_loss: 6197.8946 - val_loss: 1434.9757 - val_regression_loss: 700.4323\n","Epoch 14/50\n","537/537 [==============================] - 0s 56us/step - loss: 16686.3353 - regression_loss: 8221.2804 - val_loss: 1761.6229 - val_regression_loss: 863.8260\n","Epoch 15/50\n","537/537 [==============================] - 0s 59us/step - loss: 19599.5504 - regression_loss: 9677.4102 - val_loss: 1790.9994 - val_regression_loss: 878.5819\n","Epoch 16/50\n","537/537 [==============================] - 0s 54us/step - loss: 19891.7208 - regression_loss: 9825.9174 - val_loss: 1545.7661 - val_regression_loss: 756.0142\n","Epoch 17/50\n","537/537 [==============================] - 0s 53us/step - loss: 17399.1478 - regression_loss: 8577.5490 - val_loss: 1229.3015 - val_regression_loss: 597.8021\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 13640.5961 - regression_loss: 6699.8811 - val_loss: 1054.6724 - val_regression_loss: 510.4795\n","\n","Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 19/50\n","537/537 [==============================] - 0s 67us/step - loss: 11603.5973 - regression_loss: 5681.5182 - val_loss: 1056.9313 - val_regression_loss: 511.6025\n","Epoch 20/50\n","537/537 [==============================] - 0s 72us/step - loss: 10793.0796 - regression_loss: 5273.0085 - val_loss: 1105.8871 - val_regression_loss: 536.0815\n","Epoch 21/50\n","537/537 [==============================] - 0s 67us/step - loss: 10913.1590 - regression_loss: 5333.7001 - val_loss: 1171.4944 - val_regression_loss: 568.8972\n","Epoch 22/50\n","537/537 [==============================] - 0s 54us/step - loss: 10960.0487 - regression_loss: 5357.6647 - val_loss: 1220.0059 - val_regression_loss: 593.1772\n","Epoch 23/50\n","537/537 [==============================] - 0s 54us/step - loss: 11132.9351 - regression_loss: 5446.9487 - val_loss: 1230.3970 - val_regression_loss: 598.4056\n","Epoch 24/50\n","537/537 [==============================] - 0s 61us/step - loss: 11019.5465 - regression_loss: 5388.5424 - val_loss: 1200.7676 - val_regression_loss: 583.6245\n","Epoch 25/50\n","537/537 [==============================] - 0s 63us/step - loss: 10750.9009 - regression_loss: 5253.1483 - val_loss: 1145.0879 - val_regression_loss: 555.8130\n","Epoch 26/50\n","537/537 [==============================] - 0s 65us/step - loss: 10586.4651 - regression_loss: 5174.7550 - val_loss: 1083.2483 - val_regression_loss: 524.9105\n","Epoch 27/50\n","537/537 [==============================] - 0s 62us/step - loss: 10209.4181 - regression_loss: 4984.0466 - val_loss: 1029.0887 - val_regression_loss: 497.8365\n","Epoch 28/50\n","537/537 [==============================] - 0s 58us/step - loss: 9726.9971 - regression_loss: 4739.8297 - val_loss: 988.7437 - val_regression_loss: 477.6589\n","Epoch 29/50\n","537/537 [==============================] - 0s 61us/step - loss: 9541.2048 - regression_loss: 4647.6221 - val_loss: 961.4995 - val_regression_loss: 464.0249\n","Epoch 30/50\n","537/537 [==============================] - 0s 62us/step - loss: 9392.6632 - regression_loss: 4575.5201 - val_loss: 942.6829 - val_regression_loss: 454.6015\n","Epoch 31/50\n","537/537 [==============================] - 0s 62us/step - loss: 9650.4017 - regression_loss: 4703.0517 - val_loss: 927.9962 - val_regression_loss: 447.2454\n","Epoch 32/50\n","537/537 [==============================] - 0s 64us/step - loss: 9386.3881 - regression_loss: 4572.4349 - val_loss: 914.8516 - val_regression_loss: 440.6647\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 9233.1965 - regression_loss: 4494.3153 - val_loss: 902.9617 - val_regression_loss: 434.7154\n","Epoch 34/50\n","537/537 [==============================] - 0s 60us/step - loss: 9001.7036 - regression_loss: 4380.3537 - val_loss: 894.2215 - val_regression_loss: 430.3459\n","Epoch 35/50\n","537/537 [==============================] - 0s 60us/step - loss: 8754.2768 - regression_loss: 4255.8820 - val_loss: 889.1921 - val_regression_loss: 427.8361\n","Epoch 36/50\n","537/537 [==============================] - 0s 64us/step - loss: 8517.7286 - regression_loss: 4136.0393 - val_loss: 886.6713 - val_regression_loss: 426.5814\n","Epoch 37/50\n","537/537 [==============================] - 0s 70us/step - loss: 8171.8132 - regression_loss: 3965.1699 - val_loss: 885.1312 - val_regression_loss: 425.8183\n","Epoch 38/50\n","537/537 [==============================] - 0s 63us/step - loss: 8027.5882 - regression_loss: 3888.9637 - val_loss: 881.0610 - val_regression_loss: 423.7887\n","Epoch 39/50\n","537/537 [==============================] - 0s 59us/step - loss: 8073.8430 - regression_loss: 3914.9197 - val_loss: 873.1403 - val_regression_loss: 419.8341\n","Epoch 40/50\n","537/537 [==============================] - 0s 57us/step - loss: 7535.8512 - regression_loss: 3645.8642 - val_loss: 860.2575 - val_regression_loss: 413.3959\n","Epoch 41/50\n","537/537 [==============================] - 0s 60us/step - loss: 7666.8682 - regression_loss: 3712.9098 - val_loss: 846.4069 - val_regression_loss: 406.4728\n","Epoch 42/50\n","537/537 [==============================] - 0s 60us/step - loss: 7539.8096 - regression_loss: 3648.2729 - val_loss: 830.1363 - val_regression_loss: 398.3367\n","Epoch 43/50\n","537/537 [==============================] - 0s 55us/step - loss: 7309.1740 - regression_loss: 3533.6318 - val_loss: 813.4839 - val_regression_loss: 390.0086\n","Epoch 44/50\n","537/537 [==============================] - 0s 66us/step - loss: 7161.8149 - regression_loss: 3459.8799 - val_loss: 797.6204 - val_regression_loss: 382.0725\n","Epoch 45/50\n","537/537 [==============================] - 0s 61us/step - loss: 6954.6137 - regression_loss: 3355.6021 - val_loss: 783.2459 - val_regression_loss: 374.8804\n","Epoch 46/50\n","537/537 [==============================] - 0s 60us/step - loss: 6585.1506 - regression_loss: 3170.6654 - val_loss: 770.4758 - val_regression_loss: 368.4900\n","Epoch 47/50\n","537/537 [==============================] - 0s 63us/step - loss: 6479.3842 - regression_loss: 3116.5241 - val_loss: 759.7950 - val_regression_loss: 363.1449\n","Epoch 48/50\n","537/537 [==============================] - 0s 56us/step - loss: 6460.6762 - regression_loss: 3109.3503 - val_loss: 751.2633 - val_regression_loss: 358.8747\n","Epoch 49/50\n","537/537 [==============================] - 0s 59us/step - loss: 6271.6248 - regression_loss: 3013.5543 - val_loss: 744.9691 - val_regression_loss: 355.7240\n","Epoch 50/50\n","537/537 [==============================] - 0s 60us/step - loss: 5872.9124 - regression_loss: 2812.8810 - val_loss: 739.1002 - val_regression_loss: 352.7850\n","***************************** elapsed_time is:  3.8770244121551514\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 590691.8636 - regression_loss: 296109.8374 - val_loss: 51957.2070 - val_regression_loss: 26051.6953\n","Epoch 2/50\n","537/537 [==============================] - 0s 57us/step - loss: 552662.4090 - regression_loss: 277047.1680 - val_loss: 48014.4805 - val_regression_loss: 24073.2949\n","Epoch 3/50\n","537/537 [==============================] - 0s 62us/step - loss: 510881.5039 - regression_loss: 256124.2425 - val_loss: 42476.5234 - val_regression_loss: 21293.7559\n","Epoch 4/50\n","537/537 [==============================] - 0s 59us/step - loss: 469978.1096 - regression_loss: 235589.1771 - val_loss: 35110.4062 - val_regression_loss: 17596.0391\n","Epoch 5/50\n","537/537 [==============================] - 0s 56us/step - loss: 395381.5697 - regression_loss: 198145.0023 - val_loss: 26361.6699 - val_regression_loss: 13202.0166\n","Epoch 6/50\n","537/537 [==============================] - 0s 54us/step - loss: 319956.1345 - regression_loss: 160280.3558 - val_loss: 17696.0508 - val_regression_loss: 8843.6641\n","Epoch 7/50\n","537/537 [==============================] - 0s 57us/step - loss: 220285.4737 - regression_loss: 110221.3984 - val_loss: 11815.1748 - val_regression_loss: 5869.0039\n","Epoch 8/50\n","537/537 [==============================] - 0s 54us/step - loss: 160332.8753 - regression_loss: 79992.6610 - val_loss: 12388.4258 - val_regression_loss: 6107.7422\n","Epoch 9/50\n","537/537 [==============================] - 0s 58us/step - loss: 145564.5896 - regression_loss: 72239.0672 - val_loss: 17706.7988 - val_regression_loss: 8723.2188\n","Epoch 10/50\n","537/537 [==============================] - 0s 54us/step - loss: 169859.7327 - regression_loss: 84031.0629 - val_loss: 17559.6016 - val_regression_loss: 8654.6328\n","Epoch 11/50\n","537/537 [==============================] - 0s 60us/step - loss: 162183.1258 - regression_loss: 80215.5117 - val_loss: 13161.9717 - val_regression_loss: 6490.1665\n","Epoch 12/50\n","537/537 [==============================] - 0s 56us/step - loss: 137254.6804 - regression_loss: 68017.8658 - val_loss: 9142.2852 - val_regression_loss: 4516.3530\n","Epoch 13/50\n","537/537 [==============================] - 0s 53us/step - loss: 114298.7159 - regression_loss: 56849.7032 - val_loss: 7170.8281 - val_regression_loss: 3556.9583\n","Epoch 14/50\n","537/537 [==============================] - 0s 60us/step - loss: 107430.6079 - regression_loss: 53608.6143 - val_loss: 6754.4937 - val_regression_loss: 3364.7339\n","Epoch 15/50\n","537/537 [==============================] - 0s 63us/step - loss: 106693.8245 - regression_loss: 53340.2290 - val_loss: 6786.1685 - val_regression_loss: 3387.5964\n","Epoch 16/50\n","537/537 [==============================] - 0s 55us/step - loss: 114674.4949 - regression_loss: 57394.9664 - val_loss: 6466.7139 - val_regression_loss: 3227.2183\n","Epoch 17/50\n","537/537 [==============================] - 0s 54us/step - loss: 102604.2444 - regression_loss: 51329.3627 - val_loss: 5804.6226 - val_regression_loss: 2890.2988\n","Epoch 18/50\n","537/537 [==============================] - 0s 57us/step - loss: 100743.9073 - regression_loss: 50351.7078 - val_loss: 5233.3794 - val_regression_loss: 2595.4946\n","Epoch 19/50\n","537/537 [==============================] - 0s 58us/step - loss: 86358.6230 - regression_loss: 43072.4058 - val_loss: 5231.9009 - val_regression_loss: 2585.0403\n","Epoch 20/50\n","537/537 [==============================] - 0s 59us/step - loss: 78214.7087 - regression_loss: 38912.3477 - val_loss: 5754.2070 - val_regression_loss: 2838.5461\n","Epoch 21/50\n","537/537 [==============================] - 0s 53us/step - loss: 79187.9801 - regression_loss: 39333.4249 - val_loss: 6127.9053 - val_regression_loss: 3021.9858\n","Epoch 22/50\n","537/537 [==============================] - 0s 59us/step - loss: 68150.7420 - regression_loss: 33759.9808 - val_loss: 5656.7666 - val_regression_loss: 2788.5547\n","Epoch 23/50\n","537/537 [==============================] - 0s 56us/step - loss: 70774.7990 - regression_loss: 35095.8088 - val_loss: 4706.1060 - val_regression_loss: 2319.5042\n","Epoch 24/50\n","537/537 [==============================] - 0s 62us/step - loss: 65439.1127 - regression_loss: 32469.1445 - val_loss: 3898.6863 - val_regression_loss: 1924.7455\n","Epoch 25/50\n","537/537 [==============================] - 0s 67us/step - loss: 59558.7215 - regression_loss: 29590.2983 - val_loss: 3545.1836 - val_regression_loss: 1756.9163\n","Epoch 26/50\n","537/537 [==============================] - 0s 59us/step - loss: 56801.3469 - regression_loss: 28272.2631 - val_loss: 3395.9592 - val_regression_loss: 1688.6400\n","Epoch 27/50\n","537/537 [==============================] - 0s 62us/step - loss: 57128.7674 - regression_loss: 28482.8235 - val_loss: 3233.7988 - val_regression_loss: 1610.7637\n","Epoch 28/50\n","537/537 [==============================] - 0s 60us/step - loss: 51654.6787 - regression_loss: 25745.0575 - val_loss: 3159.4534 - val_regression_loss: 1575.5227\n","Epoch 29/50\n","537/537 [==============================] - 0s 58us/step - loss: 48348.8148 - regression_loss: 24081.9912 - val_loss: 3219.5845 - val_regression_loss: 1607.9302\n","Epoch 30/50\n","537/537 [==============================] - 0s 58us/step - loss: 45864.5308 - regression_loss: 22825.5219 - val_loss: 3079.7209 - val_regression_loss: 1540.9644\n","Epoch 31/50\n","537/537 [==============================] - 0s 61us/step - loss: 44279.3963 - regression_loss: 22043.2762 - val_loss: 2625.3101 - val_regression_loss: 1315.5452\n","Epoch 32/50\n","537/537 [==============================] - 0s 59us/step - loss: 40126.6445 - regression_loss: 19990.3939 - val_loss: 2140.6541 - val_regression_loss: 1072.1688\n","Epoch 33/50\n","537/537 [==============================] - 0s 58us/step - loss: 38040.7885 - regression_loss: 18974.3402 - val_loss: 1850.8490 - val_regression_loss: 921.8226\n","Epoch 34/50\n","537/537 [==============================] - 0s 59us/step - loss: 36427.7297 - regression_loss: 18183.6923 - val_loss: 1699.8512 - val_regression_loss: 834.7616\n","Epoch 35/50\n","537/537 [==============================] - 0s 61us/step - loss: 29860.1616 - regression_loss: 14875.1008 - val_loss: 1637.7633 - val_regression_loss: 787.3976\n","Epoch 36/50\n","537/537 [==============================] - 0s 59us/step - loss: 31680.2592 - regression_loss: 15753.5308 - val_loss: 1715.4333 - val_regression_loss: 807.6161\n","Epoch 37/50\n","537/537 [==============================] - 0s 64us/step - loss: 28286.0181 - regression_loss: 14000.7781 - val_loss: 1768.0558 - val_regression_loss: 819.9979\n","Epoch 38/50\n","537/537 [==============================] - 0s 76us/step - loss: 28780.3532 - regression_loss: 14233.1297 - val_loss: 1679.0413 - val_regression_loss: 768.9994\n","Epoch 39/50\n","537/537 [==============================] - 0s 66us/step - loss: 26655.0427 - regression_loss: 13145.4908 - val_loss: 1521.6383 - val_regression_loss: 691.2924\n","Epoch 40/50\n","537/537 [==============================] - 0s 62us/step - loss: 24455.8083 - regression_loss: 12068.2909 - val_loss: 1446.1697 - val_regression_loss: 656.1708\n","Epoch 41/50\n","537/537 [==============================] - 0s 61us/step - loss: 23872.0502 - regression_loss: 11797.8377 - val_loss: 1455.8176 - val_regression_loss: 661.8077\n","Epoch 42/50\n","537/537 [==============================] - 0s 66us/step - loss: 22470.6501 - regression_loss: 11095.1719 - val_loss: 1528.1462 - val_regression_loss: 698.2941\n","Epoch 43/50\n","537/537 [==============================] - 0s 57us/step - loss: 20838.0706 - regression_loss: 10279.0677 - val_loss: 1557.9238 - val_regression_loss: 716.0517\n","Epoch 44/50\n","537/537 [==============================] - 0s 63us/step - loss: 19956.4949 - regression_loss: 9821.2578 - val_loss: 1450.2362 - val_regression_loss: 669.2051\n","Epoch 45/50\n","537/537 [==============================] - 0s 62us/step - loss: 18554.5924 - regression_loss: 9135.5025 - val_loss: 1311.4758 - val_regression_loss: 607.1223\n","Epoch 46/50\n","537/537 [==============================] - 0s 63us/step - loss: 17154.6813 - regression_loss: 8455.2953 - val_loss: 1238.2146 - val_regression_loss: 574.4180\n","Epoch 47/50\n","537/537 [==============================] - 0s 59us/step - loss: 16200.9375 - regression_loss: 7975.1615 - val_loss: 1223.4880 - val_regression_loss: 567.8226\n","Epoch 48/50\n","537/537 [==============================] - 0s 63us/step - loss: 12459.7692 - regression_loss: 6084.8968 - val_loss: 1185.3138 - val_regression_loss: 548.9007\n","Epoch 49/50\n","537/537 [==============================] - 0s 63us/step - loss: 13808.0975 - regression_loss: 6743.2964 - val_loss: 1181.3458 - val_regression_loss: 546.3552\n","Epoch 50/50\n","537/537 [==============================] - 0s 61us/step - loss: 12814.6264 - regression_loss: 6228.1545 - val_loss: 1092.8954 - val_regression_loss: 503.3798\n","***************************** elapsed_time is:  3.788576126098633\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 1264470.8336 - regression_loss: 633085.2893 - val_loss: 84145.4375 - val_regression_loss: 42147.8906\n","Epoch 2/50\n","537/537 [==============================] - 0s 55us/step - loss: 1158420.7605 - regression_loss: 580088.8505 - val_loss: 77864.8516 - val_regression_loss: 39011.3047\n","Epoch 3/50\n","537/537 [==============================] - 0s 55us/step - loss: 1148549.6595 - regression_loss: 575225.2296 - val_loss: 69115.4609 - val_regression_loss: 34638.9492\n","Epoch 4/50\n","537/537 [==============================] - 0s 58us/step - loss: 1000768.7665 - regression_loss: 501382.6673 - val_loss: 57514.9648 - val_regression_loss: 28839.1797\n","Epoch 5/50\n","537/537 [==============================] - 0s 64us/step - loss: 885663.3202 - regression_loss: 443835.8069 - val_loss: 43510.3516 - val_regression_loss: 21833.5195\n","Epoch 6/50\n","537/537 [==============================] - 0s 51us/step - loss: 773935.7362 - regression_loss: 388117.3719 - val_loss: 29254.1172 - val_regression_loss: 14691.0566\n","Epoch 7/50\n","537/537 [==============================] - 0s 52us/step - loss: 598057.8329 - regression_loss: 300249.9035 - val_loss: 18129.0234 - val_regression_loss: 9078.6836\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 399834.6536 - regression_loss: 201004.6649 - val_loss: 15392.4678 - val_regression_loss: 7536.9590\n","Epoch 9/50\n","537/537 [==============================] - 0s 60us/step - loss: 336850.7765 - regression_loss: 169130.1325 - val_loss: 26729.5996 - val_regression_loss: 12653.4170\n","Epoch 10/50\n","537/537 [==============================] - 0s 65us/step - loss: 340964.0783 - regression_loss: 168675.1069 - val_loss: 39937.0273 - val_regression_loss: 18654.9219\n","Epoch 11/50\n","537/537 [==============================] - 0s 58us/step - loss: 390241.5037 - regression_loss: 189724.7780 - val_loss: 36054.9805 - val_regression_loss: 17008.4297\n","Epoch 12/50\n","537/537 [==============================] - 0s 63us/step - loss: 349356.9561 - regression_loss: 170683.3541 - val_loss: 24261.6230 - val_regression_loss: 11603.1846\n","Epoch 13/50\n","537/537 [==============================] - 0s 56us/step - loss: 291469.9009 - regression_loss: 144474.0205 - val_loss: 15185.7656 - val_regression_loss: 7348.7905\n","Epoch 14/50\n","537/537 [==============================] - 0s 57us/step - loss: 229096.6438 - regression_loss: 114621.9106 - val_loss: 11442.3809 - val_regression_loss: 5604.3730\n","Epoch 15/50\n","537/537 [==============================] - 0s 58us/step - loss: 257529.7467 - regression_loss: 129644.7796 - val_loss: 11097.6318 - val_regression_loss: 5483.9824\n","Epoch 16/50\n","537/537 [==============================] - 0s 61us/step - loss: 263709.6883 - regression_loss: 133007.1993 - val_loss: 11103.2568 - val_regression_loss: 5500.8936\n","Epoch 17/50\n","537/537 [==============================] - 0s 57us/step - loss: 271884.1774 - regression_loss: 137510.6060 - val_loss: 10293.7998 - val_regression_loss: 5079.4531\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 249779.2073 - regression_loss: 126835.2213 - val_loss: 9480.5381 - val_regression_loss: 4613.6538\n","Epoch 19/50\n","537/537 [==============================] - 0s 59us/step - loss: 229751.1144 - regression_loss: 117567.3941 - val_loss: 9940.7812 - val_regression_loss: 4700.5557\n","\n","Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 20/50\n","537/537 [==============================] - 0s 73us/step - loss: 209294.6523 - regression_loss: 107458.3333 - val_loss: 10670.5010 - val_regression_loss: 4957.2534\n","Epoch 21/50\n","537/537 [==============================] - 0s 63us/step - loss: 203398.0979 - regression_loss: 104179.3615 - val_loss: 11446.7607 - val_regression_loss: 5230.7847\n","Epoch 22/50\n","537/537 [==============================] - 0s 62us/step - loss: 200018.5553 - regression_loss: 102077.4569 - val_loss: 11914.1582 - val_regression_loss: 5379.3184\n","Epoch 23/50\n","537/537 [==============================] - 0s 61us/step - loss: 186459.1683 - regression_loss: 94233.3498 - val_loss: 11740.2324 - val_regression_loss: 5274.9507\n","Epoch 24/50\n","537/537 [==============================] - 0s 55us/step - loss: 186300.6587 - regression_loss: 94202.0657 - val_loss: 11001.5332 - val_regression_loss: 4952.4795\n","Epoch 25/50\n","537/537 [==============================] - 0s 55us/step - loss: 186672.0960 - regression_loss: 95105.1116 - val_loss: 9950.0098 - val_regression_loss: 4511.8286\n","Epoch 26/50\n","537/537 [==============================] - 0s 57us/step - loss: 180290.1307 - regression_loss: 92439.6065 - val_loss: 8845.3818 - val_regression_loss: 4053.5105\n","Epoch 27/50\n","537/537 [==============================] - 0s 63us/step - loss: 167429.6145 - regression_loss: 86283.4582 - val_loss: 7896.8423 - val_regression_loss: 3658.9702\n","Epoch 28/50\n","537/537 [==============================] - 0s 60us/step - loss: 168329.7263 - regression_loss: 87626.8415 - val_loss: 7228.7358 - val_regression_loss: 3377.5596\n","Epoch 29/50\n","537/537 [==============================] - 0s 54us/step - loss: 160504.1846 - regression_loss: 83750.4124 - val_loss: 6785.6641 - val_regression_loss: 3185.4302\n","Epoch 30/50\n","537/537 [==============================] - 0s 55us/step - loss: 155953.0432 - regression_loss: 81763.9761 - val_loss: 6545.7983 - val_regression_loss: 3071.0310\n","Epoch 31/50\n","537/537 [==============================] - 0s 59us/step - loss: 149486.8803 - regression_loss: 78518.1404 - val_loss: 6458.0020 - val_regression_loss: 3013.3804\n","Epoch 32/50\n","537/537 [==============================] - 0s 58us/step - loss: 148181.6488 - regression_loss: 78097.0435 - val_loss: 6518.6523 - val_regression_loss: 3012.6594\n","Epoch 33/50\n","537/537 [==============================] - 0s 56us/step - loss: 131356.0934 - regression_loss: 68970.1455 - val_loss: 6625.0107 - val_regression_loss: 3030.8220\n","Epoch 34/50\n","537/537 [==============================] - 0s 54us/step - loss: 124420.6926 - regression_loss: 64959.0526 - val_loss: 6703.3760 - val_regression_loss: 3041.0833\n","Epoch 35/50\n","537/537 [==============================] - 0s 67us/step - loss: 132884.6173 - regression_loss: 69400.0730 - val_loss: 6714.4844 - val_regression_loss: 3030.8115\n","Epoch 36/50\n","537/537 [==============================] - 0s 55us/step - loss: 126230.0294 - regression_loss: 66019.6860 - val_loss: 6576.0977 - val_regression_loss: 2965.7964\n","Epoch 37/50\n","537/537 [==============================] - 0s 60us/step - loss: 123654.9472 - regression_loss: 64660.8302 - val_loss: 6268.7861 - val_regression_loss: 2836.6140\n","Epoch 38/50\n","537/537 [==============================] - 0s 64us/step - loss: 111029.9816 - regression_loss: 58007.1958 - val_loss: 5854.1338 - val_regression_loss: 2664.5132\n","Epoch 39/50\n","537/537 [==============================] - 0s 55us/step - loss: 113843.3052 - regression_loss: 60062.6133 - val_loss: 5491.9951 - val_regression_loss: 2513.6809\n","Epoch 40/50\n","537/537 [==============================] - 0s 54us/step - loss: 107736.0784 - regression_loss: 56829.8968 - val_loss: 5179.7139 - val_regression_loss: 2381.7424\n","Epoch 41/50\n","537/537 [==============================] - 0s 53us/step - loss: 104332.0111 - regression_loss: 55325.0808 - val_loss: 4977.3608 - val_regression_loss: 2293.3333\n","Epoch 42/50\n","537/537 [==============================] - 0s 56us/step - loss: 94180.6721 - regression_loss: 49407.8487 - val_loss: 4807.7847 - val_regression_loss: 2216.7180\n","Epoch 43/50\n","537/537 [==============================] - 0s 59us/step - loss: 96218.1126 - regression_loss: 51037.1045 - val_loss: 4774.5015 - val_regression_loss: 2196.5964\n","Epoch 44/50\n","537/537 [==============================] - 0s 55us/step - loss: 86060.7267 - regression_loss: 45492.4673 - val_loss: 4755.6362 - val_regression_loss: 2183.5762\n","Epoch 45/50\n","537/537 [==============================] - 0s 58us/step - loss: 90615.5640 - regression_loss: 47962.7103 - val_loss: 4744.2710 - val_regression_loss: 2175.0767\n","Epoch 46/50\n","537/537 [==============================] - 0s 57us/step - loss: 80903.9525 - regression_loss: 42422.1220 - val_loss: 4591.2324 - val_regression_loss: 2107.4866\n","Epoch 47/50\n","537/537 [==============================] - 0s 60us/step - loss: 82187.0034 - regression_loss: 43332.2803 - val_loss: 4397.2651 - val_regression_loss: 2023.8533\n","Epoch 48/50\n","537/537 [==============================] - 0s 65us/step - loss: 79544.1858 - regression_loss: 42086.7096 - val_loss: 4177.1406 - val_regression_loss: 1929.7479\n","Epoch 49/50\n","537/537 [==============================] - 0s 61us/step - loss: 74314.1351 - regression_loss: 39226.3182 - val_loss: 3952.6101 - val_regression_loss: 1833.5579\n","Epoch 50/50\n","537/537 [==============================] - 0s 58us/step - loss: 68210.0174 - regression_loss: 36267.3094 - val_loss: 3819.4966 - val_regression_loss: 1776.1387\n","***************************** elapsed_time is:  4.100278854370117\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 1863209.1040 - regression_loss: 929675.1241 - val_loss: 211043.1406 - val_regression_loss: 105286.8750\n","Epoch 2/50\n","537/537 [==============================] - 0s 68us/step - loss: 1801416.1343 - regression_loss: 898880.2719 - val_loss: 201499.8281 - val_regression_loss: 100534.8203\n","Epoch 3/50\n","537/537 [==============================] - 0s 59us/step - loss: 1712110.9551 - regression_loss: 854360.9398 - val_loss: 187993.3906 - val_regression_loss: 93808.3359\n","Epoch 4/50\n","537/537 [==============================] - 0s 58us/step - loss: 1613797.1096 - regression_loss: 805449.8488 - val_loss: 169563.8594 - val_regression_loss: 84628.8047\n","Epoch 5/50\n","537/537 [==============================] - 0s 56us/step - loss: 1458856.5169 - regression_loss: 728267.1414 - val_loss: 146324.9219 - val_regression_loss: 73052.0156\n","Epoch 6/50\n","537/537 [==============================] - 0s 67us/step - loss: 1295676.4969 - regression_loss: 647023.2083 - val_loss: 119354.8359 - val_regression_loss: 59614.6406\n","Epoch 7/50\n","537/537 [==============================] - 0s 58us/step - loss: 1064538.9769 - regression_loss: 531851.2377 - val_loss: 89966.4922 - val_regression_loss: 44971.8086\n","Epoch 8/50\n","537/537 [==============================] - 0s 59us/step - loss: 814897.6996 - regression_loss: 407446.9725 - val_loss: 60061.9844 - val_regression_loss: 30073.1152\n","Epoch 9/50\n","537/537 [==============================] - 0s 58us/step - loss: 576986.6625 - regression_loss: 288905.8751 - val_loss: 33858.2539 - val_regression_loss: 16998.9551\n","Epoch 10/50\n","537/537 [==============================] - 0s 55us/step - loss: 358409.6355 - regression_loss: 179829.1061 - val_loss: 20639.7734 - val_regression_loss: 10231.6582\n","Epoch 11/50\n","537/537 [==============================] - 0s 58us/step - loss: 246039.8469 - regression_loss: 122030.0208 - val_loss: 31005.3438 - val_regression_loss: 14980.2520\n","Epoch 12/50\n","537/537 [==============================] - 0s 62us/step - loss: 316764.6083 - regression_loss: 153070.2200 - val_loss: 43732.7930 - val_regression_loss: 21269.2598\n","Epoch 13/50\n","537/537 [==============================] - 0s 61us/step - loss: 405580.9459 - regression_loss: 196718.9042 - val_loss: 37341.5938 - val_regression_loss: 18337.5508\n","Epoch 14/50\n","537/537 [==============================] - 0s 63us/step - loss: 349550.5585 - regression_loss: 171447.0431 - val_loss: 25405.4609 - val_regression_loss: 12606.0576\n","Epoch 15/50\n","537/537 [==============================] - 0s 57us/step - loss: 255570.4637 - regression_loss: 126810.9350 - val_loss: 18956.7969 - val_regression_loss: 9471.6025\n","Epoch 16/50\n","537/537 [==============================] - 0s 63us/step - loss: 211366.7261 - regression_loss: 105560.1770 - val_loss: 18245.2812 - val_regression_loss: 9131.0322\n","Epoch 17/50\n","537/537 [==============================] - 0s 56us/step - loss: 203491.1092 - regression_loss: 101783.0589 - val_loss: 19597.4336 - val_regression_loss: 9802.4043\n","Epoch 18/50\n","537/537 [==============================] - 0s 66us/step - loss: 227420.8886 - regression_loss: 113725.3890 - val_loss: 20331.1973 - val_regression_loss: 10162.4170\n","Epoch 19/50\n","537/537 [==============================] - 0s 60us/step - loss: 231349.8510 - regression_loss: 115637.8792 - val_loss: 19599.3262 - val_regression_loss: 9792.0312\n","Epoch 20/50\n","537/537 [==============================] - 0s 54us/step - loss: 220793.7632 - regression_loss: 110312.1902 - val_loss: 17734.2598 - val_regression_loss: 8858.4805\n","Epoch 21/50\n","537/537 [==============================] - 0s 55us/step - loss: 211938.8362 - regression_loss: 105880.3188 - val_loss: 15757.2793 - val_regression_loss: 7872.1450\n","Epoch 22/50\n","537/537 [==============================] - 0s 55us/step - loss: 193118.1330 - regression_loss: 96461.3218 - val_loss: 14776.2148 - val_regression_loss: 7386.3716\n","Epoch 23/50\n","537/537 [==============================] - 0s 63us/step - loss: 175062.9716 - regression_loss: 87476.5774 - val_loss: 15232.8037 - val_regression_loss: 7620.2124\n","Epoch 24/50\n","537/537 [==============================] - 0s 55us/step - loss: 172070.4846 - regression_loss: 86017.8122 - val_loss: 16304.0615 - val_regression_loss: 8160.0947\n","Epoch 25/50\n","537/537 [==============================] - 0s 59us/step - loss: 165215.4004 - regression_loss: 82624.4340 - val_loss: 16514.5527 - val_regression_loss: 8266.0977\n","Epoch 26/50\n","537/537 [==============================] - 0s 64us/step - loss: 171420.8711 - regression_loss: 85716.2440 - val_loss: 15404.6143 - val_regression_loss: 7708.3369\n","Epoch 27/50\n","537/537 [==============================] - 0s 72us/step - loss: 159532.1559 - regression_loss: 79755.9963 - val_loss: 13618.2764 - val_regression_loss: 6809.8228\n","Epoch 28/50\n","537/537 [==============================] - 0s 65us/step - loss: 146379.6623 - regression_loss: 73136.7595 - val_loss: 12177.2598 - val_regression_loss: 6083.7812\n","Epoch 29/50\n","537/537 [==============================] - 0s 63us/step - loss: 146791.6320 - regression_loss: 73298.3133 - val_loss: 11368.1025 - val_regression_loss: 5675.1919\n","Epoch 30/50\n","537/537 [==============================] - 0s 66us/step - loss: 143557.7467 - regression_loss: 71653.0791 - val_loss: 10902.5889 - val_regression_loss: 5440.7158\n","Epoch 31/50\n","537/537 [==============================] - 0s 62us/step - loss: 140701.8626 - regression_loss: 70208.0775 - val_loss: 10527.2617 - val_regression_loss: 5253.9727\n","Epoch 32/50\n","537/537 [==============================] - 0s 66us/step - loss: 131433.7481 - regression_loss: 65593.3894 - val_loss: 10245.9346 - val_regression_loss: 5116.2178\n","Epoch 33/50\n","537/537 [==============================] - 0s 65us/step - loss: 127927.9735 - regression_loss: 63855.0252 - val_loss: 10202.7744 - val_regression_loss: 5098.9150\n","Epoch 34/50\n","537/537 [==============================] - 0s 66us/step - loss: 122873.8802 - regression_loss: 61358.2354 - val_loss: 10415.6914 - val_regression_loss: 5209.5801\n","Epoch 35/50\n","537/537 [==============================] - 0s 64us/step - loss: 120034.6104 - regression_loss: 59970.1863 - val_loss: 10609.4268 - val_regression_loss: 5309.1128\n","Epoch 36/50\n","537/537 [==============================] - 0s 61us/step - loss: 112927.2113 - regression_loss: 56437.1103 - val_loss: 10466.7305 - val_regression_loss: 5237.9883\n","Epoch 37/50\n","537/537 [==============================] - 0s 59us/step - loss: 109987.2892 - regression_loss: 54963.0287 - val_loss: 9917.2021 - val_regression_loss: 4961.2417\n","Epoch 38/50\n","537/537 [==============================] - 0s 61us/step - loss: 104429.8608 - regression_loss: 52174.6549 - val_loss: 9154.4482 - val_regression_loss: 4576.6001\n","Epoch 39/50\n","537/537 [==============================] - 0s 63us/step - loss: 99273.8772 - regression_loss: 49569.3441 - val_loss: 8452.3086 - val_regression_loss: 4222.6050\n","Epoch 40/50\n","537/537 [==============================] - 0s 58us/step - loss: 94546.7192 - regression_loss: 47180.5660 - val_loss: 7838.8584 - val_regression_loss: 3914.1245\n","Epoch 41/50\n","537/537 [==============================] - 0s 59us/step - loss: 87111.5314 - regression_loss: 43457.4091 - val_loss: 7296.2715 - val_regression_loss: 3642.4722\n","Epoch 42/50\n","537/537 [==============================] - 0s 60us/step - loss: 82450.4400 - regression_loss: 41120.7646 - val_loss: 6862.7261 - val_regression_loss: 3426.7593\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 77103.2872 - regression_loss: 38460.4390 - val_loss: 6500.4561 - val_regression_loss: 3247.1128\n","Epoch 44/50\n","537/537 [==============================] - 0s 62us/step - loss: 70331.7842 - regression_loss: 35086.7340 - val_loss: 6123.5254 - val_regression_loss: 3059.6750\n","Epoch 45/50\n","537/537 [==============================] - 0s 85us/step - loss: 63672.4232 - regression_loss: 31762.9246 - val_loss: 5624.2754 - val_regression_loss: 2809.8616\n","Epoch 46/50\n","537/537 [==============================] - 0s 63us/step - loss: 60148.5645 - regression_loss: 29999.6292 - val_loss: 5041.0806 - val_regression_loss: 2517.0618\n","Epoch 47/50\n","537/537 [==============================] - 0s 63us/step - loss: 51882.6596 - regression_loss: 25861.6495 - val_loss: 4419.4028 - val_regression_loss: 2204.1475\n","Epoch 48/50\n","537/537 [==============================] - 0s 61us/step - loss: 46610.8463 - regression_loss: 23207.5920 - val_loss: 3868.4851 - val_regression_loss: 1926.7461\n","Epoch 49/50\n","537/537 [==============================] - 0s 61us/step - loss: 44604.6596 - regression_loss: 22190.4082 - val_loss: 3438.6528 - val_regression_loss: 1711.0029\n","Epoch 50/50\n","537/537 [==============================] - 0s 60us/step - loss: 40789.5935 - regression_loss: 20272.3026 - val_loss: 3105.5361 - val_regression_loss: 1544.5369\n","***************************** elapsed_time is:  3.805452585220337\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 56806.7604 - regression_loss: 28086.3357 - val_loss: 6243.3320 - val_regression_loss: 3080.9717\n","Epoch 2/50\n","537/537 [==============================] - 0s 56us/step - loss: 47712.5045 - regression_loss: 23563.1536 - val_loss: 4814.7935 - val_regression_loss: 2370.9954\n","Epoch 3/50\n","537/537 [==============================] - 0s 60us/step - loss: 37152.1556 - regression_loss: 18312.8103 - val_loss: 3149.3484 - val_regression_loss: 1544.9934\n","Epoch 4/50\n","537/537 [==============================] - 0s 64us/step - loss: 24219.7833 - regression_loss: 11901.3224 - val_loss: 1817.9198 - val_regression_loss: 889.5251\n","Epoch 5/50\n","537/537 [==============================] - 0s 63us/step - loss: 14657.5746 - regression_loss: 7201.3775 - val_loss: 1659.7773 - val_regression_loss: 821.3015\n","Epoch 6/50\n","537/537 [==============================] - 0s 63us/step - loss: 14147.7219 - regression_loss: 7030.0648 - val_loss: 1466.3239 - val_regression_loss: 724.6194\n","Epoch 7/50\n","537/537 [==============================] - 0s 59us/step - loss: 12367.8875 - regression_loss: 6135.6241 - val_loss: 957.8360 - val_regression_loss: 463.7653\n","Epoch 8/50\n","537/537 [==============================] - 0s 64us/step - loss: 7902.0200 - regression_loss: 3846.1685 - val_loss: 746.4511 - val_regression_loss: 350.3915\n","Epoch 9/50\n","537/537 [==============================] - 0s 61us/step - loss: 5718.0993 - regression_loss: 2689.7891 - val_loss: 882.8861 - val_regression_loss: 413.3778\n","Epoch 10/50\n","537/537 [==============================] - 0s 62us/step - loss: 6393.3025 - regression_loss: 2983.7730 - val_loss: 1059.7809 - val_regression_loss: 500.1522\n","Epoch 11/50\n","537/537 [==============================] - 0s 60us/step - loss: 7484.7360 - regression_loss: 3517.4026 - val_loss: 1056.1388 - val_regression_loss: 499.4854\n","Epoch 12/50\n","537/537 [==============================] - 0s 64us/step - loss: 7755.7216 - regression_loss: 3660.3474 - val_loss: 877.6735 - val_regression_loss: 413.0558\n","Epoch 13/50\n","537/537 [==============================] - 0s 59us/step - loss: 5965.0492 - regression_loss: 2789.4610 - val_loss: 651.7275 - val_regression_loss: 303.5632\n","Epoch 14/50\n","537/537 [==============================] - 0s 63us/step - loss: 5082.9479 - regression_loss: 2375.9044 - val_loss: 505.4229 - val_regression_loss: 233.9322\n","Epoch 15/50\n","537/537 [==============================] - 0s 63us/step - loss: 4094.6661 - regression_loss: 1909.4152 - val_loss: 494.7120 - val_regression_loss: 231.4770\n","Epoch 16/50\n","537/537 [==============================] - 0s 64us/step - loss: 4246.9026 - regression_loss: 2007.5037 - val_loss: 542.6546 - val_regression_loss: 257.2964\n","Epoch 17/50\n","537/537 [==============================] - 0s 62us/step - loss: 4579.8152 - regression_loss: 2185.7210 - val_loss: 547.6008 - val_regression_loss: 260.3848\n","Epoch 18/50\n","537/537 [==============================] - 0s 58us/step - loss: 4568.4467 - regression_loss: 2187.7926 - val_loss: 513.6627 - val_regression_loss: 242.9235\n","Epoch 19/50\n","537/537 [==============================] - 0s 64us/step - loss: 4144.6230 - regression_loss: 1971.2552 - val_loss: 498.1802 - val_regression_loss: 234.0021\n","Epoch 20/50\n","537/537 [==============================] - 0s 61us/step - loss: 3784.7542 - regression_loss: 1780.1790 - val_loss: 506.8845 - val_regression_loss: 236.9612\n","Epoch 21/50\n","537/537 [==============================] - 0s 70us/step - loss: 3703.1614 - regression_loss: 1731.0299 - val_loss: 506.6883 - val_regression_loss: 235.5763\n","Epoch 22/50\n","537/537 [==============================] - 0s 59us/step - loss: 3723.3470 - regression_loss: 1728.1964 - val_loss: 481.4110 - val_regression_loss: 221.9041\n","Epoch 23/50\n","537/537 [==============================] - 0s 62us/step - loss: 3565.1728 - regression_loss: 1638.5678 - val_loss: 451.2503 - val_regression_loss: 206.1509\n","Epoch 24/50\n","537/537 [==============================] - 0s 60us/step - loss: 3365.4486 - regression_loss: 1536.7463 - val_loss: 431.6197 - val_regression_loss: 196.1852\n","Epoch 25/50\n","537/537 [==============================] - 0s 61us/step - loss: 3276.2660 - regression_loss: 1488.5234 - val_loss: 414.3355 - val_regression_loss: 188.0380\n","Epoch 26/50\n","537/537 [==============================] - 0s 63us/step - loss: 3226.8282 - regression_loss: 1471.0819 - val_loss: 394.2405 - val_regression_loss: 178.8541\n","Epoch 27/50\n","537/537 [==============================] - 0s 64us/step - loss: 3066.4587 - regression_loss: 1393.9841 - val_loss: 381.0037 - val_regression_loss: 173.0938\n","Epoch 28/50\n","537/537 [==============================] - 0s 59us/step - loss: 3010.3880 - regression_loss: 1379.4373 - val_loss: 378.8738 - val_regression_loss: 172.5622\n","Epoch 29/50\n","537/537 [==============================] - 0s 62us/step - loss: 2867.4928 - regression_loss: 1309.1681 - val_loss: 376.5889 - val_regression_loss: 171.6968\n","Epoch 30/50\n","537/537 [==============================] - 0s 62us/step - loss: 2928.2325 - regression_loss: 1342.4101 - val_loss: 366.1647 - val_regression_loss: 166.5712\n","Epoch 31/50\n","537/537 [==============================] - 0s 65us/step - loss: 2839.3006 - regression_loss: 1298.4269 - val_loss: 353.3600 - val_regression_loss: 160.0871\n","Epoch 32/50\n","537/537 [==============================] - 0s 62us/step - loss: 2650.2218 - regression_loss: 1204.4102 - val_loss: 343.0227 - val_regression_loss: 154.6261\n","Epoch 33/50\n","537/537 [==============================] - 0s 61us/step - loss: 2615.7438 - regression_loss: 1181.7688 - val_loss: 334.8665 - val_regression_loss: 150.1530\n","Epoch 34/50\n","537/537 [==============================] - 0s 58us/step - loss: 2638.5305 - regression_loss: 1190.7366 - val_loss: 328.2354 - val_regression_loss: 146.3977\n","Epoch 35/50\n","537/537 [==============================] - 0s 62us/step - loss: 2590.0939 - regression_loss: 1164.8290 - val_loss: 322.2815 - val_regression_loss: 143.1024\n","Epoch 36/50\n","537/537 [==============================] - 0s 57us/step - loss: 2440.2995 - regression_loss: 1085.3149 - val_loss: 316.7380 - val_regression_loss: 140.3113\n","Epoch 37/50\n","537/537 [==============================] - 0s 61us/step - loss: 2456.7445 - regression_loss: 1095.4677 - val_loss: 307.8587 - val_regression_loss: 136.2160\n","Epoch 38/50\n","537/537 [==============================] - 0s 59us/step - loss: 2408.4738 - regression_loss: 1073.6778 - val_loss: 298.2857 - val_regression_loss: 131.9148\n","Epoch 39/50\n","537/537 [==============================] - 0s 66us/step - loss: 2210.4897 - regression_loss: 980.8601 - val_loss: 288.5953 - val_regression_loss: 127.3542\n","Epoch 40/50\n","537/537 [==============================] - 0s 60us/step - loss: 2313.0638 - regression_loss: 1031.3995 - val_loss: 280.1727 - val_regression_loss: 123.2835\n","Epoch 41/50\n","537/537 [==============================] - 0s 61us/step - loss: 2268.3984 - regression_loss: 1010.9766 - val_loss: 273.0073 - val_regression_loss: 119.5720\n","Epoch 42/50\n","537/537 [==============================] - 0s 60us/step - loss: 2212.6438 - regression_loss: 982.5055 - val_loss: 268.1060 - val_regression_loss: 116.9771\n","Epoch 43/50\n","537/537 [==============================] - 0s 63us/step - loss: 2034.2237 - regression_loss: 892.6428 - val_loss: 262.8246 - val_regression_loss: 114.2061\n","Epoch 44/50\n","537/537 [==============================] - 0s 57us/step - loss: 2116.3642 - regression_loss: 933.1905 - val_loss: 257.6448 - val_regression_loss: 111.6403\n","Epoch 45/50\n","537/537 [==============================] - 0s 57us/step - loss: 2077.7316 - regression_loss: 912.5918 - val_loss: 251.7208 - val_regression_loss: 108.7206\n","Epoch 46/50\n","537/537 [==============================] - 0s 58us/step - loss: 2048.3560 - regression_loss: 899.8836 - val_loss: 245.1692 - val_regression_loss: 105.4250\n","Epoch 47/50\n","537/537 [==============================] - 0s 59us/step - loss: 2005.7946 - regression_loss: 878.0521 - val_loss: 241.0999 - val_regression_loss: 103.4375\n","Epoch 48/50\n","537/537 [==============================] - 0s 60us/step - loss: 1865.4669 - regression_loss: 807.9952 - val_loss: 237.6180 - val_regression_loss: 101.8366\n","Epoch 49/50\n","537/537 [==============================] - 0s 53us/step - loss: 1959.8545 - regression_loss: 857.1079 - val_loss: 233.4255 - val_regression_loss: 99.8886\n","Epoch 50/50\n","537/537 [==============================] - 0s 58us/step - loss: 1903.0091 - regression_loss: 830.5323 - val_loss: 227.7995 - val_regression_loss: 97.1253\n","***************************** elapsed_time is:  3.81878662109375\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 7961.7797 - regression_loss: 3821.8599 - val_loss: 641.4937 - val_regression_loss: 299.1511\n","Epoch 2/50\n","537/537 [==============================] - 0s 59us/step - loss: 5098.4077 - regression_loss: 2390.7027 - val_loss: 389.2503 - val_regression_loss: 173.3465\n","Epoch 3/50\n","537/537 [==============================] - 0s 64us/step - loss: 3155.8254 - regression_loss: 1421.6753 - val_loss: 260.9682 - val_regression_loss: 109.9992\n","Epoch 4/50\n","537/537 [==============================] - 0s 60us/step - loss: 2137.5694 - regression_loss: 919.7274 - val_loss: 197.2749 - val_regression_loss: 79.1606\n","Epoch 5/50\n","537/537 [==============================] - 0s 57us/step - loss: 1654.1550 - regression_loss: 686.6309 - val_loss: 230.8002 - val_regression_loss: 96.6397\n","Epoch 6/50\n","537/537 [==============================] - 0s 55us/step - loss: 1878.6297 - regression_loss: 804.2246 - val_loss: 269.0468 - val_regression_loss: 116.2095\n","Epoch 7/50\n","537/537 [==============================] - 0s 56us/step - loss: 1996.6879 - regression_loss: 866.7336 - val_loss: 220.3886 - val_regression_loss: 92.2970\n","Epoch 8/50\n","537/537 [==============================] - 0s 53us/step - loss: 1595.1790 - regression_loss: 669.2113 - val_loss: 171.2228 - val_regression_loss: 68.1025\n","Epoch 9/50\n","537/537 [==============================] - 0s 62us/step - loss: 1281.0398 - regression_loss: 516.1270 - val_loss: 161.3157 - val_regression_loss: 63.4546\n","Epoch 10/50\n","537/537 [==============================] - 0s 58us/step - loss: 1338.1233 - regression_loss: 545.8884 - val_loss: 169.3934 - val_regression_loss: 67.7022\n","Epoch 11/50\n","537/537 [==============================] - 0s 51us/step - loss: 1440.3405 - regression_loss: 598.3962 - val_loss: 167.9660 - val_regression_loss: 67.1531\n","Epoch 12/50\n","537/537 [==============================] - 0s 51us/step - loss: 1440.7188 - regression_loss: 600.3749 - val_loss: 157.1161 - val_regression_loss: 61.8751\n","Epoch 13/50\n","537/537 [==============================] - 0s 54us/step - loss: 1321.8464 - regression_loss: 541.8353 - val_loss: 150.5065 - val_regression_loss: 58.6748\n","Epoch 14/50\n","537/537 [==============================] - 0s 59us/step - loss: 1246.7206 - regression_loss: 504.2445 - val_loss: 153.7981 - val_regression_loss: 60.3521\n","Epoch 15/50\n","537/537 [==============================] - 0s 62us/step - loss: 1198.4267 - regression_loss: 481.2812 - val_loss: 160.7572 - val_regression_loss: 63.8053\n","Epoch 16/50\n","537/537 [==============================] - 0s 61us/step - loss: 1255.3082 - regression_loss: 511.1140 - val_loss: 161.1891 - val_regression_loss: 63.9888\n","Epoch 17/50\n","537/537 [==============================] - 0s 57us/step - loss: 1295.6785 - regression_loss: 529.2315 - val_loss: 155.5415 - val_regression_loss: 61.1714\n","Epoch 18/50\n","537/537 [==============================] - 0s 52us/step - loss: 1238.3485 - regression_loss: 501.8115 - val_loss: 149.2745 - val_regression_loss: 58.0551\n","Epoch 19/50\n","537/537 [==============================] - 0s 55us/step - loss: 1191.9514 - regression_loss: 483.4013 - val_loss: 146.9003 - val_regression_loss: 56.8646\n","Epoch 20/50\n","537/537 [==============================] - 0s 54us/step - loss: 1176.0168 - regression_loss: 473.9532 - val_loss: 148.0143 - val_regression_loss: 57.4029\n","Epoch 21/50\n","537/537 [==============================] - 0s 61us/step - loss: 1183.6324 - regression_loss: 478.5732 - val_loss: 149.5769 - val_regression_loss: 58.1737\n","Epoch 22/50\n","537/537 [==============================] - 0s 65us/step - loss: 1215.1881 - regression_loss: 494.7041 - val_loss: 150.0938 - val_regression_loss: 58.4442\n","Epoch 23/50\n","537/537 [==============================] - 0s 58us/step - loss: 1186.2224 - regression_loss: 482.7492 - val_loss: 149.8971 - val_regression_loss: 58.3810\n","Epoch 24/50\n","537/537 [==============================] - 0s 53us/step - loss: 1181.5841 - regression_loss: 477.2755 - val_loss: 150.5180 - val_regression_loss: 58.7207\n","Epoch 25/50\n","537/537 [==============================] - 0s 64us/step - loss: 1178.2497 - regression_loss: 477.0663 - val_loss: 152.4360 - val_regression_loss: 59.6857\n","\n","Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 26/50\n","537/537 [==============================] - 0s 63us/step - loss: 1177.0473 - regression_loss: 474.4314 - val_loss: 153.2677 - val_regression_loss: 60.0906\n","Epoch 27/50\n","537/537 [==============================] - 0s 63us/step - loss: 1164.3880 - regression_loss: 469.2993 - val_loss: 153.8369 - val_regression_loss: 60.3540\n","Epoch 28/50\n","537/537 [==============================] - 0s 64us/step - loss: 1185.0709 - regression_loss: 479.6059 - val_loss: 154.1365 - val_regression_loss: 60.4774\n","Epoch 29/50\n","537/537 [==============================] - 0s 65us/step - loss: 1152.3280 - regression_loss: 463.0417 - val_loss: 154.1361 - val_regression_loss: 60.4481\n","Epoch 30/50\n","537/537 [==============================] - 0s 54us/step - loss: 1156.4752 - regression_loss: 466.5399 - val_loss: 153.9684 - val_regression_loss: 60.3426\n","Epoch 31/50\n","537/537 [==============================] - 0s 56us/step - loss: 1151.0205 - regression_loss: 465.2608 - val_loss: 153.8542 - val_regression_loss: 60.2704\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 1165.3337 - regression_loss: 470.8444 - val_loss: 153.6827 - val_regression_loss: 60.1817\n","Epoch 33/50\n","537/537 [==============================] - 0s 57us/step - loss: 1170.1989 - regression_loss: 471.5847 - val_loss: 153.5443 - val_regression_loss: 60.1149\n","Epoch 34/50\n","537/537 [==============================] - 0s 60us/step - loss: 1140.9319 - regression_loss: 460.9225 - val_loss: 153.5678 - val_regression_loss: 60.1349\n","Epoch 35/50\n","537/537 [==============================] - 0s 56us/step - loss: 1162.2693 - regression_loss: 471.0871 - val_loss: 153.6050 - val_regression_loss: 60.1607\n","Epoch 36/50\n","537/537 [==============================] - 0s 55us/step - loss: 1168.3827 - regression_loss: 472.6436 - val_loss: 153.6950 - val_regression_loss: 60.2103\n","Epoch 37/50\n","537/537 [==============================] - 0s 58us/step - loss: 1145.2631 - regression_loss: 462.5719 - val_loss: 154.0654 - val_regression_loss: 60.3953\n","Epoch 38/50\n","537/537 [==============================] - 0s 59us/step - loss: 1161.6822 - regression_loss: 469.5999 - val_loss: 154.5093 - val_regression_loss: 60.6106\n","Epoch 39/50\n","537/537 [==============================] - 0s 54us/step - loss: 1158.2291 - regression_loss: 468.0016 - val_loss: 154.9561 - val_regression_loss: 60.8201\n","\n","Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 40/50\n","537/537 [==============================] - 0s 57us/step - loss: 1162.6992 - regression_loss: 471.0013 - val_loss: 155.1320 - val_regression_loss: 60.8990\n","Epoch 41/50\n","537/537 [==============================] - 0s 60us/step - loss: 1156.5323 - regression_loss: 467.6433 - val_loss: 155.1597 - val_regression_loss: 60.9035\n","Epoch 42/50\n","537/537 [==============================] - 0s 58us/step - loss: 1151.0034 - regression_loss: 467.1942 - val_loss: 155.1591 - val_regression_loss: 60.8930\n","Epoch 43/50\n","537/537 [==============================] - 0s 60us/step - loss: 1123.4540 - regression_loss: 451.2625 - val_loss: 155.1145 - val_regression_loss: 60.8614\n","Epoch 44/50\n","537/537 [==============================] - 0s 57us/step - loss: 1148.7740 - regression_loss: 464.0037 - val_loss: 155.1149 - val_regression_loss: 60.8528\n","Epoch 45/50\n","537/537 [==============================] - 0s 54us/step - loss: 1154.7663 - regression_loss: 465.9308 - val_loss: 155.1113 - val_regression_loss: 60.8411\n","Epoch 46/50\n","537/537 [==============================] - 0s 58us/step - loss: 1153.3434 - regression_loss: 468.9340 - val_loss: 155.0880 - val_regression_loss: 60.8209\n","Epoch 47/50\n","537/537 [==============================] - 0s 61us/step - loss: 1144.3840 - regression_loss: 462.7974 - val_loss: 155.0583 - val_regression_loss: 60.8006\n","Epoch 48/50\n","537/537 [==============================] - 0s 59us/step - loss: 1123.7681 - regression_loss: 452.9694 - val_loss: 155.0406 - val_regression_loss: 60.7876\n","\n","Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 49/50\n","537/537 [==============================] - 0s 57us/step - loss: 1156.2058 - regression_loss: 468.8348 - val_loss: 155.0613 - val_regression_loss: 60.7950\n","Epoch 50/50\n","537/537 [==============================] - 0s 53us/step - loss: 1152.8072 - regression_loss: 465.7471 - val_loss: 155.0821 - val_regression_loss: 60.8023\n","***************************** elapsed_time is:  3.728386878967285\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 60935.9945 - regression_loss: 30158.5338 - val_loss: 7224.9922 - val_regression_loss: 3574.9109\n","Epoch 2/50\n","537/537 [==============================] - 0s 51us/step - loss: 50904.0523 - regression_loss: 25175.6868 - val_loss: 5628.1519 - val_regression_loss: 2781.7500\n","Epoch 3/50\n","537/537 [==============================] - 0s 54us/step - loss: 38824.6657 - regression_loss: 19177.7011 - val_loss: 3763.6355 - val_regression_loss: 1856.2626\n","Epoch 4/50\n","537/537 [==============================] - 0s 57us/step - loss: 24637.4828 - regression_loss: 12140.5593 - val_loss: 2148.2427 - val_regression_loss: 1056.1489\n","Epoch 5/50\n","537/537 [==============================] - 0s 58us/step - loss: 13404.4595 - regression_loss: 6581.0622 - val_loss: 1719.4491 - val_regression_loss: 847.6443\n","Epoch 6/50\n","537/537 [==============================] - 0s 53us/step - loss: 11297.9814 - regression_loss: 5573.6050 - val_loss: 2014.6495 - val_regression_loss: 994.4731\n","Epoch 7/50\n","537/537 [==============================] - 0s 58us/step - loss: 14289.2474 - regression_loss: 7063.2182 - val_loss: 1792.0256 - val_regression_loss: 879.0216\n","Epoch 8/50\n","537/537 [==============================] - 0s 57us/step - loss: 11993.6975 - regression_loss: 5883.4562 - val_loss: 1349.4534 - val_regression_loss: 654.2667\n","Epoch 9/50\n","537/537 [==============================] - 0s 58us/step - loss: 7496.3098 - regression_loss: 3607.5470 - val_loss: 1243.4506 - val_regression_loss: 598.6559\n","Epoch 10/50\n","537/537 [==============================] - 0s 56us/step - loss: 6428.8018 - regression_loss: 3052.0251 - val_loss: 1405.5691 - val_regression_loss: 678.3341\n","Epoch 11/50\n","537/537 [==============================] - 0s 61us/step - loss: 7921.0730 - regression_loss: 3786.7748 - val_loss: 1431.9537 - val_regression_loss: 692.1254\n","Epoch 12/50\n","537/537 [==============================] - 0s 63us/step - loss: 8336.7322 - regression_loss: 3997.8160 - val_loss: 1239.3976 - val_regression_loss: 597.9839\n","Epoch 13/50\n","537/537 [==============================] - 0s 64us/step - loss: 7062.8917 - regression_loss: 3375.9511 - val_loss: 1021.7927 - val_regression_loss: 491.7845\n","Epoch 14/50\n","537/537 [==============================] - 0s 66us/step - loss: 5503.4219 - regression_loss: 2615.3713 - val_loss: 948.3185 - val_regression_loss: 457.1471\n","Epoch 15/50\n","537/537 [==============================] - 0s 63us/step - loss: 4919.7186 - regression_loss: 2339.1518 - val_loss: 998.8232 - val_regression_loss: 483.5115\n","Epoch 16/50\n","537/537 [==============================] - 0s 60us/step - loss: 5406.7739 - regression_loss: 2590.9598 - val_loss: 1020.7563 - val_regression_loss: 494.6723\n","Epoch 17/50\n","537/537 [==============================] - 0s 62us/step - loss: 5596.8220 - regression_loss: 2685.6382 - val_loss: 946.0023 - val_regression_loss: 456.8058\n","Epoch 18/50\n","537/537 [==============================] - 0s 64us/step - loss: 5039.4727 - regression_loss: 2405.8686 - val_loss: 854.0470 - val_regression_loss: 409.8297\n","Epoch 19/50\n","537/537 [==============================] - 0s 63us/step - loss: 4097.5600 - regression_loss: 1924.6268 - val_loss: 827.1102 - val_regression_loss: 395.1482\n","Epoch 20/50\n","537/537 [==============================] - 0s 63us/step - loss: 3911.8858 - regression_loss: 1821.4859 - val_loss: 841.8105 - val_regression_loss: 401.5208\n","Epoch 21/50\n","537/537 [==============================] - 0s 67us/step - loss: 3899.9152 - regression_loss: 1807.0730 - val_loss: 831.6100 - val_regression_loss: 396.0585\n","Epoch 22/50\n","537/537 [==============================] - 0s 67us/step - loss: 4053.5420 - regression_loss: 1879.6971 - val_loss: 775.9003 - val_regression_loss: 368.5857\n","Epoch 23/50\n","537/537 [==============================] - 0s 62us/step - loss: 3592.6719 - regression_loss: 1652.5696 - val_loss: 719.0962 - val_regression_loss: 341.0549\n","Epoch 24/50\n","537/537 [==============================] - 0s 64us/step - loss: 3287.2803 - regression_loss: 1508.9710 - val_loss: 693.8154 - val_regression_loss: 329.4035\n","Epoch 25/50\n","537/537 [==============================] - 0s 60us/step - loss: 3164.8106 - regression_loss: 1451.9476 - val_loss: 685.0844 - val_regression_loss: 325.7953\n","Epoch 26/50\n","537/537 [==============================] - 0s 61us/step - loss: 3099.5978 - regression_loss: 1428.3863 - val_loss: 668.7939 - val_regression_loss: 317.8781\n","Epoch 27/50\n","537/537 [==============================] - 0s 68us/step - loss: 2924.4013 - regression_loss: 1341.8828 - val_loss: 652.3634 - val_regression_loss: 309.3509\n","Epoch 28/50\n","537/537 [==============================] - 0s 62us/step - loss: 2729.5899 - regression_loss: 1239.6604 - val_loss: 646.9017 - val_regression_loss: 306.1060\n","Epoch 29/50\n","537/537 [==============================] - 0s 58us/step - loss: 2598.5679 - regression_loss: 1168.2679 - val_loss: 636.2371 - val_regression_loss: 300.4113\n","Epoch 30/50\n","537/537 [==============================] - 0s 63us/step - loss: 2545.3496 - regression_loss: 1144.4359 - val_loss: 604.6542 - val_regression_loss: 284.6327\n","Epoch 31/50\n","537/537 [==============================] - 0s 57us/step - loss: 2362.1599 - regression_loss: 1049.5694 - val_loss: 567.2154 - val_regression_loss: 266.1387\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 2167.9250 - regression_loss: 957.6103 - val_loss: 542.2871 - val_regression_loss: 253.7994\n","Epoch 33/50\n","537/537 [==============================] - 0s 65us/step - loss: 2131.4729 - regression_loss: 939.9773 - val_loss: 524.7991 - val_regression_loss: 244.9756\n","Epoch 34/50\n","537/537 [==============================] - 0s 62us/step - loss: 2057.9891 - regression_loss: 903.6784 - val_loss: 509.5401 - val_regression_loss: 237.0398\n","Epoch 35/50\n","537/537 [==============================] - 0s 65us/step - loss: 1978.1033 - regression_loss: 861.2536 - val_loss: 500.9493 - val_regression_loss: 232.4884\n","Epoch 36/50\n","537/537 [==============================] - 0s 60us/step - loss: 1871.1421 - regression_loss: 808.1219 - val_loss: 490.6740 - val_regression_loss: 227.3036\n","Epoch 37/50\n","537/537 [==============================] - 0s 65us/step - loss: 1842.2097 - regression_loss: 792.1022 - val_loss: 470.9668 - val_regression_loss: 217.6802\n","Epoch 38/50\n","537/537 [==============================] - 0s 63us/step - loss: 1783.8798 - regression_loss: 766.4786 - val_loss: 448.8058 - val_regression_loss: 206.8912\n","Epoch 39/50\n","537/537 [==============================] - 0s 60us/step - loss: 1714.4224 - regression_loss: 734.7076 - val_loss: 432.0853 - val_regression_loss: 198.6598\n","Epoch 40/50\n","537/537 [==============================] - 0s 57us/step - loss: 1684.0844 - regression_loss: 721.7107 - val_loss: 418.4654 - val_regression_loss: 191.7183\n","Epoch 41/50\n","537/537 [==============================] - 0s 61us/step - loss: 1646.8959 - regression_loss: 699.4478 - val_loss: 408.7982 - val_regression_loss: 186.6758\n","Epoch 42/50\n","537/537 [==============================] - 0s 60us/step - loss: 1593.1581 - regression_loss: 674.3729 - val_loss: 400.3441 - val_regression_loss: 182.2988\n","Epoch 43/50\n","537/537 [==============================] - 0s 59us/step - loss: 1586.4371 - regression_loss: 667.5554 - val_loss: 387.3456 - val_regression_loss: 175.8996\n","Epoch 44/50\n","537/537 [==============================] - 0s 65us/step - loss: 1523.7659 - regression_loss: 636.9236 - val_loss: 373.3122 - val_regression_loss: 169.0919\n","Epoch 45/50\n","537/537 [==============================] - 0s 68us/step - loss: 1497.4434 - regression_loss: 625.8254 - val_loss: 363.0590 - val_regression_loss: 164.1548\n","Epoch 46/50\n","537/537 [==============================] - 0s 66us/step - loss: 1515.3677 - regression_loss: 632.6841 - val_loss: 356.5312 - val_regression_loss: 160.9522\n","Epoch 47/50\n","537/537 [==============================] - 0s 63us/step - loss: 1448.1217 - regression_loss: 601.5034 - val_loss: 350.2071 - val_regression_loss: 157.7553\n","Epoch 48/50\n","537/537 [==============================] - 0s 63us/step - loss: 1460.3523 - regression_loss: 607.4956 - val_loss: 345.8276 - val_regression_loss: 155.5173\n","Epoch 49/50\n","537/537 [==============================] - 0s 63us/step - loss: 1372.1075 - regression_loss: 561.9201 - val_loss: 335.3273 - val_regression_loss: 150.3244\n","Epoch 50/50\n","537/537 [==============================] - 0s 65us/step - loss: 1421.1575 - regression_loss: 588.6941 - val_loss: 328.5429 - val_regression_loss: 146.9885\n","***************************** elapsed_time is:  4.0474324226379395\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 95432.7901 - regression_loss: 47377.8775 - val_loss: 9319.6934 - val_regression_loss: 4623.3936\n","Epoch 2/50\n","537/537 [==============================] - 0s 58us/step - loss: 81370.6608 - regression_loss: 40386.9633 - val_loss: 7521.2549 - val_regression_loss: 3728.5723\n","Epoch 3/50\n","537/537 [==============================] - 0s 57us/step - loss: 66104.2199 - regression_loss: 32791.4895 - val_loss: 5366.8159 - val_regression_loss: 2656.6709\n","Epoch 4/50\n","537/537 [==============================] - 0s 57us/step - loss: 46499.3837 - regression_loss: 23032.0065 - val_loss: 3170.3831 - val_regression_loss: 1564.5231\n","Epoch 5/50\n","537/537 [==============================] - 0s 61us/step - loss: 27271.4017 - regression_loss: 13470.1658 - val_loss: 1704.8599 - val_regression_loss: 838.0347\n","Epoch 6/50\n","537/537 [==============================] - 0s 54us/step - loss: 12744.1047 - regression_loss: 6258.1194 - val_loss: 1789.0663 - val_regression_loss: 885.1219\n","Epoch 7/50\n","537/537 [==============================] - 0s 55us/step - loss: 11782.4987 - regression_loss: 5817.6001 - val_loss: 2235.3135 - val_regression_loss: 1108.8192\n","Epoch 8/50\n","537/537 [==============================] - 0s 60us/step - loss: 14886.6977 - regression_loss: 7370.5582 - val_loss: 1832.1925 - val_regression_loss: 904.1731\n","Epoch 9/50\n","537/537 [==============================] - 0s 66us/step - loss: 12695.4077 - regression_loss: 6245.5455 - val_loss: 1109.1633 - val_regression_loss: 538.8699\n","Epoch 10/50\n","537/537 [==============================] - 0s 55us/step - loss: 8411.6563 - regression_loss: 4070.3360 - val_loss: 623.8904 - val_regression_loss: 293.7963\n","Epoch 11/50\n","537/537 [==============================] - 0s 54us/step - loss: 5803.2254 - regression_loss: 2748.0703 - val_loss: 519.9677 - val_regression_loss: 240.8396\n","Epoch 12/50\n","537/537 [==============================] - 0s 55us/step - loss: 6026.1211 - regression_loss: 2851.7106 - val_loss: 599.5373 - val_regression_loss: 280.6349\n","Epoch 13/50\n","537/537 [==============================] - 0s 56us/step - loss: 6984.5035 - regression_loss: 3335.0096 - val_loss: 641.6909 - val_regression_loss: 302.3373\n","Epoch 14/50\n","537/537 [==============================] - 0s 53us/step - loss: 7472.1279 - regression_loss: 3585.9024 - val_loss: 592.9301 - val_regression_loss: 278.9486\n","Epoch 15/50\n","537/537 [==============================] - 0s 57us/step - loss: 6474.8987 - regression_loss: 3095.5179 - val_loss: 511.5605 - val_regression_loss: 239.3865\n","Epoch 16/50\n","537/537 [==============================] - 0s 65us/step - loss: 5479.6963 - regression_loss: 2610.2779 - val_loss: 466.4076 - val_regression_loss: 217.9026\n","Epoch 17/50\n","537/537 [==============================] - 0s 63us/step - loss: 4567.3311 - regression_loss: 2160.8477 - val_loss: 493.3608 - val_regression_loss: 232.2664\n","Epoch 18/50\n","537/537 [==============================] - 0s 55us/step - loss: 4096.2680 - regression_loss: 1932.4866 - val_loss: 568.1367 - val_regression_loss: 270.2144\n","Epoch 19/50\n","537/537 [==============================] - 0s 53us/step - loss: 4354.6178 - regression_loss: 2067.1507 - val_loss: 627.3077 - val_regression_loss: 299.9901\n","Epoch 20/50\n","537/537 [==============================] - 0s 61us/step - loss: 4560.9709 - regression_loss: 2171.9061 - val_loss: 623.9224 - val_regression_loss: 298.1676\n","Epoch 21/50\n","537/537 [==============================] - 0s 52us/step - loss: 4651.2933 - regression_loss: 2218.1471 - val_loss: 561.8318 - val_regression_loss: 266.7632\n","Epoch 22/50\n","537/537 [==============================] - 0s 50us/step - loss: 4245.6213 - regression_loss: 2010.4700 - val_loss: 483.3322 - val_regression_loss: 227.0312\n","Epoch 23/50\n","537/537 [==============================] - 0s 66us/step - loss: 3820.4153 - regression_loss: 1795.2658 - val_loss: 425.5487 - val_regression_loss: 197.6315\n","Epoch 24/50\n","537/537 [==============================] - 0s 53us/step - loss: 3717.4914 - regression_loss: 1737.6930 - val_loss: 397.0277 - val_regression_loss: 182.9290\n","Epoch 25/50\n","537/537 [==============================] - 0s 64us/step - loss: 3659.0364 - regression_loss: 1705.9888 - val_loss: 385.4507 - val_regression_loss: 176.8175\n","Epoch 26/50\n","537/537 [==============================] - 0s 60us/step - loss: 3655.3392 - regression_loss: 1700.5233 - val_loss: 377.3541 - val_regression_loss: 172.6015\n","Epoch 27/50\n","537/537 [==============================] - 0s 54us/step - loss: 3684.2123 - regression_loss: 1715.6564 - val_loss: 368.1775 - val_regression_loss: 167.9919\n","Epoch 28/50\n","537/537 [==============================] - 0s 51us/step - loss: 3409.6514 - regression_loss: 1574.6384 - val_loss: 362.7401 - val_regression_loss: 165.3770\n","Epoch 29/50\n","537/537 [==============================] - 0s 50us/step - loss: 3259.6375 - regression_loss: 1503.1153 - val_loss: 365.8575 - val_regression_loss: 167.1269\n","Epoch 30/50\n","537/537 [==============================] - 0s 58us/step - loss: 3210.3967 - regression_loss: 1478.0136 - val_loss: 375.4300 - val_regression_loss: 172.1516\n","Epoch 31/50\n","537/537 [==============================] - 0s 56us/step - loss: 3068.2923 - regression_loss: 1412.0533 - val_loss: 382.0686 - val_regression_loss: 175.6881\n","Epoch 32/50\n","537/537 [==============================] - 0s 56us/step - loss: 3008.2110 - regression_loss: 1382.2816 - val_loss: 379.3617 - val_regression_loss: 174.4672\n","Epoch 33/50\n","537/537 [==============================] - 0s 55us/step - loss: 2912.3757 - regression_loss: 1335.9673 - val_loss: 367.9937 - val_regression_loss: 168.8044\n","Epoch 34/50\n","537/537 [==============================] - 0s 57us/step - loss: 2820.2461 - regression_loss: 1290.4564 - val_loss: 353.0104 - val_regression_loss: 161.2159\n","Epoch 35/50\n","537/537 [==============================] - 0s 56us/step - loss: 2623.0270 - regression_loss: 1191.5043 - val_loss: 340.1629 - val_regression_loss: 154.6379\n","Epoch 36/50\n","537/537 [==============================] - 0s 54us/step - loss: 2614.4846 - regression_loss: 1185.9763 - val_loss: 329.2259 - val_regression_loss: 149.0026\n","Epoch 37/50\n","537/537 [==============================] - 0s 53us/step - loss: 2522.5377 - regression_loss: 1138.1786 - val_loss: 318.5610 - val_regression_loss: 143.5309\n","Epoch 38/50\n","537/537 [==============================] - 0s 59us/step - loss: 2442.4578 - regression_loss: 1096.0593 - val_loss: 309.0406 - val_regression_loss: 138.6655\n","Epoch 39/50\n","537/537 [==============================] - 0s 64us/step - loss: 2391.0535 - regression_loss: 1069.7088 - val_loss: 303.1232 - val_regression_loss: 135.6440\n","Epoch 40/50\n","537/537 [==============================] - 0s 77us/step - loss: 2323.7127 - regression_loss: 1036.9914 - val_loss: 297.3821 - val_regression_loss: 132.7253\n","Epoch 41/50\n","537/537 [==============================] - 0s 62us/step - loss: 2323.6196 - regression_loss: 1035.4484 - val_loss: 289.6999 - val_regression_loss: 128.8393\n","Epoch 42/50\n","537/537 [==============================] - 0s 57us/step - loss: 2272.0318 - regression_loss: 1010.2231 - val_loss: 280.5704 - val_regression_loss: 124.2268\n","Epoch 43/50\n","537/537 [==============================] - 0s 57us/step - loss: 2205.9582 - regression_loss: 976.3592 - val_loss: 273.6383 - val_regression_loss: 120.7199\n","Epoch 44/50\n","537/537 [==============================] - 0s 57us/step - loss: 2163.4240 - regression_loss: 954.9660 - val_loss: 269.6666 - val_regression_loss: 118.7428\n","Epoch 45/50\n","537/537 [==============================] - 0s 59us/step - loss: 2109.0244 - regression_loss: 927.2550 - val_loss: 265.1166 - val_regression_loss: 116.5099\n","Epoch 46/50\n","537/537 [==============================] - 0s 63us/step - loss: 2079.0703 - regression_loss: 915.8544 - val_loss: 261.6758 - val_regression_loss: 114.8455\n","Epoch 47/50\n","537/537 [==============================] - 0s 64us/step - loss: 2047.6932 - regression_loss: 899.5191 - val_loss: 255.6043 - val_regression_loss: 111.8338\n","Epoch 48/50\n","537/537 [==============================] - 0s 59us/step - loss: 1960.7583 - regression_loss: 856.0708 - val_loss: 247.5571 - val_regression_loss: 107.7975\n","Epoch 49/50\n","537/537 [==============================] - 0s 61us/step - loss: 1960.8607 - regression_loss: 858.8069 - val_loss: 239.5728 - val_regression_loss: 103.7597\n","Epoch 50/50\n","537/537 [==============================] - 0s 69us/step - loss: 1930.6507 - regression_loss: 841.4287 - val_loss: 233.1995 - val_regression_loss: 100.5196\n","***************************** elapsed_time is:  3.6686763763427734\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 9958.9671 - regression_loss: 4852.3554 - val_loss: 918.6310 - val_regression_loss: 438.0118\n","Epoch 2/50\n","537/537 [==============================] - 0s 61us/step - loss: 6362.3926 - regression_loss: 2998.0833 - val_loss: 569.5516 - val_regression_loss: 255.4239\n","Epoch 3/50\n","537/537 [==============================] - 0s 56us/step - loss: 4065.8369 - regression_loss: 1783.5058 - val_loss: 405.2125 - val_regression_loss: 169.3244\n","Epoch 4/50\n","537/537 [==============================] - 0s 55us/step - loss: 3157.8433 - regression_loss: 1299.0013 - val_loss: 269.7408 - val_regression_loss: 108.5383\n","Epoch 5/50\n","537/537 [==============================] - 0s 65us/step - loss: 1971.1709 - regression_loss: 767.9506 - val_loss: 295.7282 - val_regression_loss: 132.7873\n","Epoch 6/50\n","537/537 [==============================] - 0s 55us/step - loss: 1874.9619 - regression_loss: 813.9334 - val_loss: 406.2027 - val_regression_loss: 194.3277\n","Epoch 7/50\n","537/537 [==============================] - 0s 60us/step - loss: 2530.5855 - regression_loss: 1193.8994 - val_loss: 341.9442 - val_regression_loss: 160.9067\n","Epoch 8/50\n","537/537 [==============================] - 0s 57us/step - loss: 2101.7713 - regression_loss: 970.3035 - val_loss: 242.3325 - val_regression_loss: 106.8625\n","Epoch 9/50\n","537/537 [==============================] - 0s 58us/step - loss: 1511.6887 - regression_loss: 638.4011 - val_loss: 209.1701 - val_regression_loss: 86.2095\n","Epoch 10/50\n","537/537 [==============================] - 0s 55us/step - loss: 1439.6111 - regression_loss: 570.1310 - val_loss: 216.2469 - val_regression_loss: 87.3465\n","Epoch 11/50\n","537/537 [==============================] - 0s 56us/step - loss: 1627.2466 - regression_loss: 642.7602 - val_loss: 224.7708 - val_regression_loss: 91.1893\n","Epoch 12/50\n","537/537 [==============================] - 0s 61us/step - loss: 1659.5578 - regression_loss: 655.1295 - val_loss: 228.3167 - val_regression_loss: 93.9876\n","Epoch 13/50\n","537/537 [==============================] - 0s 59us/step - loss: 1527.9425 - regression_loss: 602.2357 - val_loss: 224.4459 - val_regression_loss: 93.8506\n","Epoch 14/50\n","537/537 [==============================] - 0s 59us/step - loss: 1425.9149 - regression_loss: 565.3127 - val_loss: 217.4426 - val_regression_loss: 92.5446\n","Epoch 15/50\n","537/537 [==============================] - 0s 59us/step - loss: 1350.3735 - regression_loss: 548.5417 - val_loss: 214.6053 - val_regression_loss: 93.1599\n","Epoch 16/50\n","537/537 [==============================] - 0s 55us/step - loss: 1333.8166 - regression_loss: 556.1094 - val_loss: 213.9095 - val_regression_loss: 94.0950\n","Epoch 17/50\n","537/537 [==============================] - 0s 58us/step - loss: 1370.3405 - regression_loss: 589.6659 - val_loss: 206.2095 - val_regression_loss: 90.2317\n","Epoch 18/50\n","537/537 [==============================] - 0s 60us/step - loss: 1361.7419 - regression_loss: 582.6562 - val_loss: 196.3157 - val_regression_loss: 84.2866\n","Epoch 19/50\n","537/537 [==============================] - 0s 96us/step - loss: 1324.0116 - regression_loss: 558.0697 - val_loss: 188.1892 - val_regression_loss: 78.7549\n","Epoch 20/50\n","537/537 [==============================] - 0s 64us/step - loss: 1300.5877 - regression_loss: 531.4272 - val_loss: 184.0744 - val_regression_loss: 75.4001\n","Epoch 21/50\n","537/537 [==============================] - 0s 59us/step - loss: 1273.5295 - regression_loss: 509.3761 - val_loss: 183.7376 - val_regression_loss: 74.4272\n","Epoch 22/50\n","537/537 [==============================] - 0s 62us/step - loss: 1265.9881 - regression_loss: 497.5887 - val_loss: 186.0202 - val_regression_loss: 75.4680\n","Epoch 23/50\n","537/537 [==============================] - 0s 65us/step - loss: 1290.2232 - regression_loss: 508.6862 - val_loss: 187.3180 - val_regression_loss: 76.6746\n","Epoch 24/50\n","537/537 [==============================] - 0s 65us/step - loss: 1275.4277 - regression_loss: 508.4907 - val_loss: 187.0103 - val_regression_loss: 77.3800\n","Epoch 25/50\n","537/537 [==============================] - 0s 63us/step - loss: 1249.2386 - regression_loss: 503.5351 - val_loss: 186.3971 - val_regression_loss: 77.9225\n","Epoch 26/50\n","537/537 [==============================] - 0s 69us/step - loss: 1216.9182 - regression_loss: 490.7585 - val_loss: 184.4921 - val_regression_loss: 77.5000\n","Epoch 27/50\n","537/537 [==============================] - 0s 65us/step - loss: 1237.1133 - regression_loss: 509.2020 - val_loss: 179.7366 - val_regression_loss: 75.1429\n","Epoch 28/50\n","537/537 [==============================] - 0s 61us/step - loss: 1255.4426 - regression_loss: 515.0734 - val_loss: 177.6662 - val_regression_loss: 73.8574\n","Epoch 29/50\n","537/537 [==============================] - 0s 61us/step - loss: 1242.2326 - regression_loss: 507.9204 - val_loss: 177.5578 - val_regression_loss: 73.3843\n","Epoch 30/50\n","537/537 [==============================] - 0s 60us/step - loss: 1222.4871 - regression_loss: 496.3210 - val_loss: 178.5893 - val_regression_loss: 73.4796\n","Epoch 31/50\n","537/537 [==============================] - 0s 58us/step - loss: 1213.6252 - regression_loss: 488.4777 - val_loss: 180.3689 - val_regression_loss: 74.1711\n","Epoch 32/50\n","537/537 [==============================] - 0s 60us/step - loss: 1205.9112 - regression_loss: 483.0185 - val_loss: 179.3125 - val_regression_loss: 73.6502\n","Epoch 33/50\n","537/537 [==============================] - 0s 60us/step - loss: 1206.4508 - regression_loss: 484.5853 - val_loss: 177.0860 - val_regression_loss: 72.7151\n","Epoch 34/50\n","537/537 [==============================] - 0s 58us/step - loss: 1201.5675 - regression_loss: 480.9634 - val_loss: 175.6941 - val_regression_loss: 72.3474\n","Epoch 35/50\n","537/537 [==============================] - 0s 61us/step - loss: 1195.0153 - regression_loss: 483.1270 - val_loss: 176.2605 - val_regression_loss: 72.9136\n","Epoch 36/50\n","537/537 [==============================] - 0s 59us/step - loss: 1210.4645 - regression_loss: 490.4814 - val_loss: 176.6487 - val_regression_loss: 73.1972\n","Epoch 37/50\n","537/537 [==============================] - 0s 64us/step - loss: 1186.1945 - regression_loss: 479.6223 - val_loss: 177.1603 - val_regression_loss: 73.3861\n","Epoch 38/50\n","537/537 [==============================] - 0s 72us/step - loss: 1189.5824 - regression_loss: 483.4387 - val_loss: 176.6631 - val_regression_loss: 72.8987\n","Epoch 39/50\n","537/537 [==============================] - 0s 65us/step - loss: 1188.5897 - regression_loss: 477.8916 - val_loss: 176.7258 - val_regression_loss: 72.7435\n","Epoch 40/50\n","537/537 [==============================] - 0s 58us/step - loss: 1169.1315 - regression_loss: 468.3592 - val_loss: 177.1319 - val_regression_loss: 72.8950\n","Epoch 41/50\n","537/537 [==============================] - 0s 60us/step - loss: 1190.0392 - regression_loss: 480.1527 - val_loss: 175.0287 - val_regression_loss: 71.8260\n","Epoch 42/50\n","537/537 [==============================] - 0s 61us/step - loss: 1186.1604 - regression_loss: 476.6913 - val_loss: 174.1693 - val_regression_loss: 71.4938\n","Epoch 43/50\n","537/537 [==============================] - 0s 64us/step - loss: 1167.9827 - regression_loss: 469.1623 - val_loss: 174.2076 - val_regression_loss: 71.6662\n","Epoch 44/50\n","537/537 [==============================] - 0s 59us/step - loss: 1177.4284 - regression_loss: 473.4023 - val_loss: 175.7974 - val_regression_loss: 72.6087\n","Epoch 45/50\n","537/537 [==============================] - 0s 62us/step - loss: 1172.4284 - regression_loss: 470.9934 - val_loss: 175.9637 - val_regression_loss: 72.6247\n","Epoch 46/50\n","537/537 [==============================] - 0s 62us/step - loss: 1176.5342 - regression_loss: 475.9501 - val_loss: 175.9225 - val_regression_loss: 72.4845\n","Epoch 47/50\n","537/537 [==============================] - 0s 61us/step - loss: 1187.5318 - regression_loss: 478.4199 - val_loss: 175.0536 - val_regression_loss: 71.9362\n","Epoch 48/50\n","537/537 [==============================] - 0s 60us/step - loss: 1159.3529 - regression_loss: 465.7092 - val_loss: 175.4443 - val_regression_loss: 72.1361\n","Epoch 49/50\n","537/537 [==============================] - 0s 59us/step - loss: 1166.6065 - regression_loss: 469.8358 - val_loss: 175.1467 - val_regression_loss: 72.0235\n","Epoch 50/50\n","537/537 [==============================] - 0s 57us/step - loss: 1162.5734 - regression_loss: 466.9622 - val_loss: 174.3230 - val_regression_loss: 71.6588\n","***************************** elapsed_time is:  3.7219748497009277\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 95525.4158 - regression_loss: 47444.6272 - val_loss: 10056.4043 - val_regression_loss: 4989.6182\n","Epoch 2/50\n","537/537 [==============================] - 0s 68us/step - loss: 81615.3260 - regression_loss: 40514.7609 - val_loss: 8060.5352 - val_regression_loss: 3995.3787\n","Epoch 3/50\n","537/537 [==============================] - 0s 67us/step - loss: 65123.0999 - regression_loss: 32296.8438 - val_loss: 5564.5874 - val_regression_loss: 2752.2371\n","Epoch 4/50\n","537/537 [==============================] - 0s 57us/step - loss: 45779.4622 - regression_loss: 22664.9953 - val_loss: 3083.8330 - val_regression_loss: 1518.1829\n","Epoch 5/50\n","537/537 [==============================] - 0s 58us/step - loss: 25776.6546 - regression_loss: 12715.0877 - val_loss: 1947.8202 - val_regression_loss: 957.7056\n","Epoch 6/50\n","537/537 [==============================] - 0s 62us/step - loss: 16126.4444 - regression_loss: 7951.8094 - val_loss: 2731.2168 - val_regression_loss: 1354.3420\n","Epoch 7/50\n","537/537 [==============================] - 0s 70us/step - loss: 21852.3880 - regression_loss: 10852.6847 - val_loss: 2574.3186 - val_regression_loss: 1274.7466\n","Epoch 8/50\n","537/537 [==============================] - 0s 63us/step - loss: 19811.4411 - regression_loss: 9821.0634 - val_loss: 1721.7911 - val_regression_loss: 844.7536\n","Epoch 9/50\n","537/537 [==============================] - 0s 61us/step - loss: 12757.7820 - regression_loss: 6264.2670 - val_loss: 1277.5787 - val_regression_loss: 618.8731\n","Epoch 10/50\n","537/537 [==============================] - 0s 60us/step - loss: 9461.5990 - regression_loss: 4584.1521 - val_loss: 1344.3184 - val_regression_loss: 649.6837\n","Epoch 11/50\n","537/537 [==============================] - 0s 60us/step - loss: 10202.9042 - regression_loss: 4935.1896 - val_loss: 1523.3340 - val_regression_loss: 738.2783\n","Epoch 12/50\n","537/537 [==============================] - 0s 62us/step - loss: 11782.0046 - regression_loss: 5718.3879 - val_loss: 1518.8950 - val_regression_loss: 736.6091\n","Epoch 13/50\n","537/537 [==============================] - 0s 59us/step - loss: 12110.5287 - regression_loss: 5886.5447 - val_loss: 1310.7540 - val_regression_loss: 634.0492\n","Epoch 14/50\n","537/537 [==============================] - 0s 71us/step - loss: 10226.9510 - regression_loss: 4958.3914 - val_loss: 1053.7473 - val_regression_loss: 507.4403\n","Epoch 15/50\n","537/537 [==============================] - 0s 60us/step - loss: 8258.8512 - regression_loss: 3990.1732 - val_loss: 911.7630 - val_regression_loss: 438.2711\n","Epoch 16/50\n","537/537 [==============================] - 0s 61us/step - loss: 7276.2116 - regression_loss: 3513.3567 - val_loss: 931.0917 - val_regression_loss: 449.4122\n","Epoch 17/50\n","537/537 [==============================] - 0s 64us/step - loss: 7464.1734 - regression_loss: 3620.4684 - val_loss: 994.4997 - val_regression_loss: 482.1243\n","Epoch 18/50\n","537/537 [==============================] - 0s 64us/step - loss: 7682.3980 - regression_loss: 3734.2408 - val_loss: 967.3525 - val_regression_loss: 469.0327\n","Epoch 19/50\n","537/537 [==============================] - 0s 60us/step - loss: 7628.0374 - regression_loss: 3712.1789 - val_loss: 868.9761 - val_regression_loss: 419.7861\n","Epoch 20/50\n","537/537 [==============================] - 0s 64us/step - loss: 6741.7106 - regression_loss: 3266.3950 - val_loss: 791.9153 - val_regression_loss: 380.7542\n","Epoch 21/50\n","537/537 [==============================] - 0s 60us/step - loss: 6150.6637 - regression_loss: 2967.8376 - val_loss: 766.8878 - val_regression_loss: 367.5310\n","Epoch 22/50\n","537/537 [==============================] - 0s 59us/step - loss: 6014.7719 - regression_loss: 2890.5209 - val_loss: 746.3295 - val_regression_loss: 356.5900\n","Epoch 23/50\n","537/537 [==============================] - 0s 62us/step - loss: 5996.9609 - regression_loss: 2875.8583 - val_loss: 691.7665 - val_regression_loss: 328.8656\n","Epoch 24/50\n","537/537 [==============================] - 0s 77us/step - loss: 5497.8447 - regression_loss: 2623.4438 - val_loss: 624.0118 - val_regression_loss: 294.7644\n","Epoch 25/50\n","537/537 [==============================] - 0s 62us/step - loss: 4942.8688 - regression_loss: 2346.7333 - val_loss: 581.0515 - val_regression_loss: 273.2519\n","Epoch 26/50\n","537/537 [==============================] - 0s 59us/step - loss: 4560.0807 - regression_loss: 2152.4671 - val_loss: 562.2310 - val_regression_loss: 263.9818\n","Epoch 27/50\n","537/537 [==============================] - 0s 59us/step - loss: 4481.3155 - regression_loss: 2115.6708 - val_loss: 530.4161 - val_regression_loss: 248.2990\n","Epoch 28/50\n","537/537 [==============================] - 0s 68us/step - loss: 4111.7303 - regression_loss: 1934.1046 - val_loss: 478.6042 - val_regression_loss: 222.5515\n","Epoch 29/50\n","537/537 [==============================] - 0s 64us/step - loss: 3697.5083 - regression_loss: 1726.6520 - val_loss: 436.7277 - val_regression_loss: 201.6318\n","Epoch 30/50\n","537/537 [==============================] - 0s 66us/step - loss: 3470.7240 - regression_loss: 1611.3439 - val_loss: 416.0139 - val_regression_loss: 191.3200\n","Epoch 31/50\n","537/537 [==============================] - 0s 65us/step - loss: 3575.8600 - regression_loss: 1667.5628 - val_loss: 395.2108 - val_regression_loss: 181.1577\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 3393.5126 - regression_loss: 1574.7404 - val_loss: 377.3651 - val_regression_loss: 172.6385\n","Epoch 33/50\n","537/537 [==============================] - 0s 61us/step - loss: 3227.1478 - regression_loss: 1496.1492 - val_loss: 373.7268 - val_regression_loss: 171.1120\n","Epoch 34/50\n","537/537 [==============================] - 0s 63us/step - loss: 3054.2160 - regression_loss: 1411.7000 - val_loss: 362.5721 - val_regression_loss: 165.4951\n","Epoch 35/50\n","537/537 [==============================] - 0s 63us/step - loss: 2869.3060 - regression_loss: 1316.7327 - val_loss: 338.8644 - val_regression_loss: 153.2945\n","Epoch 36/50\n","537/537 [==============================] - 0s 62us/step - loss: 2785.9951 - regression_loss: 1272.1156 - val_loss: 322.2199 - val_regression_loss: 144.6298\n","Epoch 37/50\n","537/537 [==============================] - 0s 61us/step - loss: 2720.5799 - regression_loss: 1236.9550 - val_loss: 314.7365 - val_regression_loss: 140.8269\n","Epoch 38/50\n","537/537 [==============================] - 0s 62us/step - loss: 2649.7211 - regression_loss: 1200.5976 - val_loss: 310.5057 - val_regression_loss: 138.9398\n","Epoch 39/50\n","537/537 [==============================] - 0s 64us/step - loss: 2373.8763 - regression_loss: 1064.3135 - val_loss: 306.7147 - val_regression_loss: 137.2972\n","Epoch 40/50\n","537/537 [==============================] - 0s 62us/step - loss: 2486.9772 - regression_loss: 1124.1018 - val_loss: 296.3396 - val_regression_loss: 132.2344\n","Epoch 41/50\n","537/537 [==============================] - 0s 62us/step - loss: 2237.1255 - regression_loss: 999.6365 - val_loss: 275.0508 - val_regression_loss: 121.4972\n","Epoch 42/50\n","537/537 [==============================] - 0s 67us/step - loss: 2371.0628 - regression_loss: 1066.3625 - val_loss: 259.6728 - val_regression_loss: 113.7047\n","Epoch 43/50\n","537/537 [==============================] - 0s 63us/step - loss: 2309.3639 - regression_loss: 1032.9391 - val_loss: 254.0695 - val_regression_loss: 110.9413\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 2219.9622 - regression_loss: 988.8211 - val_loss: 255.4972 - val_regression_loss: 111.7724\n","Epoch 45/50\n","537/537 [==============================] - 0s 59us/step - loss: 2159.1733 - regression_loss: 960.4073 - val_loss: 255.5362 - val_regression_loss: 111.8094\n","Epoch 46/50\n","537/537 [==============================] - 0s 63us/step - loss: 2118.2868 - regression_loss: 941.0838 - val_loss: 246.9805 - val_regression_loss: 107.3908\n","Epoch 47/50\n","537/537 [==============================] - 0s 73us/step - loss: 2066.1559 - regression_loss: 913.5048 - val_loss: 235.5220 - val_regression_loss: 101.4794\n","Epoch 48/50\n","537/537 [==============================] - 0s 69us/step - loss: 2035.2593 - regression_loss: 896.7893 - val_loss: 231.3136 - val_regression_loss: 99.3585\n","Epoch 49/50\n","537/537 [==============================] - 0s 65us/step - loss: 1963.1630 - regression_loss: 861.6404 - val_loss: 229.5219 - val_regression_loss: 98.5136\n","Epoch 50/50\n","537/537 [==============================] - 0s 61us/step - loss: 1943.0011 - regression_loss: 850.0280 - val_loss: 229.0787 - val_regression_loss: 98.3580\n","***************************** elapsed_time is:  4.1327619552612305\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 802214.2704 - regression_loss: 400728.9166 - val_loss: 95189.6953 - val_regression_loss: 47545.1680\n","Epoch 2/50\n","537/537 [==============================] - 0s 56us/step - loss: 782175.8897 - regression_loss: 390721.3067 - val_loss: 89298.4453 - val_regression_loss: 44603.4258\n","Epoch 3/50\n","537/537 [==============================] - 0s 60us/step - loss: 742802.2674 - regression_loss: 371063.0809 - val_loss: 81323.2188 - val_regression_loss: 40620.8125\n","Epoch 4/50\n","537/537 [==============================] - 0s 52us/step - loss: 681363.3982 - regression_loss: 340387.2329 - val_loss: 70927.6953 - val_regression_loss: 35429.0703\n","Epoch 5/50\n","537/537 [==============================] - 0s 52us/step - loss: 585845.2314 - regression_loss: 292670.1086 - val_loss: 58401.1406 - val_regression_loss: 29172.6270\n","Epoch 6/50\n","537/537 [==============================] - 0s 55us/step - loss: 495383.9856 - regression_loss: 247494.4316 - val_loss: 44822.7578 - val_regression_loss: 22390.4824\n","Epoch 7/50\n","537/537 [==============================] - 0s 54us/step - loss: 387823.4171 - regression_loss: 193767.9740 - val_loss: 32063.8105 - val_regression_loss: 16017.5508\n","Epoch 8/50\n","537/537 [==============================] - 0s 60us/step - loss: 291623.1335 - regression_loss: 145718.5094 - val_loss: 22983.4941 - val_regression_loss: 11482.5957\n","Epoch 9/50\n","537/537 [==============================] - 0s 62us/step - loss: 229127.0190 - regression_loss: 114510.1059 - val_loss: 20955.3145 - val_regression_loss: 10470.5693\n","Epoch 10/50\n","537/537 [==============================] - 0s 65us/step - loss: 217305.9007 - regression_loss: 108610.9267 - val_loss: 22755.7656 - val_regression_loss: 11364.3936\n","Epoch 11/50\n","537/537 [==============================] - 0s 63us/step - loss: 232751.8809 - regression_loss: 116257.8511 - val_loss: 21109.9590 - val_regression_loss: 10527.3877\n","Epoch 12/50\n","537/537 [==============================] - 0s 61us/step - loss: 223029.1932 - regression_loss: 111249.8261 - val_loss: 17681.4785 - val_regression_loss: 8806.2305\n","Epoch 13/50\n","537/537 [==============================] - 0s 63us/step - loss: 189232.5853 - regression_loss: 94268.5435 - val_loss: 15657.0703 - val_regression_loss: 7797.2949\n","Epoch 14/50\n","537/537 [==============================] - 0s 62us/step - loss: 168319.3339 - regression_loss: 83876.7302 - val_loss: 15665.4229 - val_regression_loss: 7806.7354\n","Epoch 15/50\n","537/537 [==============================] - 0s 59us/step - loss: 160766.2058 - regression_loss: 80148.3478 - val_loss: 16467.4863 - val_regression_loss: 8211.3076\n","Epoch 16/50\n","537/537 [==============================] - 0s 62us/step - loss: 157775.6826 - regression_loss: 78696.9876 - val_loss: 16793.2480 - val_regression_loss: 8376.0029\n","Epoch 17/50\n","537/537 [==============================] - 0s 61us/step - loss: 167725.5420 - regression_loss: 83697.3309 - val_loss: 16109.4258 - val_regression_loss: 8035.0864\n","Epoch 18/50\n","537/537 [==============================] - 0s 59us/step - loss: 160157.2760 - regression_loss: 79920.6880 - val_loss: 14651.7246 - val_regression_loss: 7306.9673\n","Epoch 19/50\n","537/537 [==============================] - 0s 61us/step - loss: 136344.3881 - regression_loss: 68023.8231 - val_loss: 13189.3574 - val_regression_loss: 6576.3691\n","Epoch 20/50\n","537/537 [==============================] - 0s 62us/step - loss: 128345.5141 - regression_loss: 64027.9024 - val_loss: 12462.4893 - val_regression_loss: 6213.3774\n","Epoch 21/50\n","537/537 [==============================] - 0s 71us/step - loss: 118096.1090 - regression_loss: 58908.7029 - val_loss: 12588.1182 - val_regression_loss: 6276.4238\n","Epoch 22/50\n","537/537 [==============================] - 0s 62us/step - loss: 115319.1488 - regression_loss: 57523.6719 - val_loss: 12702.8037 - val_regression_loss: 6333.7510\n","Epoch 23/50\n","537/537 [==============================] - 0s 63us/step - loss: 118546.5129 - regression_loss: 59137.2913 - val_loss: 12028.5645 - val_regression_loss: 5996.3662\n","Epoch 24/50\n","537/537 [==============================] - 0s 61us/step - loss: 111016.5899 - regression_loss: 55372.2359 - val_loss: 10831.3252 - val_regression_loss: 5397.3032\n","Epoch 25/50\n","537/537 [==============================] - 0s 58us/step - loss: 101938.2071 - regression_loss: 50827.4181 - val_loss: 9856.8301 - val_regression_loss: 4909.5825\n","Epoch 26/50\n","537/537 [==============================] - 0s 58us/step - loss: 95565.5559 - regression_loss: 47635.7092 - val_loss: 9202.0791 - val_regression_loss: 4581.8843\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 92796.4174 - regression_loss: 46248.4242 - val_loss: 8580.5781 - val_regression_loss: 4271.0996\n","Epoch 28/50\n","537/537 [==============================] - 0s 58us/step - loss: 83384.7419 - regression_loss: 41542.1197 - val_loss: 8002.3721 - val_regression_loss: 3982.2102\n","Epoch 29/50\n","537/537 [==============================] - 0s 56us/step - loss: 70798.2003 - regression_loss: 35249.1704 - val_loss: 7688.7334 - val_regression_loss: 3825.7356\n","Epoch 30/50\n","537/537 [==============================] - 0s 59us/step - loss: 55322.5980 - regression_loss: 27513.3891 - val_loss: 7559.0586 - val_regression_loss: 3761.2251\n","Epoch 31/50\n","537/537 [==============================] - 0s 61us/step - loss: 64835.4956 - regression_loss: 32273.2068 - val_loss: 7140.5811 - val_regression_loss: 3552.1738\n","Epoch 32/50\n","537/537 [==============================] - 0s 58us/step - loss: 58408.5244 - regression_loss: 29059.7184 - val_loss: 6138.0854 - val_regression_loss: 3050.9104\n","Epoch 33/50\n","537/537 [==============================] - 0s 61us/step - loss: 52064.8159 - regression_loss: 25887.5534 - val_loss: 5160.8804 - val_regression_loss: 2562.2524\n","Epoch 34/50\n","537/537 [==============================] - 0s 65us/step - loss: 44687.2622 - regression_loss: 22200.2538 - val_loss: 4485.2778 - val_regression_loss: 2224.5188\n","Epoch 35/50\n","537/537 [==============================] - 0s 74us/step - loss: 41775.6626 - regression_loss: 20743.0481 - val_loss: 4103.2319 - val_regression_loss: 2033.7769\n","Epoch 36/50\n","537/537 [==============================] - 0s 62us/step - loss: 39593.9120 - regression_loss: 19655.8931 - val_loss: 4005.2627 - val_regression_loss: 1985.1357\n","Epoch 37/50\n","537/537 [==============================] - 0s 65us/step - loss: 36192.1886 - regression_loss: 17959.3523 - val_loss: 3777.0098 - val_regression_loss: 1871.2021\n","Epoch 38/50\n","537/537 [==============================] - 0s 66us/step - loss: 33053.0835 - regression_loss: 16392.4245 - val_loss: 3173.2664 - val_regression_loss: 1569.3149\n","Epoch 39/50\n","537/537 [==============================] - 0s 65us/step - loss: 29926.6568 - regression_loss: 14829.2848 - val_loss: 2609.8811 - val_regression_loss: 1287.5206\n","Epoch 40/50\n","537/537 [==============================] - 0s 64us/step - loss: 27818.8974 - regression_loss: 13776.4187 - val_loss: 2365.2292 - val_regression_loss: 1165.2101\n","Epoch 41/50\n","537/537 [==============================] - 0s 66us/step - loss: 25898.7008 - regression_loss: 12815.2922 - val_loss: 2361.0610 - val_regression_loss: 1163.2782\n","Epoch 42/50\n","537/537 [==============================] - 0s 66us/step - loss: 23960.5559 - regression_loss: 11848.0560 - val_loss: 2410.0217 - val_regression_loss: 1187.8876\n","Epoch 43/50\n","537/537 [==============================] - 0s 66us/step - loss: 22596.0346 - regression_loss: 11170.5269 - val_loss: 2187.9382 - val_regression_loss: 1076.8369\n","Epoch 44/50\n","537/537 [==============================] - 0s 68us/step - loss: 20925.6166 - regression_loss: 10333.7922 - val_loss: 1918.7476 - val_regression_loss: 942.1907\n","Epoch 45/50\n","537/537 [==============================] - 0s 61us/step - loss: 19190.5426 - regression_loss: 9466.8507 - val_loss: 1788.6459 - val_regression_loss: 877.1423\n","Epoch 46/50\n","537/537 [==============================] - 0s 59us/step - loss: 18090.5941 - regression_loss: 8916.8855 - val_loss: 1815.8712 - val_regression_loss: 890.8230\n","Epoch 47/50\n","537/537 [==============================] - 0s 63us/step - loss: 17016.1484 - regression_loss: 8381.6706 - val_loss: 1797.2605 - val_regression_loss: 881.5309\n","Epoch 48/50\n","537/537 [==============================] - 0s 62us/step - loss: 15885.3696 - regression_loss: 7813.6186 - val_loss: 1678.7789 - val_regression_loss: 822.2394\n","Epoch 49/50\n","537/537 [==============================] - 0s 61us/step - loss: 14648.8338 - regression_loss: 7198.8155 - val_loss: 1611.5823 - val_regression_loss: 788.6018\n","Epoch 50/50\n","537/537 [==============================] - 0s 57us/step - loss: 9000.3473 - regression_loss: 4376.0728 - val_loss: 1571.5470 - val_regression_loss: 768.5645\n","***************************** elapsed_time is:  3.756725788116455\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 40980.6929 - regression_loss: 20460.8502 - val_loss: 4344.8398 - val_regression_loss: 2164.0322\n","Epoch 2/50\n","537/537 [==============================] - 0s 64us/step - loss: 32514.4765 - regression_loss: 16203.4509 - val_loss: 3222.2927 - val_regression_loss: 1597.8695\n","Epoch 3/50\n","537/537 [==============================] - 0s 62us/step - loss: 23943.9618 - regression_loss: 11876.6311 - val_loss: 2001.8748 - val_regression_loss: 979.7089\n","Epoch 4/50\n","537/537 [==============================] - 0s 64us/step - loss: 14011.0708 - regression_loss: 6850.2327 - val_loss: 1062.4084 - val_regression_loss: 497.1576\n","Epoch 5/50\n","537/537 [==============================] - 0s 54us/step - loss: 7327.4175 - regression_loss: 3405.1350 - val_loss: 864.2676 - val_regression_loss: 384.6893\n","Epoch 6/50\n","537/537 [==============================] - 0s 54us/step - loss: 6701.6965 - regression_loss: 2986.6721 - val_loss: 829.2339 - val_regression_loss: 368.7132\n","Epoch 7/50\n","537/537 [==============================] - 0s 55us/step - loss: 7218.5276 - regression_loss: 3259.1331 - val_loss: 619.0496 - val_regression_loss: 275.1860\n","Epoch 8/50\n","537/537 [==============================] - 0s 58us/step - loss: 6121.2155 - regression_loss: 2807.5657 - val_loss: 403.8117 - val_regression_loss: 178.9190\n","Epoch 9/50\n","537/537 [==============================] - 0s 66us/step - loss: 4165.0390 - regression_loss: 1918.9903 - val_loss: 388.7094 - val_regression_loss: 178.6234\n","Epoch 10/50\n","537/537 [==============================] - 0s 58us/step - loss: 3684.3505 - regression_loss: 1739.7387 - val_loss: 501.3112 - val_regression_loss: 237.5816\n","Epoch 11/50\n","537/537 [==============================] - 0s 60us/step - loss: 4063.8454 - regression_loss: 1948.9079 - val_loss: 555.5041 - val_regression_loss: 264.0237\n","Epoch 12/50\n","537/537 [==============================] - 0s 62us/step - loss: 4337.8225 - regression_loss: 2082.8632 - val_loss: 499.5579 - val_regression_loss: 233.7681\n","Epoch 13/50\n","537/537 [==============================] - 0s 52us/step - loss: 3794.3357 - regression_loss: 1790.1095 - val_loss: 398.7587 - val_regression_loss: 180.7505\n","Epoch 14/50\n","537/537 [==============================] - 0s 50us/step - loss: 3048.4978 - regression_loss: 1397.4359 - val_loss: 325.2522 - val_regression_loss: 141.7533\n","Epoch 15/50\n","537/537 [==============================] - 0s 62us/step - loss: 2633.4187 - regression_loss: 1172.7589 - val_loss: 310.0746 - val_regression_loss: 132.7335\n","Epoch 16/50\n","537/537 [==============================] - 0s 60us/step - loss: 2630.5135 - regression_loss: 1160.1205 - val_loss: 324.3656 - val_regression_loss: 139.4778\n","Epoch 17/50\n","537/537 [==============================] - 0s 62us/step - loss: 2839.7612 - regression_loss: 1266.0546 - val_loss: 322.6939 - val_regression_loss: 139.0925\n","Epoch 18/50\n","537/537 [==============================] - 0s 52us/step - loss: 2891.6445 - regression_loss: 1295.0166 - val_loss: 295.1124 - val_regression_loss: 126.2417\n","Epoch 19/50\n","537/537 [==============================] - 0s 59us/step - loss: 2645.1598 - regression_loss: 1181.2580 - val_loss: 268.7019 - val_regression_loss: 114.0409\n","Epoch 20/50\n","537/537 [==============================] - 0s 55us/step - loss: 2417.6299 - regression_loss: 1075.4611 - val_loss: 263.7385 - val_regression_loss: 112.4123\n","Epoch 21/50\n","537/537 [==============================] - 0s 57us/step - loss: 2242.8166 - regression_loss: 997.4382 - val_loss: 273.8581 - val_regression_loss: 118.0714\n","Epoch 22/50\n","537/537 [==============================] - 0s 66us/step - loss: 2262.4258 - regression_loss: 1011.4512 - val_loss: 279.3694 - val_regression_loss: 121.1757\n","Epoch 23/50\n","537/537 [==============================] - 0s 61us/step - loss: 2355.5058 - regression_loss: 1062.8013 - val_loss: 269.1208 - val_regression_loss: 116.1068\n","Epoch 24/50\n","537/537 [==============================] - 0s 55us/step - loss: 2339.9012 - regression_loss: 1054.9681 - val_loss: 249.2018 - val_regression_loss: 105.9589\n","Epoch 25/50\n","537/537 [==============================] - 0s 54us/step - loss: 2188.9791 - regression_loss: 978.7362 - val_loss: 233.4847 - val_regression_loss: 97.7094\n","Epoch 26/50\n","537/537 [==============================] - 0s 53us/step - loss: 2033.1816 - regression_loss: 897.6228 - val_loss: 228.1777 - val_regression_loss: 94.4563\n","Epoch 27/50\n","537/537 [==============================] - 0s 67us/step - loss: 1982.5845 - regression_loss: 868.9733 - val_loss: 228.2580 - val_regression_loss: 93.8394\n","Epoch 28/50\n","537/537 [==============================] - 0s 74us/step - loss: 1985.2686 - regression_loss: 869.2992 - val_loss: 227.1517 - val_regression_loss: 92.7435\n","Epoch 29/50\n","537/537 [==============================] - 0s 69us/step - loss: 1883.9548 - regression_loss: 809.1120 - val_loss: 224.0076 - val_regression_loss: 90.9077\n","Epoch 30/50\n","537/537 [==============================] - 0s 68us/step - loss: 1839.5093 - regression_loss: 786.2700 - val_loss: 221.6564 - val_regression_loss: 89.7549\n","Epoch 31/50\n","537/537 [==============================] - 0s 60us/step - loss: 1790.6721 - regression_loss: 763.0760 - val_loss: 218.6775 - val_regression_loss: 88.5248\n","Epoch 32/50\n","537/537 [==============================] - 0s 56us/step - loss: 1756.0148 - regression_loss: 750.1248 - val_loss: 212.9118 - val_regression_loss: 85.9674\n","Epoch 33/50\n","537/537 [==============================] - 0s 65us/step - loss: 1710.6742 - regression_loss: 731.3309 - val_loss: 206.1560 - val_regression_loss: 82.8648\n","Epoch 34/50\n","537/537 [==============================] - 0s 65us/step - loss: 1657.2752 - regression_loss: 705.0797 - val_loss: 202.4711 - val_regression_loss: 81.1805\n","Epoch 35/50\n","537/537 [==============================] - 0s 63us/step - loss: 1631.0332 - regression_loss: 695.4417 - val_loss: 202.2034 - val_regression_loss: 81.0635\n","Epoch 36/50\n","537/537 [==============================] - 0s 65us/step - loss: 1661.9955 - regression_loss: 711.6241 - val_loss: 202.7235 - val_regression_loss: 81.1655\n","Epoch 37/50\n","537/537 [==============================] - 0s 62us/step - loss: 1606.8398 - regression_loss: 683.9340 - val_loss: 202.9491 - val_regression_loss: 81.0981\n","Epoch 38/50\n","537/537 [==============================] - 0s 61us/step - loss: 1599.1815 - regression_loss: 678.2372 - val_loss: 204.7323 - val_regression_loss: 81.8351\n","Epoch 39/50\n","537/537 [==============================] - 0s 60us/step - loss: 1576.6093 - regression_loss: 664.3655 - val_loss: 206.6088 - val_regression_loss: 82.6717\n","Epoch 40/50\n","537/537 [==============================] - 0s 66us/step - loss: 1538.8024 - regression_loss: 648.1016 - val_loss: 207.7517 - val_regression_loss: 83.0954\n","Epoch 41/50\n","537/537 [==============================] - 0s 68us/step - loss: 1535.1497 - regression_loss: 641.8118 - val_loss: 205.9425 - val_regression_loss: 82.1314\n","Epoch 42/50\n","537/537 [==============================] - 0s 61us/step - loss: 1535.8589 - regression_loss: 643.0689 - val_loss: 203.1978 - val_regression_loss: 80.8186\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 1495.2999 - regression_loss: 626.5072 - val_loss: 200.3601 - val_regression_loss: 79.5138\n","Epoch 44/50\n","537/537 [==============================] - 0s 57us/step - loss: 1533.5641 - regression_loss: 644.5465 - val_loss: 197.0614 - val_regression_loss: 78.0518\n","Epoch 45/50\n","537/537 [==============================] - 0s 58us/step - loss: 1492.8097 - regression_loss: 626.3935 - val_loss: 195.3693 - val_regression_loss: 77.3114\n","Epoch 46/50\n","537/537 [==============================] - 0s 57us/step - loss: 1474.0299 - regression_loss: 620.0219 - val_loss: 194.9827 - val_regression_loss: 77.1018\n","Epoch 47/50\n","537/537 [==============================] - 0s 60us/step - loss: 1457.0456 - regression_loss: 609.0876 - val_loss: 194.6903 - val_regression_loss: 76.8545\n","Epoch 48/50\n","537/537 [==============================] - 0s 56us/step - loss: 1426.9088 - regression_loss: 595.7732 - val_loss: 194.2087 - val_regression_loss: 76.4600\n","Epoch 49/50\n","537/537 [==============================] - 0s 58us/step - loss: 1440.2657 - regression_loss: 600.9075 - val_loss: 192.9350 - val_regression_loss: 75.7913\n","Epoch 50/50\n","537/537 [==============================] - 0s 65us/step - loss: 1395.9566 - regression_loss: 578.2044 - val_loss: 190.3145 - val_regression_loss: 74.5769\n","***************************** elapsed_time is:  3.663071870803833\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 74395.5257 - regression_loss: 36946.5028 - val_loss: 6762.1724 - val_regression_loss: 3352.5850\n","Epoch 2/50\n","537/537 [==============================] - 0s 57us/step - loss: 61118.3790 - regression_loss: 30334.7564 - val_loss: 5042.5327 - val_regression_loss: 2496.5151\n","Epoch 3/50\n","537/537 [==============================] - 0s 60us/step - loss: 44633.3561 - regression_loss: 22122.2464 - val_loss: 3194.2739 - val_regression_loss: 1576.8020\n","Epoch 4/50\n","537/537 [==============================] - 0s 57us/step - loss: 28282.4294 - regression_loss: 13983.4528 - val_loss: 1904.0365 - val_regression_loss: 936.6470\n","Epoch 5/50\n","537/537 [==============================] - 0s 59us/step - loss: 14652.2239 - regression_loss: 7207.4037 - val_loss: 2139.6670 - val_regression_loss: 1058.8257\n","Epoch 6/50\n","537/537 [==============================] - 0s 60us/step - loss: 14091.6561 - regression_loss: 6961.9451 - val_loss: 2249.7112 - val_regression_loss: 1114.3839\n","Epoch 7/50\n","537/537 [==============================] - 0s 52us/step - loss: 14571.8064 - regression_loss: 7204.0677 - val_loss: 1402.5750 - val_regression_loss: 688.6859\n","Epoch 8/50\n","537/537 [==============================] - 0s 50us/step - loss: 9289.1070 - regression_loss: 4545.9609 - val_loss: 638.2602 - val_regression_loss: 303.5419\n","Epoch 9/50\n","537/537 [==============================] - 0s 61us/step - loss: 4782.0543 - regression_loss: 2265.5324 - val_loss: 426.0994 - val_regression_loss: 194.8461\n","Epoch 10/50\n","537/537 [==============================] - 0s 63us/step - loss: 4413.0189 - regression_loss: 2059.1243 - val_loss: 542.3824 - val_regression_loss: 251.4449\n","Epoch 11/50\n","537/537 [==============================] - 0s 72us/step - loss: 6186.5855 - regression_loss: 2934.6337 - val_loss: 624.8999 - val_regression_loss: 292.4064\n","Epoch 12/50\n","537/537 [==============================] - 0s 66us/step - loss: 6825.8973 - regression_loss: 3254.1800 - val_loss: 557.6422 - val_regression_loss: 259.4221\n","Epoch 13/50\n","537/537 [==============================] - 0s 61us/step - loss: 5769.7450 - regression_loss: 2733.6657 - val_loss: 429.5543 - val_regression_loss: 196.5277\n","Epoch 14/50\n","537/537 [==============================] - 0s 59us/step - loss: 4028.8261 - regression_loss: 1873.6118 - val_loss: 363.6266 - val_regression_loss: 164.8121\n","Epoch 15/50\n","537/537 [==============================] - 0s 59us/step - loss: 3107.6318 - regression_loss: 1423.4287 - val_loss: 403.9422 - val_regression_loss: 186.0646\n","Epoch 16/50\n","537/537 [==============================] - 0s 60us/step - loss: 2958.1545 - regression_loss: 1361.0646 - val_loss: 490.2001 - val_regression_loss: 229.9826\n","Epoch 17/50\n","537/537 [==============================] - 0s 53us/step - loss: 3280.1023 - regression_loss: 1526.3753 - val_loss: 528.4254 - val_regression_loss: 249.4970\n","Epoch 18/50\n","537/537 [==============================] - 0s 57us/step - loss: 3484.4717 - regression_loss: 1636.1398 - val_loss: 493.2289 - val_regression_loss: 231.8766\n","Epoch 19/50\n","537/537 [==============================] - 0s 68us/step - loss: 3106.8058 - regression_loss: 1445.7859 - val_loss: 436.7692 - val_regression_loss: 203.2595\n","Epoch 20/50\n","537/537 [==============================] - 0s 60us/step - loss: 2843.5314 - regression_loss: 1311.2306 - val_loss: 399.3818 - val_regression_loss: 183.9643\n","Epoch 21/50\n","537/537 [==============================] - 0s 63us/step - loss: 2657.6398 - regression_loss: 1213.2625 - val_loss: 375.4318 - val_regression_loss: 171.3659\n","Epoch 22/50\n","537/537 [==============================] - 0s 63us/step - loss: 2571.1906 - regression_loss: 1166.2155 - val_loss: 346.9170 - val_regression_loss: 156.5907\n","Epoch 23/50\n","537/537 [==============================] - 0s 63us/step - loss: 2508.8747 - regression_loss: 1131.1439 - val_loss: 318.2412 - val_regression_loss: 141.8635\n","Epoch 24/50\n","537/537 [==============================] - 0s 65us/step - loss: 2308.5358 - regression_loss: 1026.4580 - val_loss: 308.0560 - val_regression_loss: 136.4983\n","Epoch 25/50\n","537/537 [==============================] - 0s 59us/step - loss: 2208.2831 - regression_loss: 974.2225 - val_loss: 312.1231 - val_regression_loss: 138.4330\n","Epoch 26/50\n","537/537 [==============================] - 0s 67us/step - loss: 2219.8186 - regression_loss: 979.8736 - val_loss: 312.0404 - val_regression_loss: 138.4795\n","Epoch 27/50\n","537/537 [==============================] - 0s 62us/step - loss: 2170.3106 - regression_loss: 956.4309 - val_loss: 299.5302 - val_regression_loss: 132.4142\n","Epoch 28/50\n","537/537 [==============================] - 0s 66us/step - loss: 2009.2298 - regression_loss: 877.8584 - val_loss: 285.4488 - val_regression_loss: 125.5445\n","Epoch 29/50\n","537/537 [==============================] - 0s 59us/step - loss: 1899.9781 - regression_loss: 827.2322 - val_loss: 278.4600 - val_regression_loss: 122.1419\n","Epoch 30/50\n","537/537 [==============================] - 0s 61us/step - loss: 1872.9453 - regression_loss: 816.2211 - val_loss: 275.2933 - val_regression_loss: 120.6175\n","Epoch 31/50\n","537/537 [==============================] - 0s 58us/step - loss: 1865.2362 - regression_loss: 812.9564 - val_loss: 269.8130 - val_regression_loss: 117.9407\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 1780.8952 - regression_loss: 770.7957 - val_loss: 262.8789 - val_regression_loss: 114.5234\n","Epoch 33/50\n","537/537 [==============================] - 0s 63us/step - loss: 1728.9295 - regression_loss: 745.1803 - val_loss: 256.4640 - val_regression_loss: 111.2969\n","Epoch 34/50\n","537/537 [==============================] - 0s 63us/step - loss: 1712.9559 - regression_loss: 734.5243 - val_loss: 249.5594 - val_regression_loss: 107.7457\n","Epoch 35/50\n","537/537 [==============================] - 0s 73us/step - loss: 1670.6831 - regression_loss: 714.8441 - val_loss: 239.6353 - val_regression_loss: 102.6236\n","Epoch 36/50\n","537/537 [==============================] - 0s 65us/step - loss: 1627.1295 - regression_loss: 691.2741 - val_loss: 229.9886 - val_regression_loss: 97.6234\n","Epoch 37/50\n","537/537 [==============================] - 0s 62us/step - loss: 1563.9116 - regression_loss: 658.9095 - val_loss: 223.2424 - val_regression_loss: 94.1199\n","Epoch 38/50\n","537/537 [==============================] - 0s 62us/step - loss: 1576.4497 - regression_loss: 663.5745 - val_loss: 219.8198 - val_regression_loss: 92.3978\n","Epoch 39/50\n","537/537 [==============================] - 0s 61us/step - loss: 1571.8364 - regression_loss: 662.2239 - val_loss: 217.5294 - val_regression_loss: 91.3313\n","Epoch 40/50\n","537/537 [==============================] - 0s 64us/step - loss: 1531.8078 - regression_loss: 644.2306 - val_loss: 217.2346 - val_regression_loss: 91.3083\n","Epoch 41/50\n","537/537 [==============================] - 0s 60us/step - loss: 1502.8591 - regression_loss: 630.5794 - val_loss: 215.8464 - val_regression_loss: 90.7252\n","Epoch 42/50\n","537/537 [==============================] - 0s 61us/step - loss: 1457.6264 - regression_loss: 608.1656 - val_loss: 212.1331 - val_regression_loss: 88.9325\n","Epoch 43/50\n","537/537 [==============================] - 0s 59us/step - loss: 1470.5582 - regression_loss: 616.4173 - val_loss: 207.0663 - val_regression_loss: 86.4067\n","Epoch 44/50\n","537/537 [==============================] - 0s 61us/step - loss: 1466.1155 - regression_loss: 612.2467 - val_loss: 203.3659 - val_regression_loss: 84.5517\n","Epoch 45/50\n","537/537 [==============================] - 0s 64us/step - loss: 1458.0523 - regression_loss: 608.0511 - val_loss: 200.6400 - val_regression_loss: 83.1837\n","Epoch 46/50\n","537/537 [==============================] - 0s 62us/step - loss: 1451.4791 - regression_loss: 605.5100 - val_loss: 198.9450 - val_regression_loss: 82.3343\n","Epoch 47/50\n","537/537 [==============================] - 0s 71us/step - loss: 1410.7784 - regression_loss: 583.7087 - val_loss: 196.5387 - val_regression_loss: 81.1111\n","Epoch 48/50\n","537/537 [==============================] - 0s 60us/step - loss: 1407.3544 - regression_loss: 584.4662 - val_loss: 193.6913 - val_regression_loss: 79.6697\n","Epoch 49/50\n","537/537 [==============================] - 0s 59us/step - loss: 1440.0502 - regression_loss: 598.0963 - val_loss: 190.5609 - val_regression_loss: 78.1101\n","Epoch 50/50\n","537/537 [==============================] - 0s 58us/step - loss: 1399.6193 - regression_loss: 579.3395 - val_loss: 187.2674 - val_regression_loss: 76.4713\n","***************************** elapsed_time is:  4.00443959236145\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 83153.4427 - regression_loss: 41001.6640 - val_loss: 7991.3950 - val_regression_loss: 3941.6011\n","Epoch 2/50\n","537/537 [==============================] - 0s 64us/step - loss: 68929.1455 - regression_loss: 34024.3410 - val_loss: 6077.4868 - val_regression_loss: 3003.2007\n","Epoch 3/50\n","537/537 [==============================] - 0s 71us/step - loss: 51723.2316 - regression_loss: 25553.3417 - val_loss: 3835.3167 - val_regression_loss: 1903.0078\n","Epoch 4/50\n","537/537 [==============================] - 0s 57us/step - loss: 32756.9174 - regression_loss: 16257.0245 - val_loss: 1875.8306 - val_regression_loss: 944.9695\n","Epoch 5/50\n","537/537 [==============================] - 0s 57us/step - loss: 15568.5296 - regression_loss: 7842.3954 - val_loss: 1452.2369 - val_regression_loss: 749.0833\n","Epoch 6/50\n","537/537 [==============================] - 0s 62us/step - loss: 10713.2226 - regression_loss: 5546.1448 - val_loss: 2023.7024 - val_regression_loss: 1026.3861\n","Epoch 7/50\n","537/537 [==============================] - 0s 62us/step - loss: 14394.0220 - regression_loss: 7302.8705 - val_loss: 1684.4934 - val_regression_loss: 827.4937\n","Epoch 8/50\n","537/537 [==============================] - 0s 61us/step - loss: 12009.5389 - regression_loss: 5861.7605 - val_loss: 1012.7673 - val_regression_loss: 471.4677\n","Epoch 9/50\n","537/537 [==============================] - 0s 57us/step - loss: 7085.0995 - regression_loss: 3230.7401 - val_loss: 694.6884 - val_regression_loss: 308.0344\n","Epoch 10/50\n","537/537 [==============================] - 0s 60us/step - loss: 4879.0435 - regression_loss: 2114.6078 - val_loss: 767.9941 - val_regression_loss: 346.0768\n","Epoch 11/50\n","537/537 [==============================] - 0s 59us/step - loss: 6347.3311 - regression_loss: 2875.7532 - val_loss: 860.5112 - val_regression_loss: 395.5511\n","Epoch 12/50\n","537/537 [==============================] - 0s 61us/step - loss: 7443.7201 - regression_loss: 3448.5578 - val_loss: 792.0024 - val_regression_loss: 365.9153\n","Epoch 13/50\n","537/537 [==============================] - 0s 65us/step - loss: 6851.9601 - regression_loss: 3190.5787 - val_loss: 628.4420 - val_regression_loss: 289.8646\n","Epoch 14/50\n","537/537 [==============================] - 0s 64us/step - loss: 5375.1051 - regression_loss: 2500.0165 - val_loss: 498.1934 - val_regression_loss: 230.8686\n","Epoch 15/50\n","537/537 [==============================] - 0s 63us/step - loss: 3910.3853 - regression_loss: 1815.1672 - val_loss: 483.1910 - val_regression_loss: 228.8540\n","Epoch 16/50\n","537/537 [==============================] - 0s 64us/step - loss: 3554.8150 - regression_loss: 1681.6089 - val_loss: 556.0096 - val_regression_loss: 269.1379\n","Epoch 17/50\n","537/537 [==============================] - 0s 64us/step - loss: 3884.8936 - regression_loss: 1877.4964 - val_loss: 610.1318 - val_regression_loss: 297.7614\n","Epoch 18/50\n","537/537 [==============================] - 0s 67us/step - loss: 4382.7682 - regression_loss: 2141.1199 - val_loss: 575.0098 - val_regression_loss: 279.4080\n","Epoch 19/50\n","537/537 [==============================] - 0s 60us/step - loss: 4076.6309 - regression_loss: 1984.0967 - val_loss: 484.5444 - val_regression_loss: 231.5594\n","Epoch 20/50\n","537/537 [==============================] - 0s 62us/step - loss: 3366.8084 - regression_loss: 1608.3279 - val_loss: 411.3316 - val_regression_loss: 191.3699\n","Epoch 21/50\n","537/537 [==============================] - 0s 65us/step - loss: 3166.2834 - regression_loss: 1479.5820 - val_loss: 385.6537 - val_regression_loss: 174.9849\n","Epoch 22/50\n","537/537 [==============================] - 0s 61us/step - loss: 3118.3727 - regression_loss: 1431.5197 - val_loss: 387.3203 - val_regression_loss: 173.0615\n","Epoch 23/50\n","537/537 [==============================] - 0s 59us/step - loss: 3196.7141 - regression_loss: 1446.5126 - val_loss: 386.0865 - val_regression_loss: 171.0116\n","Epoch 24/50\n","537/537 [==============================] - 0s 63us/step - loss: 3097.8767 - regression_loss: 1388.5677 - val_loss: 373.0331 - val_regression_loss: 164.4228\n","Epoch 25/50\n","537/537 [==============================] - 0s 63us/step - loss: 3015.3360 - regression_loss: 1343.8616 - val_loss: 359.1976 - val_regression_loss: 158.5290\n","Epoch 26/50\n","537/537 [==============================] - 0s 60us/step - loss: 2809.0354 - regression_loss: 1249.7492 - val_loss: 354.8247 - val_regression_loss: 157.9818\n","Epoch 27/50\n","537/537 [==============================] - 0s 62us/step - loss: 2643.0799 - regression_loss: 1178.7832 - val_loss: 354.5870 - val_regression_loss: 159.6228\n","Epoch 28/50\n","537/537 [==============================] - 0s 59us/step - loss: 2624.4027 - regression_loss: 1184.5515 - val_loss: 348.6791 - val_regression_loss: 158.1046\n","Epoch 29/50\n","537/537 [==============================] - 0s 68us/step - loss: 2621.3123 - regression_loss: 1194.2261 - val_loss: 337.0842 - val_regression_loss: 153.1011\n","Epoch 30/50\n","537/537 [==============================] - 0s 61us/step - loss: 2531.6668 - regression_loss: 1156.6443 - val_loss: 326.7039 - val_regression_loss: 147.9795\n","Epoch 31/50\n","537/537 [==============================] - 0s 60us/step - loss: 2384.9066 - regression_loss: 1080.6202 - val_loss: 321.5351 - val_regression_loss: 144.9470\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 2372.2381 - regression_loss: 1074.4875 - val_loss: 319.1700 - val_regression_loss: 143.1219\n","Epoch 33/50\n","537/537 [==============================] - 0s 62us/step - loss: 2341.6317 - regression_loss: 1053.5454 - val_loss: 315.8296 - val_regression_loss: 140.9620\n","Epoch 34/50\n","537/537 [==============================] - 0s 61us/step - loss: 2310.1144 - regression_loss: 1033.8923 - val_loss: 312.5025 - val_regression_loss: 138.9955\n","Epoch 35/50\n","537/537 [==============================] - 0s 61us/step - loss: 2253.7260 - regression_loss: 999.5613 - val_loss: 311.7203 - val_regression_loss: 138.4489\n","Epoch 36/50\n","537/537 [==============================] - 0s 63us/step - loss: 2206.0748 - regression_loss: 975.6608 - val_loss: 311.7441 - val_regression_loss: 138.3128\n","Epoch 37/50\n","537/537 [==============================] - 0s 63us/step - loss: 2153.6028 - regression_loss: 945.1066 - val_loss: 309.5005 - val_regression_loss: 137.1311\n","Epoch 38/50\n","537/537 [==============================] - 0s 63us/step - loss: 2087.0319 - regression_loss: 914.6942 - val_loss: 305.7924 - val_regression_loss: 135.2590\n","Epoch 39/50\n","537/537 [==============================] - 0s 61us/step - loss: 2069.2991 - regression_loss: 904.2231 - val_loss: 303.6109 - val_regression_loss: 134.2719\n","Epoch 40/50\n","537/537 [==============================] - 0s 60us/step - loss: 1992.7158 - regression_loss: 868.7984 - val_loss: 302.8694 - val_regression_loss: 134.1782\n","Epoch 41/50\n","537/537 [==============================] - 0s 61us/step - loss: 2030.6622 - regression_loss: 887.5442 - val_loss: 301.7920 - val_regression_loss: 134.1132\n","Epoch 42/50\n","537/537 [==============================] - 0s 60us/step - loss: 1955.4801 - regression_loss: 853.7421 - val_loss: 300.4442 - val_regression_loss: 133.9573\n","Epoch 43/50\n","537/537 [==============================] - 0s 69us/step - loss: 1949.5532 - regression_loss: 854.7889 - val_loss: 299.2213 - val_regression_loss: 133.6727\n","Epoch 44/50\n","537/537 [==============================] - 0s 64us/step - loss: 1926.9753 - regression_loss: 844.9641 - val_loss: 297.8530 - val_regression_loss: 132.9744\n","Epoch 45/50\n","537/537 [==============================] - 0s 61us/step - loss: 1896.6890 - regression_loss: 828.2977 - val_loss: 295.9167 - val_regression_loss: 131.6692\n","Epoch 46/50\n","537/537 [==============================] - 0s 66us/step - loss: 1818.5022 - regression_loss: 787.8066 - val_loss: 294.1920 - val_regression_loss: 130.2652\n","Epoch 47/50\n","537/537 [==============================] - 0s 65us/step - loss: 1821.4028 - regression_loss: 786.1883 - val_loss: 293.5384 - val_regression_loss: 129.5274\n","Epoch 48/50\n","537/537 [==============================] - 0s 61us/step - loss: 1810.4478 - regression_loss: 777.7514 - val_loss: 292.1625 - val_regression_loss: 128.8006\n","Epoch 49/50\n","537/537 [==============================] - 0s 60us/step - loss: 1808.3232 - regression_loss: 775.2425 - val_loss: 290.3403 - val_regression_loss: 128.1355\n","Epoch 50/50\n","537/537 [==============================] - 0s 65us/step - loss: 1769.8355 - regression_loss: 761.2022 - val_loss: 288.8055 - val_regression_loss: 127.6710\n","***************************** elapsed_time is:  3.7967803478240967\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 66681.3910 - regression_loss: 33294.0627 - val_loss: 6750.7046 - val_regression_loss: 3365.3789\n","Epoch 2/50\n","537/537 [==============================] - 0s 62us/step - loss: 57142.1601 - regression_loss: 28510.1487 - val_loss: 5399.7192 - val_regression_loss: 2687.3530\n","Epoch 3/50\n","537/537 [==============================] - 0s 67us/step - loss: 45863.2401 - regression_loss: 22852.0247 - val_loss: 3744.5940 - val_regression_loss: 1856.0062\n","Epoch 4/50\n","537/537 [==============================] - 0s 63us/step - loss: 31997.0376 - regression_loss: 15886.8502 - val_loss: 2036.8899 - val_regression_loss: 996.7632\n","Epoch 5/50\n","537/537 [==============================] - 0s 61us/step - loss: 16932.2065 - regression_loss: 8317.4232 - val_loss: 1014.6531 - val_regression_loss: 478.7609\n","Epoch 6/50\n","537/537 [==============================] - 0s 55us/step - loss: 8596.6634 - regression_loss: 4089.6930 - val_loss: 1307.9788 - val_regression_loss: 620.6678\n","Epoch 7/50\n","537/537 [==============================] - 0s 54us/step - loss: 10405.9009 - regression_loss: 4957.2246 - val_loss: 1419.6370 - val_regression_loss: 679.5965\n","Epoch 8/50\n","537/537 [==============================] - 0s 51us/step - loss: 11234.4805 - regression_loss: 5399.1550 - val_loss: 951.8314 - val_regression_loss: 452.0891\n","Epoch 9/50\n","537/537 [==============================] - 0s 52us/step - loss: 8141.4176 - regression_loss: 3904.0964 - val_loss: 538.8959 - val_regression_loss: 251.3945\n","Epoch 10/50\n","537/537 [==============================] - 0s 67us/step - loss: 5404.2650 - regression_loss: 2581.0152 - val_loss: 476.0130 - val_regression_loss: 223.6367\n","Epoch 11/50\n","537/537 [==============================] - 0s 52us/step - loss: 5041.4309 - regression_loss: 2428.8778 - val_loss: 594.2752 - val_regression_loss: 284.1581\n","Epoch 12/50\n","537/537 [==============================] - 0s 66us/step - loss: 5917.3244 - regression_loss: 2876.5560 - val_loss: 644.5434 - val_regression_loss: 308.8805\n","Epoch 13/50\n","537/537 [==============================] - 0s 56us/step - loss: 6130.8131 - regression_loss: 2979.0551 - val_loss: 572.4262 - val_regression_loss: 271.3452\n","Epoch 14/50\n","537/537 [==============================] - 0s 50us/step - loss: 5269.2169 - regression_loss: 2537.8161 - val_loss: 453.2997 - val_regression_loss: 209.8091\n","Epoch 15/50\n","537/537 [==============================] - 0s 48us/step - loss: 4362.4712 - regression_loss: 2063.0284 - val_loss: 375.4170 - val_regression_loss: 168.9349\n","Epoch 16/50\n","537/537 [==============================] - 0s 49us/step - loss: 3697.1983 - regression_loss: 1713.3177 - val_loss: 376.6426 - val_regression_loss: 168.0541\n","Epoch 17/50\n","537/537 [==============================] - 0s 54us/step - loss: 3550.4109 - regression_loss: 1627.6407 - val_loss: 419.6407 - val_regression_loss: 188.7904\n","Epoch 18/50\n","537/537 [==============================] - 0s 63us/step - loss: 3830.3582 - regression_loss: 1762.9627 - val_loss: 434.4215 - val_regression_loss: 196.2493\n","Epoch 19/50\n","537/537 [==============================] - 0s 64us/step - loss: 3946.8688 - regression_loss: 1820.5955 - val_loss: 398.2863 - val_regression_loss: 178.9101\n","Epoch 20/50\n","537/537 [==============================] - 0s 66us/step - loss: 3572.5184 - regression_loss: 1638.1816 - val_loss: 351.4417 - val_regression_loss: 156.6004\n","Epoch 21/50\n","537/537 [==============================] - 0s 71us/step - loss: 3145.1100 - regression_loss: 1435.4221 - val_loss: 339.1705 - val_regression_loss: 151.6702\n","Epoch 22/50\n","537/537 [==============================] - 0s 64us/step - loss: 2906.5303 - regression_loss: 1325.6181 - val_loss: 358.0739 - val_regression_loss: 162.1449\n","Epoch 23/50\n","537/537 [==============================] - 0s 65us/step - loss: 2939.1199 - regression_loss: 1347.8068 - val_loss: 372.1552 - val_regression_loss: 169.8340\n","Epoch 24/50\n","537/537 [==============================] - 0s 61us/step - loss: 3089.9911 - regression_loss: 1431.3346 - val_loss: 360.3307 - val_regression_loss: 164.1460\n","Epoch 25/50\n","537/537 [==============================] - 0s 64us/step - loss: 3012.0910 - regression_loss: 1396.3337 - val_loss: 332.4992 - val_regression_loss: 150.0584\n","Epoch 26/50\n","537/537 [==============================] - 0s 59us/step - loss: 2765.0144 - regression_loss: 1268.5916 - val_loss: 310.2686 - val_regression_loss: 138.5201\n","Epoch 27/50\n","537/537 [==============================] - 0s 60us/step - loss: 2628.2116 - regression_loss: 1196.6061 - val_loss: 302.8528 - val_regression_loss: 134.2330\n","Epoch 28/50\n","537/537 [==============================] - 0s 71us/step - loss: 2596.3730 - regression_loss: 1175.5946 - val_loss: 301.9608 - val_regression_loss: 133.1960\n","Epoch 29/50\n","537/537 [==============================] - 0s 75us/step - loss: 2557.2522 - regression_loss: 1150.7299 - val_loss: 297.6149 - val_regression_loss: 130.5978\n","Epoch 30/50\n","537/537 [==============================] - 0s 61us/step - loss: 2490.9133 - regression_loss: 1115.7982 - val_loss: 289.6004 - val_regression_loss: 126.4063\n","Epoch 31/50\n","537/537 [==============================] - 0s 66us/step - loss: 2221.2968 - regression_loss: 974.6760 - val_loss: 284.1829 - val_regression_loss: 123.8085\n","Epoch 32/50\n","537/537 [==============================] - 0s 63us/step - loss: 2331.0605 - regression_loss: 1032.8483 - val_loss: 284.4398 - val_regression_loss: 124.2115\n","Epoch 33/50\n","537/537 [==============================] - 0s 59us/step - loss: 2215.6942 - regression_loss: 978.6904 - val_loss: 284.2621 - val_regression_loss: 124.4093\n","Epoch 34/50\n","537/537 [==============================] - 0s 65us/step - loss: 2230.1548 - regression_loss: 988.4792 - val_loss: 280.4797 - val_regression_loss: 122.7291\n","Epoch 35/50\n","537/537 [==============================] - 0s 61us/step - loss: 2180.5691 - regression_loss: 962.6675 - val_loss: 276.0878 - val_regression_loss: 120.6552\n","Epoch 36/50\n","537/537 [==============================] - 0s 59us/step - loss: 2105.7643 - regression_loss: 926.4399 - val_loss: 272.5557 - val_regression_loss: 118.8924\n","Epoch 37/50\n","537/537 [==============================] - 0s 59us/step - loss: 2094.5764 - regression_loss: 924.1844 - val_loss: 268.9814 - val_regression_loss: 117.0736\n","Epoch 38/50\n","537/537 [==============================] - 0s 58us/step - loss: 2039.6336 - regression_loss: 893.8151 - val_loss: 262.5272 - val_regression_loss: 113.7930\n","Epoch 39/50\n","537/537 [==============================] - 0s 59us/step - loss: 1962.3720 - regression_loss: 856.6539 - val_loss: 255.5058 - val_regression_loss: 110.2522\n","Epoch 40/50\n","537/537 [==============================] - 0s 60us/step - loss: 1926.6879 - regression_loss: 838.8316 - val_loss: 250.6534 - val_regression_loss: 107.7773\n","Epoch 41/50\n","537/537 [==============================] - 0s 62us/step - loss: 1932.7435 - regression_loss: 840.7475 - val_loss: 248.7211 - val_regression_loss: 106.6994\n","Epoch 42/50\n","537/537 [==============================] - 0s 58us/step - loss: 1889.3710 - regression_loss: 819.1597 - val_loss: 248.8892 - val_regression_loss: 106.6578\n","Epoch 43/50\n","537/537 [==============================] - 0s 56us/step - loss: 1846.6921 - regression_loss: 798.8406 - val_loss: 250.2781 - val_regression_loss: 107.2477\n","Epoch 44/50\n","537/537 [==============================] - 0s 58us/step - loss: 1835.0607 - regression_loss: 790.4867 - val_loss: 251.0963 - val_regression_loss: 107.6488\n","Epoch 45/50\n","537/537 [==============================] - 0s 60us/step - loss: 1812.3993 - regression_loss: 781.7525 - val_loss: 249.3296 - val_regression_loss: 106.8746\n","Epoch 46/50\n","537/537 [==============================] - 0s 55us/step - loss: 1768.5840 - regression_loss: 758.0142 - val_loss: 245.9008 - val_regression_loss: 105.3137\n","Epoch 47/50\n","537/537 [==============================] - 0s 59us/step - loss: 1711.8248 - regression_loss: 730.3734 - val_loss: 242.0660 - val_regression_loss: 103.5683\n","Epoch 48/50\n","537/537 [==============================] - 0s 59us/step - loss: 1761.6908 - regression_loss: 757.5434 - val_loss: 240.3226 - val_regression_loss: 102.7021\n","Epoch 49/50\n","537/537 [==============================] - 0s 62us/step - loss: 1743.9513 - regression_loss: 749.2455 - val_loss: 239.6117 - val_regression_loss: 102.2802\n","Epoch 50/50\n","537/537 [==============================] - 0s 56us/step - loss: 1680.5405 - regression_loss: 717.8150 - val_loss: 240.2506 - val_regression_loss: 102.4866\n","***************************** elapsed_time is:  3.723633289337158\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 574976.0109 - regression_loss: 287377.5048 - val_loss: 63397.4492 - val_regression_loss: 31684.9141\n","Epoch 2/50\n","537/537 [==============================] - 0s 57us/step - loss: 532546.6315 - regression_loss: 266174.0541 - val_loss: 57495.9609 - val_regression_loss: 28735.3867\n","Epoch 3/50\n","537/537 [==============================] - 0s 56us/step - loss: 476352.9398 - regression_loss: 238087.8428 - val_loss: 49298.1406 - val_regression_loss: 24637.2383\n","Epoch 4/50\n","537/537 [==============================] - 0s 59us/step - loss: 412128.5205 - regression_loss: 205980.2989 - val_loss: 39183.8984 - val_regression_loss: 19579.9492\n","Epoch 5/50\n","537/537 [==============================] - 0s 62us/step - loss: 334309.6947 - regression_loss: 167075.2716 - val_loss: 28202.7695 - val_regression_loss: 14087.4219\n","Epoch 6/50\n","537/537 [==============================] - 0s 60us/step - loss: 237464.1888 - regression_loss: 118636.9721 - val_loss: 18104.6797 - val_regression_loss: 9031.6699\n","Epoch 7/50\n","537/537 [==============================] - 0s 55us/step - loss: 166566.7228 - regression_loss: 83160.2531 - val_loss: 11522.8242 - val_regression_loss: 5715.2305\n","Epoch 8/50\n","537/537 [==============================] - 0s 57us/step - loss: 111765.0965 - regression_loss: 55598.9686 - val_loss: 11456.6396 - val_regression_loss: 5593.9326\n","Epoch 9/50\n","537/537 [==============================] - 0s 55us/step - loss: 110196.8548 - regression_loss: 54241.7182 - val_loss: 14077.8379 - val_regression_loss: 6824.1851\n","Epoch 10/50\n","537/537 [==============================] - 0s 58us/step - loss: 131490.3312 - regression_loss: 64329.4613 - val_loss: 12594.9756 - val_regression_loss: 6121.6660\n","Epoch 11/50\n","537/537 [==============================] - 0s 65us/step - loss: 120578.5156 - regression_loss: 59162.8039 - val_loss: 9257.8164 - val_regression_loss: 4527.6543\n","Epoch 12/50\n","537/537 [==============================] - 0s 63us/step - loss: 101686.2873 - regression_loss: 50223.4208 - val_loss: 7304.4468 - val_regression_loss: 3597.3987\n","Epoch 13/50\n","537/537 [==============================] - 0s 55us/step - loss: 88925.9031 - regression_loss: 44132.2506 - val_loss: 7276.2607 - val_regression_loss: 3603.5962\n","Epoch 14/50\n","537/537 [==============================] - 0s 58us/step - loss: 87644.7934 - regression_loss: 43621.5499 - val_loss: 7816.2036 - val_regression_loss: 3880.9202\n","Epoch 15/50\n","537/537 [==============================] - 0s 54us/step - loss: 89512.3494 - regression_loss: 44592.3738 - val_loss: 7920.4077 - val_regression_loss: 3934.5437\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 89893.4419 - regression_loss: 44787.6373 - val_loss: 7452.2471 - val_regression_loss: 3698.9224\n","Epoch 17/50\n","537/537 [==============================] - 0s 63us/step - loss: 83594.1936 - regression_loss: 41621.0004 - val_loss: 6745.5566 - val_regression_loss: 3342.1448\n","Epoch 18/50\n","537/537 [==============================] - 0s 60us/step - loss: 79674.9900 - regression_loss: 39641.2207 - val_loss: 6186.5220 - val_regression_loss: 3057.9292\n","Epoch 19/50\n","537/537 [==============================] - 0s 57us/step - loss: 68557.1619 - regression_loss: 34040.6891 - val_loss: 6023.3472 - val_regression_loss: 2971.5654\n","Epoch 20/50\n","537/537 [==============================] - 0s 61us/step - loss: 69427.0295 - regression_loss: 34445.0387 - val_loss: 6163.3101 - val_regression_loss: 3037.9832\n","Epoch 21/50\n","537/537 [==============================] - 0s 56us/step - loss: 68388.2762 - regression_loss: 33898.2419 - val_loss: 6220.0620 - val_regression_loss: 3065.5830\n","Epoch 22/50\n","537/537 [==============================] - 0s 58us/step - loss: 66305.7444 - regression_loss: 32854.4559 - val_loss: 5883.4678 - val_regression_loss: 2899.5420\n","Epoch 23/50\n","537/537 [==============================] - 0s 55us/step - loss: 63759.9360 - regression_loss: 31596.5916 - val_loss: 5275.9482 - val_regression_loss: 2599.9771\n","Epoch 24/50\n","537/537 [==============================] - 0s 53us/step - loss: 58947.6598 - regression_loss: 29222.8948 - val_loss: 4762.8472 - val_regression_loss: 2347.8650\n","Epoch 25/50\n","537/537 [==============================] - 0s 59us/step - loss: 52919.6754 - regression_loss: 26241.5713 - val_loss: 4501.9312 - val_regression_loss: 2220.9973\n","Epoch 26/50\n","537/537 [==============================] - 0s 52us/step - loss: 51200.5489 - regression_loss: 25408.3524 - val_loss: 4386.6689 - val_regression_loss: 2165.5940\n","Epoch 27/50\n","537/537 [==============================] - 0s 59us/step - loss: 49793.7025 - regression_loss: 24720.7598 - val_loss: 4256.1782 - val_regression_loss: 2101.2505\n","Epoch 28/50\n","537/537 [==============================] - 0s 53us/step - loss: 47124.3730 - regression_loss: 23394.9132 - val_loss: 4120.0420 - val_regression_loss: 2033.0706\n","Epoch 29/50\n","537/537 [==============================] - 0s 63us/step - loss: 40667.6794 - regression_loss: 20162.6432 - val_loss: 4072.5183 - val_regression_loss: 2008.8761\n","Epoch 30/50\n","537/537 [==============================] - 0s 57us/step - loss: 40660.4321 - regression_loss: 20157.6418 - val_loss: 4117.3721 - val_regression_loss: 2031.0601\n","Epoch 31/50\n","537/537 [==============================] - 0s 59us/step - loss: 38455.4785 - regression_loss: 19051.7382 - val_loss: 4030.2593 - val_regression_loss: 1988.1006\n","Epoch 32/50\n","537/537 [==============================] - 0s 55us/step - loss: 35613.3645 - regression_loss: 17634.4621 - val_loss: 3696.2053 - val_regression_loss: 1822.4622\n","Epoch 33/50\n","537/537 [==============================] - 0s 60us/step - loss: 32183.3905 - regression_loss: 15928.3051 - val_loss: 3276.2417 - val_regression_loss: 1614.1898\n","Epoch 34/50\n","537/537 [==============================] - 0s 65us/step - loss: 30450.6957 - regression_loss: 15071.7552 - val_loss: 2948.8958 - val_regression_loss: 1451.9725\n","Epoch 35/50\n","537/537 [==============================] - 0s 62us/step - loss: 28302.2599 - regression_loss: 14006.3562 - val_loss: 2739.2786 - val_regression_loss: 1347.8981\n","Epoch 36/50\n","537/537 [==============================] - 0s 54us/step - loss: 25870.8132 - regression_loss: 12794.8176 - val_loss: 2645.5029 - val_regression_loss: 1301.0164\n","Epoch 37/50\n","537/537 [==============================] - 0s 57us/step - loss: 24404.0645 - regression_loss: 12061.0626 - val_loss: 2654.8850 - val_regression_loss: 1305.5267\n","Epoch 38/50\n","537/537 [==============================] - 0s 55us/step - loss: 22647.0498 - regression_loss: 11180.4573 - val_loss: 2577.3247 - val_regression_loss: 1266.9946\n","Epoch 39/50\n","537/537 [==============================] - 0s 59us/step - loss: 21378.4700 - regression_loss: 10545.8127 - val_loss: 2334.2717 - val_regression_loss: 1146.2008\n","Epoch 40/50\n","537/537 [==============================] - 0s 67us/step - loss: 19415.2064 - regression_loss: 9568.0986 - val_loss: 2078.9175 - val_regression_loss: 1019.3749\n","Epoch 41/50\n","537/537 [==============================] - 0s 65us/step - loss: 17650.9239 - regression_loss: 8691.4299 - val_loss: 1923.0872 - val_regression_loss: 941.9722\n","Epoch 42/50\n","537/537 [==============================] - 0s 56us/step - loss: 16842.1826 - regression_loss: 8290.6896 - val_loss: 1881.5660 - val_regression_loss: 921.1764\n","Epoch 43/50\n","537/537 [==============================] - 0s 59us/step - loss: 15476.3040 - regression_loss: 7606.2251 - val_loss: 1882.6309 - val_regression_loss: 921.5373\n","Epoch 44/50\n","537/537 [==============================] - 0s 54us/step - loss: 14283.6993 - regression_loss: 7008.9500 - val_loss: 1818.4358 - val_regression_loss: 889.5397\n","Epoch 45/50\n","537/537 [==============================] - 0s 60us/step - loss: 13192.6072 - regression_loss: 6461.9861 - val_loss: 1670.1946 - val_regression_loss: 815.8743\n","Epoch 46/50\n","537/537 [==============================] - 0s 53us/step - loss: 12503.9243 - regression_loss: 6122.3919 - val_loss: 1586.0693 - val_regression_loss: 774.0730\n","Epoch 47/50\n","537/537 [==============================] - 0s 56us/step - loss: 11581.1653 - regression_loss: 5661.0241 - val_loss: 1560.2368 - val_regression_loss: 761.1440\n","Epoch 48/50\n","537/537 [==============================] - 0s 57us/step - loss: 10992.1638 - regression_loss: 5368.4170 - val_loss: 1542.9557 - val_regression_loss: 752.4446\n","Epoch 49/50\n","537/537 [==============================] - 0s 60us/step - loss: 10737.5356 - regression_loss: 5238.6148 - val_loss: 1488.4335 - val_regression_loss: 725.3488\n","Epoch 50/50\n","537/537 [==============================] - 0s 66us/step - loss: 10032.2077 - regression_loss: 4886.4167 - val_loss: 1419.4254 - val_regression_loss: 691.1261\n","***************************** elapsed_time is:  3.6350529193878174\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 130100.6290 - regression_loss: 64953.9562 - val_loss: 14932.0654 - val_regression_loss: 7452.2344\n","Epoch 2/50\n","537/537 [==============================] - 0s 59us/step - loss: 117304.2374 - regression_loss: 58553.9036 - val_loss: 12515.3604 - val_regression_loss: 6243.2598\n","Epoch 3/50\n","537/537 [==============================] - 0s 65us/step - loss: 97830.0336 - regression_loss: 48812.3526 - val_loss: 9291.6328 - val_regression_loss: 4629.7979\n","Epoch 4/50\n","537/537 [==============================] - 0s 56us/step - loss: 71998.7872 - regression_loss: 35881.4409 - val_loss: 5778.8105 - val_regression_loss: 2870.6255\n","Epoch 5/50\n","537/537 [==============================] - 0s 55us/step - loss: 44702.3854 - regression_loss: 22210.5086 - val_loss: 2901.1377 - val_regression_loss: 1428.0264\n","Epoch 6/50\n","537/537 [==============================] - 0s 61us/step - loss: 23402.1079 - regression_loss: 11533.0812 - val_loss: 2041.0243 - val_regression_loss: 994.3382\n","Epoch 7/50\n","537/537 [==============================] - 0s 56us/step - loss: 18234.6279 - regression_loss: 8917.8931 - val_loss: 2600.7271 - val_regression_loss: 1274.0035\n","Epoch 8/50\n","537/537 [==============================] - 0s 52us/step - loss: 24580.4172 - regression_loss: 12088.7848 - val_loss: 2365.9524 - val_regression_loss: 1159.9739\n","Epoch 9/50\n","537/537 [==============================] - 0s 57us/step - loss: 22887.6900 - regression_loss: 11270.4898 - val_loss: 1654.5239 - val_regression_loss: 808.1273\n","Epoch 10/50\n","537/537 [==============================] - 0s 69us/step - loss: 16362.5908 - regression_loss: 8039.8218 - val_loss: 1226.5620 - val_regression_loss: 597.0479\n","Epoch 11/50\n","537/537 [==============================] - 0s 56us/step - loss: 12010.9468 - regression_loss: 5887.3791 - val_loss: 1315.8503 - val_regression_loss: 643.2976\n","Epoch 12/50\n","537/537 [==============================] - 0s 58us/step - loss: 11431.2152 - regression_loss: 5610.3870 - val_loss: 1604.4047 - val_regression_loss: 788.0065\n","Epoch 13/50\n","537/537 [==============================] - 0s 60us/step - loss: 12809.8924 - regression_loss: 6303.7581 - val_loss: 1723.2899 - val_regression_loss: 847.0027\n","Epoch 14/50\n","537/537 [==============================] - 0s 52us/step - loss: 14072.4294 - regression_loss: 6933.5907 - val_loss: 1561.8856 - val_regression_loss: 765.2612\n","Epoch 15/50\n","537/537 [==============================] - 0s 55us/step - loss: 12922.2027 - regression_loss: 6351.1092 - val_loss: 1249.9996 - val_regression_loss: 607.9711\n","Epoch 16/50\n","537/537 [==============================] - 0s 56us/step - loss: 10725.2714 - regression_loss: 5244.7936 - val_loss: 979.9256 - val_regression_loss: 471.5423\n","Epoch 17/50\n","537/537 [==============================] - 0s 65us/step - loss: 8704.4504 - regression_loss: 4222.5311 - val_loss: 867.1765 - val_regression_loss: 414.0632\n","Epoch 18/50\n","537/537 [==============================] - 0s 58us/step - loss: 8082.8696 - regression_loss: 3902.1221 - val_loss: 877.5259 - val_regression_loss: 418.6270\n","Epoch 19/50\n","537/537 [==============================] - 0s 57us/step - loss: 8971.6387 - regression_loss: 4346.1563 - val_loss: 883.3959 - val_regression_loss: 421.4583\n","Epoch 20/50\n","537/537 [==============================] - 0s 54us/step - loss: 9204.8457 - regression_loss: 4459.5297 - val_loss: 819.4614 - val_regression_loss: 389.8338\n","Epoch 21/50\n","537/537 [==============================] - 0s 55us/step - loss: 8323.1252 - regression_loss: 4021.7085 - val_loss: 743.3196 - val_regression_loss: 352.3678\n","Epoch 22/50\n","537/537 [==============================] - 0s 57us/step - loss: 7549.5295 - regression_loss: 3640.7918 - val_loss: 724.8622 - val_regression_loss: 343.8315\n","Epoch 23/50\n","537/537 [==============================] - 0s 66us/step - loss: 6852.3311 - regression_loss: 3298.1030 - val_loss: 761.5430 - val_regression_loss: 362.8046\n","Epoch 24/50\n","537/537 [==============================] - 0s 63us/step - loss: 6629.9558 - regression_loss: 3190.1014 - val_loss: 791.8488 - val_regression_loss: 378.3919\n","Epoch 25/50\n","537/537 [==============================] - 0s 53us/step - loss: 6337.3746 - regression_loss: 3050.5624 - val_loss: 762.3348 - val_regression_loss: 363.7805\n","Epoch 26/50\n","537/537 [==============================] - 0s 57us/step - loss: 6493.9989 - regression_loss: 3128.6704 - val_loss: 676.7881 - val_regression_loss: 320.8434\n","Epoch 27/50\n","537/537 [==============================] - 0s 61us/step - loss: 5806.9060 - regression_loss: 2784.9886 - val_loss: 592.5068 - val_regression_loss: 278.2823\n","Epoch 28/50\n","537/537 [==============================] - 0s 55us/step - loss: 5341.6062 - regression_loss: 2547.8852 - val_loss: 544.5488 - val_regression_loss: 253.7228\n","Epoch 29/50\n","537/537 [==============================] - 0s 62us/step - loss: 5117.6947 - regression_loss: 2431.7094 - val_loss: 526.0546 - val_regression_loss: 243.9161\n","Epoch 30/50\n","537/537 [==============================] - 0s 64us/step - loss: 4881.2417 - regression_loss: 2306.5016 - val_loss: 516.6631 - val_regression_loss: 238.8792\n","Epoch 31/50\n","537/537 [==============================] - 0s 64us/step - loss: 4622.2620 - regression_loss: 2176.5772 - val_loss: 520.8052 - val_regression_loss: 240.9385\n","Epoch 32/50\n","537/537 [==============================] - 0s 61us/step - loss: 4415.4005 - regression_loss: 2072.9110 - val_loss: 544.9598 - val_regression_loss: 253.2776\n","Epoch 33/50\n","537/537 [==============================] - 0s 53us/step - loss: 3983.6805 - regression_loss: 1859.6857 - val_loss: 566.3715 - val_regression_loss: 264.3227\n","Epoch 34/50\n","537/537 [==============================] - 0s 56us/step - loss: 4055.1261 - regression_loss: 1896.3611 - val_loss: 553.7014 - val_regression_loss: 258.2037\n","Epoch 35/50\n","537/537 [==============================] - 0s 62us/step - loss: 3784.8679 - regression_loss: 1765.4523 - val_loss: 524.8621 - val_regression_loss: 243.8375\n","Epoch 36/50\n","537/537 [==============================] - 0s 65us/step - loss: 3530.5113 - regression_loss: 1638.8954 - val_loss: 506.7728 - val_regression_loss: 234.7412\n","Epoch 37/50\n","537/537 [==============================] - 0s 56us/step - loss: 3319.2949 - regression_loss: 1531.6071 - val_loss: 503.1360 - val_regression_loss: 232.8864\n","Epoch 38/50\n","537/537 [==============================] - 0s 60us/step - loss: 3270.1318 - regression_loss: 1505.1977 - val_loss: 506.6513 - val_regression_loss: 234.7080\n","Epoch 39/50\n","537/537 [==============================] - 0s 53us/step - loss: 2931.6050 - regression_loss: 1336.6870 - val_loss: 515.8657 - val_regression_loss: 239.4657\n","Epoch 40/50\n","537/537 [==============================] - 0s 56us/step - loss: 2897.4911 - regression_loss: 1321.3955 - val_loss: 522.4646 - val_regression_loss: 242.8630\n","Epoch 41/50\n","537/537 [==============================] - 0s 58us/step - loss: 2898.7012 - regression_loss: 1322.3359 - val_loss: 513.7397 - val_regression_loss: 238.5042\n","Epoch 42/50\n","537/537 [==============================] - 0s 54us/step - loss: 2861.9812 - regression_loss: 1304.7542 - val_loss: 493.3763 - val_regression_loss: 228.2638\n","Epoch 43/50\n","537/537 [==============================] - 0s 59us/step - loss: 2754.6134 - regression_loss: 1249.9172 - val_loss: 473.1787 - val_regression_loss: 218.1507\n","Epoch 44/50\n","537/537 [==============================] - 0s 63us/step - loss: 2567.8930 - regression_loss: 1154.6912 - val_loss: 457.2430 - val_regression_loss: 210.3414\n","Epoch 45/50\n","537/537 [==============================] - 0s 57us/step - loss: 2563.0417 - regression_loss: 1154.3644 - val_loss: 443.9181 - val_regression_loss: 203.9205\n","Epoch 46/50\n","537/537 [==============================] - 0s 54us/step - loss: 2357.4361 - regression_loss: 1052.5212 - val_loss: 433.6564 - val_regression_loss: 199.0344\n","Epoch 47/50\n","537/537 [==============================] - 0s 55us/step - loss: 2250.4364 - regression_loss: 1001.1143 - val_loss: 420.3515 - val_regression_loss: 192.4823\n","Epoch 48/50\n","537/537 [==============================] - 0s 63us/step - loss: 2308.0839 - regression_loss: 1029.6613 - val_loss: 405.2422 - val_regression_loss: 184.8835\n","Epoch 49/50\n","537/537 [==============================] - 0s 58us/step - loss: 2233.0053 - regression_loss: 992.9954 - val_loss: 393.5291 - val_regression_loss: 178.9569\n","Epoch 50/50\n","537/537 [==============================] - 0s 53us/step - loss: 2144.5132 - regression_loss: 945.7582 - val_loss: 383.7163 - val_regression_loss: 174.0982\n","***************************** elapsed_time is:  3.998629331588745\n","Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 110927.2016 - regression_loss: 55513.5028 - val_loss: 12357.7969 - val_regression_loss: 6183.7163\n","Epoch 2/50\n","537/537 [==============================] - 0s 55us/step - loss: 95520.0420 - regression_loss: 47801.4789 - val_loss: 10158.4072 - val_regression_loss: 5080.9951\n","Epoch 3/50\n","537/537 [==============================] - 0s 55us/step - loss: 78377.4468 - regression_loss: 39208.4063 - val_loss: 7422.2485 - val_regression_loss: 3707.6016\n","Epoch 4/50\n","537/537 [==============================] - 0s 62us/step - loss: 56177.6682 - regression_loss: 28069.3979 - val_loss: 4476.3364 - val_regression_loss: 2225.6074\n","Epoch 5/50\n","537/537 [==============================] - 0s 55us/step - loss: 33310.0243 - regression_loss: 16565.0067 - val_loss: 2136.3845 - val_regression_loss: 1040.6344\n","Epoch 6/50\n","537/537 [==============================] - 0s 54us/step - loss: 15006.4550 - regression_loss: 7292.6412 - val_loss: 1526.6697 - val_regression_loss: 714.8209\n","Epoch 7/50\n","537/537 [==============================] - 0s 53us/step - loss: 10763.4145 - regression_loss: 5003.8536 - val_loss: 2190.7014 - val_regression_loss: 1037.1433\n","Epoch 8/50\n","537/537 [==============================] - 0s 60us/step - loss: 17030.6982 - regression_loss: 8067.2448 - val_loss: 2158.4795 - val_regression_loss: 1030.3508\n","Epoch 9/50\n","537/537 [==============================] - 0s 66us/step - loss: 17624.3331 - regression_loss: 8439.9771 - val_loss: 1445.9058 - val_regression_loss: 688.9182\n","Epoch 10/50\n","537/537 [==============================] - 0s 59us/step - loss: 11706.0935 - regression_loss: 5603.1268 - val_loss: 970.6950 - val_regression_loss: 462.9957\n","Epoch 11/50\n","537/537 [==============================] - 0s 54us/step - loss: 7349.7915 - regression_loss: 3513.7128 - val_loss: 1008.3940 - val_regression_loss: 488.4323\n","Epoch 12/50\n","537/537 [==============================] - 0s 51us/step - loss: 7142.3107 - regression_loss: 3465.2362 - val_loss: 1231.1295 - val_regression_loss: 602.0757\n","Epoch 13/50\n","537/537 [==============================] - 0s 53us/step - loss: 8464.8037 - regression_loss: 4141.3179 - val_loss: 1335.5205 - val_regression_loss: 653.9043\n","Epoch 14/50\n","537/537 [==============================] - 0s 63us/step - loss: 9322.9414 - regression_loss: 4567.8093 - val_loss: 1246.2001 - val_regression_loss: 607.5350\n","Epoch 15/50\n","537/537 [==============================] - 0s 60us/step - loss: 8700.7655 - regression_loss: 4242.5800 - val_loss: 1046.9305 - val_regression_loss: 505.7484\n","Epoch 16/50\n","537/537 [==============================] - 0s 55us/step - loss: 7298.1650 - regression_loss: 3523.3226 - val_loss: 858.5405 - val_regression_loss: 409.5059\n","Epoch 17/50\n","537/537 [==============================] - 0s 61us/step - loss: 5928.8821 - regression_loss: 2824.4628 - val_loss: 769.8546 - val_regression_loss: 363.6500\n","Epoch 18/50\n","537/537 [==============================] - 0s 58us/step - loss: 5403.4068 - regression_loss: 2549.8109 - val_loss: 777.8787 - val_regression_loss: 366.9498\n","Epoch 19/50\n","537/537 [==============================] - 0s 55us/step - loss: 5512.7473 - regression_loss: 2598.9495 - val_loss: 809.5735 - val_regression_loss: 382.9034\n","Epoch 20/50\n","537/537 [==============================] - 0s 71us/step - loss: 5856.0967 - regression_loss: 2773.7127 - val_loss: 797.5456 - val_regression_loss: 377.5688\n","Epoch 21/50\n","537/537 [==============================] - 0s 61us/step - loss: 5719.9982 - regression_loss: 2708.6127 - val_loss: 741.7773 - val_regression_loss: 350.6518\n","Epoch 22/50\n","537/537 [==============================] - 0s 54us/step - loss: 5248.1101 - regression_loss: 2484.8608 - val_loss: 692.7156 - val_regression_loss: 327.0043\n","Epoch 23/50\n","537/537 [==============================] - 0s 56us/step - loss: 4801.8314 - regression_loss: 2266.4612 - val_loss: 677.9918 - val_regression_loss: 320.3412\n","Epoch 24/50\n","537/537 [==============================] - 0s 59us/step - loss: 4686.9372 - regression_loss: 2215.0249 - val_loss: 688.2880 - val_regression_loss: 325.9601\n","Epoch 25/50\n","537/537 [==============================] - 0s 57us/step - loss: 4500.2513 - regression_loss: 2122.7679 - val_loss: 694.9386 - val_regression_loss: 329.5343\n","Epoch 26/50\n","537/537 [==============================] - 0s 57us/step - loss: 4741.7291 - regression_loss: 2249.6836 - val_loss: 679.2565 - val_regression_loss: 321.7030\n","Epoch 27/50\n","537/537 [==============================] - 0s 62us/step - loss: 4520.5735 - regression_loss: 2137.7200 - val_loss: 644.0569 - val_regression_loss: 303.9705\n","Epoch 28/50\n","537/537 [==============================] - 0s 59us/step - loss: 4297.6209 - regression_loss: 2024.8174 - val_loss: 608.1321 - val_regression_loss: 285.7733\n","Epoch 29/50\n","537/537 [==============================] - 0s 56us/step - loss: 4027.8309 - regression_loss: 1887.1108 - val_loss: 584.7352 - val_regression_loss: 273.8000\n","Epoch 30/50\n","537/537 [==============================] - 0s 60us/step - loss: 4037.8293 - regression_loss: 1892.3354 - val_loss: 572.7559 - val_regression_loss: 267.4958\n","Epoch 31/50\n","537/537 [==============================] - 0s 60us/step - loss: 4017.5946 - regression_loss: 1879.3382 - val_loss: 563.0160 - val_regression_loss: 262.3248\n","Epoch 32/50\n","537/537 [==============================] - 0s 64us/step - loss: 3876.2173 - regression_loss: 1804.7691 - val_loss: 548.6213 - val_regression_loss: 254.9198\n","Epoch 33/50\n","537/537 [==============================] - 0s 58us/step - loss: 3762.1780 - regression_loss: 1748.5555 - val_loss: 533.8223 - val_regression_loss: 247.4488\n","Epoch 34/50\n","537/537 [==============================] - 0s 60us/step - loss: 3595.0537 - regression_loss: 1665.0282 - val_loss: 522.1415 - val_regression_loss: 241.6922\n","Epoch 35/50\n","537/537 [==============================] - 0s 55us/step - loss: 3533.4914 - regression_loss: 1633.0214 - val_loss: 513.5056 - val_regression_loss: 237.5819\n","Epoch 36/50\n","537/537 [==============================] - 0s 54us/step - loss: 3275.6457 - regression_loss: 1509.2339 - val_loss: 503.1674 - val_regression_loss: 232.6629\n","Epoch 37/50\n","537/537 [==============================] - 0s 58us/step - loss: 3250.4113 - regression_loss: 1495.9014 - val_loss: 488.3439 - val_regression_loss: 225.4918\n","Epoch 38/50\n","537/537 [==============================] - 0s 64us/step - loss: 3032.0943 - regression_loss: 1391.5104 - val_loss: 472.1346 - val_regression_loss: 217.5089\n","Epoch 39/50\n","537/537 [==============================] - 0s 57us/step - loss: 2923.8764 - regression_loss: 1339.9074 - val_loss: 457.9555 - val_regression_loss: 210.4498\n","Epoch 40/50\n","537/537 [==============================] - 0s 55us/step - loss: 2821.6337 - regression_loss: 1289.6613 - val_loss: 446.7507 - val_regression_loss: 204.8568\n","Epoch 41/50\n","537/537 [==============================] - 0s 62us/step - loss: 2660.2466 - regression_loss: 1208.8455 - val_loss: 435.6942 - val_regression_loss: 199.3226\n","Epoch 42/50\n","537/537 [==============================] - 0s 62us/step - loss: 2619.4714 - regression_loss: 1186.8843 - val_loss: 424.4969 - val_regression_loss: 193.7583\n","Epoch 43/50\n","537/537 [==============================] - 0s 65us/step - loss: 2507.4606 - regression_loss: 1130.8656 - val_loss: 415.3519 - val_regression_loss: 189.2114\n","Epoch 44/50\n","537/537 [==============================] - 0s 59us/step - loss: 2476.5976 - regression_loss: 1117.9059 - val_loss: 405.8559 - val_regression_loss: 184.4488\n","Epoch 45/50\n","537/537 [==============================] - 0s 54us/step - loss: 2372.0072 - regression_loss: 1063.1303 - val_loss: 394.9918 - val_regression_loss: 178.9742\n","Epoch 46/50\n","537/537 [==============================] - 0s 56us/step - loss: 2270.5206 - regression_loss: 1014.0903 - val_loss: 383.4781 - val_regression_loss: 173.1353\n","Epoch 47/50\n","537/537 [==============================] - 0s 53us/step - loss: 2283.7933 - regression_loss: 1020.2641 - val_loss: 372.6791 - val_regression_loss: 167.6994\n","Epoch 48/50\n","537/537 [==============================] - 0s 60us/step - loss: 2204.7574 - regression_loss: 981.7463 - val_loss: 361.7910 - val_regression_loss: 162.2940\n","Epoch 49/50\n","537/537 [==============================] - 0s 57us/step - loss: 2113.9186 - regression_loss: 940.3484 - val_loss: 349.0251 - val_regression_loss: 155.9628\n","Epoch 50/50\n","537/537 [==============================] - 0s 66us/step - loss: 2018.1933 - regression_loss: 893.7979 - val_loss: 335.5306 - val_regression_loss: 149.3071\n","***************************** elapsed_time is:  3.6005756855010986\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"32zLJ4yfLOj8","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"4a71d51d-a1e8-4384-d142-1cb3e3464fb3"},"source":["df_dragonnet=pd.DataFrame([train_result, test_result])\n","df_dragonnet"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.919889</td>\n","      <td>1.107028</td>\n","      <td>4.972546</td>\n","      <td>0.826205</td>\n","      <td>0.353679</td>\n","      <td>1.817544</td>\n","      <td>Dragonnet</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.036828</td>\n","      <td>1.033174</td>\n","      <td>4.930624</td>\n","      <td>0.925162</td>\n","      <td>0.348241</td>\n","      <td>2.146797</td>\n","      <td>Dragonnet</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pehe_score-mean  pehe_score-median  ...     method  train\n","0         2.919889           1.107028  ...  Dragonnet   True\n","1         3.036828           1.033174  ...  Dragonnet  False\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"imeaA1mMLOkD"},"source":["### 1.7.1 Neura Network Visualization"]},{"cell_type":"code","metadata":{"id":"i9IhdaTnLOkF","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"bc733ef2-d4ba-48fa-feb8-9d5fc20b1871"},"source":["random_state = 1\n","\n","results_df = list()\n","test_scores = list()\n","train_scores = list()\n","\n","\n","\n","train, test = train_test_split(\n","        replications[n], train_size=train_size, random_state=random_state\n","    )\n","\n","# REPLACE this with the function you implemented and want to evaluate\n","train_ite, test_ite = causal_forest(train, test, model)\n","\n","# Calculate the scores and append them to a dataframe\n","train_scores.append(calc_scores(train[Col.ite], train_ite, metrics))\n","test_scores.append(calc_scores(test[Col.ite], test_ite, metrics))\n","\n","# Summarize the scores and save them in a dataframe\n","train_result, test_result = summarize_scores(train_scores), summarize_scores(test_scores)\n","train_result.update({'method': 'Dragonnet', 'train': True})\n","test_result.update({'method': 'Dragonnet', 'train': False})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 537 samples, validate on 60 samples\n","Epoch 1/50\n","537/537 [==============================] - 1s 2ms/step - loss: 14622.1565 - regression_loss: 7142.0761 - val_loss: 1554.1144 - val_regression_loss: 755.8400\n","Epoch 2/50\n","537/537 [==============================] - 0s 63us/step - loss: 10591.8192 - regression_loss: 5141.3342 - val_loss: 1069.6041 - val_regression_loss: 514.9556\n","Epoch 3/50\n","537/537 [==============================] - 0s 74us/step - loss: 7188.3933 - regression_loss: 3450.9330 - val_loss: 747.3391 - val_regression_loss: 355.0568\n","Epoch 4/50\n","537/537 [==============================] - 0s 58us/step - loss: 5443.8089 - regression_loss: 2587.9504 - val_loss: 519.1975 - val_regression_loss: 241.8520\n","Epoch 5/50\n","537/537 [==============================] - 0s 58us/step - loss: 3824.1067 - regression_loss: 1785.5284 - val_loss: 324.8045 - val_regression_loss: 144.9832\n","Epoch 6/50\n","537/537 [==============================] - 0s 67us/step - loss: 2084.9452 - regression_loss: 918.4310 - val_loss: 360.0456 - val_regression_loss: 162.4598\n","Epoch 7/50\n","537/537 [==============================] - 0s 68us/step - loss: 2111.3164 - regression_loss: 928.9492 - val_loss: 461.3327 - val_regression_loss: 212.9567\n","Epoch 8/50\n","537/537 [==============================] - 0s 64us/step - loss: 2942.2585 - regression_loss: 1342.6565 - val_loss: 382.3994 - val_regression_loss: 173.6419\n","Epoch 9/50\n","537/537 [==============================] - 0s 70us/step - loss: 2554.8332 - regression_loss: 1149.7320 - val_loss: 285.3643 - val_regression_loss: 125.3883\n","Epoch 10/50\n","537/537 [==============================] - 0s 64us/step - loss: 1996.6745 - regression_loss: 875.4456 - val_loss: 246.3229 - val_regression_loss: 106.0607\n","Epoch 11/50\n","537/537 [==============================] - 0s 70us/step - loss: 1729.7948 - regression_loss: 741.0916 - val_loss: 248.6249 - val_regression_loss: 107.2856\n","Epoch 12/50\n","537/537 [==============================] - 0s 72us/step - loss: 1698.9022 - regression_loss: 728.7212 - val_loss: 275.3362 - val_regression_loss: 120.6522\n","Epoch 13/50\n","537/537 [==============================] - 0s 66us/step - loss: 1868.1098 - regression_loss: 814.8615 - val_loss: 283.6924 - val_regression_loss: 124.8783\n","Epoch 14/50\n","537/537 [==============================] - 0s 67us/step - loss: 1921.0537 - regression_loss: 838.8734 - val_loss: 265.3604 - val_regression_loss: 115.7743\n","Epoch 15/50\n","537/537 [==============================] - 0s 63us/step - loss: 1762.2659 - regression_loss: 762.1662 - val_loss: 246.8041 - val_regression_loss: 106.4930\n","Epoch 16/50\n","537/537 [==============================] - 0s 63us/step - loss: 1643.2683 - regression_loss: 702.9095 - val_loss: 239.3306 - val_regression_loss: 102.6617\n","Epoch 17/50\n","537/537 [==============================] - 0s 66us/step - loss: 1578.8224 - regression_loss: 668.1296 - val_loss: 243.0766 - val_regression_loss: 104.3995\n","Epoch 18/50\n","537/537 [==============================] - 0s 62us/step - loss: 1467.6196 - regression_loss: 610.0276 - val_loss: 255.7105 - val_regression_loss: 110.6073\n","Epoch 19/50\n","537/537 [==============================] - 0s 73us/step - loss: 1549.9896 - regression_loss: 654.5805 - val_loss: 256.1873 - val_regression_loss: 110.8094\n","Epoch 20/50\n","537/537 [==============================] - 0s 65us/step - loss: 1559.8493 - regression_loss: 657.6596 - val_loss: 241.0820 - val_regression_loss: 103.3085\n","Epoch 21/50\n","537/537 [==============================] - 0s 69us/step - loss: 1487.3401 - regression_loss: 623.0497 - val_loss: 226.5077 - val_regression_loss: 96.1230\n","Epoch 22/50\n","537/537 [==============================] - 0s 61us/step - loss: 1433.7975 - regression_loss: 596.6026 - val_loss: 219.5548 - val_regression_loss: 92.7451\n","Epoch 23/50\n","537/537 [==============================] - 0s 61us/step - loss: 1443.4099 - regression_loss: 601.0642 - val_loss: 222.7244 - val_regression_loss: 94.3753\n","Epoch 24/50\n","537/537 [==============================] - 0s 67us/step - loss: 1422.2602 - regression_loss: 593.7358 - val_loss: 227.4698 - val_regression_loss: 96.7642\n","Epoch 25/50\n","537/537 [==============================] - 0s 71us/step - loss: 1427.9429 - regression_loss: 596.9113 - val_loss: 224.0708 - val_regression_loss: 95.0876\n","Epoch 26/50\n","537/537 [==============================] - 0s 65us/step - loss: 1325.1134 - regression_loss: 546.4428 - val_loss: 218.4160 - val_regression_loss: 92.2805\n","Epoch 27/50\n","537/537 [==============================] - 0s 59us/step - loss: 1372.5731 - regression_loss: 571.0425 - val_loss: 215.2655 - val_regression_loss: 90.7119\n","Epoch 28/50\n","537/537 [==============================] - 0s 65us/step - loss: 1360.2651 - regression_loss: 565.4245 - val_loss: 218.2835 - val_regression_loss: 92.1983\n","Epoch 29/50\n","537/537 [==============================] - 0s 67us/step - loss: 1323.8710 - regression_loss: 545.2133 - val_loss: 220.9643 - val_regression_loss: 93.5170\n","Epoch 30/50\n","537/537 [==============================] - 0s 69us/step - loss: 1328.6819 - regression_loss: 549.5610 - val_loss: 215.0301 - val_regression_loss: 90.5575\n","Epoch 31/50\n","537/537 [==============================] - 0s 63us/step - loss: 1305.6557 - regression_loss: 539.0312 - val_loss: 208.5293 - val_regression_loss: 87.3312\n","Epoch 32/50\n","537/537 [==============================] - 0s 69us/step - loss: 1305.5675 - regression_loss: 536.8521 - val_loss: 206.1131 - val_regression_loss: 86.1433\n","Epoch 33/50\n","537/537 [==============================] - 0s 64us/step - loss: 1297.6665 - regression_loss: 535.4303 - val_loss: 208.5167 - val_regression_loss: 87.3542\n","Epoch 34/50\n","537/537 [==============================] - 0s 59us/step - loss: 1292.0077 - regression_loss: 531.0391 - val_loss: 208.6234 - val_regression_loss: 87.4217\n","Epoch 35/50\n","537/537 [==============================] - 0s 59us/step - loss: 1276.6782 - regression_loss: 522.3815 - val_loss: 204.5030 - val_regression_loss: 85.3803\n","Epoch 36/50\n","537/537 [==============================] - 0s 64us/step - loss: 1233.0236 - regression_loss: 502.1640 - val_loss: 200.6161 - val_regression_loss: 83.4497\n","Epoch 37/50\n","537/537 [==============================] - 0s 69us/step - loss: 1266.4465 - regression_loss: 519.0699 - val_loss: 200.1945 - val_regression_loss: 83.2375\n","Epoch 38/50\n","537/537 [==============================] - 0s 63us/step - loss: 1224.7188 - regression_loss: 498.5643 - val_loss: 202.8111 - val_regression_loss: 84.5324\n","Epoch 39/50\n","537/537 [==============================] - 0s 62us/step - loss: 1225.3874 - regression_loss: 500.8966 - val_loss: 199.1351 - val_regression_loss: 82.6892\n","Epoch 40/50\n","537/537 [==============================] - 0s 64us/step - loss: 1191.3659 - regression_loss: 482.9496 - val_loss: 192.4677 - val_regression_loss: 79.3583\n","Epoch 41/50\n","537/537 [==============================] - 0s 68us/step - loss: 1197.8269 - regression_loss: 485.4293 - val_loss: 192.4802 - val_regression_loss: 79.3553\n","Epoch 42/50\n","537/537 [==============================] - 0s 67us/step - loss: 1195.2109 - regression_loss: 485.9787 - val_loss: 191.7144 - val_regression_loss: 78.9681\n","Epoch 43/50\n","537/537 [==============================] - 0s 61us/step - loss: 1177.6635 - regression_loss: 476.5420 - val_loss: 188.3350 - val_regression_loss: 77.2823\n","Epoch 44/50\n","537/537 [==============================] - 0s 68us/step - loss: 1153.7197 - regression_loss: 463.2598 - val_loss: 185.9891 - val_regression_loss: 76.1088\n","Epoch 45/50\n","537/537 [==============================] - 0s 58us/step - loss: 1138.4874 - regression_loss: 456.5851 - val_loss: 185.2569 - val_regression_loss: 75.7380\n","Epoch 46/50\n","537/537 [==============================] - 0s 58us/step - loss: 1152.7438 - regression_loss: 462.8130 - val_loss: 184.3398 - val_regression_loss: 75.2689\n","Epoch 47/50\n","537/537 [==============================] - 0s 63us/step - loss: 1142.4432 - regression_loss: 458.9440 - val_loss: 184.4334 - val_regression_loss: 75.3013\n","Epoch 48/50\n","537/537 [==============================] - 0s 66us/step - loss: 1132.6585 - regression_loss: 453.7048 - val_loss: 180.4813 - val_regression_loss: 73.3185\n","Epoch 49/50\n","537/537 [==============================] - 0s 59us/step - loss: 1132.6103 - regression_loss: 453.8608 - val_loss: 178.4081 - val_regression_loss: 72.2719\n","Epoch 50/50\n","537/537 [==============================] - 0s 59us/step - loss: 1142.2405 - regression_loss: 459.2547 - val_loss: 181.3660 - val_regression_loss: 73.7344\n","***************************** elapsed_time is:  3.9246158599853516\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VvttPAc8LOkJ","colab":{"base_uri":"https://localhost:8080/","height":333},"outputId":"1052e7e2-13ce-4ebd-b3f2-fec4a6b994b7"},"source":["import matplotlib.pyplot as plt\n","colors = (0,0,0)\n","# Plot\n","plt.scatter(test.np.ite, test_ite,  c=colors, alpha=0.5)\n","plt.title('Scatter of treatment effects')\n","plt.xlabel('real treatment effect')\n","plt.ylabel('estimated treatment effect')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"E6ZW3Us6LOkN"},"source":["## QUESTION 6\n","\n","IS THE DRAGONNET NEURAL NETWORK ESTIMATING INDIVIDUAL TREATMENT EFFECTS WELL? IF YES/NO -WHY SO?"]},{"cell_type":"markdown","metadata":{"id":"D09tYZt9LOkO"},"source":["## 1.8 Comparison of the methods"]},{"cell_type":"code","metadata":{"id":"pti3CRzkLOkO","colab":{"base_uri":"https://localhost:8080/","height":458},"outputId":"367b6cb2-52de-4f95-fa6d-5ec3252603a9"},"source":["pd.concat([df_S_learner_LR, df_PSW_LR, df_S_learner_RF, df_T_learner_LR, df_T_learner_RF, df_causal_forest, df_dragonnet ], ignore_index=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pehe_score-mean</th>\n","      <th>pehe_score-median</th>\n","      <th>pehe_score-std</th>\n","      <th>mean_absolute-mean</th>\n","      <th>mean_absolute-median</th>\n","      <th>mean_absolute-std</th>\n","      <th>method</th>\n","      <th>train</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.633660</td>\n","      <td>2.623297</td>\n","      <td>8.362125</td>\n","      <td>0.732443</td>\n","      <td>0.238185</td>\n","      <td>1.493276</td>\n","      <td>S-Learner LR</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5.625971</td>\n","      <td>2.635993</td>\n","      <td>8.213626</td>\n","      <td>1.292668</td>\n","      <td>0.396246</td>\n","      <td>2.474603</td>\n","      <td>S-Learner LR</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.595322</td>\n","      <td>2.537818</td>\n","      <td>8.244302</td>\n","      <td>0.412006</td>\n","      <td>0.284332</td>\n","      <td>0.457697</td>\n","      <td>PSW</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6.837997</td>\n","      <td>3.484394</td>\n","      <td>8.323623</td>\n","      <td>3.783440</td>\n","      <td>2.649187</td>\n","      <td>3.225824</td>\n","      <td>PSW</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.133693</td>\n","      <td>1.047904</td>\n","      <td>4.861394</td>\n","      <td>0.511598</td>\n","      <td>0.125918</td>\n","      <td>0.971366</td>\n","      <td>S-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3.320106</td>\n","      <td>1.234456</td>\n","      <td>5.197415</td>\n","      <td>0.451143</td>\n","      <td>0.137152</td>\n","      <td>1.047357</td>\n","      <td>S-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>3.106716</td>\n","      <td>1.051650</td>\n","      <td>4.788258</td>\n","      <td>0.500543</td>\n","      <td>0.124951</td>\n","      <td>0.933657</td>\n","      <td>T-Learner LR</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3.311096</td>\n","      <td>1.267110</td>\n","      <td>5.155404</td>\n","      <td>0.445728</td>\n","      <td>0.134426</td>\n","      <td>1.022285</td>\n","      <td>T-Learner LR</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.783826</td>\n","      <td>0.947208</td>\n","      <td>2.263913</td>\n","      <td>0.130565</td>\n","      <td>0.104154</td>\n","      <td>0.130342</td>\n","      <td>T-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2.512660</td>\n","      <td>1.109184</td>\n","      <td>3.501475</td>\n","      <td>0.210897</td>\n","      <td>0.123812</td>\n","      <td>0.277706</td>\n","      <td>T-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4.172661</td>\n","      <td>1.921242</td>\n","      <td>6.330031</td>\n","      <td>0.441738</td>\n","      <td>0.198733</td>\n","      <td>0.823171</td>\n","      <td>T-Learner RF</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>4.378083</td>\n","      <td>1.797608</td>\n","      <td>6.606150</td>\n","      <td>0.683703</td>\n","      <td>0.233695</td>\n","      <td>1.354973</td>\n","      <td>T-Learner RF</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2.919889</td>\n","      <td>1.107028</td>\n","      <td>4.972546</td>\n","      <td>0.826205</td>\n","      <td>0.353679</td>\n","      <td>1.817544</td>\n","      <td>Dragonnet</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>3.036828</td>\n","      <td>1.033174</td>\n","      <td>4.930624</td>\n","      <td>0.925162</td>\n","      <td>0.348241</td>\n","      <td>2.146797</td>\n","      <td>Dragonnet</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    pehe_score-mean  pehe_score-median  ...        method  train\n","0          5.633660           2.623297  ...  S-Learner LR   True\n","1          5.625971           2.635993  ...  S-Learner LR  False\n","2          5.595322           2.537818  ...           PSW   True\n","3          6.837997           3.484394  ...           PSW  False\n","4          3.133693           1.047904  ...  S-Learner RF   True\n","5          3.320106           1.234456  ...  S-Learner RF  False\n","6          3.106716           1.051650  ...  T-Learner LR   True\n","7          3.311096           1.267110  ...  T-Learner LR  False\n","8          1.783826           0.947208  ...  T-Learner RF   True\n","9          2.512660           1.109184  ...  T-Learner RF  False\n","10         4.172661           1.921242  ...  T-Learner RF   True\n","11         4.378083           1.797608  ...  T-Learner RF  False\n","12         2.919889           1.107028  ...     Dragonnet   True\n","13         3.036828           1.033174  ...     Dragonnet  False\n","\n","[14 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"5kl0lnoxLOkU"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","## QUESTION 7\n","\n","HOW DO THE DIFFERENT MODELS COMPARE IN TERMS OF MEAN PEHE ACCURACY? WHAT ASPECTS DO YOU THINK DETERMINE WHETHER ONE MODEL PERFOMS BETTER THAN ANOTER?"]},{"cell_type":"code","metadata":{"id":"-fzmpIfvaEbu"},"source":[""],"execution_count":null,"outputs":[]}]}