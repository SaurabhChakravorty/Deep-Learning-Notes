{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Correlation does not equal causation - examples\n",
    "\n",
    "**Correlation vs. Causation - - Example 1**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1WltNRmI3S1X3FlqxfFw9fUqPFaNvXLaW\" width=50%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation vs. Causation - - Example 2**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1_OqTFn4ENRgXW4D6WNL50P839YryPoR4\" width=70%>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation vs. Causation - - Example 3**\n",
    "Everyday practical relevance. Did the advert cause me to click on the Frankfurt School link or would I have cliked anyhow?\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1fNkG2XF2SpondJAtrav5XYCu80W52JGx\" width=40%>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Correlation does not equal causation - differences defined\n",
    "\n",
    "**correlation does not imply causation**. The following are  **differences** between the two:\n",
    "\n",
    "- The direction of causation may be reversed, because **from correlation we can never know the direction of causation**. For instance, the temperature and the thermometer are directly correlated, but we cannot know the direction of causation from looking at the two quantities.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1CeEKjWZi56M3a0qG_HNlo6SJuXJ8aUUv\" width=75%>\n",
    "\n",
    "\n",
    "Example of why this matters: **Amazon's recommender system**. When searching for a laptop rucksack the system recommends also buying a laptop to go along with it. This is reasonable from a machine learning point of view, as the to items are closely related and probably often bought together. Looking at the recommendation from a causal point of view, however, we are tempted to say that he who searches a laptop rucksack already has a laptop. Recommending a laptop to everyone who buys rucksack is like recommending to buy a house to everyone who buys a doormat.\n",
    "\n",
    "- The correlation is **spurious or coincidental**. E.g. the number of Nobel price laureates in a country correlates with the per capita consumption of chocolate.\n",
    "\n",
    "<img src=\"https://swflreia.com/wp-content/uploads/2017/05/ScreenHunter_117-May.-17-11.23.jpg\" width=70%>\n",
    "\n",
    "\n",
    "\n",
    "- There is a **common cause C of A and B**. For example, the age a child causally influences its reading ability and its shoe size thus creating a non-causal correlation between shoe size and reading ability.\n",
    "\n",
    "<img src=\"http://wps.pearsoned.com.au/wps/media/objects/3031/3104229/_images_/md10_1420.gif\" width=60%>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Prediction does not equal causation -  confounders (endogenous variation)/ common cause\n",
    "\n",
    "**Prediction and causality are not the same - the relevance of confounders**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1Dj5Zph8rMkxNcBZfojBaFtnxllzLejoG\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1PtF3xzlNNQ7vIt_tctd5FJqQJsmBls7X\" width=75%>\n",
    "\n",
    "\n",
    "Source: Ebbes, P.; Papies, D. Tackling endogeneity concerns with and without\n",
    "instrumental variables [here](http://bit.ly/Code_Endog_AMA2020). \n",
    "\n",
    "\n",
    "An estimator is **consistent** if, as the sample size increases, the estimates (produced by the estimator) \"converge\" to the true value of the parameter being estimated. To be slightly more precise - consistency means that, as the sample size increases, the sampling distribution of the estimator becomes increasingly concentrated at the true parameter value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. When to use causal models and when not\n",
    "\n",
    "__When prediction is not enough__\n",
    "\n",
    "\n",
    "__When causality matters__\n",
    "- We want to __intervene__ into the system e.g.\n",
    "- Drug development\n",
    "- Marketing actions (coupons, online marketing)\n",
    "- Strategic decision (and interventions)\n",
    "- Robots and self driving cars (any AI system learning from its interactions with the environment e.g. in reinforcement learning)\n",
    "- Effect of shocks on financial markets (e.g. news, weather change)\n",
    "- Etc.\n",
    "\n",
    "- If there is covariate shift or missing data\n",
    "\n",
    "__When causality does not matter__\n",
    "- Predict next day’s demand e.g. for ice cream\n",
    "- Predict the weather\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Causality \"defined\"\n",
    "\n",
    "Causality has been defined in many ways and been studied over millennia across fields ranging from philosophy to more recent approaches in statistics, economics, law, and computer science.  Famous examples include David Hume’s 1748 treatise on induction and its response by Immanuel Kant. \n",
    "\n",
    "__Hume: Whenever A then B and, if A would not have occurred B would not have occurred (counterfactual)__\n",
    "\n",
    "Computer science and econometrics: causal- and correlational relationships both modeled with conditional distributions, 𝑝(y|𝐱) \n",
    "\n",
    "But **two aspects specific** for **causality**:\n",
    "- Effects of **intervention**: **“what if I do X?” 𝑝(y│𝑑𝑜(𝐱) )** [(Galles and Pearl, 1997) ](https://www.sciencedirect.com/science/article/pii/S0004370297000477) \n",
    "- **Counterfactual 𝑝(y′│𝑥′ );  x’ and y’ did not happen**. If we know a causal relationship between X and Y, we know what were to happen with Y (e.g., revenue lift) regardless of whether X (e.g., ad spending) actually happened or not [(Rubin, 2005)](https://www.tandfonline.com/doi/pdf/10.1198/016214504000001880?casa_token=tLy-RK-yJ6oAAAAA%3AzUdCp5JT3UPQpsfdywXCyOfKNIzu2b2rdiXMvnNMm6SPHJ3l1U_IcuBpL-EQtRmz3G44LTYEWSn0&): **potential outcomes framework**\n",
    "\n",
    "\n",
    "\n",
    "Table: three conceptually different ways of thinking about the world\n",
    " <img src=\"https://i.imgur.com/fAr7FFd.jpg\" width=75%>\n",
    "\n",
    "Source: \"https://i.imgur.com/fAr7FFd.jpg\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Defining causality in terms of potential outcomes\n",
    "\n",
    "**Potential Outcomes & Causal Effect - the way we will model causality**\n",
    "\n",
    "Suppose T is an indicator variable which is T=1 when the treatment is given and T=0, when the treatment is not given. \n",
    "\n",
    "- Potential outcomes (Neyman-Rubin causal model$^1$) for treatment outcome: $𝑌_0, 𝑌_1$,   𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑: $𝑌=(1−𝑇) 𝑌_0+𝑇𝑌_1$  -  **Only observe one**\n",
    "\n",
    "\n",
    "- Goal: individual treatment effect, $𝐼𝑇𝐸(𝑋)≔𝔼[𝑌_1−𝑌_0│𝑋=𝑥]$   -    **Never observed**\n",
    "\n",
    "\n",
    "- **If interventions are random (RCT gold standard!)**:  ACE=𝔼[Y│T=1]−𝔼[Y│T=0]   -  **Average Causal Effect**\n",
    "\n",
    "\n",
    "- Assume strong **ignorability $^2$:   $𝒀_𝟎, 𝒀_𝟏  \t\\perp  𝑻|𝑿$ $^3$ and consistency and SUTVA**\n",
    "\n",
    "\n",
    "\n",
    "1 Ruben, JASA, 2011, ITE is also called conditional average treatment effect CATE,   \n",
    "2 Aka – exogeneity, no hidden confounders, selection on the observables, or no omitted variable bias   \n",
    "3 More precisely the condition shown here is conditional ignobility\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditions explained (a more detailed definition will follow)**\n",
    "- Ignorability simply means we can ignore how one ended up in one vs. the other group (‘treated’ $T_x = 1$, or ‘control’ $T_x = 0$) when it comes to the potential outcome (say Y). Once we condition for X whether a treatment is received or not received should be at random. \n",
    "- Consistency: same cause same effect\n",
    "- SUTVA - Stable unit treatment value assumption:\n",
    "- We require that \"the potential outcome observation on one unit should be unaffected by the particular assignment of treatments to the other units\" [(Cox 1958, §2.4)](https://projecteuclid.org/euclid.aoms/1177706618). This is called the Stable Unit Treatment Value Assumption (SUTVA), which goes beyond the concept of independence.\n",
    "- In the context of our example, Joe's blood pressure should not depend on whether or not Mary received the treatment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fundamental Problem of Causal Inference\n",
    "\n",
    "For any unit $i$, we can only **observer** the **outcome either after treatment or after a placebo** has been administered, **never both**. Dealing with this problem is the core task of causal inferences. \n",
    "\n",
    "Essentially we are trying to **derive counterfactual conclusions**, i.e. causes, from only looking at factual data, the observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal effect estimation vs. causal discovery**\n",
    "\n",
    "**Causal discovery**: retrieval of causal structure from data we collected from the world\n",
    "\n",
    "**Causal effect estimation**: estimation of the effect of a specific action, often called treatment\n",
    "\n",
    "Main difference: for causal effect estimation we already have a hypothesis about a causal relationship and want to estimate whether it is there and  to which extent the effect holds in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand\\independent{\\protect\\mathpalette{\\protect\\independenT}{\\perp}}$\n",
    "$\\def\\independenT#1#2{\\mathrel{\\rlap{$#1#2$}\\mkern2mu{#1#2}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Randomization and Pseudo Randomization\n",
    "\n",
    "\n",
    "In practice we cannot observer the counterfactual, but under fully randomized conditions the mean of a group that is not treated can be deducted from the mean of a group that is treated to obtain the average causal effect. Thus, under “perfect” randomization, causes can be inferred without observing the counterfactual. \n",
    "\n",
    "Causal inference thus requires randomization within the data (i.e., X) to identify causal effects and control for confounders. This requires either “true” randomization (e.g., randomized controlled trials (RCT), or instead some form of “pseudo-randomization” (e.g., diff-in-diff) obtained by making assumptions on the collected data that are then used to mathematically construct an equivalent of a randomized controlled trial.\n",
    "\n",
    "Randomized controlled trials are the gold standard of causality, in the sense that we are not forced to make often strong (and potentially unrealistic) assumptions about the data. If we can set up a trial where two groups only differ by the treatment they have received, controlling for all other potentially confounding factors, we can make sure that the treatment is actually causally having the effect we claim it to have.\n",
    "\n",
    "Unfortunately, randomized controlled trials are often impractical in business situations, e.g. the same company having and not having an ecommerce channel, or it is at least difficult to control for all potentially confounding factors or unethical. Rather, a more realistic scenario is that we make some changes and observe the effect of those changes. Based on the observed effects we then need to infer the relevant causal relationship.\n",
    "\n",
    "Thus, when the gold standard of randomized controlled trials is not possible, we need to infer causality through statistical methods based on pseudo-randomization of the data. Commonly used generic meta-learners are:\n",
    "- Propensity score weighting / matching\n",
    "- Conditional mean regression (S and  T learner)\n",
    "- Doubly robust estimators \n",
    "- K-Nearest Neighbours\n",
    "- X-Learner\n",
    "- R-Learner\n",
    "\n",
    "It is important not to confuse the meta learner with a particular statistical estimation technique (linear regression, neural networks etc). \n",
    "\n",
    "The statistical approaches share the following notion in one way or another: We cannot experimentally create two groups that do not differ by anything else than treatments, so we instead try to mathematically adjust to create “hypothetical twins” which only differ by the treatment and are similar otherwise. If we can find for example two customers that are extremely similar or can be mathematically “simulated” as being very similar, and one has obtained the treatment whilst the other has not, then we can establish whether the treatment has the expected causal effect. \n",
    "\n",
    "This creates __two difficulties__. First, the measurement of __similarity__ requires the definition of a __distance function__. Yet, for high dimensional spaces which are common in the social sciences, __distances shrink__ by the __curse of dimensionality__. The shrinking of distances in a high dimensional space implies that we cannot clearly determine how similar mathematical objects (such as a feature-based descriptions of a customer) are. Thus, if there are a lot of potentially relevant input variables seemingly close subjects for treatment and control may actually be quite different. \n",
    "\n",
    "Second, high dimensional data is frequently subject to the effect of __confounders__, variables that __influence and are correlated with the outcome of interest__. If these confounders are not accounted for, __causal attributions are incorrect__.\n",
    "When confounders can be measured, they can be accounted for using techniques to adjust for their effects such as propensity reweighing or covariance shift. However, in a real world application it is typically difficult to decipher the true confounding effect as data is high dimensional and measurements are noisy. For instance, in the case of an online advertising campaign we are interested in identifying treatments that produce better outcomes for companies but have to adjust causal measurements for many potentially confounding variables such as age, socioeconomic status of customers, the particular products the company is selling, the layout of the website or which channels customers came from. This can be problematic as illustrated by the Simpsons Paradox, where hidden confounding factors lead to paradoxical results.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observational data and Confounders**:\n",
    "\n",
    "- **Observational** data: no interventions (treatment not random)\n",
    "- **Handling of confounders**: factors that affect both an intervention and its outcome\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definition of a confounder__\n",
    "\n",
    " - Expressed by causal graphs\n",
    " \n",
    " - Here we consider the following causal graph\n",
    " \n",
    " \n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1eVYxPYkp93I8eJstdKYV8tET7MA-ygTt\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Confounders**\n",
    "\n",
    "- Attempt to measure all important confounders\n",
    "- De facto: create __simulated randomized controlled trial (choosing appropriate statistical technique)__\n",
    "- **Statistical twin** (similar in a mathematical measurement space - in respect to the outcome y)\n",
    "\n",
    "\n",
    "If we have a __randomized controlled trial, we always have a close statistical twin__ with similar level of covariates X!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following generic meta-learners are often used for \"pseudo-randomization\" of the data:\n",
    "- Propensity score weighting / matching\n",
    "- Conditional mean regression (S and  T learner)\n",
    "- Doubly robust estimators \n",
    "- K- Nearest Neighbours \n",
    "- X-Learner\n",
    "- R-Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulated randomized controlled trial- representation learning**\n",
    "\n",
    "- learn a functional representation that ensures that treatment groups are similarly distributed (form of randomization)\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1ShQtqStYeoUOKzL-9dacEgnZN7q4hucf\" width=75%>\n",
    "\n",
    "\n",
    "**Some Terminology**\n",
    "*Balanced representation*: distribution of covariates of treatment and control group are the same (in terms of covariates that have an influence on both treatment and outcome). An unbalanced representation is one where the covariate distribution of treatment and control group are different - this amounts to selection bias/ confoundedness. If a distribution is balanced we have achieved ignorability.\n",
    "\n",
    "**Other ways of seeing this**: \n",
    "- Clustering (find the clusters where observations are the most similar in terms of the covariates (X) -> should be the statistical twins)\n",
    "- weighing\n",
    "- Propensities - likelihood of being treated/ not treated given a particular level of the covariates X (adjust for for this to get a balanced distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some comment:**\n",
    "Note that the problem of causality is very closely related to the problems of:\n",
    "- Domain adaptation\n",
    "- Sample selection bias\n",
    "- Covariate shift\n",
    "\n",
    "And thus **generalization** at broad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice non-quantitative introduction can also be found [here](https://www.markhw.com/blog/causalforestintro). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
