{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short story of change\n",
    "\n",
    "As with all major systems in technology (like browsers) there is a constant state of \"platform wars\", where the different solutions and ecosystems vie for dominance. This is also the case with deep learning frameworks.\n",
    "\n",
    "During it's course of development, Tensorflow amassed a seizable community, just by the virtue of being a large scale, deployable system with a giant as Google backing it. None the less, developing TF 1.x based code was absolutely not a smooth experience. \n",
    "\n",
    "The concept of execution Sessions and \"define-and-run\" made debugging clumsy and complicated, the programming solutions used were unlike other \"standard\" systems, and the whole thing was just somehow \"un-pythonic\".\n",
    "\n",
    "This also paved the way for the emergence of new competitors, most notably [Pytorch](https://pytorch.org/) backed by Facebook Research (and very much inspired by Chainer). It is a full \"pythonic\" framework, with native conditional statements, command flow control, iteration, etc. It is a great tool for experimental code for new architectures, thus it became hugely popular in the research community. It was originally not an industry grade deployable framework, but it is being enhanced in that direction. \n",
    "\n",
    "None the less the popularity of Pytorch amongst the research community proved to be a major challenge for Tensorflow. In fact, in 2019 [this](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/) article argued, that Pytorch is winning the game, since it has the backing of the research community. (Which is arguable...)\n",
    "\n",
    "<mg src=\"https://thegradient.pub/content/images/2019/10/number_medium.png\" width=45%>\n",
    "\n",
    "Thus, with the 2.0 release of Tensorflow, Google announced a major overhaul, to become competitive again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main changes\n",
    "\n",
    "- Usability:\n",
    "    - tf.keras as the de-facto high level API in TF\n",
    "    -\"Eager execution\" enabled by default\n",
    "- Clarity:\n",
    "    - Removal of duplicate functionality\n",
    "    - Consistent API\n",
    "    - Consistent API in the whole ecosystem\n",
    "- Flexibility:\n",
    "    - Keeping the low level API\n",
    "    - Deep access to internal ops (`tf.raw_ops`)\n",
    "    - Inheritable interfaces for layers (and everything else)\n",
    "\n",
    "Video of the [TF 2.0 intro event](https://www.youtube.com/watch?v=k5c-vg4rjBw) details the motivation and main changes, but the main takeaway is:\n",
    "\n",
    "- **tf.Keras became the default layer abstraction**\n",
    "- **Eager execution** is the norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TF 2.x ecosystem\n",
    "\n",
    "What remained - or in fact became more powerful - is the \"production\" ecosystem around TF.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=18sayJe87HgVX1UpLeLDucX6TsWQoI9xt\" width=65%>\n",
    "\n",
    "From mobile and edge devices to hugely scalable server side deployments, there is a tool for everything.\n",
    "\n",
    "Especially remarkable is [Tensorflow Serving](https://www.tensorflow.org/tfx/guide/serving), which enables huge scale deployment.\n",
    "\n",
    "([This video](https://www.youtube.com/watch?v=q_IkJcPyNl0) is a nice intro to Serving.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing models with TF 2.x\n",
    "\n",
    "The \"intended usage\" of TF also changed a lot. According to the official sources, this is the suggested way / levels of development:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1pbSvYLtwO_8Q31S7kXk1KDbflSIanrYl\" width=65%>\n",
    "\n",
    "As we can see, tf.Keras is dominating the field, nearly no mention about \"classic\" TF ops and ways (which is now called \"Tensorflow Core\"). We also agree, that for most use cases, tf.Keras is the way to go - so maybe __learning the functional API__ is the most efficient use of one's time.\n",
    "\n",
    "A very nice intro to the layers of development with TF 2.x can be found [here](https://www.youtube.com/watch?v=5ECD8J3dvDQ). We will follow it with our discussion below.\n",
    "\n",
    "None the less, let's revisit, how TF \"core\" changed with Eager. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:43:36.418065Z",
     "start_time": "2020-05-07T15:43:36.413860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dominating change: it is now **\"define-by-run\", so whatever we declare, it gets executed instantly!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:43:36.479458Z",
     "start_time": "2020-05-07T15:43:36.465024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=69, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 0.64984685, -0.75238955, -0.13366635],\n",
       "       [ 0.63856125, -0.6594663 ,  0.08644307]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2,3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:43:36.485654Z",
     "start_time": "2020-05-07T15:43:36.480756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=76, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.694922  ,  0.37309778, -1.7960442 ],\n",
       "       [ 2.3307428 , -1.2173747 , -0.6468137 ]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.random.normal(shape=(2,3))\n",
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:27:45.787988Z",
     "start_time": "2020-05-07T15:27:45.784854Z"
    }
   },
   "source": [
    "Inside TF, there is a normal Numpy array, that is neatly accessible, we can manipulate it freely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:43:36.539300Z",
     "start_time": "2020-05-07T15:43:36.534673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0450752 ,  1.1254873 , -1.6623778 ],\n",
       "       [ 1.6921815 , -0.55790836, -0.73325676]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:43:36.550674Z",
     "start_time": "2020-05-07T15:43:36.544240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([4, 5, 6], dtype=int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tf.Variable([1,2,3])\n",
    "print(c)\n",
    "c.assign([4,5,6]) \n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Caveat: a slight distinction of general Tensors and Variables still lingers, only Variables can be assigned.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient tape\n",
    "\n",
    "Now - akin to Pytorch - the \"gradient tape\" is a strong abstraction in low level TF 2.x. It allows for the collection of gradient information, and it's application to dynamically defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:43:36.608040Z",
     "start_time": "2020-05-07T15:43:36.602973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=92, shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y=x*x\n",
    "dy_dx = g.gradient(y,x)\n",
    "\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:46:13.968703Z",
     "start_time": "2020-05-07T15:46:13.964661Z"
    }
   },
   "source": [
    "Please observe, that variables in a tape scope get automatically tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T15:45:43.672997Z",
     "start_time": "2020-05-07T15:45:43.636572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=678, shape=(32,), dtype=float32, numpy=\n",
       "array([ 0.2144123 ,  0.9980367 ,  2.523283  , -0.05389541,  1.2221451 ,\n",
       "        1.6887089 ,  0.38739684,  2.1451378 , -0.7038405 ,  2.1398394 ,\n",
       "       -0.33315706,  0.65045196, -0.25495082, -2.2888103 ,  0.6497345 ,\n",
       "        1.2448461 , -0.62869006,  0.13058892, -0.4034389 ,  0.53868127,\n",
       "       -0.07362868,  1.2114792 , -0.28120014, -1.1203902 ,  0.3054561 ,\n",
       "       -2.00901   , -0.591593  , -0.9884967 ,  0.71254337,  1.5557951 ,\n",
       "       -0.969301  ,  0.09261519], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense1 = tf.keras.layers.Dense(32)\n",
    "dense2 = tf.keras.layers.Dense(32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    result = dense2(dense1(tf.zeros([1,10])))\n",
    "    grad = tape.gradient(result, dense1.variables)\n",
    "\n",
    "grad[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This facility is enabling the rapid development and automatic differentiation of complex architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple linear regression model pretty close to TF1, but already in TF2, see [here](https://towardsdatascience.com/get-started-with-tensorflow-2-0-and-linear-regression-29b5dbd65977) or for a different style [here](https://heartbeat.fritz.ai/linear-regression-using-tensorflow-2-0-1cd51e211e1f).\n",
    "\n",
    "If you want to know, what happens under the hood of TF 2.x, eg. for the handling of conditionals, see [here](https://www.youtube.com/watch?v=IzKXEbpT9Lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding up custom functions\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1IROhjuSRSzfEPMLWXmNUc_QiH70hTV5z\" width=65%>\n",
    "\n",
    "[Tensorflow @function decorator](https://www.tensorflow.org/api_docs/python/tf/function) is another nice facility. Even if we omit it, we can - as we saw above - write Pythonic code that gets executed eagerly, so we can build up complex structures in a functional manner. None the less, if we would like to make the code __run more efficiently__, we can use  the decorator, that results in TF trying to __compile our ops to a static graph, thus speeding it up__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:10_days_AI]",
   "language": "python",
   "name": "conda-env-10_days_AI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
