{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "mH8yC7Z1-ya5",
    "outputId": "bb27dc0f-0ad0-4e3f-c53c-6c5ace273f64"
   },
   "outputs": [],
   "source": [
    "! python -m spacy download en\n",
    "! pip install wordcloud\n",
    "! wget https://gitlab.com/andras.simonyi/10_days_AI_training_data/raw/master/sentiment.tsv?inline=false -O sentiment.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKNqb75Iy-Yw",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Task: sentiment classification\n",
    "\n",
    "The task is to classify one-sentence long movie reviews/opinions according to the sentiment they express. There are only two categories: positive and negative sentiment.\n",
    "\n",
    "\n",
    "> \"Data source: [UMICH SI650 - Sentiment Classification](https://www.kaggle.com/c/si650winter11/data)\n",
    "\n",
    "> Training data: 7086 lines. \n",
    "  \n",
    "> Format: 1|0 (tab) sentence\n",
    "\n",
    "> Test data: 33052 lines, each contains one sentence. \n",
    "\n",
    "> The data was originally collected from opinmind.com (which is no longer active).\"\n",
    "\n",
    "The data is in the file \"sentiment.tsv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWjIpjZBy-Y2",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "S9liEvjzy-Y7",
    "outputId": "ad510800-bf38-4ac9-b8c9-cfc42b07ff62"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('sentiment.tsv', sep='\\t', \n",
    "                 quoting=3, # Quotes are _never_ field separators\n",
    "                 header=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "HRLzOOkny-ZO",
    "outputId": "0f8f2b7b-f991-4be1-f894-7989e5548e61"
   },
   "outputs": [],
   "source": [
    "df = df[[1,0]] # rearrange columns\n",
    "\n",
    "df.rename(columns={1:\"text\", 0:\"sentiment\"}, inplace=True) # rename columns\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-urvA8RDy-Zc"
   },
   "source": [
    "# Splitting into train, validation and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BS5wjfjEy-Ze"
   },
   "source": [
    "Before doing anything else (!) we divide our data into train, validation and test parts,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xcMx4I5gy-Zh",
    "outputId": "24010650-3147-4035-b14e-a436c7781f86"
   },
   "outputs": [],
   "source": [
    "# Import the necessary function from Scikit\n",
    "from ...\n",
    "\n",
    "# Please observe, that we can only do a split into two\n",
    "# hence our best option is to call the function twice in a chain\n",
    "# Don't forget to fix the random seed also, eg to 13, since that is a lucky number! :-)\n",
    "df_train, df_test_valid = ...\n",
    "\n",
    "df_test, df_valid = ...\n",
    "\n",
    "assert len(df_train)==5668 and len(df_valid)==709 and len(df_test)==709\n",
    "print(len(df_train), len(df_valid), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCbUUTBty-Zq"
   },
   "source": [
    "# Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "pMZv_My5y-Zt",
    "outputId": "f790377c-fd93-41b7-b0d4-e24a512a6a2e"
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhMtyNyGy-Z4"
   },
   "source": [
    "We can examine the lengths of sentences as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "I_a-pzhcy-Z7",
    "outputId": "560733be-f404-4a8c-bffe-09b120a56571"
   },
   "outputs": [],
   "source": [
    "n_chars = df_train.text.apply(lambda x: len(x))\n",
    "\n",
    "n_chars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UalJeEyOy-aJ"
   },
   "source": [
    "The first sentence with the maximal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "tujl0aK-y-aO",
    "outputId": "8ed584b8-dcd2-4660-ab72-9c2635019a8d"
   },
   "outputs": [],
   "source": [
    "long_sentence = df_train.loc[n_chars.idxmax(), \"text\"]\n",
    "long_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fr0k2LGsy-ag",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Extra task: Let's do a word cloud!\n",
    "\n",
    "Let us visualize together and separately (by category) the sentences!\n",
    "\n",
    "Tool: https://github.com/amueller/word_cloud\n",
    "\n",
    "\n",
    "Good example: https://github.com/amueller/word_cloud/blob/master/examples/simple.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_Wo1OyNdy-ak",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function for displaying a word cloud\n",
    "# Input: one _UNIFIED_, space separated string!\n",
    "# Protip: https://www.tutorialspoint.com/python/string_join.htm\n",
    "def do_wordcloud(text):\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    # the matplotlib way:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # lower max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B8vuQclYy-av",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### TASK !!! ####\n",
    "#Put here the world cloud!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "S2j3sI2Dy-a0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### TASK !!! ####\n",
    "# Here only the cloud for sentences with negative sentiment!\n",
    "# Help: the shape of the DataFrame with only the negative sentences is: (2975, 2)\n",
    "# Source: https://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-GafZ4Py-a9"
   },
   "source": [
    "# Bag of words (BoW) representation of the texts\n",
    "\n",
    "We will represent each text as a (sparse) vector of lemma (word root) counts for frequent lemmas in the training data. \n",
    "\n",
    "For tokenization and lemmatization we use [spaCy](https://spacy.io/), an open source Python NLP library, which can produce a list of unique lemma ids from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msjbzya6y-a_"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\"]) \n",
    "# We only need the tokenizer, all higher functions are now unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-wKS3rZy-bE"
   },
   "source": [
    "spaCy can produce spaCy Doc objects from texts that contain their linguistic analysis, among others lemmas and their unique spaCy string ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0mNl6cRyy-bH",
    "outputId": "1d8d8e1a-d6d7-486e-b571-560433aea1d9"
   },
   "outputs": [],
   "source": [
    "doc = nlp(long_sentence)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "RDVWmOamy-bP",
    "outputId": "996988d4-3855-4460-d2b9-1c19a626f6d7"
   },
   "outputs": [],
   "source": [
    "print([token.lemma_ for token in doc ]) # Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "kklNRbxey-bX",
    "outputId": "3fd9c516-267c-4f8d-88d1-3f8346dea834"
   },
   "outputs": [],
   "source": [
    "print([token.lemma for token in doc]) # Connected unique ID-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVDIxcI_y-br"
   },
   "source": [
    "Now we have to convert these lists into BoW vectors. We could \"roll our own\", but, fortunately, scikit-learn has a feature extractor doing exactly that, the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) so, for the sake of simplicity, we will use that along with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "EIPLl6eDy-bx",
    "outputId": "c17e218b-4174-4fc9-8b9b-480d7216f4ed"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(analyzer=lambda s: [token.lemma for token in nlp(s)], #spaCy for analysis\n",
    "                     min_df= 0.001) # We ignore the lemmas with low document frequency\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Ye0Hc8hOy-cP",
    "outputId": "9cee4569-3866-407b-9db2-95a9e1d3b907"
   },
   "outputs": [],
   "source": [
    "sents = [\"I hate this movie.\", \"The movie is the worst I've seen.\"]\n",
    "bows = cv.fit_transform(sents).toarray() \n",
    "# A CountVectorizer produces a sparse matrix, we convert to ndarray\n",
    "bows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BMQTcX5y-cg"
   },
   "source": [
    "Using the CountVectorizer we convert the text columns of our train, validation and  test data into three sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "eo9nOv7jy-ci",
    "outputId": "50005f78-ccdc-46a9-b7a4-722757af7532"
   },
   "outputs": [],
   "source": [
    "bows_train = cv.fit_transform(df_train.text)\n",
    "bows_train.sort_indices() # comes from TF2.0 sparse implementation, obscure requirement\n",
    "bow_length = bows_train.shape[1]\n",
    "print(\"BoW length:\", bow_length)\n",
    "bows_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLd4g4oMy-cz"
   },
   "outputs": [],
   "source": [
    "bows_valid = cv.transform(df_valid.text)\n",
    "bows_valid.sort_indices() # comes from TF2.0 sparse implementation, obscure requirement\n",
    "bows_test = cv.transform(df_test.text)\n",
    "bows_test.sort_indices() # comes from TF2.0 sparse implementation, obscure requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "57_ybJKay-c5",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Task: The model\n",
    "\n",
    "We build a feed-forward neural network in Keras for our binary classification task, which will be trained with cross-entropy loss and minibatch SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# USE KERAS FUNKTIONAl API!\n",
    "\n",
    "\n",
    "# Parameters\n",
    "############\n",
    "\n",
    "hidden_size = 100\n",
    "\n",
    "# Model\n",
    "#######\n",
    "# Define (instantiate) the input layer!\n",
    "# Give the shape parameter the length of a BoW vector as length\n",
    "# WARNING: shape only accepts a tuple, even is one dimensional!\n",
    "inputs = ... \n",
    "\n",
    "# Hidden layer\n",
    "##############\n",
    "# Define a fully connected hidden layer that can be modified by the parameters above!\n",
    "# Use the ReLU activation function\n",
    "# Give the inputs to the hidden layer\n",
    "# Please be aware, that in Keras Functional, the parameters defining the layer are \n",
    "# \"instantiation\" parameters, but the input of the layer is already a \"call\" parameter!\n",
    "# (The magic lies in the brackets... ;-)\n",
    "\n",
    "hidden_output = ...\n",
    "\n",
    "# Softmax \n",
    "#########\n",
    "# Define the output softmax\n",
    "# (Which is a fully connected layer with activation accordingly...)\n",
    "# Please remember, we have exactly two classes! \n",
    "# (We choose to use this generalized, Softmax approach...)\n",
    "# We feed the layer with the output of the hidden one.\n",
    "\n",
    "predictions = ...\n",
    "\n",
    "# Whole model\n",
    "##############\n",
    "# Nothing more is left, than to instantiate the model\n",
    "# Please ensure input and output is right!\n",
    "\n",
    "model = ...\n",
    "\n",
    "# Optimization\n",
    "##############\n",
    "# For now, we stick to this.\n",
    "optimizer = SGD(lr=0.1)\n",
    " \n",
    "\n",
    "# Compilation and teaching\n",
    "##########################\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy', # use this cross entropy variant\n",
    "                                                      # since the input is not one-hot encoded\n",
    "              metrics=['accuracy']) #We measure and print accuracy during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=bows_train, \n",
    "          y=df_train.sentiment.values,\n",
    "          validation_data=(bows_valid, df_valid.sentiment.values),\n",
    "          epochs=10,\n",
    "          batch_size=200)\n",
    "\n",
    "# Please don't just run, understand!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== INTERACTIVE DEMO ===\")\n",
    "while True:\n",
    "    s = input(\"Enter a short text to evaluate or press return to quit: \")\n",
    "    if s == \"\":\n",
    "        break\n",
    "    else:\n",
    "        bow = cv.transform([s])\n",
    "        prob_pred = model.predict(bow[0])\n",
    "        print(f\"Positive vs negative sentiment probability: {prob_pred[0,1]} vs {prob_pred[0,0]}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Day6_Task.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
