{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"necessary\"></a>\n",
    "\n",
    "# Are LSTM-s really necessary for time series?\n",
    "\n",
    "LSTMs, when combined with dropout and other techniques became **hugely successful** and were considered the workhorse of NLP and time series applications, as well as sequence to sequence problems, moreover they serve as basis for all memory network architectures. They were and are still dominant in these fields.\n",
    "\n",
    "None the less as of 2017-8, multiple findings emerged that question the necessity for LSTMs in many fields. The leading field in this regard was neural machine translation, where Facebook Research developed it's [ConvNet based  machine translation](https://code.fb.com/ml-applications/a-novel-approach-to-neural-machine-translation/), as well as Google publishing the [transformer architecture](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html). Both approaches were motivated by the fact that LSTM computation is not easily parallelized, but ConvNets and transformers are, so training can be scaled up rapidly.\n",
    "\n",
    "Based on these networks multiple analyses tried to justify the usage of LSTMs and found, that though they have in theory infinite memory ability, in practice, a limited memory is good enough, which can be modeled by ConvNets, especially 1D and dilated convolutions.\n",
    "\n",
    "For more information see [here](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0) [here](http://www.offconvex.org/2018/07/27/approximating-recurrent/) [here](http://blog.aylien.com/acl-2018-highlights-understanding-representations-and-evaluation-in-more-challenging-settings/) [here](https://arxiv.org/abs/1803.01271)\n",
    "\n",
    "(Interesting snippets or remarks from the above sources:\n",
    "“LSTMs work in practice, but can they work in theory?”\n",
    "\" “According to Chomsky, sequential recency is not the right bias for learning human language. RNNs thus don’t seem to have the right bias for modeling language, which in practice can lead to statistical inefficiency and poor generalization behaviour.\")\n",
    "\n",
    "\n",
    "Alternative approaches as [\"recursive neural networks\"](https://en.wikipedia.org/wiki/Recursive_neural_network) and [\"recurrent neural network grammars\"](https://arxiv.org/abs/1602.07776) also exist, but not that widespread.\n",
    "\n",
    "\n",
    "## Convolutions for time series\n",
    "\n",
    "We use convolution operators, but only in one dimension over the data.\n",
    "\n",
    "<img src=\"http://mblogthumb2.phinf.naver.net/MjAxNjEyMTBfMjMx/MDAxNDgxMjk1ODk2NDAz.kn9JN93v9X2Xn9vJloqupV5c5GB09YNYwPrvDB8yKU8g.Hh1wT30ySu0JFWNqj2qoSTiX-pRnrjH2VWhMI2EAo30g.PNG.atelierjpro/%EC%8A%AC%EB%9D%BC%EC%9D%B4%EB%93%9C2.PNG?type=w2\" width=600 heigth=600>\n",
    "\n",
    "Look, look, we have reinvented sliding windows! :-(\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-523434af0d21bb0b59454aa9563cc90b-c\" width=600 heigth=600>\n",
    "\n",
    "Though if we [calculate the receptive fileds](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807) of these models, with enough depth they can be formidable!\n",
    "\n",
    "### Dilated convolutions\n",
    "\n",
    "Dilated convolutions are used to radically increase the total receptive field of a network.\n",
    "\n",
    "<img src=\"http://sergeiturukin.com/assets/2017-02-23-155956_803x294_scrot.png\" width=600 heigth=600>\n",
    "\n",
    "\n",
    "<img src=\"https://mlblr.com/images/dilated.gif\" width=400 heigth=400>\n",
    "\n",
    "Original paper [here](https://arxiv.org/abs/1511.07122)\n",
    "\n",
    "[Wavenet](http://sergeiturukin.com/2017/03/02/wavenet.html) and other successful models (especially in voice recognition) use this approach effectively.\n",
    "\n",
    "### Convolutional filters and wavelets\n",
    "\n",
    "It is also interesting to note, that convolutions over a timeseries can be understood as generalizations of \"wavelet / shapelet\" approaches, with the added benefit, that here the definition of \"mother wavelet\" is not that problematic.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Zoltan_German-Sallo/publication/266056525/figure/fig1/AS:295770143117318@1447528505764/The-Continuous-Wavelet-Transform-as-a-convolution-between-data-signal-and-scaled-and.png\" width=45%>\n",
    "\n",
    "\n",
    "# Neural models for time series similarity\n",
    "\n",
    "Some really successful neural models eg. for time series similarity (like [this](https://arxiv.org/pdf/1812.08306.pdf)) also capitalize on the flexibility of neural models for pattern recognition in timeseries, thus forming effective competition to the tried and true Fourier spectral methods and [Dynamic Time Warping](https://en.wikipedia.org/wiki/Dynamic_time_warping).\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1WrrsD273HFeWQkKnTDGnv-PxP4cWU6Ph\" widht=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seq\"></a>\n",
    "# What if outputs are also sequences? - Seq2seq\n",
    "\n",
    "Till this point we always assumed, that though the inputs of the modeling problems are sequences, but the outputs were single categorical values or scalars representing either one prediction step or a similarity score. But what if we would like to move into problems where **the input and the output are both sequences**? (For example parallel texts to translate, questions and answers,...)\n",
    "\n",
    "We are quite lucky, since it turns out, that full sequence models are quite flexible in this regard also: \n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/WSOie.png\" width=65%>\n",
    "\n",
    "**We can use the sequence models to capture (\"encode\") some inputs and generate (\"decode\") the desired outputs for us (utilizing their hidden states).**\n",
    "\n",
    "## Sidenote: more heroes\n",
    "\n",
    "A noteworthy \"hero\" of DL is [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever), who was instrumental in the elaboration of Sequence-to-sequence learning methods. \n",
    "\n",
    "(Not surprisingly, also a student of Hinton.)\n",
    "\n",
    "<img src=\"http://r.com.pk/wp-content/uploads/2018/04/ilya-sutskever.jpg\" width=400 heigth=400>\n",
    "\n",
    "Now he is the founder of the OpenAI foundation and research group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ysasa_jejAK"
   },
   "source": [
    "## The innen states of LSTMs as dense vector representations of series\n",
    "\n",
    "As mentioned before the inner states of LSTMs represent an arbitrary long sequence of inputs as a fixed length hidden state vector, thus LSTMs can be regarded as sequence encoders.\n",
    "\n",
    "The produced representations can be for example used to:\n",
    "\n",
    "- classification (eg. sentiment analysis in NLP)\n",
    "- the measurement of similarities of series, thus __search__\n",
    "- for sequence to sequence transformations, where we generate a new series just in case of language models by applying for example \"beam search\" from the hidden representations.\n",
    "\n",
    "<img src=\"http://suriyadeepan.github.io/img/seq2seq/seq2seq2.png\">\n",
    "\n",
    "LSTM based sequence-to-sequence transformations are used at:\n",
    "\n",
    "- **neural machine translation**\n",
    "- Summarization\n",
    "- Question answering\n",
    "- Dialogue systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EyXa4P6Cmdke"
   },
   "source": [
    "## Architectural improvements\n",
    "\n",
    "### Bi-directional RNN layer\n",
    "\n",
    "The big deficiency of the models seen before is that they only take into account the information coming from the \"left context\" of the datapoint (in the form of passed on hidden state), but this is not a realistic assumption form language processing perspective this is not totally plausible, since humans also read in a \"back-and-forth\" manner. \n",
    "\n",
    "This problem is mitigated by the introduction of a forward and a backward looking LSTM layer.:\n",
    "\n",
    "<img src=\"http://opennmt.net/OpenNMT/img/gnmt-encoder.png\" width=\"500px\">\n",
    "\n",
    "The two layers produce the sequence element independently, which later on gets combined into the final output -- most frequently only as a concatenation. \n",
    "\n",
    "Naturally we can stack BiLSTMs on top of each-other:\n",
    "\n",
    "<img src=\"http://brightliao.me/attaches/2016/2016-12-11-dl-workshop-rnn-and-lstm-1/deep-bidirectional-rnn.png\" width=\"400px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw1G1fc1TzQg"
   },
   "source": [
    "### Two more complex LSTM NLP architectures\n",
    "\n",
    "#### Tree-LSTM\n",
    "\n",
    "It is capable of processing a tree structured input - intead of a traditional sequence based input. (It was developed for parse-tree processing, but can have usages in molecular graphs also - though alternative approaches as [\"recursive neural networks\"](https://en.wikipedia.org/wiki/Recursive_neural_network) are also present in that field.) The input of a higher level node is - in a simple case - formed by the summation of the output of the child nodes. In a more complex case with maximalized branching factor each cell has an input and forget gate for all children.\n",
    "\n",
    "<img src=\"https://adeshpande3.github.io/assets/NLP28.png\" width=\"400px\">\n",
    "\n",
    "Original paper where they use it for sentiment-analysis: [Tai et al (2015): Improved Semantic Representations From\n",
    "Tree-Structured Long Short-Term Memory Networks (2015)](https://arxiv.org/pdf/1503.00075.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy11GY5mn8fA"
   },
   "source": [
    "#### Hierarchic models\n",
    "\n",
    "It is capitalizing on the hierarchic structure of textual data: the encoder first generates the representation of the sentences, then a paragraph is being modelled as a sequence of dense sentence vectors.\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/538991962b2e6e832183225a5b555d55a630dcef/3-Figure3-1.png\" width=\"600px\">\n",
    "\n",
    "Usage: abstract generation from text ([Zou 2016 A Hierarchical model for text autosummarization](https://pdfs.semanticscholar.org/5389/91962b2e6e832183225a5b555d55a630dcef.pdf)) and as a general purpose autoencoder also ([Li et al 2015: A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://arxiv.org/pdf/1506.01057.pdf)): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "97nstyY8GDMk"
   },
   "source": [
    "### A sidenote: Google Seq2Seq\n",
    "\n",
    "The encoder-decoder based seq2seq RNN architectures became so important, but so complex in recent years, that some dedicated frameworks for building up these kind of models appeared. One of the most well known of them is Google's  [seq2seq](https://github.com/google/seq2seq) based on TensorFlow, with which we can define complex seq2seq architectures with the help of simple `yml` description files (naturally more atypical architectrues are also possible). An example of a simple seq2seq model in `yml`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LdRbSuYPGBtY"
   },
   "source": [
    "```model: BasicSeq2Seq\n",
    "model_params:\n",
    "  bridge.class: seq2seq.models.bridges.InitialStateBridge\n",
    "  embedding.dim: 128\n",
    "  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n",
    "  encoder.params:\n",
    "    rnn_cell:\n",
    "      cell_class: GRUCell\n",
    "      cell_params:\n",
    "        num_units: 128\n",
    "      dropout_input_keep_prob: 0.8\n",
    "      dropout_output_keep_prob: 1.0\n",
    "      num_layers: 1\n",
    "  decoder.class: seq2seq.decoders.BasicDecoder\n",
    "  decoder.params:\n",
    "    rnn_cell:\n",
    "      cell_class: GRUCell\n",
    "      cell_params:\n",
    "        num_units: 128\n",
    "      dropout_input_keep_prob: 0.8\n",
    "      dropout_output_keep_prob: 1.0\n",
    "      num_layers: 1\n",
    "  optimizer.name: Adam\n",
    "  optimizer.params:\n",
    "    epsilon: 0.0000008\n",
    "  optimizer.learning_rate: 0.0001\n",
    "  source.max_seq_len: 50\n",
    "  source.reverse: false\n",
    "  target.max_seq_len: 50```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LrE1IzWJjhW"
   },
   "source": [
    "Training and prediction can be done in this case with simple shell commands.\n",
    "\n",
    "More than one \"concurent\" seq2seq framework exists, like [this](https://github.com/eladhoffer/seq2seq.pytorch)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
