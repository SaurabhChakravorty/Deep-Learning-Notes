{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "# https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Regressor\n",
    "\n",
    "XGBoost the (alleged) king among all decision trees, offers a very potent API for regression tasks like ours.\n",
    "It is extremly scalable, offers GPU support, a good amount of parameters to tune and overall fantastic results.\n",
    "\n",
    "#### THE BIGGEST DOWNSIDE\n",
    "The biggest downside to XGBoost for this particular now-regression task is the fact that we chose to transform\n",
    "our time variables to a corresponding sin-cos pair. Decision trees will struggle to pick up the intended relation among these two feature columns. Adding XGBoosts feature_constraints via nested lists did not bring the desired fix and overall XGBoost stayed behind our expecations for this regression task.\n",
    "\n",
    "#### Bayesian Optimization\n",
    "For this notebook/model we chose to search for optimal parameters using a Python Bayesian Optimization implementation,\n",
    "which can be a lot more cost effective compared to extensive grid-searches while still delivering sufficient results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"hourly_resampled_contracts_ohlcsv_weather.csv\", index_col=[0], header=[0, 1], parse_dates=True)\n",
    "df = pd.read_pickle(data_path+'hourly_resampled_contracts_ohlcsv_weather.pkl')\n",
    "features = ['t', 'weekday_sin', 'weekday_cos', 'run_hour', \n",
    "            'n_prev_hour_contracts', 'hour_sin', 'hour_cos', \n",
    "            'air_temp', 'rel_humidity', 'wind_speed', 'wind_dir',\n",
    "            'holidays', 'qty_open', 'qty_high', 'qty_low', 'qty_close',\n",
    "            'qty_var' ,'qty_sum', 'px_open','px_high', 'px_low', 'px_var']\n",
    "WINDOW_SIZE = 5\n",
    "forecast_df = lib.create_rolling_windows(df, WINDOW_SIZE, features, save_to_pickle=False)\n",
    "forecast_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test= lib.train_test_valid_split(forecast_df, WINDOW_SIZE, \n",
    "                                                            len(features), test_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = X_train.hist(bins=(30),figsize=(25,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain XGB model test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [[5,21,37,53,69,84],[1,2,3,4,17,18,19,20,33,34,35,36,49,50,51,52,65,66,67,68,80,81,82,83],\n",
    "               [0,6,7,8,9,10,11,12,13,14,15,16,22,23,24,25,26,27,28,29,30,31,32,\n",
    "                38,39,40,41,42,43,44,45,46,47,48,54,55,56,57,58,59,60,61,62,63,64,70,71,72,73,74,75,76,77,78,79]]\n",
    "# turns out, feature constraints did not add any value, regardless of \"nested list\" combinations\n",
    "#interaction_constraints = constraints\n",
    "\n",
    "xgbr = xgb.XGBRegressor(verbosity=1) \n",
    "xgbr.fit(X_train, y_train,eval_set =[(X_train,y_train),(X_valid,y_valid)],eval_metric=\"mae\", early_stopping_rounds=30)\n",
    "preds = xgbr.predict(X_test)\n",
    "mae = mean_absolute_error(y_test,preds)\n",
    "r2 = r2_score(y_test,preds)\n",
    "print(\"Test MAE: \", mae)\n",
    "print(\"Test R2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Training score:  2.868287162863178\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance:\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "xgb.plot_importance(xgbr, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, price values appear to be the most important driver in feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Bays. Opt.:\n",
    "\n",
    "Implementing Bays. Opt. to search for optimal parameter combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_xgbox_function(n_estimators , max_depth , learning_rate ,gamma , min_child_weight, \n",
    "                       subsample, colsample_bytree,max_delta_step , reg_alpha ,\n",
    "                      reg_lambda,scale_pos_weight):\n",
    "    \n",
    "    \"\"\"Function with unknown internals we wish to maximize.\n",
    "\n",
    "    This is just serving as an example, for all intents and\n",
    "    purposes think of the internals of this function, i.e.: the process\n",
    "    which generates its output values, as unknown.\n",
    "    \"\"\"\n",
    "    n_estimators_xgbr = int(round(n_estimators,0))\n",
    "    max_depth_xgbr = int(round(max_depth,0))\n",
    "    max_delta_step_xgbr = int(round(max_delta_step,0))\n",
    "    \n",
    "    \n",
    "    params = {\"n_estimators\": n_estimators_xgbr,\n",
    "              \"max_depth\": max_depth_xgbr,\n",
    "              \"learning_rate\": learning_rate,\n",
    "              \"gamma\": gamma,\n",
    "              \"min_child_weight\": min_child_weight,\n",
    "              \"subsample\": subsample,\n",
    "              \"colsample_bytree\": colsample_bytree,\n",
    "              \"max_delta_step\": max_delta_step_xgbr,\n",
    "              \"reg_alpha\": reg_alpha,\n",
    "              \"reg_lambda\": reg_lambda,\n",
    "              \"scale_pos_weight\": scale_pos_weight,\n",
    "              \"tree_method\": \"gpu_hist\",\n",
    "              \"eval_metric\":\"mae\",\n",
    "              \"verbosity\":0,\n",
    "              \"random_state\": 42}\n",
    "\n",
    "    #\"deterministic_histogram\":\"true\"\n",
    "    xgbr = xgb.XGBRegressor(objective='reg:squarederror', **params) \n",
    "    xgbr.fit(X_train, y_train,eval_set =[(X_train,y_train),(X_valid,y_valid)], early_stopping_rounds=30)\n",
    "    preds = xgbr.predict(X_valid)\n",
    "    mae = mean_absolute_error(y_valid,preds)\n",
    "    \n",
    "    return -1*mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(X_train, X_valid, X_test, y_train, y_valid, y_test):\n",
    "    \n",
    "    pbounds_xgb = {\"n_estimators\": (100,200),\n",
    "                  \"max_depth\": (4,15),\n",
    "                  \"learning_rate\": (0.1,0.8),\n",
    "                  \"gamma\": (0,0.8),\n",
    "                  \"min_child_weight\": (1,2),\n",
    "                  \"subsample\": (.5,1.),\n",
    "                  \"colsample_bytree\": (.5,1.),\n",
    "                  \"max_delta_step\": (0,10),\n",
    "                  \"reg_alpha\": (0,1),\n",
    "                  \"reg_lambda\": (0,1),\n",
    "                  \"scale_pos_weight\": (0,1)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "    f=gpu_xgbox_function,\n",
    "    pbounds=pbounds_xgb,\n",
    "    verbose=1, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=42,)\n",
    "    \n",
    "    optimizer.maximize(\n",
    "    init_points=20,\n",
    "    n_iter=100,\n",
    "    acq=\"ei\", \n",
    "    xi=1e-1)\n",
    "    \n",
    "    best_params = optimizer.max[\"params\"].copy()\n",
    "    best_params[\"n_estimators\"] = int(round(best_params[\"n_estimators\"],0))\n",
    "    best_params[\"max_depth\"] = int(round(best_params[\"max_depth\"],0))\n",
    "    best_params[\"max_delta_step\"] = int(round(best_params[\"max_delta_step\"],0))\n",
    "    best_params[\"tree_method\"] = \"gpu_hist\"\n",
    "    best_params[\"eval_metric\"] = \"mae\"\n",
    "    #best_params[\"deterministic_histogram\"] = \"true\"\n",
    "    best_params[\"verbosity\"] = 1\n",
    "    best_params[\"random_state\"] = 42\n",
    "    \n",
    "    xgbr2 = xgb.XGBRegressor(objective='reg:squarederror', **best_params) \n",
    "    xgbr2.fit(X_train, y_train,eval_set =[(X_train,y_train),(X_valid,y_valid)], early_stopping_rounds=30)\n",
    "    preds2 = xgbr2.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test,preds2)\n",
    "    print(\"Test MAE: \",mae)\n",
    "    return best_params, preds, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, preds, mae = call_model(X_train, X_valid, X_test, y_train, y_valid, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some further error analysis and notes on the xgb results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While results were not as promising as anticipated, the MAE is now down to ~2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"preds\"] = preds\n",
    "X_test[\"true\"] = y_test\n",
    "X_test[\"error\"] = abs(X_test[\"true\"]-X_test[\"preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = X_test.plot(kind=\"scatter\", x=\"preds\",y=\"true\",title=\"XGB True vs. Pred. Values (size = error size)\",\n",
    "                  s=np.array(X_test.error), figsize=(10,10),c=\"error\",colormap=\"viridis\",\n",
    "                  colorbar=False,alpha=.5)\n",
    "ax.plot([-100, 100], [-100, 100], color='black',linewidth=1)\n",
    "ax.plot([-100, 100], [0, 0], color='black', linestyle=\"--\", linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the Lasso benchmark, the negative values now have a more even error spread and seem to have improved. The batch between 0 and 10 we predict to be around 15-25 is still as present as before. The record outliers also appear to be the same, pointing towards a more substantial problem withing the events causing these prices. To solve these errors we either need to get back to the data and improve our features or now find an appropriate framework that can find the pattern XGB is so far missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
