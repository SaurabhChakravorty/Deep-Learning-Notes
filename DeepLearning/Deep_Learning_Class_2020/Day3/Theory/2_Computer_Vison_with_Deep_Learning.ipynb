{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"motivation\"></a>\n",
    "# Motivation: Problems of computer vision \n",
    "\n",
    "Computer vision (since the earliest perceptrons) has been traditionally a crucial field of AI research.\n",
    "One of the reasons is, that the perceptual \"circuitry\" of vision in humans got considerable attention in many fields (like cognitive science) and is directly relevant for the \"biological inspiration\" of AI methods.\n",
    "On the other hand it is a challenging field, model performance was way lower than human baseline till most recently (think 2014-15), though it is a common ability of many animals.\n",
    "Finally the field is interesting, since we have a strong understanding of physical processes forming it's baseline and would \"expect it to be simple\". It is not.\n",
    "\n",
    "\n",
    "## Invariances in the field of vision\n",
    "\n",
    "\"Pictures come from objects, not random pixels\"\n",
    "\n",
    "<img src=\"https://cms-assets.tutsplus.com/uploads/users/108/posts/19997/image/color-fundamentals-value-1.png\" width=400 heigth=400>\n",
    "\n",
    "The basic mechanism of light reflection and absorption are responsible for the detected \"pixel distribution\" on a picture, so we can assume that the empirically observed distribution is coming from a directly not observed, but very well defined \"generative distribution\" of physical objects. While \"practicing\" vision, we reconstruct the domain of objects from the visible data of pixels. Our expectation would be the same for AI systems. \n",
    "\n",
    "<img src=\"http://www.mstworkbooks.co.za/natural-sciences/gr8/images/gr8ec04-gd-0052.png\" width=400 heigth=400>\n",
    "\n",
    "Colour vision, especially how the abstract categories of \"colour\" arise from the raw perceptions is non-trivial. (We perceive the same object as \"red\", even though the lighting conditions influence the perception strongly, and we categorise different things as pink or red according to culture, or even see things differently [based on our culture](https://en.wikipedia.org/wiki/Linguistic_relativity_and_the_color_naming_debate). More [here](https://books.google.hu/books?id=O9SIAgAAQBAJ&dq=Color+vision:+A+case+study+in+the+Foundations+of+Cognitive+Science))\n",
    "\n",
    "From these it follows, that there are many \"invariances\" in the visual field, like rotational, translation, lighting,... which the model should handle well. \n",
    "\n",
    "Another \"adjacent\" field in computer science concerns itself with the realistic generation of pictures (or sequences, as animations) from a known set of objects. This is how computer graphics and special effects are made with the inverse procedure, \"ray tracing\".\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Ray_trace_diagram.svg/600px-Ray_trace_diagram.svg.png\" width=400 heigth=400>\n",
    "\n",
    "It is important to note, that the development of Graphics Processing Units was a direct result of the need for better computer graphics, thus it is by no chance, that the first applications of GPUs to deep learning came from the field of image recognition. The hardware was already somewhat specialized for the task. \n",
    "\n",
    "\n",
    "## Compositionality\n",
    "\n",
    "\"Objects are made of parts, not just blobs\"\n",
    "\n",
    "<img src=\"https://kevinkaixu.net/projects/scores/teaser.jpg\" width=600 heigth=600>\n",
    "\n",
    "It is also crucially important, that \"objects\" themselves (though sometimes vaguely defined in themselves) are results of a compositional structure, that is they many times are composed of \"parts\" (themselves objects). This presupposes that a good model can handle a hierarchic composition of objects.\n",
    "\n",
    "(The recent advances of machine learning made a \"full circle\", and deep learning methods are aiding the composition of computer graphics, like in: [\"SCORES: Shape Composition with Recursive Substructure Priors\"](https://kevinkaixu.net/projects/scores.html).)\n",
    "\n",
    "Further complicating the problem, objects in the visual field can be described by edges, shapes, textures, so their description is non-trivial.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1zgVDLXoJCPTIS1JAwbwlGR5MnxDsvNBt\"  width=600 heigth=600>\n",
    "\n",
    "\n",
    "## Why traditional methods suffered?\n",
    "\n",
    "\"Could you please build up a list of cat parts with visual specification, please?\"\n",
    "\n",
    "Image input is notoriously high in dimensionality of measurements, though we annoyingly know, that is is a very well copressable data, just we don't know how.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1VJZaGkHUVAZXpS-vRMRgEmzIQPia5JWV\"  width=600 heigth=600>\n",
    "\n",
    "\n",
    "I can describe a scene as \"there are two cardinal red spheres of 1cm diameter in an empty white space\", and I have described the scene with a very good approximation, though it can have literally myriad instantiations under viewing angles, ligthing conditions,...\n",
    "\n",
    "<img src=\"https://cdn8.bigcommerce.com/s-e2p82/images/stencil/500x659/products/2992/7729/Q010145__80365.1458131489.jpg?c=2\" width=400 height=400>\n",
    "\n",
    "As stated before: Compressability and \"understandability\", \"discernability\" has it's deep connections. \n",
    "\n",
    "\n",
    "## Crucial importance of end-to-end learning\n",
    "\n",
    "In the visual field it is all the more obvious, that the historical progress did not come from the better or stronger classifiers - an SVM will do fine - but the elaboration of features, that is, the representation of the input, is essential. Years upon years of human engineering went into feature extractors (see some at [wikipedia](https://en.wikipedia.org/wiki/Feature_detection_(computer_vision)#Feature_extraction) though there are many many more), and the big breakthrough in 2012 for deep learning came from the fact, that a neural net with no engineered features, working on raw pixels (though with a clever architecture) significantly beat the state of the art.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1v8yieDdObp74czCbGMg1J50xPYL20XhZ\"  width=600 heigth=600>\n",
    "\n",
    "[source](https://www.youtube.com/watch?v=o8otywnWwKc&t=301s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"architecture\"></a>\n",
    "# Convolutional Neural Network (CNN) architecture\n",
    "\n",
    "Breakthrough solution for image recognition the introduction of **Convolutional Neural Network** architecture: \n",
    "- De-facto standard now\n",
    "- Numerous applications outside of it\n",
    "\n",
    "## Some sung and unsung heroes \n",
    "\n",
    "### Fukushima\n",
    "\n",
    "Worked on **neocognitron model** in the 1980s, which predates CNN-s.\n",
    "- Hierarchical, multilayered artificial neural network. \n",
    "- Used for handwritten character recognition and other pattern recognition tasks.\n",
    "- Served as inspiration for convolutional neural networks.\n",
    "\n",
    "<img src=\"http://personalpage.flsi.or.jp/fukushima/files/FukushimaImg.jpg\" width=300 heigth=300>\n",
    "\n",
    "\n",
    "\n",
    "Inspired by model proposed by Hubel & Wiesel in 1959. \n",
    "- Found two types of cells in the visual primary cortex:\n",
    "- Simple cell (S-cell) and complex cell (C-cell)\n",
    "- Simple cell: Takes input from complex cells. Responds primarily to oriented edges and gratings (bars of particular orientations) in small regions of the visual field\n",
    "- Complex cell: Responds (like simple cell) to oriented edges and gratings; however has a degree of spatial invariance. This means that its receptive field cannot be mapped into fixed excitatory and inhibitory zones. Rather, it will respond to patterns of light in a certain orientation within a large receptive field, regardless of the exact location. Some complex cells respond optimally only to movement in a certain direction. Insensitive to exact location of the edge in the field.\n",
    "- Cascading model of these two types of cells for use in pattern recognition tasks, one in a sense a local edge detector and one a more global one.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saurabh\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py:689: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Cw5PKV9Rj3o\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Cw5PKV9Rj3o\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neocognitron is natural extension of cascading models. \n",
    "- Consists of multiple types of cells: S-cells, C-cells. \n",
    "- Local features are extracted by S-cells (convolutional layer), and these features' deformation, such as local shifts, are tolerated by C-cells (downsampling layer). \n",
    "- Local features in the input are integrated gradually and classified in the higher layers\" [Wikipedia](https://en.wikipedia.org/wiki/Neocognitron)\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/581528b2215e017eba96ef4ee16d33a74645755f/3-Figure1-1.png\" width=400 heigth=400>\n",
    "\n",
    "Many of the features of later CNNs present in Fukushima's work, but ideas were based on learning rules of classical era, predating backprop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Yann LeCun\n",
    "\n",
    "It is common to attribute the elaboration of Convolutional Neural Nets - as well as many advances till this day - to [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun), a student of Hinton, who worked extensively on image recognition tasks, especially in hadwritten digit recognition for numeric cheques. He is also attributed for creating our \"workhorse\", the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-thumb-48611829-200-wjnraisajwlkqlmolpgmqnkfnxvuezwr.jpeg\" width=400 heigth=400>\n",
    "\n",
    "Till this day he is one of the most inlfuential figureheads of the deep learning revolution, he's talks and publications are well worth following.\n",
    "\n",
    "## Basic building blocks of CNNs\n",
    "\n",
    "Basic building blocks of a CNN:\n",
    "\n",
    "- Convolutional layers - edge detectors; analogous to simple cells\n",
    "- Subsampling layers (\"Pool\" or \"Pooling\") - going beyond exact location/ reducing information (analogous to complex cells)\n",
    "- Fully connected layers - combining the information\n",
    "- A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.\n",
    "\n",
    "<img src=\"https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/lenet-5-825x285.png?x31195\" width=600 heigth=600>\n",
    "\n",
    "A good summary introduction of CNN elements can be found [here](https://www.wikiwand.com/en/Convolutional_neural_network) and [here](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/).\n",
    "\n",
    "### Basic ideas\n",
    "\n",
    "Remember the idea of *\"Embed and cut\"* from representation learning? Convolutional and pooling layers essentially building up pipeline of capturing a hierarchy of more and more abstract features of the image until \"embedding space\" provides enough information for final classifier - the fully connected layers. \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Siva_Chaitanya_Mynepalli/publication/281607765/figure/fig1/AS:284643598323714@1444875730488/Learning-hierarchy-of-visual-features-in-CNN-architecture.png\" width=600 heigth=600>\n",
    "\n",
    "[source](https://www.researchgate.net/publication/281607765_Hierarchical_Deep_Learning_Architecture_For_10K_Objects_Classification)\n",
    "\n",
    "<img src=\"https://devblogs.nvidia.com/wp-content/uploads/2015/11/hierarchical_features.png\" width=600 heigth=600>\n",
    "\n",
    "We could not have defined these features so exactly by hand!\n",
    "\n",
    "#### Hierarchy of \"detectors\" or \"filters\"\n",
    "\n",
    "Alternatively think of architecture as hierarchy of \"filters\" that \"scans\" through input field and gives most \"signal\" when the input partis overlayed with specific pattern it is looking for.\n",
    "\n",
    "<img src=\"http://mathworld.wolfram.com/images/gifs/convgaus.gif\" height=\"300\">\n",
    "\n",
    "\n",
    "#### Engineered prior for invariances\n",
    "\n",
    "Usage of filters and convolution (see below):\n",
    "- Realize a kind of invariance, since **the filters detect the appropriate pattern irrespective of it's location in the picture**. \n",
    "- Combined with successive abstraction of hierarchy - can effectively solve problems of location invariance.\n",
    "\n",
    "#### Weight sharing\n",
    "\n",
    "Application of \"subsampling\" layers together with weight sharing in case of convolution allows for radical decrease of number of weights used for a model, making training time feasible for _huge_ neural networks and work on _high resolution_ (high dimensionality) input data. \n",
    "\n",
    "(By the way, weight sharing helps to **decrease the number of parameters**, thus can be considered as an implicit \"regularization\".)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution \n",
    "\n",
    "#### Mathematiclal definition\n",
    "General definition:\n",
    "\n",
    "\n",
    "$$y(t) = x(t) * h(t) = \\int_{-\\infty}^{\\infty}x(\\tau)h(t - \\tau)d\\tau$$\n",
    "\n",
    "- f and g are both functions which operate on the same domain of inputs\n",
    "- x(t) is the input, h(t) is the impulse response\n",
    "- In the context of probability, you might frame these as density functions; in audio processing, they might be waveforms; \n",
    "- When you convolve two functions together, the convolution itself acts as a function, insofar as we can calculate the value of the convolution at a particular point t, just as we can with a function.\n",
    "\n",
    "- To calculate a convolution between f and g, at point t, we take the integral over all values tau between negative and positive infinity, and, at each point, multiply the value of f(x) at position tau by the value of h(x) at t — tau, that is, the difference between the point for which the convolution is being calculated and the tau at a given point in the integral.\n",
    "\n",
    "- We have $h(t - \\tau)$, as one function is reversed and \"moves through the other\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://mathworld.wolfram.com/images/gifs/convgaus.gif\" height=\"300\">\n",
    "\n",
    "\n",
    "In the figure \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution, cross correlation and auto-correlation\n",
    "Some features of convolution are similar to cross-correlation: for real-valued functions, of a continuous or discrete variable, it differs from cross-correlation only in that either f (x) or g(x) is reflected about the y-axis; thus it is a cross-correlation of f (x) and g(−x), or f (−x) and g(x)\n",
    "\n",
    "\n",
    "__Convolution:__\n",
    "\n",
    "Continuous function:\n",
    "$$y(t) = x(t) * h(t) = \\int_{-\\infty}^{\\infty}x(\\tau)h(t - \\tau)d\\tau$$\n",
    "\n",
    "Discrete function (1d):\n",
    "$${\\displaystyle (f*g)[n]=\\sum _{m=-\\infty }^{\\infty }f[m]g[n-m]}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Cross-correlation:__\n",
    "\n",
    "Continuous function:\n",
    "$${\\displaystyle (f\\star g)(\\tau )\\ \\triangleq \\int _{-\\infty }^{\\infty }{\\overline {f(t)}}g(t+\\tau )\\,dt}$$\t\n",
    "\n",
    "\n",
    "where ${\\displaystyle {\\overline {f(t)}}}$ denotes the complex conjugate of $f(t)$, and $\\tau$  is the displacement, also known as lag (a feature in $f$ at $t$ occurs in $g$ at $t+\\tau$.\n",
    "\n",
    "Discrete function (1d):\n",
    "\n",
    "$${\\displaystyle (f\\star g)[n]\\ \\triangleq \\sum _{m=-\\infty }^{\\infty }{\\overline {f[m]}}g[m+n]}$$\t\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/21/Comparison_convolution_correlation.svg\" height=\"300\">\n",
    "\n",
    "\n",
    "Visual comparison of convolution, cross-correlation, and autocorrelation. For the operations involving function f, and assuming the height of f is 1.0, the value of the result at 5 different points is indicated by the shaded area below each point. \n",
    "\n",
    "\n",
    "If $X$ and $Y$ are two independent random variables with probability density functions $f$ and $g$, respectively, then the probability density of the difference $Y-X$ is formally given by the cross-correlation (in the signal-processing sense) $f\\star g$; however this terminology is not used in probability and statistics. In contrast, the convolution $f*g$ (equivalent to the cross-correlation of $f(t)$ and  $g(-t)$ gives the probability density function of the sum $X+Y$.\n",
    "\n",
    "Note that in statistics cross-correlation is always normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D example\n",
    "Discrete 2D convolution:\n",
    "\n",
    "\n",
    "\n",
    "$$o[m,n]=f[m,n]*g[m,n]=\\sum_{u=-\\infty}^{\\infty}\\sum_{v=-\\infty}^{\\infty}f[u,v]g[m-u,n-v]$$\n",
    "\n",
    "\n",
    "Generally the idea of convolution is as follows:\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-578748437404fe6733bc7823755e813c-c\" width=600 heigth=600>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Or more in detail:**\n",
    "\n",
    "This is the pixel intensity representation of the input image:\n",
    "\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/07/screen-shot-2016-07-24-at-11-25-13-pm.png?w=127&h=115\" height=\"200\">\n",
    "\n",
    "This is the weight matrix of the neuron, that is the \"filter\":\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/07/screen-shot-2016-07-24-at-11-25-24-pm.png?w=74&h=64\" height=\"100\">\n",
    "\n",
    "Convolution is the \"shifting\" of the \"filters\" throughout the input: \n",
    "\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/07/convolution_schematic.gif?w=268&h=196\" height=\"300\">\n",
    "\n",
    "[source](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n",
    "\n",
    "The convolution of filters on an original image:\n",
    "\n",
    "<img src=\"https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif?w=748\" height=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another intuition for discrete convolutions (and cross-correlation): dot product\n",
    "\n",
    "Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. These definitions are equivalent when using Cartesian coordinates\n",
    "\n",
    "##### Algebraically\n",
    "\n",
    "The dot product of two vectors $\\textbf{a}=[a_1, a_2, ...., a_n]$ and $\\textbf{b}=[b_1, b_2, ...., b_n]$ is defined as:\n",
    "\n",
    "$$\\textbf{a}.\\textbf{b}=\\sum_{i=1}^{n} {a_i*b_i}=a_1*b_1+a_2*b_2+...+a_n*b_n$$ \n",
    "\n",
    "where $\\sum$ denotes summation and n is the dimensions of the ector space. For instance, in three-dimensional space the dot product of vectors $[1, 3, -5]$ and $[4, -2, 1]$ is:\n",
    "\n",
    "$$[1, 3, -5].[4, -2, -1]=(1*4)+(3*-2)+(-5*-1)$$\n",
    "\n",
    "$=4-6+5$\n",
    "$=3$\n",
    "\n",
    "##### Geometrically\n",
    "\n",
    "$$a\\dot{}b=a_xb_x + a_yb_y = |a||b|\\cos{\\theta}$$\n",
    "\n",
    "Note that with a and b two sides of a triangle and θ the angle between them, the third side is b−a and (cosine rule)\n",
    "\n",
    "$$|b-a|^2=|a|^2+|b|^2-2|a||b|\\cos \\theta$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$2|a||b|\\cos\\theta=\\Sigma a_i^2+\\Sigma b_i^2-\\Sigma (b_i-a_i)^2=2\\Sigma a_ib_i$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$|a||b|\\cos\\theta=\\Sigma a_ib_i$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://betterexplained.com/wp-content/uploads/dotproduct/dot_product_components.png\" height=\"300\">\n",
    "\n",
    "\n",
    "<img src=\"https://betterexplained.com/wp-content/uploads/dotproduct/dot_product_rotation.png\" height=\"300\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More detailed example of 1D convolutional neural network\n",
    "[\"Source\"](https://colah.github.io/posts/2014-07-Understanding-Convolutions/) \n",
    "\n",
    "So, how does convolution relate to convolutional neural networks?\n",
    "\n",
    "Consider a 1-dimensional convolutional layer with inputs $\\{x_n\\}$ and outputs $\\{y_n\\}$:\n",
    "\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY.png\" width=400 heigth=400>\n",
    "\n",
    "As we observed, we can describe the outputs in terms of the inputs:\n",
    "\n",
    "$$y_n = A(x_{n}, x_{n+1}, ...)$$\n",
    "\n",
    "Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.\n",
    "\n",
    "Recall that a typical neuron in a neural network is described by:\n",
    "\n",
    "$$\\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~...~ + b)$$\n",
    "\n",
    "Where $x_0$, $x_1$,... are the inputs. The weights, $w_0$, $w_1$, ... describe how the neuron connets to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior. Saying that multiple neurons are identical is the same thing as saying that the weights are the same.\n",
    "\n",
    "It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.\n",
    "\n",
    "Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:\n",
    "\n",
    "$$y = \\sigma(Wx + b)$$\n",
    "\n",
    "For example, we get:\n",
    "\n",
    "$$y_0 = \\sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 ...)$$\n",
    "\n",
    "$$y_1 = \\sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 ...)$$\n",
    "\n",
    "Each row of the matrix describes the weights connecting a neuron to its inputs.\n",
    "\n",
    "Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY-W.png\" width=400 heigth=400>\n",
    "\n",
    "Which corresponds to the equations:\n",
    "\n",
    "$$y_0 = \\sigma(W_0x_0 + W_1x_1 -b)$$\n",
    "$$y_1 = \\sigma(W_0x_1 + W_1x_2 -b)$$\n",
    "\n",
    "So while, normally, a weight matrix connects every input to every neuron with different weights:\n",
    "\n",
    "$$W = \\left[\\begin{array}{ccccc} \n",
    "W_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\\\\n",
    "W_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\\\\n",
    "W_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\\\\n",
    "W_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\\\\n",
    "...     &   ...   &   ...   &  ...    & ...\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don’t connect to many possible inputs, there’s lots of zeros.\n",
    "\n",
    "$$ W = \\left[\\begin{array}{ccccc} \n",
    "w_0 & w_1 &  0  &  0  & ...\\\\\n",
    " 0  & w_0 & w_1 &  0  & ...\\\\\n",
    " 0  &  0  & w_0 & w_1 & ...\\\\\n",
    " 0  &  0  &  0  & w_0 & ...\\\\\n",
    "... & ... & ... & ... & ...\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "\n",
    "Multiplying by the above matrix is the same thing as convolving with $[...0, w_1, w_0, 0...]$ The function sliding to different positions corresponds to having neurons at those positions.\n",
    "\n",
    "What about two-dimensional convolutional layers?\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv2-5x5-Conv2-XY.png\" width=400 heigth=400>\n",
    "\n",
    "The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.\n",
    "\n",
    "Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receptive fields: \"size of the convolution\" in x*y pixels\n",
    "\n",
    "#### Stride and padding\n",
    "\n",
    "\"Step size\" (in number of pixels of picture, in our case) of filters, the so called _stride_ is a crucial parameter.\n",
    "\n",
    "- Pixels at the edges of the picture taking part in fewer convolutions: nothing to be convolved on one of their sides. \n",
    "- Effect can be mitigated by _\"padding\"_ input with virtual pixels. \n",
    "- Most widespread padding is _zero-padding_: fill up the space with zeros, but other solutions like \"warping\" \"clamping\" and \"mirroring\" also exist. (Further reading [here](https://www.cs.toronto.edu/~urtasun/courses/CV/lecture02.pdf))\n",
    "\n",
    "<img src=\"https://adeshpande3.github.io/assets/Pad.png\" height=\"300\">\n",
    "\n",
    "With careful choice of padding and stride we can make the successive layers of the network constant in width:\n",
    "\n",
    "The new representation, the _\"activation map\"_ calculated as follows:\n",
    "\n",
    "Let the picture be of size $W\\times W$, the size of the filter (_\"receptive field\"_) $F\\times F$, stride $S$ and padding $P$. The resulting activation map is then:\n",
    "$O=(W-F+2P)/S+1$. \n",
    "\n",
    "\n",
    "For example on a 7x7 picture we convolve a 3x3 filter with stride 1 and 0 padding, we get a $5\\times 5$- output. If the stride is 2 pixels, get $3\\times 3$ as output. \n",
    "\n",
    "\n",
    "\n",
    "#### Weight sharing\n",
    "\n",
    "- Can also look at the weights in above example, as $5\\times 5$ neuron forming the new representation.\n",
    "- Major difference from fully connected layers, since in case of CNNs **all neurons have the same weights**. \n",
    "- Called _weight sharing_.\n",
    "- Different neurons receive inputs from different parts of the picture. \n",
    "\n",
    "#### Spatial arrangement of convolutional units\n",
    "\n",
    "- Convolution operation can be extended to more dimensions.\n",
    "- At the end always get a scalar value. \n",
    "- Including the color channels (RGB), the picture can be described as a $W\\times W \\times D$ 3D matrix\n",
    "- \"Depth\" equals number of channels (in most cases 3)\n",
    "- In such a case, the filters are also 3D. \n",
    "- If we follow reasoning, we can stack layers of activation maps on top of each-other\n",
    "- We again get a 3D input at the next layer, like:\n",
    "\n",
    "\n",
    "<img src=\"http://deeplearning.net/tutorial/_images/cnn_explained.png\" height=\"300\">\n",
    "\n",
    "#### Connection between hyperparameters\n",
    "\n",
    "Input size: $W_1 \\times H_1 \\times D_1$  (width, length, depth - we were using rectangular inputs above, but that is not necessary)\n",
    "\n",
    "Hyperparameters:\n",
    " - Number of filters: K\n",
    " - Filter size: F\n",
    " - Stride: S\n",
    " - Padding: P\n",
    " \n",
    " Output: $W_2\\times H_2 \\times D_2$,\n",
    "  where \n",
    "  \n",
    "  - $W_2 = (W_1-F-2P)/S+1$\n",
    "  - $H_2 = (H_1-F-2P)/S+1$\n",
    "  - $D_2 = K$\n",
    "  \n",
    "  \n",
    "  Total parameters for a layer: $(F\\times F\\times D_1 +1)*K$, where 1 is representing the bias. \n",
    "  \n",
    "  (In case of fully connected layers it is $W_1 \\times H_1 \\times D_1 \\times W_2 \\times H_2$ parameters, which is by order of magnitude larger...)\n",
    "  \n",
    "  \n",
    "#### Comparison to fully connected layer\n",
    "\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table {\n",
    "    font-family: arial, sans-serif;\n",
    "    border-collapse: collapse;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "    border: 1px solid #dddddd;\n",
    "    text-align: left;\n",
    "    padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even) {\n",
    "    background-color: #dddddd;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h2>Connection between sub parts</h2>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>MLP</th>\n",
    "    <th>Convolutional network</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" height=\"300\"></td>\n",
    "    <td><img src=\"http://cs231n.github.io/assets/cnn/cnn.jpeg\" height=\"300\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "Left side: traditional 3 layer network. \n",
    "Right side: ConvNet, where we arrange inputs in 3 dimensions:\n",
    "- Output of filters for each part of picture single scalar, values form an activation map for the given filter over the whole image. \n",
    "- Maps stacked on top of each-other to get to 3D representation, forming the input for the next layers. \n",
    "- In case of pictures, RGB input can be considered 3D by default. [Source](http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "\n",
    "### Pooling\n",
    "\n",
    "- Follow logic of \"detectors\" in case of convolutional filters\n",
    "- Accept premise that there is a compositional hierarchy of object parts, \n",
    "\n",
    "--> Most relevant information whether filter detected certain pattern somewhere in the region of the input picture. \n",
    "\n",
    "Exact location is not important, only the fact that there is \"strong\" (confident) signal.\n",
    "\n",
    "Following this logic additional _pooling layer_ introduced in CNNs:\n",
    "- Downsampling strategy, reducing the dimensionality of the inputs for a next convolutional layer, \n",
    "- Aiding the \"invariant behavior\" of the network as well as a hierarchic form of compressing.\n",
    "\n",
    "Different pooling strategies exist (like \"average pooling\"), but most widespread form is _max pooling_:\n",
    "\n",
    "- The pooling layer does no learning, has no weights. \n",
    "- For each $k*k$  input a single value which is the $maximum$ of activation in that region.\n",
    "- In case of input with dimensions $N*N$ will then output a $ \\frac{N}k *\\frac{N}k$ layer, as each $k*k$ has an output of only a scalar because of the application of the $max$ function.\n",
    "\n",
    "<img src=\"https://adeshpande3.github.io/assets/MaxPool.png\" width=400 heigth=400>\n",
    "\n",
    "Some more justification for this procedure can be, that in a certain local region the presence of a feature is unlikely be \"double\", unless the picture is somehow blurry, so we only need a decision of maximal presence.  Additional resources [here](http://yann.lecun.com/exdb/publis/pdf/boureau-icml-10.pdf).\n",
    "\n",
    "#### Sidenote: some disadvantages of CNN\n",
    "\n",
    "Recently the researchers at Uber's AI lab started to investigate the failings of ConvNets, especially their inability to solve tasks requiring exact coordinates, for example dealing with single pixel based tasks (like given a coordinate pair, the network should output a singel pixel of white at that coordinate). The basic convolutional architectures fail miserably, thus implying that we laid too much emphasis on invariances, and with it, crippled the ability of the networks to use accurate coordinates.\n",
    "\n",
    "<img src=\"https://eng.uber.com/wp-content/uploads/2018/07/image15.png\" width=300 heigth=300>\n",
    "\n",
    "The researchers propose a simple extension of connectivity, thus a new architecture they call [\"CoordConv\"](https://eng.uber.com/coordconv/) to solve this isssue.\n",
    "\n",
    "<img src=\"https://eng.uber.com/wp-content/uploads/2018/07/image8-768x321.jpg\" width=400 heigth=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of a convolutional neural network\n",
    "[\"Click here for a simple visualization of a convoluational neural network\"](http://scs.ryerson.ca/~aharley/vis/conv/flat.html) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"specarch\"></a>\n",
    "# Specific architecture examples\n",
    "\n",
    "After the initial success of ConvNet a _huge_ amount of development went into refining techniques and architectures that fueled the rapid development of the visual processing field, which surpassed (on certain tasks!!!) human performance.\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/media/DDwtxn6U0AADbz0.jpg\" width=400 heigth=400>\n",
    "\n",
    "One obvious source of development was, that with the \"tricks\" discussed before, as well as the increase of computation power the number of layers inside modern image recognition models also rises rapidly.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Kien_Nguyen26/publication/321896881/figure/fig1/AS:573085821489153@1513645715549/The-evolution-of-the-winning-entries-on-the-ImageNet-Large-Scale-Visual-Recognition.png\" width=600 height=600>\n",
    "\n",
    "[Source](https://www.researchgate.net/figure/The-evolution-of-the-winning-entries-on-the-ImageNet-Large-Scale-Visual-Recognition_fig1_321896881)\n",
    "\n",
    "But some structural modifications were also necessary for these results to become achiveable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4qJAh_LzdcP"
   },
   "source": [
    "--------------------------------------------\n",
    "## LeNet\n",
    "\n",
    "[Original paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "\n",
    "This is the original ConvNet by LeCun et al. with the first implementation of the conv, pool and FC layers used for character recognition.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1Yw1NECPVDmrO2a3JoPwJVvnsNU8BU1Ul\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11d4hcgAtKNZ"
   },
   "source": [
    "---------------------------------\n",
    "## AlexNet\n",
    "\n",
    "Papers:\n",
    "\n",
    "- [Here](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "- [and here](https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html)\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*qyc21qM0oxWEuRaj-XJKcw.png\" height=\"300\">\n",
    "\n",
    "\n",
    "Winner of the \"ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) 2012\", 37.5% top-1 and 17.0% top-5 error rate, 650000 neuron, 60 million parmeters.\n",
    "\n",
    "### Main ideas:\n",
    "- **RELU in all layers**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=14p7awodIcVsqEj-N2buGty74uz2doXuA\" height=\"500\">\n",
    "\n",
    "- **Parallel processing on two GPUs**\n",
    "\n",
    "The processing is done in two separate channels. This \"innovation\" came to be because of the fact that GPU memory was extremely limited back then, and the only way to train a bigger model was to \"divide it by half\". None the less a mechanic had to be implemented that ensured communication between the two channels at some layers.\n",
    "\n",
    "- **Local response normalization**\n",
    "\n",
    "As a linear form of \"inhibition\", we divide the activation of a neuron at a position with a scalar that depends on the activations of other neurons in the same position. It has some faint connections to batch norm, and resulted in 1.2-1.4% gain in accuracy. This is also called \"brightness normalization\" and fell out of favour recently.\n",
    "\n",
    "- **Overlapping pooling**\n",
    "\n",
    "Stride is smaller than the pooling filter. This caused 0.3-0.4% increase in accuracy.\n",
    "\n",
    "- **Techniques against overfitting**\n",
    "\n",
    "    - Data augmentation\n",
    "    - DropOut (only on the last FC layers - it doubled the number of training iterations none the less for convergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6R0L3U0tQqB"
   },
   "source": [
    "-----------------------------\n",
    "## VGG (Simonyan and Zisserman, Visual Geometry Group, University of Oxford)\n",
    "\n",
    "Sources:\n",
    "- https://arxiv.org/abs/1409.1556\n",
    "- http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "- https://github.com/machrisaa/tensorflow-vgg\n",
    "- https://github.com/tensorflow/models/blob/master/research/slim/nets/vgg.py\n",
    "- https://gist.github.com/omoindrot/dedc857cdc0e680dfb1be99762990c9c\n",
    "\n",
    "<img src=\"http://www.hirokatsukataoka.net/research/cnnfeatureevaluation/cnnarchitecture.jpg\" width=600 heigth=600>\n",
    "\n",
    "Second place in 2014 ImageNet. It had 16 convolutional and 3 FC layers 3x3 convolutional and 2x2 pooling filters. The model's appeal was it's simplicity, though the FC layers at the end made it quite demanding in computational terms. Decreasing FC layers was later found to mitigate this probelm without meaningful loss of performance.\n",
    "\n",
    "Main ideas:\n",
    "\n",
    "- more convolutional layers\n",
    "- only small, but overlapping filters (3x3, stride=1)\n",
    "- no local response normalization\n",
    "- Sampling from multi-scale pictures\n",
    "- usage of 1x1 convolutional layers\n",
    "- ensemble of multiple models (2 made a large improvement, no need for large ensembles)\n",
    "\n",
    "VGG is still a widely used architecture - especially in transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XK31BL7tNrD"
   },
   "source": [
    "---------------------------------------------\n",
    "## GoogLeNet-Inception models\n",
    "\n",
    "Winner of ILSVRC 2014 in detection and classification also, model of the Google team.\n",
    "\n",
    "**Sources:**\n",
    "- https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n",
    "- https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/googlenet.html\n",
    "\n",
    "\n",
    "**Design objective:**\n",
    "Lower level of energy and memory consumption\n",
    "\n",
    "**Problems:**\n",
    "1. More layers mean higher precision, but too many parameters (overfitting, training time, computational power,...)\n",
    "2. ConvNet libraries (as well as general numeric libraries and hardware) is designed to deal with dense matrices, but computer vision representations work with sparse distributions\n",
    "\n",
    "**Ideas:**\n",
    "\n",
    "1. idea, based on: M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013. Using 1x1-es convolutions, increasing depth but not increasing dimensions. Concatenateing this structure repeatedly resulted in GoogLeNet\n",
    "2. idea: Inspired by biology, trying to reach scalar-invariance by applying variable size filters in parallel (This was also present in the classical model of Serre, HMAX just with non-learned filters.) Optimally we would work with many filters that are only sparsely active.\n",
    "\n",
    "Inception model combines the two ideas, it does not enforce sparsity explicitly, but approximates it instead.\n",
    "\n",
    "The building blocks, \"inception modules\" are composed of different size pooling and convolutional filters, their output is concatenated to become the input for the next layer. The ratio of different size filters changes as we get further from the original input, since we can expect to see lower and lower levels of spatial correlations in the higher levels of representation. The number of parameters is controlled by 1x1 dimension reduction convolutions:\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/inception_1x1.png\" width=700 height=700>\n",
    "\n",
    "The structure of GoogLeNet:\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/GoogleNet.png\" width=700 height=700>\n",
    "\n",
    "Because of memory constraints, in the first two layers they used \"classical\" conv and pool layers, then multiple inception modules were chained after each-other. Finally they used _avg pooling_ (instead of FC layers), dropoutot and softmax. If we consider a module one \"layer\", the number of operators with parameters was 22, all in all the number of layers was 100\n",
    "\n",
    "3. idea: Put external classifiers on some of the intermediate layers, this contributes to the error minimized during training. This ensures that strong gradient flow is maintained in bottom layers also. (The motivation came from the fact that not so deep models work also, so there is a gradient signal that can be extracted from them.) During testing, the auxillary classifiers are not used.\n",
    "\n",
    "4. idea: The usage of photometric distortion help the model becoming invariant against the properties of the photos taking (camera, detector type,...) [soruce](https://arxiv.org/abs/1312.5402)\n",
    "\n",
    "5. idea: They have tought 7 (6) identical models on subsets of the training data, and finally made an ensemble of them. (Ensembling always helps a bit. :-( )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8btKTu-5CnGD"
   },
   "source": [
    "---------------------------------------------\n",
    "## Highway Networks\n",
    "\n",
    "**Sources:**\n",
    "- https://arxiv.org/abs/1505.00387\n",
    "- http://people.idsia.ch/~rupesh/very_deep_learning/\n",
    "- https://arxiv.org/abs/1507.06228\n",
    "- https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa\n",
    "- https://github.com/wujysh/highway-networks-tensorflow\n",
    "\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "The teaching of very deep networks is difficult (unsurprisingly) \n",
    "\n",
    "**Idea:**\n",
    "\n",
    "The flow of information could be enhanced by passing through **the original as well as the transformed input**, thus the signal is essentially not weakened during the process. (And by the way, gradients can flow backwards more easily!). This general idea is also called a skip connection, although highway networks are just one particular way of implementing skip connections.\n",
    "\n",
    "$\\mathbf y=H(\\mathbf{x}, \\mathbf{W_H})\\odot T(\\mathbf{x}, \\mathbf{W_T})+ \\mathbf{x}\\odot C(\\mathbf{x}, \\mathbf{W_C})$, \n",
    "\n",
    "where T is the \"transform\" gate, C is the \"carry over\" gate. \n",
    "\n",
    "Most simple form is when \n",
    "\n",
    "$C()=1-T()$, \n",
    "\n",
    "thus \n",
    "\n",
    "$\\mathbf y=H(\\mathbf{x}, \\mathbf{W_H})\\odot T(\\mathbf{x}, \\mathbf{W_T})+ \\mathbf{x}\\odot (1-T(\\mathbf{x}, \\mathbf{W_T}))$.\n",
    "\n",
    "In the original paper, the transform gate is concretely a sigmoid layer: $T(\\mathbf x)=\\sigma(\\mathbf{W_T}^T \\mathbf x + \\mathbf{b_T})$.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=17asvsfH4S6O2TVmE8dLQBF7uhJmalCNt\" width=500 height=500>\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1ab6Z6Sr8_tc30s6ZtvaBy-f4ycdyXWOI\" width=500 height=500>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93JL4amjuX5W"
   },
   "source": [
    "------------------------------------------\n",
    "## ResNet, Wide ResNet, ResNeXt\n",
    "\n",
    "\n",
    "**Sources:**\n",
    "- https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet\n",
    "- https://github.com/tensorflow/models/tree/master/research/resnet\n",
    "- https://arxiv.org/abs/1512.03385\n",
    "- https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf\n",
    "- https://arxiv.org/pdf/1602.07261.pdf   combining Inception and ResNet leads to quicker training and higher accuracy\n",
    "- https://blog.waya.ai/deep-residual-learning-9610bb62c355\n",
    "\n",
    "\n",
    "Originally a result of Microsoft Research Asia, the lead author, Professor He is now at Facebook AI...\n",
    "\n",
    "ResNets\t@\tILSVRC\t&\tCOCO\t2015\twinner in all categories!\n",
    "- ImageNet\tClassification:\t“Ultra-deep”\t152-layer nets\t\n",
    "- ImageNet\tDetection: 16% better\tthan\t2nd\n",
    "- ImageNet\tLocalization: 27% better\tthan\t2nd\n",
    "- COCO\tDetection: 11% better\tthan\t2nd\n",
    "- COCO\tSegmentation: 12% better\tthan\t2nd\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/ImagenetTable.png\" width=500 height=500>\n",
    "\n",
    "\n",
    "### ResNet vs Highway networks\n",
    "\n",
    "Motivation:\n",
    "\n",
    "Depth has a limit: \"degradation problem\"\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1g9laFRfDIdW0Bl9PC58et2z9DqyB5DNf\" width=600 height=600>\n",
    "\n",
    "\n",
    "You need more and more tricks to keep up the gradient flow in case of deeper networks.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=141aDrpQAtXdN8HLKg3eopbO3RB7YSXeo\" width=400 height=400>\n",
    "\n",
    "\n",
    "ResNets and Highway networks time to realise the same goal, so similar concept, different realization:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/311842587/figure/fig1/AS:442294890438658@1482462727725/Illustrating-our-usage-of-blocks-and-stages-in-Highway-and-Residual-networks-Note-also.png\" width=400 height=400>\n",
    "\n",
    "[Source](https://www.researchgate.net/publication/311842587_Highway_and_Residual_Networks_learn_Unrolled_Iterative_Estimation)\n",
    "\n",
    "Both solutions try to implement a \"shortcut\" in depth, though in case of Highway networks it is done through a \"gating function\". The gates are dependent on the input and other parameters. If the value of the gate converges to 0, it does not learn a residual mapping (so it is in essence a variable amount of \"residuality\"), while ResNet always operates on the basis of residuals. It is not clear that a Highway net can always capiatalize on the increased depth.\n",
    "\n",
    "The big idea of ResNets is to learn the simplest of \"transformations\", namely _identity mapping_, that is we introduce \"skip connections\", so that the old mapping is the \"residual\" part of the new mapping. (It had some precursors in [VLAD (Vector of Locally Aggregated Descriptors](http://www.vlfeat.org/api/vlad-fundamentals.html).)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=17vkfFKkbNBxtc4v_tqM9GFRonE-hbPNw\" width=400 height=400>\n",
    "\n",
    "Introduction of identity transformation helps optimization.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1NWGv-2LtzBHZODghTTJsVlccye3CaEpT\" width=400 height=400>\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1MLyeQ6fpeQbLn71jmuclDn0ECO5i2utC\" width=400 height=400>\n",
    "\n",
    "\n",
    "Building blocks of original ResNet, \"improved\" and \"generalized to more layers\":\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1dWnfQSp0JBR1jLQ-iPXzBlB9V-vQ1FqU\" width=600 height=600>\n",
    "\n",
    "The difference lies in the transformations used. In smaller networks they use non-linearities (ReLU or ReLU+Batchnorm), and in bigger networks identity.\n",
    "\n",
    "The whole network is made up of such units, but before the classification there are no FC layers, only average pooling.\n",
    "\n",
    "\n",
    "The bottleneck structure for reducing parameters is useful in big tasks:\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=19W-y5pAJCNTPO0iRzb5tRfguUj3P1Jq_\" width=600 height=600>\n",
    "\n",
    "\n",
    "Two ResNet components are:\n",
    "- basic:two convolutional layers, 3x3 filters, batch normalization and ReLU\n",
    "- bottleneck: dimensionality reduction and 1x1 expanding convolutional layer and a 3x3 conv layer inbetween\n",
    "\n",
    "\n",
    "**Results:**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1Wp9eO5k0KV8BPRhdqx89I2CtCkAlb6KM\" width=600 height=600>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bupmpKsGG4AL"
   },
   "source": [
    "## Wide ResNet\n",
    "\n",
    "[**Source**](https://arxiv.org/abs/1605.07146)\n",
    "\n",
    "The introduction of identity mapping was the key in ResNets but with enough layers, the so called \"diminishing feature reuse\" problem arises, that is, the information is preferredly flowing through the skip connections and nothing is learned on the other branch.\n",
    "\n",
    "\n",
    "**Ideas:**\n",
    "- Changing the order of operators: BN->ReLU->conv, results in faster training\n",
    "- Since the bottleneck layer is \"narrow\", and the goal is to build wide nets, they have left it out\n",
    "- Increase of representation power\n",
    "    - more layers\n",
    "    - more conv layers in one ResNet module\n",
    "    - change of receptive fields of filters (smaller filters are generally better\n",
    "    - Widening: more filters inside the conv layer of the module\n",
    "- Widening (more filters) allowed for better GPU utilization, drastic speed increase in comparison to classic ResNet\n",
    "- Batch Norm instead of DropOut in convolutional layers\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1txjchc8PNT9_UGxNYMdAQaG39Pe7e3UO\" width=500 height=500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_EOtKEtuzXZ"
   },
   "source": [
    "------------------------------------------------------\n",
    "## DenseNet\n",
    "\n",
    "**Sources:** \n",
    "- https://arxiv.org/abs/1608.06993\n",
    "- https://github.com/liuzhuang13/DenseNet\n",
    "- https://github.com/YixuanLi/densenet-tensorflow\n",
    "\n",
    "**Problem:** \n",
    "- Many other models were experimenting with \"shortening the paths\", but there were many variants for it\n",
    "\n",
    "**Idea:**\n",
    "- What if we enable connection between all layers, the diverse filters together increase representation potential?\n",
    "- Few filters per layer, further compression between \"dense\" blocks\n",
    "\n",
    "\n",
    "<img src=\"https://cloud.githubusercontent.com/assets/8370623/17981494/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg\" width=500 height=500>\n",
    "\n",
    "<img src=\"https://cloud.githubusercontent.com/assets/8370623/17981496/fa648b32-6ad1-11e6-9625-02fdd72fdcd3.jpg\" width=700 height=500>\n",
    "\n",
    "\n",
    "**Comparison with other models:**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1av2mGDY1aH0GAq7bpyjwdA9sN7d2C09u\" width=500 height=500>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
