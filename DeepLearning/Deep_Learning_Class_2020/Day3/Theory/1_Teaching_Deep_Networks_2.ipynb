{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"regularize\"></a>\n",
    "# Regularization / overfitting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"Empirical risk minimization\" situation\n",
    "\n",
    "\"Holdout set\", \"test error\", we would like to measure generalization.\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1dgGMzI4cEwVeyQsSJKcIJSBcTrEimc9v\"  width=900 heigth=900>\n",
    "\n",
    "\n",
    "<img src=\"https://media.nature.com/m685/nature-assets/nmeth/journal/v13/n9/images/nmeth.3968-F1.jpg\">\n",
    "\n",
    "\n",
    "\n",
    "## Neural networks should not work...\n",
    "\n",
    "- Neural networks even with one hidden layers are universal approximators, and massively overparametrized\n",
    "- Degrees of freedom, capacity or ability to have absurd amount of \"curvature\" as functions is huge\n",
    "- Just gets worse with if number of layers is greater than 2.\n",
    "\n",
    "<img src=\"https://cs231n.github.io/assets/nn1/layer_sizes.jpeg\">\n",
    "\n",
    "\n",
    "## ...And did in fact not work for a while\n",
    "\n",
    "\"... neural network should be able to overfit training data given sufficient training iterations and a legitimate learning algorithm, especially considering that Brady et al. (1989) showed that an inferior algorithm was able to overfit the data. Therefore, this phenomenon should have played a critical role\n",
    "in the research of improving the optimization techniques. Recently, the studying of cost surfaces of neural networks have indicated the existence of saddle points (Choromanska et al., 2015; Dauphin et al., 2014; Pascanu et al., 2014), which may explain the findings of Brady et al back in the late 80s.\"\n",
    "\n",
    "[Source](https://arxiv.org/pdf/1702.07800.pdf)\n",
    "\n",
    "\n",
    "## But now works none the less. WHY?\n",
    "\n",
    "As mentioned many times: \n",
    "\n",
    "- \"Some technical developments\"\n",
    "    - Part of this is the material for this class\n",
    "- Hardware + amount of data...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. \"Capacity\" usage as the progressive growth of weights during training\n",
    "\n",
    "Change of decision surface equals direction of downward \"step\" in direction of error gradients.\n",
    "\n",
    "<img src=\"https://iamtrask.github.io/img/sgd_optimal.png\">\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/23104835/Qc281.jpg\">\n",
    "\n",
    "- If data is not linearly separable, curvature of decision surface increases progressively. \n",
    "- With some simplification, we can assume, that the weights will increase to produce a higher order polynomial.\n",
    "\n",
    "Training can be understood to **increase the absolute value of weights** to utilize more and more capacity.\n",
    "\n",
    "Small SGD based NN training on MNIST: charted the sum of absolute values of weights during the epochs.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1mbWqsUFHX-suGkJ15ZxNQcZvchBJprPf\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Why do we check validation data in every epoch?\n",
    "\n",
    "<img src=\"https://deeplearning4j.org/img/earlystopping.png\">\n",
    "\n",
    "<img src=\"https://elitedatascience.com/wp-content/uploads/2017/09/early-stopping-graphic.jpg\">\n",
    "\n",
    "**We stop, where validation error starts to increase.**\n",
    "\n",
    "### \"Starts to\"?\n",
    "\n",
    "An example of a validation error curve might look like this:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1tfV_C9pS8QZUKqVbXxY47tXEa1hDOMzK\" width=\"70%\">\n",
    "\n",
    "- If we get a constantly updating, noisy series, it might not be the best idea to stop the process if *one* epoch caused an increase in validation error. \n",
    "- Should \"wait a bit\", do some smoothing. (Moving average? - The default guess of TensorBoard, Continuous increase for some epochs?...)\n",
    "\n",
    "#### Early stopping metaalgorithm\n",
    "\n",
    "On the one hand we have to \"get a feel\" for it, on the other hand there are attempts to come up with heuristics.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1XhKuVB6mqj5ZMxsUBK8VgWF8uceP0ra-\" width=\"70%\">\n",
    "\n",
    "([Source: Goodfellow, Bengio & Courville: Deep Learning (MIT, 2016)](http://www.deeplearningbook.org/contents/regularization.html))\n",
    "\n",
    "There are several approaches for the systematic timing of early stopping.\n",
    "**A good summary is well worth reading [here](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic normalization - weight norm\n",
    "\n",
    "We would like to achieve the increase of performance with the smallest possible amount of weight increase during training. \"Do your best with the least capacity\".\n",
    "\n",
    "### Added terms to the cost function\n",
    "\n",
    "How can we achieve this?\n",
    "We should add some regularizing terms to the cost. These are the \"additional constraints\" mentioned in the beginning, and - no surprise - can be same as in case of linear models.\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/10-170929054651/95/neural-network-part2-7-638.jpg?cb=1506665197\">\n",
    "\n",
    "\"Eror\" :-D\n",
    "\n",
    "[Source](https://www.slideshare.net/21_venkat/neural-network-part2)\n",
    "\n",
    "\n",
    "\n",
    "### L1-L2 penalty\n",
    "No change from the linear models, same concept.\n",
    "\n",
    "<img src=\"http://slideplayer.com/slide/2511721/9/images/3/Cost+function+Logistic+regression:+Neural+network:.jpg\">\n",
    "\n",
    "#### L1\n",
    "**If the size of the covariants is small, the degree of freedom for the function is constrained.**\n",
    "<img src=\"https://jamesmccaffrey.files.wordpress.com/2017/06/l1_regularization_1.jpg?w=640&h=361\">\n",
    "\n",
    "[Forrás](https://jamesmccaffrey.wordpress.com/2017/06/27/implementing-neural-network-l1-regularization/)\n",
    "\n",
    "\n",
    "#### L2\n",
    "**If parallelly few covariants have a \"non zero\" value, the degree of freedom for the function is constrained.**\n",
    "\n",
    "<img src=\"https://jamesmccaffrey.files.wordpress.com/2017/06/l2_regularization_equations.jpg?w=640&h=379\">\n",
    "\n",
    "[Source](https://jamesmccaffrey.wordpress.com/2017/06/29/implementing-neural-network-l2-regularization/)\n",
    "\n",
    "#### Result\n",
    "<img src=\"https://cs231n.github.io/assets/nn1/reg_strengths.jpeg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Increase of cross-coupling during training (“antirobust” model)\n",
    "\n",
    "### Dropout\n",
    "\n",
    "#### What is it?\n",
    "\n",
    "The basic idea is to **randomly choose** some neurons (typically with 0.5 probability \"coin flip\") that we \"switch off\", we regard them as temporarily not being part of the network. We set their activation to 0.\n",
    "\n",
    "We calculate gradients according to this, since their contribution to the error will be zero.\n",
    "\n",
    "In the next forward pass, we \"flip the coins\" anew.\n",
    "\n",
    "<img src=\"https://everglory99.github.io/Intro_DL_TCC/intro_dl_images/dropout1.png\">\n",
    "\n",
    "**When we finish the training, we use THE WHOLE NETWORK as one**, but we have to norm the outputs with a normalization term.\n",
    "\n",
    "**[Original paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)**\n",
    "\n",
    "\n",
    "#### WHYYYY?\n",
    "\n",
    "1. First of all: we don't know exactly. There are some ideas, and an emerging consensus.\n",
    "2. It is plausible that the \"interdependency\" (\"coupling\", \"entanglement of hidden factors\") is also increasing during training (quasi complexity, more like dependency), which causes more \"brittle\" or arbitrary decision boundaries (not a large margin one). This is a nice metaphor, but the literature is not backing it fully up .\n",
    "3. It is in practice a form of model averaging, that is, a self contained \"ensemble model\" - see below.\n",
    "\n",
    "More: [here](https://arxiv.org/abs/1512.05287)\n",
    "\n",
    "\n",
    "#### Observations\n",
    "\n",
    "1. Dropout makes training more difficult, **decreases training performance**\n",
    "2. As a rule of thumb, using dropout **_approximately_ doubles required training time**, but epoch time usually decreases (remember, you are zeroing out half of the gradients...)\n",
    "3. In case of $h$ hidden neurons when we apply dropout with probability $p$, we train approximately $2^h$ models, and we do the prediction with their \"ensemble\", decreasing activations by a factor of $p$\n",
    "4. Dropout forces the network to learn *robust* features, which are useful for many other random neuron groups in their decisions\n",
    "\n",
    "#### Connection with ensemble methods\n",
    "\n",
    "According to the most widespread opinion, during dropout, we create a weakly coupled group or *ensemble* of learners.\n",
    "\n",
    "\"Dropout training is similar to bagging (Breiman, 1994), where many different models are trained on different subsets of the data. Dropout training differs from bagging in that each model is trained for only one step and all of the models share parameters. For this training procedure (dropout) to behave as if it is training an ensemble rather than a single model, each update must have a large effect, so that it makes the sub-model induced by that µ fit the current input v well.\"\n",
    "\n",
    "[Source](http://proceedings.mlr.press/v28/goodfellow13.pdf)\n",
    "\n",
    "It is interesting to think about the connection of deep neural nets with *\"boosted trees\"* and \"traditional\" models like *RandomForest* and *XGBoost*.\n",
    "\n",
    "<img src=\"https://i0.wp.com/dimensionless.in/wp-content/uploads/RandomForest_blog_files/figure-html/voting.png?w=1080&ssl=1\">\n",
    "\n",
    "\n",
    "\n",
    "#### \"Dropout\" wrapper\n",
    "\n",
    "It became such a standard practice, that there is a \"wrapper\" function in TensorFlow (and all other frameworks) as a standard operation.\n",
    "\n",
    "It is considered here as separate layer, that \"wraps\" the prior one, executes the \"switch off\" (technically by multiplicating with zero or other tricks).\n",
    "\n",
    "```python\n",
    "# Fully connected layer (in tf contrib folder for now)\n",
    "fc1 = tf.layers.dense(fc1, 1024)\n",
    "# Apply Dropout (if is_training is False, dropout is not applied)\n",
    "fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "```\n",
    "        \n",
    "\n",
    "Details in the [documentation](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)\n",
    "\n",
    "(WARNING!, `tf.layers.dropout` is a wrapper around `tf.nn.dopout` - at least till the random \"lords\" of TF don't decide otherwise!) \n",
    "\n",
    "#### OK, but where to use it? \n",
    "\n",
    "Non trivial question is where to place the dropout \"layer\"?\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1zvL2s2gXB4ymfcnOXXraR8QFSm3Gy54Z\">\n",
    "\n",
    "This does not yet have a settled universal answer, just partial aspects were investigated.\n",
    "\n",
    "For example in case of handwriting recognition the result is [this](http://ieeexplore.ieee.org/document/7333848/?reload=true).\n",
    "\n",
    "This strongly depends on the architecture and represents an even more complex problem in case of recurrent models. A three part summary of the problem can be found [here](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307).\n",
    "\n",
    "#### Sidenote: Dropout may be more important than we think\n",
    "\n",
    "There is also work, (eg. by the group of [René Vidal](http://openaccess.thecvf.com/content_cvpr_2017/papers/Haeffele_Global_Optimality_in_CVPR_2017_paper.pdf)) which tries to characterize - under certain constraints - the supposedly non-convex optimization challenge of training a neural network as a convex problem. This can lead to a deeper understanding of how deep networks can - in theory, not just in practice - reach their high performance.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1lzDovsqObw6RizIEx5duSBX0G5Cx2EOf\" width=600 heigth=600>\n",
    "\n",
    "In this framework, they propose, that dropout - together with structural properties of deep NN-s and the optimization process is essentially causing the problem to become convex. This line of research is pretty novel, not well understood, but definitely worth following.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Robustness\n",
    "\n",
    "### \"Information dropout\"\n",
    "\n",
    "#### What? \n",
    "\n",
    "Achille & Soatto (2015)\n",
    "\n",
    "What if we don't \"switch off\" nodes, as in dropout, but we consciously add noise from a known distribution to the inputs?\n",
    "\n",
    "\"We call “representation” any function of the data that is useful for a task. An optimal representation is most useful(sufficient), parsimonious (minimal), and minimally affected by nuisance factors (invariant). Do deep neural networks approximate such invariants sufficiently?\n",
    "The cross-entropy loss most commonly used in deeplearning does indeed enforce the creation of sufficient representations, but the other defining properties of optimal representations do not seem to be explicitly enforced by the commonly used training procedures. \n",
    "However, **we show that this can be done by adding a regularizer, which is related to the injection of multiplicative noise in the activations, with the surprising result that noisy computation facilitates the approximation of optimal representations.** In this paper we establish connections between the theory of optimal representations for classification tasks, variational inference, dropout and disentangling in deep neural networks.\"\n",
    "\n",
    "**\"(3)We show that, counter-intuitively, injecting multiplicative noise to the computation improves the properties of a representation and results in better approximation of anoptimal one (Section 6).**\n",
    "\n",
    "\n",
    " (4)We relate such a multiplicative noise to the regularizer,and  show  that  in  the  special  case  of  Bernoulli  noise, regularization reduces to dropout [3], thus establishing a connection to information theoretic principles. We also provide a more efficient alternative, called InformationDropout, that makes better use of limited capacity, adapts to  the  data, and  is related to Variational Dropout\"\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1b0gwNXYvJ7J2ZlF26dP4a4iM3iSI2U6D\" width=500 heigth=500>\n",
    "\n",
    "Original paper [here](https://arxiv.org/abs/1611.01353), implementation is not yet \"standard\", but can be found [here](https://github.com/ucla-vision/information-dropout)\n",
    "\n",
    "**Learning: Noise is _not_ necessarily your enemy in Deep learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connections to other methods\n",
    "\n",
    "#### Variational autoencoders\n",
    "\n",
    "Even the original paper of information dropout makes an explicit Reference to variational autoencoders, so we have to keep this in mind when we get back to those later on.\n",
    "\n",
    "#### Data augmentation\n",
    "\n",
    "What if we have too little data, and \"make some more\" - by applying transformations to the original data, which are plausible in the given domain - like rotation and scaling in image domains?\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=17zfUo3UoD0AqwnEgCkROIQc_2X8cZx51\">\n",
    "\n",
    "Detailed analysis in case of images can be found [here](http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf)\n",
    "\n",
    "And additionally if we do such \"plausible\" transformations, **we greatly enhance the generalization ability of our models**.\n",
    "\n",
    "Remember: More data is better! (Moser) \n",
    "\n",
    "Also, augmenting data is a form of injecting domain specific knowledge.\n",
    "\n",
    "Source: Lecture series of Michael C. Mozer at DeepLearn2017 Bilbao\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=17IyTGGXib9u9cXDbADGXyDsLUwUIuwOc\">\n",
    "\n",
    "And again it is notable, that even Hinton admitted, that deep learning did not work because the lack of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Again connection:\n",
    "\n",
    "This topic is in strong connection with \"data imputation\" whereby we try to mitigate the effect of missing data by \"imputing\" missing variables. There are also additional approaches, which we can not discuss in detail now, but you can find [here](http://www.stat.columbia.edu/~gelman/arm/missing.pdf).\n",
    "\n",
    "Even one step further is something like \"Cluttered MNIST\", where the database is saved in an altered form. Downloadable from [here](https://github.com/deepmind/mnist-cluttered).\n",
    "\n",
    "\n",
    "\n",
    "#### Adversarial examples\n",
    "\n",
    "What if we add _targeted_ noise to the dataset?\n",
    "\n",
    "For the human eye the noise is absolutely not perceptible (in extreme cases it can be **one pixel**), and with high confidence we can force the network to classify the example into another class?\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1400/1*Nj_toOwx_Hc5NLn97Jv-ww.png\">\n",
    "\n",
    "**This is one of the most serious challange for neural models presently!!! Just think about the security implications!!!**\n",
    "\n",
    "Naturally the experiments started to test out this attack vector in physical settings by adding minimal optical noise (in form of colorful stickers) to stop signs. Well, it succeeded.\n",
    "\n",
    "<img src=\"http://bair.berkeley.edu/blog/assets/yolo/image1.png\" width=600 heigth=600>\n",
    "\n",
    "Original [here](http://bair.berkeley.edu/blog/2017/12/30/yolo-attack/)\n",
    "\n",
    "**This shows that the decision boundary between the classes is absolutely _not robust_!**\n",
    "\n",
    "On the other hand, nothing prevents us from **regarding adversarial examples as a form of data augmentation**, and adding them to the datasets.\n",
    "\n",
    "This method though went into the extreme recently, where Google researchers utilized it to \"reprogram\" a trained network to carry out a different task than the original.\n",
    "\n",
    "[Google Brain researchers demo method to hijack neural networks](https://venturebeat.com/2018/07/02/google-brain-researchers-demo-method-to-hijack-neural-networks/)\n",
    "\n",
    "<img src=\"https://venturebeat.com/wp-content/uploads/2018/07/Capture-boring.png?fit=578%2C451&strip=all\" width=600 heigth=600>\n",
    "\n",
    "(This has some relevance for transfer learning also, since it can be considered as domain mapping - see later.)\n",
    "\n",
    "#### Mixup - “Vicinal Risk Minimization” \n",
    "\n",
    "So getting back to the question of \"large margins\", we attack the base paradigm of empirical risk minimization, and we move over to \"vicinical risk minimization\"?\n",
    "\n",
    "This means that we do not just try to fit a model based on the error on the observed data (\"empirical risk\"), but use a method to force the model to learn something \"between the classes\", so we in essence try to constrain the behavior of the model in the space \"inbetween\".\n",
    "(Just think about embedding and representation learning!)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1xXp9PUoKg5TpsEO5v_q62D32bTZaF00o\" width=800 heigth=800>\n",
    "\n",
    "<img src=\"https://www.inference.vc/content/images/2017/11/download-87.png\" width=60%>\n",
    "\n",
    "Original paper [here](https://arxiv.org/abs/1710.09412), a very good in-detail analysis [here](https://www.inference.vc/mixup-data-dependent-data-augmentation/).\n",
    "\n",
    "This is yet a \"fringe\" movement in deep learning, but it can have great potential!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. “Covariate shift”\n",
    "\n",
    "### The problem\n",
    "\n",
    "\"Covariate shift\" is a general and rather nasty problem, when the distribution of your input shifts in a subtle way, implying that the learned model is no longer appropriate.\n",
    "\n",
    "<img src=\"http://slideplayer.com/slide/5237435/16/images/5/Covariate+Shift+Adaptation.jpg\" width=700 heigth=700>\n",
    "\n",
    "If we can realize this - eg. with testing in time, or with performance monitoring \"a bit late\" - we can try to re-train the model. (See eg. [\"importance sampling\"](https://en.wikipedia.org/wiki/Importance_sampling), or some kind of transfer learning, as elaborated later).\n",
    "\n",
    "More about detecting covariate shift can be found [here](https://datascience.stackexchange.com/questions/8278/covariate-shift-detection).\n",
    "\n",
    "For our purposes it is important that Szegedy et al. demonstrated, that even **during training, inside backprop, covariate shift is happening** - if the network is deep enough, since when we update the weights, we update the output distribution of a layer, thus modifying the input distribution for the next layer - obviously, since we want to learn something. With this we are effectively \"shooting at a moving target\" during training.\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/dlmmdcud1l06optimization-170427160940/95/optimizing-deep-networks-d1l6-insightdcu-machine-learning-workshop-2017-8-638.jpg?cb=1493309658\" width=600 heigth=600>\n",
    "\n",
    "### The solution: Batchnorm\n",
    "\n",
    "\n",
    "\"Batch normalization\", that is normalizing the activations of the nodes during backprop with the minibatch mean and variance.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1S55NF6zKwnOp8WEE6woWyDlw4LuF1ZaR\" width=400 heigth=400>\n",
    "\n",
    "A nice analysis of the original method can be found [here](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b), original paper [here](https://arxiv.org/abs/1502.03167). \n",
    "\n",
    "### Why does it work?\n",
    "\n",
    "The normalization inside a batch ensures that even with changing weights, which change the input distribution, it's mean and variance remains the same. It can not \"creep away\" in the numeric range.\n",
    "\n",
    "Detailed explanationby the famous Andrew Ng [here](https://www.coursera.org/learn/deep-neural-network/lecture/81oTm/why-does-batch-norm-work).\n",
    "\n",
    "Recently - as of June 2018 - some doubt came up, if batchnorm is counteracting covariate shift at all (see [this](https://arxiv.org/pdf/1805.11604.pdf) paper) - though it is still considered beneficial (by smoothing the error surface).\n",
    "\n",
    "\n",
    "## A sidenote again: Synthetic gradients\n",
    "\n",
    "The research on parallelizability of neural network training brought Google DeepMind to reflect upon the fact, that backprop on large enough NN-s is inherently bound by the fact that the gradients of the layers are to be calculated in a sequential order. Telling is the fact, that for problems on DeepMind horizon, this is considered a constraint, thus they try to parallelize the layer update calculation. \n",
    "\n",
    "For this to be achievable, they have introduced the concept of *\"synthetic gradients\"*. (Details can be found [here](https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/).)\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms/images/3-3.width-1500_qGOdtUS.png\" width=400 heigth=400>\n",
    "\n",
    "\"So, how can one decouple neural interfaces - that is decouple the connections between network modules - and still allow the modules to learn to interact? In this paper, we remove the reliance on backpropagation to get error gradients, and instead learn a parametric model which predicts what the gradients will be based upon only local information. We call these predicted gradients *synthetic gradients*.\"\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms/images/3-5.width-1500_EZm1qhu.png\" width=600 heigth=600>\n",
    "\"The synthetic gradient model takes in the activations from a module and produces what it predicts will be the error gradients - the gradient of the loss of the network with respect to the activations.\n",
    "\n",
    "... and use the synthetic gradients (blue) to *update Layer 1 before the rest of the network has even been executed.*\"\n",
    "\n",
    "This is again a paradigmatically interesting concept, since it goes back to the roots of Hebbian learning, whereby only local information is necessary for the update of weights, but with a twist: sooner or later *some* global information is necessary, but much less. \n",
    "\n",
    "This view is shared by other scholars (eg. Piere Baldi in his talk [\"Deep Learning: Theory, Algorithms and Applications to Natural Sciences\"](https://drive.google.com/file/d/1lbqi2EB24dhm5lHGdmRhGWrwsyRvOcrf/view?usp=sharing) at DeepLeran2018 Summer School, Genova) who state, that purely hebbian learning is not possible, at least some partial \"distant\" supervision is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gdparams\"></a>\n",
    "# GD training hyperparameters - Learning rate and co.\n",
    "<img src=\"https://i1.wp.com/theaviationist.com/wp-content/uploads/2017/12/XB-70-cockpit.jpg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What control points do we have over our model?\n",
    "\n",
    "- Structure\n",
    "    - Architecture (Fully connected layers - till this point, but we will see more)   \n",
    "        - Selection from \"general\" or standard architectures \n",
    "        - Custom \"wiring\"\n",
    "    - Layer number\n",
    "    - Layer size\n",
    "- Learning parameters\n",
    "    - Optimization method (Adam vs SGD vs…)\n",
    "    - Paameters of these  \n",
    "        - Pl learning Learning rate\n",
    "        - In case of Adam $\\beta$1,  $\\beta$2\n",
    "        ...\n",
    "- Epoch number \n",
    "    - Woth constant regard to “early stopping”\n",
    "- Regularization parameters\n",
    "    - Cost function regularization parameters\n",
    "    - Dropout rate\n",
    "    - Inf dropout noise level\n",
    "- Standardization approach\n",
    "    - Batch norm\n",
    "   \n",
    "If we consider all these \"control points\" and their combinations, we still ahve a *huge* space we can choose from. \n",
    "\n",
    "Since we do not have any analytic insights about the appropriate settings of these, we have to rely on \"expert intuition\" or \"experience\" (so much so, that some researchers [criticize](https://medium.com/@Synced/lecun-vs-rahimi-has-machine-learning-become-alchemy-21cb1557920d) their own field as \"alchemy\"), or we choose to explore the space of these settings - which can be casted itself as a search or optimization problem.\n",
    "\n",
    "Naturall enough, researchers tried to attack these optimization problem with the well known machine learning methods, specificly:\n",
    "\n",
    "- Grid search (standard practice for other ML methods, implemented in [ScikitLearn](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), usable [for DL](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/))\n",
    "- Random search: [Bergstra and Bengio 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "- [Bayesian optimization](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n",
    "- [Genetic algorithms](https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html)\n",
    "- [Reinforcement learning](https://arxiv.org/abs/1602.04062)\n",
    "\n",
    "The usage of these techniques is out of scope of this course, but it is worth mentioning, that \"AutoML\" as a trend is the product of this approach. It's proponents argue, that it is a perfect automatic solution for data science problems, it's opponents though tend to point out, that it is exorbitantly costly to train as well as acts as a kind of \"replacement\" for well understood solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above parameter space, we restrict ourselves to discussing Learning Rate, because it's peculiar properties.\n",
    "\n",
    "## Learning rate and it's \"schedules\"\n",
    "\n",
    "Although, as we have already seen, there are GD variants that adaptively set the learning rate during training (Adadelta etc.), non-adaptive GD variants, e.g. vanilla SGD and Momentum are still frequently used for optimization and require manual setting.\n",
    "\n",
    "For these variants, after setting an initial learning rate (which is a crucial hyperparameter), the rate is typically kept constant during an epoch, but is changed (typically decreased) after every epoch (or every $n$ epochs with a fixed $n$) according to a schedule. The most frequently used schedules are\n",
    "\n",
    "+ **Step decay**: The learning rate is reduced by a $\\rho$ factor after every epoch or $n$ epochs, that is an\n",
    "$$\\text{lr} = \\varrho \\cdot \\text{lr}$$ \n",
    "update is performed where $0<\\varrho<1$. \n",
    "\n",
    "+ **Exponential decay**: The learning rate is set to be\n",
    "$$\\text{lr} = \\text{lr}_0\\cdot e^{-k t}\n",
    "$$\n",
    "where $k$ is a hyperparameter and $t$ is the number of already trained epochs (or $n$ epochs). It is easy to see that this is equivalent to a step decay with $\\varrho = e^{-k}$.\n",
    "+ **${1}/{t}$ decay**: The learning rate is set to\n",
    "$$\\text{lr} = \\frac{\\text{lr}_0 }{1 + k t}$$ where $k$ is a hyperparameter and $t$ is the number of already trained epochs (or $n$ epochs).\n",
    "+ **Constant learning rate** across epochs.\n",
    "\n",
    "These schedules can be combined into more complex ones, e.g. a schedule may keep the learning rate constant for a number of epochs and then start a step decay. The switch between the simple schedules can happen simply at a predetermined epoch, but it is frequently connected to a validation metric, e.g. can be triggered when a metric has stopped improving.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*VQkTnjr2VJOz0R2m4hDucQ.jpeg\" width=600 heigth=600>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*iSZv0xuVCsCCK7Z4UiXf2g.jpeg\" width=600 heigth=600>\n",
    "\n",
    "[source](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Connection of LR and batch size \n",
    "\n",
    "Furthermore a recently published paper draws attention to the fact that the decrease of the learning rate according to schedule can be subsituted by the **gradual increase of the batch size**.\n",
    "\n",
    "[Don’t decay the learning rate, increase batch size](https://arxiv.org/abs/1711.00489)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1ka9PN4zecdNVT6uE05VXEvbY_8EygEAv\" width=700 heigth=700>\n",
    "\n",
    "A potential cause of this phenomenon is, that when we train on  a bigger dataset (relatively for each gradinet update, by the increase of the batch size), we get a more and more \"regularized\", \"smooth\" gradient.\n",
    "\n",
    "If this is true, we can benefit, since the larger batch size speeds up training considerably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caution with big batches!\n",
    "\n",
    "A bit on contrary with the statements above, it is worth mentioning, that there is emerging evidence, that small batch training has it's very distinctive advantages that are worth capitalizing on.\n",
    "\n",
    "In a paper [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612) the authors conduct large scale training runs to determine the effect of batch size, and find, that large batch sizes actually decrease generalization performance of models, and there is a rather small optimal batch size for training. \n",
    "\n",
    "<img src=\"https://www.graphcore.ai/hs-fs/hubfs/ResNet32_CIFAR100_Aug_VB_val_dist.png?t=1536059639530&width=600&height=521&name=ResNet32_CIFAR100_Aug_VB_val_dist.png\" width=400 heigth=400>\n",
    "\n",
    "This is directly counteracting the trend of larger batches for parallelism that gained traction with the advent of large-scale distributed training methods.\n",
    "\n",
    "**We should note, that this points out that batchsize is crucial, though during history was many times falsely determined by technical limitations (or luckily, in case of original SGD).**\n",
    "\n",
    "This can be caused by the effect of - under constant learning rate - a larger batch's gradients are approximations of a smaller one's, so they may prevent fine-grained convergence.\n",
    "\n",
    "<img src=\"https://www.graphcore.ai/hs-fs/hubfs/SGD_fig.png?t=1536059639530&width=900&name=SGD_fig.png\" width=400 heigh=400>\n",
    "\n",
    "A detailed exploration of the paper can be found [here](https://www.graphcore.ai/posts/revisiting-small-batch-training-for-deep-neural-networks).\n",
    "\n",
    "**Conclusion: Use small  batch sizes, it might help - though will be slower.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm restarts\n",
    "\n",
    "Original paper:\n",
    "[\"Stochastic gradient descent with warm restarts\"](https://arxiv.org/abs/1608.03983) \n",
    "\n",
    "\"In this paper, we propose to periodically simulate warm restarts of SGD, where in each restart the\n",
    "learning rate is initialized to some value and is scheduled to decrease. Four different instantiations\n",
    "of this new learning rate schedule are visualized in Figure 1. Our empirical results suggest that SGD\n",
    "with warm restarts requires 2× to 4× fewer epochs than the currently-used learning rate schedule\n",
    "schemes to achieve comparable or even better results.\"\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=12a2yIch8Nnf27g8UEQ9ydJy9MhuAzysR\" width=700 heigth=700>\n",
    "\n",
    "\n",
    "### Snapshot ensembles\n",
    "\n",
    "Warm restarts represented originally a separate line of research, but got combined with [\"Snapshot ensembles\"](https://arxiv.org/abs/1704.00109). This technique holds a connection - even in name - with ensemble models. \n",
    "\n",
    "During this procedure, we save all \"good\" model states - typically before warm restarts - and finally do an ensemble of them in the classical sense.\n",
    "\n",
    "It's biggest (proposed) advantage is, that we hope to collect solutions from similarly powerful, but distinct optima, which were visited during training, so their ensemble will be more powerful than any of them alone.\n",
    "\n",
    "<img src=\"http://ruder.io/content/images/2017/11/snapshot_ensembles.png\" width=700 heigth=700>\n",
    "\n",
    "This also gives an answer to the question - posed in early stopping context - if we should store and use the final model. No, not necessarily.\n",
    "\n",
    "Moreover if we accept the fact that a number of \"global\" optima are present for a given model, we are inclined to \"sample\" from more \"basins\", which even connects us to boosting methods.\n",
    "\n",
    "For opinions on this [this](http://www.argmin.net/2018/02/05/linearization/) post is worth reading. It can point towards a more unified view.\n",
    "\n",
    "An also very interesting summary of the developments in this regard is [this](http://ruder.io/deep-learning-optimization-2017/).\n",
    "\n",
    "\n",
    "(Sebastian and the team at AYLIEN are very up to date in NLP, worth following [here](http://ruder.io/#open).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
